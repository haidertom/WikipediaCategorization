{"id": "1758866", "url": "https://en.wikipedia.org/wiki?curid=1758866", "title": "Accelerating change", "text": "Accelerating change\n\nIn futures studies and the history of technology, accelerating change is a perceived increase in the rate of technological change throughout history, which may suggest faster and more profound change in the future and may or may not be accompanied by equally profound social and cultural change.\n\nIn 1910 during the town planning conference of London Daniel Burnham noted that \"But it is not merely in the number of facts or sorts of knowledge that progress lies: it is still more in the geometric ratio of sophistication, in the geometric widening of the sphere of knowledge, which every year is taking in a larger percentage of people as time goes on.\" and later on \"It is the argument with which I began, that a mighty change having come about in fifty years, and our pace of development having immensely accelerated, our sons and grandsons are going to demand and get results that would stagger us.\"\n\nIn 1938, Buckminster Fuller introduced the word ephemeralization to describe the trends of \"doing more with less\" in chemistry, health and other areas of industrial development. In 1946, Fuller published a chart of the discoveries of the chemical elements over time to highlight the development of accelerating acceleration in human knowledge acquisition.\n\nIn 1958, Stanislaw Ulam wrote in reference to a conversation with John von Neumann: \n\nIn a series of published articles from 1974–1979, and then in his 1988 book \"Mind Children\", computer scientist and futurist Hans Moravec generalizes Moore's law to make predictions about the future of artificial life. Moore's law describes an exponential growth pattern in the complexity of integrated semiconductor circuits. Moravec extends this to include technologies from long before the integrated circuit to future forms of technology. Moravec outlines a timeline and a scenario in which robots will evolve into a new series of artificial species, starting around 2030–2040.\nIn \"Robot: Mere Machine to Transcendent Mind\", published in 1998, Moravec further considers the implications of evolving robot intelligence, generalizing Moore's Law to technologies predating the integrated circuit, and also plotting the exponentially increasing computational power of the brains of animals in evolutionary history. Extrapolating these trends, he speculates about a coming \"mind fire\" of rapidly expanding superintelligence similar to the explosion of intelligence predicted by Vinge.\n\nIn his TV series \"Connections\" (1978)—and sequels \"Connections²\" (1994) and \"Connections³\" (1997)—James Burke explores an \"Alternative View of Change\" (the subtitle of the series) that rejects the conventional linear and teleological view of historical progress. Burke contends that one cannot consider the development of any particular piece of the modern world in isolation. Rather, the entire gestalt of the modern world is the result of a web of interconnected events, each one consisting of a person or group acting for reasons of their own motivations (e.g., profit, curiosity, religious) with no concept of the final, modern result to which the actions of either them or their contemporaries would lead. The interplay of the results of these isolated events is what drives history and innovation, and is also the main focus of the series and its sequels.\n\nBurke also explores three corollaries to his initial thesis. The first is that, if history is driven by individuals who act only on what they know at the time, and not because of any idea as to where their actions will eventually lead, then predicting the future course of technological progress is merely conjecture. Therefore, if we are astonished by the connections Burke is able to weave among past events, then we will be equally surprised to what the events of today eventually will lead, especially events we weren't even aware of at the time.\n\nThe second and third corollaries are explored most in the introductory and concluding episodes, and they represent the downside of an interconnected history. If history progresses because of the synergistic interaction of past events and innovations, then as history does progress, the number of these events and innovations increases. This increase in possible connections causes the process of innovation to not only continue, but to accelerate. Burke poses the question of what happens when this rate of innovation, or more importantly change itself, becomes too much for the average person to handle, and what this means for individual power, liberty, and privacy.\n\nIn his book \"Mindsteps to the Cosmos\" (HarperCollins, August 1983), Gerald S. Hawkins elucidated his notion of 'mindsteps', dramatic and irreversible changes to paradigms or world views. He identified five distinct mindsteps in human history, and the technology that accompanied these \"new world views\": the invention of imagery, writing, mathematics, printing, the telescope, rocket, radio, TV, computer... \"Each one takes the collective mind closer to reality, one stage further along in its understanding of the relation of humans to the cosmos.\" He noted: \"The waiting period between the mindsteps is getting shorter. One can't help noticing the acceleration.\" Hawkins' empirical 'mindstep equation' quantified this, and gave dates for future mindsteps. The date of the next mindstep (5; the series begins at 0) is given as 2021, with two further, successively closer mindsteps in 2045 and 2051, until the limit of the series in 2053. His speculations ventured beyond the technological:\n\nThe mathematician Vernor Vinge popularized his ideas about exponentially accelerating technological change in the science fiction novel \"Marooned in Realtime\" (1986), set in a world of rapidly accelerating progress leading to the emergence of more and more sophisticated technologies separated by shorter and shorter time intervals, until a point beyond human comprehension is reached. His subsequent Hugo award-winning novel \"A Fire Upon the Deep\" (1992) starts with an imaginative description of the evolution of a superintelligence passing through exponentially accelerating developmental stages ending in a transcendent, almost omnipotent power unfathomable by mere humans. His already mentioned influential 1993 paper on the technological singularity compactly summarizes the basic ideas.\n\nIn his 1999 book \"The Age of Spiritual Machines\", Ray Kurzweil proposed \"The Law of Accelerating Returns\", according to which the rate of change in a wide variety of evolutionary systems (including but not limited to the growth of technologies) tends to increase exponentially. He gave further focus to this issue in a 2001 essay entitled \"The Law of Accelerating Returns\". In it, Kurzweil, after Moravec, argued for extending Moore's Law to describe exponential growth of diverse forms of technological progress. Whenever a technology approaches some kind of a barrier, according to Kurzweil, a new technology will be invented to allow us to cross that barrier. He cites numerous past examples of this to substantiate his assertions. He predicts that such paradigm shifts have and will continue to become increasingly common, leading to \"technological change so rapid and profound it represents a rupture in the fabric of human history.\" He believes the Law of Accelerating Returns implies that a technological singularity will occur before the end of the 21st century, around 2045. The essay begins:\n\nThe Law of Accelerating Returns has in many ways altered public perception of Moore's law. It is a common (but mistaken) belief that Moore's law makes predictions regarding all forms of technology, when really it only concerns semiconductor circuits. Many futurists still use the term \"Moore's law\" to describe ideas like those put forth by Moravec, Kurzweil and others.\n\nAccording to Kurzweil, since the beginning of evolution, more complex life forms have been evolving exponentially faster, with shorter and shorter intervals between the emergence of radically new life forms, such as human beings, who have the capacity to engineer (i.e. intentionally design with efficiency) a new trait which replaces relatively blind evolutionary mechanisms of selection for efficiency. By extension, the rate of technical progress amongst humans has also been exponentially increasing, as we discover more effective ways to do things, we also discover more effective ways to learn, i.e. language, numbers, written language, philosophy, scientific method, instruments of observation, tallying devices, mechanical calculators, computers, each of these major advances in our ability to account for information occur increasingly close together. Already within the past sixty years, life in the industrialized world has changed almost beyond recognition except for living memories from the first half of the 20th century. This pattern will culminate in unimaginable technological progress in the 21st century, leading to a singularity. Kurzweil elaborates on his views in his books \"The Age of Spiritual Machines\" and \"The Singularity Is Near\".\n\nAccelerating change may not be restricted to the Anthropocene Epoch, but a general and predictable developmental feature of the universe. The physical processes that generate an acceleration such as Moore's law are positive feedback loops giving rise to exponential or superexponential technological change. These dynamics lead to increasingly efficient and dense configurations of Space, Time, Energy, and Matter (STEM efficiency and density, or STEM \"compression\"). At the physical limit, this developmental process of accelerating change leads to black hole density organizations, a conclusion also reached by studies of the ultimate physical limits of computation in the universe.\n\nApplying this vision to the search for extraterrestrial intelligence leads to the idea that advanced intelligent life reconfigures itself into a black hole. Such advanced life forms would be interested in inner space, rather than outer space and interstellar expansion. They would thus in some way transcend reality, not be observable and it would be a solution to Fermi's paradox called the \"transcension hypothesis\", Another solution is that the black holes we observe could actually be interpreted as intelligent super-civilizations feeding on stars, or \"stellivores\". \nThis dynamics of evolution and development is an invitation to study the universe itself as evolving, developing. If the universe is a kind of superorganism, it may possibly tend to reproduce, naturally or artificially, with intelligent life playing a role.\n\nExamples of large human \"buy-ins\" into technology include the computer revolution, as well as massive government projects like the Manhattan Project and the Human Genome Project. The foundation organizing the Methuselah Mouse Prize believes aging research could be the subject of such a massive project if substantial progress is made in slowing or reversing cellular aging in mice.\n\nBoth Theodore Modis and Jonathan Huebner have argued—each from different perspectives—that the rate of technological innovation has not only ceased to rise, but is actually now declining.\n\nIn fact, \"technological singularity\" is just one of a few singularities detected through the analysis of a number of characteristics of the World System development, for example, with respect to the world population, world GDP, and some other economic indices. It has been shown that the hyperbolic pattern of the world demographic, economic, cultural, urbanistic, and technological growth (observed for many centuries, if not millennia prior to the 1970s) could be accounted for by a rather simple mechanism, the nonlinear second-order positive feedback, that was shown long ago to generate precisely the hyperbolic growth, known also as the \"blow-up regime\" (implying just finite-time singularities). In our case this nonlinear second order positive feedback looks as follows: more people – more potential inventors – faster technological growth – the carrying capacity of the Earth grows faster – faster population growth – more people – more potential inventors – faster technological growth, and so on. On the other hand, this research has shown that since the 1970s the World System does not develop hyperbolically any more, its development diverges more and more from the blow-up regime, and at present it is moving \"from singularity\", rather than \"toward singularity\".\n\nJürgen Schmidhuber calls the Singularity \"Omega\", referring to Teilhard de Chardin's Omega Point (1916). For Omega = 2040, he says the series Omega - 2 human lifetimes (n < 10; one lifetime = 80 years) roughly matches the most important events in human history.\n\nKurzweil created the following graphs to illustrate his beliefs concerning and his justification for his Law of Accelerating Returns.\n\n\n"}
{"id": "22508780", "url": "https://en.wikipedia.org/wiki?curid=22508780", "title": "Alternative natural materials", "text": "Alternative natural materials\n\nAlternative natural materials are natural materials like rock or adobe that are not as commonly in use as materials such as wood or iron. Alternative natural materials have many practical uses in areas such as sustainable architecture and engineering. The main purpose of using such materials is to minimize the negative effects that our built environment can have on the planet while increasing the efficiency and adaptability of the structures.\n\nAlternative natural materials have existed for quite some time but often in very basic forms or only as ingredients to a particular material in the past. For example, earth used as a building material for walls of houses has existed for thousands of years. Much more recently, in the 1920s, the United States government promoted rammed earth as a fireproof construction method for building farmhouses. Another more common example is adobe. Adobe homes are prominent in the southwestern U.S. and several Spanish-speaking countries.\n\nStraw bale construction is a more modern concept, but there even exists evidence that straw was used to make homes in African prairies as far back as the Paleolithic times.\nAlternative natural materials, specifically their applications, have only recently made their way into more common use. The ideas of being both green and sustainable in response to global warming and climate change shifted more of a focus onto the materials and methods used to build our cityscape and homes. As environmentally conscious decisions became commonplace, the use of alternative natural materials instead of typical natural materials or man-made materials that rely heavily on natural resources became prominent.\n\nRock is a great alternative to conventional materials which contain chemicals that may be harmful to people, pets or the environment. Rocks have two great characteristics: good thermal mass and thermal insulation. These characteristics make stone a great idea because the temperature in the house stays rather constant thus requiring less air conditioning and other cooling systems. Types of rocks that can be employed are reject stone (pieces of stone that are not able to be used for another task), limestone, and flagstone.\nStraw bales can be used as a basis for walls instead of drywall. Straw provides excellent insulation and fire resistance in a traditional post-and-beam structure, where a wood frame supports the house. These straw walls are about 75% more energy efficient than standard drywalls and because no oxygen can get through the walls, fire cannot spread and there is no chance of combustion.\nIn Asian countries, bamboo is being used for structures like bridges and homes. Bamboo is surprisingly strong and rather flexible and grows incredibly fast, making it a rather abundant material. Although it can be difficult to join corners together, bamboo is immensely strong and makes up for the hardships that can be encountered while building it.\n\nCordwood is a combination of small remnants of firewood and other lumber that usually go to waste. These small blocks of wood can easily be put together to make a structure that, like stone, has great insulation as well as thermal mass. Cordwood provides the rustic look of log cabins without the use of tons of lumber. You can build an entire building with just cordwood or use stones to fill in the walls.\n\nRammed earth is a very abundant material that can be used in place of concrete and brick. Soil is packed tightly into wall molds where it is rammed together and hardened to form a durable wall packing made of nothing more than dirt, stones, and sticks. Rammed Earth also provides great thermal mass, which means great energy savings. In addition, it is very weatherproof and durable enough that it was used in the Great Wall of China.\n\nEarth sheltering is a unique building technique in which buildings are completely constructed on at least one side by some form of Earth whether it be a grass roof, clay walls, or both. This unique system usually includes plenty of windows because of the difficulty involved with using too much electricity in such a house. This adds to the energy efficiency of the house by reducing lighting costs.\n\nPapercrete is an interesting and very new material that is a good substitute for concrete. Papercrete is shredded paper, sand, and cement mixed together that forms a very durable brick-like material. Buildings utilizing papercrete are very well-insulated as well as being termite- and fire-resistant. Papercrete is very cheap as it usually only costs about $0.35 per square foot.\n\nAdobe is an age-old technique that is cheap, easy to obtain, and ideal for hot environments. A mixture of sand, clay, and water is poured into a mold and left in the sun to dry. When dried, it is exceptionally strong and heat-resistant. Adobe doesn’t let much heat through to the inside of the structure, thus providing excellent insulation during the summer to reduce energy costs. Although this clay mixture provides excellent insulation from heat, it is not very waterproof and can be dangerous in earth-quake prone areas due to its tendency to crack easily.\n\nSawdust is a good material to combine with clay or cement mixtures and use for walls. These walls turn out surprisingly sturdy and effectively recycle any trees that may need to be excavated from the building area. Depending what type of sawdust used (hardwood is best) the wood chips in the walls absorb moisture and help prevent cracking during freeze/thaw cycles. Sawdust may be combined with water and frozen to produce a material commonly known as pykrete, which is strong, and less prone to melting than regular ice.\n\nAlthough this is a newer technology there are some buildings that have already employed these materials, as well as other tactics, to make themselves green.\n\n"}
{"id": "1260420", "url": "https://en.wikipedia.org/wiki?curid=1260420", "title": "Anti-predator adaptation", "text": "Anti-predator adaptation\n\nAnti-predator adaptations are mechanisms developed through evolution that assist prey organisms in their constant struggle against predators. Throughout the animal kingdom, adaptations have evolved for every stage of this struggle, namely by avoiding detection, warding off attack, fighting back, or escaping when caught.\n\nThe first line of defence consists in avoiding detection, through mechanisms such as camouflage, masquerade, apostatic selection, living underground, or nocturnality. \n\nAlternatively, prey animals may ward off attack, whether by advertising the presence of strong defences in aposematism, by mimicking animals which do possess such defences, by startling the attacker, by signalling to the predator that pursuit is not worthwhile, by distraction, by using defensive structures such as spines, and by living in a group. Members of groups are at reduced risk of predation, despite the increased conspicuousness of a group, through improved vigilance, predator confusion, and the likelihood that the predator will attack some other individual.\n\nSome prey species are capable of fighting back against predators, whether with chemicals, through communal defence, or by ejecting noxious materials. Many animals can escape by fleeing rapidly, outrunning or outmanoeuvring their attacker. \n\nFinally, some species are able to escape even when caught by sacrificing certain body parts: crabs can shed a claw, while lizards can shed their tails, often distracting predators long enough to permit the prey to escape.\n\nAnimals may avoid becoming prey by living out of sight of predators, whether in caves, underground, or by being nocturnal. Nocturnality is an animal behavior characterized by activity during the night and sleeping during the day. This is a behavioral form of detection avoidance called crypsis used by animals to either avoid predation or to enhance prey hunting. Predation risk has long been recognized as critical in shaping behavioral decisions. For example, this predation risk is of prime importance in determining the time of evening emergence in echolocating bats. Although early access during brighter times permits easier foraging, it also leads to a higher predation risk from bat hawks and bat falcons. This results in an optimum evening emergence time that is a compromise between the conflicting demands.\nAnother nocturnal adaptation can be seen in kangaroo rats, which avoid moonlight. They forage in relatively open habitats and reduce their activity outside their nest burrows in response to moonlight. During a full moon, they shift their activity towards areas of relatively dense cover to compensate for the extra brightness.\n\nCamouflage uses any combination of materials, coloration, or illumination for concealment to make the organism hard to detect by sight. It is common in both terrestrial and marine animals. Camouflage can be achieved in many different ways, such as through resemblance to surroundings, disruptive coloration, shadow elimination by countershading or counter-illumination, self-decoration, cryptic behavior, or changeable skin patterns and colour. Animals such as the flat-tail horned lizard of North America have evolved to eliminate their shadow and blend in with the ground. The bodies of these lizards are flattened, and their sides thin towards the edge. This body form, along with the white scales fringed along their sides, allows the lizards to effectively hide their shadows. In addition, these lizards hide any remaining shadows by pressing their bodies to the ground.\n\nAnimals can hide in plain sight by masquerading as inedible objects. For example, the potoo, a South American bird, habitually perches on a tree, convincingly resembling a broken stump of a branch, while a butterfly, \"Kallima\", looks just like a dead leaf.\n\nAnother way to remain unattacked in plain sight is to look different from other members of the same species. Predators such as tits selectively hunt for abundant types of insect, ignoring less common types that were present, forming search images of the desired prey. This creates a mechanism for negative frequency-dependent selection, apostatic selection.\n\nMany species make use of behavioral strategies to deter predators.\n\nMany weakly-defended animals, including moths, butterflies, mantises, phasmids, and cephalopods such as octopuses, make use of patterns of threatening or startling behaviour, such as suddenly displaying conspicuous eyespots, so as to scare off or momentarily distract a predator, thus giving the prey animal an opportunity to escape. In the absence of toxins or other defences, this is essentially bluffing, in contrast to aposematism which involves honest signals.\n\nPursuit-deterrent signals are behavioral signals used by prey that convince predators not to pursue them. For example, gazelles stot, jumping high with stiff legs and an arched back. This is thought to signal to predators that they have a high level of fitness and can outrun the predator. As a result, predators may choose to pursue a different prey that is less likely to outrun them.\nWhite-tailed deer and other prey mammals flag with conspicuous (often black and white) tail markings when alarmed, informing the predator that it has been detected.\nWarning calls given by birds such as the Eurasian jay are similarly honest signals, benefiting both predator and prey: the predator is informed that it has been detected and might as well save time and energy by giving up the chase, while the prey is protected from attack.\n\nAnother pursuit-deterrent signal is thanatosis or playing dead. Thanatosis is a form of bluff in which an animal mimics its own dead body, feigning death to avoid being attacked by predators seeking live prey. Thanatosis can also be used by the predator in order to lure prey into approaching.\nAn example of this is seen in white-tailed deer fawns, which experience a drop in heart rate in response to approaching predators. This response, referred to as \"alarm bradycardia\", causes the fawn's heart rate to drop from 155 to 38 beats per minute within one beat of the heart. This drop in heart rate can last up to two minutes, causing the fawn to experience a depressed breathing rate and decrease in movement, called tonic immobility. Tonic immobility is a reflex response that causes the fawn to enter a low body position that simulates the position of a dead corpse. Upon discovery of the fawn, the predator loses interest in the \"dead\" prey. Other symptoms of alarm bradycardia, such as salivation, urination, and defecation, can also cause the predator to lose interest.\n\nMarine molluscs such as sea hares, cuttlefish, squid and octopuses give themselves a last chance to escape by distracting their attackers. To do this, they eject a mixture of chemicals, which may mimic food or otherwise confuse predators. In response to a predator, animals in these groups release ink, creating a cloud, and opaline, affecting the predator's feeding senses, causing it to attack the cloud.\n\nDistraction displays attract the attention of predators away from an object, typically the nest or young, that is being protected. Distraction displays are performed by some species of birds, which may feign a broken wing while hopping about on the ground, and by some species of fish.\n\nMimicry occurs when an organism (the mimic) simulates signal properties of another organism (the model) to confuse a third organism. This results in the mimic gaining protection, food, and mating advantages. There are two classical types of defensive mimicry: Batesian and Müllerian. Both involve aposematic coloration, or warning signals, to avoid being attacked by a predator.\n\nIn Batesian mimicry, a palatable, harmless prey species mimics the appearance of another species that is noxious to predators, thus reducing the mimic's risk of attack. This form of mimicry is seen in many insects. The idea behind Batesian mimicry is that predators that have tried to eat the unpalatable species learn to associate its colors and markings with an unpleasant taste. This results in the predator learning to avoid species displaying similar colours and markings, including Batesian mimics, which are in effect parasitic on the chemical or other defences of the unprofitable models. Some species of octopus can mimic a selection of other animals by changing their skin color, skin pattern and body motion. When a damselfish attacks an octopus, the octopus mimics a banded sea-snake. The model chosen varies with the octopus's predator and habitat. Most of these octopuses use Batesian mimicry, selecting an organism repulsive to predators as a model.\n\nIn Müllerian mimicry, two or more aposematic forms share the same warning signals, as in viceroy and monarch butterflies. Birds avoid eating both species because their wing patterns honestly signal their unpleasant taste.\n\nMany animals are protected against predators with armour in the form of hard shells (such as most molluscs), leathery or scaly skin (as in reptiles), or tough chitinous exoskeletons (as in arthropods).\n\nA spine is a sharp, needle-like structure used to inflict pain on predators. An example of this seen in nature is in the Sohal surgeonfish. These fish have a sharp scalpel-like spine on the front of each of their tail fins, able to inflict deep wounds. The area around the spines is often brightly colored to advertise the defensive capability; predators often avoid the Sohal surgeonfish. Defensive spines may be detachable, barbed or poisonous. Porcupine spines are long, stiff, break at the tip, and are barbed to stick into a would-be predator. In contrast, the hedgehog's short spines, which are modified hairs, readily bend, and are barbed into the body, so they are not easily lost; they may be jabbed at an attacker.\n\nMany species of slug caterpillar, Limacodidae, have numerous protuberances and stinging spines along their dorsal surfaces. Species that possess these stinging spines suffer less predation than larvae that lack them, and a predator, the paper wasp, chooses larvae without spines when given a choice.\n\nGroup living can decrease the risk of predation to the individual in a variety of ways, as described below.\n\nA dilution effect is seen when animals living in a group \"dilute\" their risk of attack, each individual being just one of many in the group. George C. Williams and W.D. Hamilton proposed that group living evolved because it provides benefits to the individual rather than to the group as a whole, which becomes more conspicuous as it becomes larger. One common example is the shoaling of fish. Experiments provide direct evidence for the decrease in individual attack rate seen with group living, for example in Camargue horses in Southern France. The horse-fly often attacks these horses, sucking blood and carrying diseases. When the flies are most numerous, the horses gather in large groups, and individuals are indeed attacked less frequently. Water striders are insects that live on the surface of fresh water, and are attacked from beneath by predatory fish. Experiments varying the group size of the water striders showed that the attack rate per individual water strider decreases as group size increases.\n\nThe selfish herd theory was proposed by W.D. Hamilton to explain why animals seek central positions in a group. The theory's central idea is to reduce the individual's domain of danger. A domain of danger is the area within the group in which the individual is more likely to be attacked by a predator. The center of the group has the lowest domain of danger, so animals are predicted to strive constantly to gain this position. Testing Hamilton's selfish herd effect, Alta De Vos and Justin O'Rainn (2010) studied brown fur seal predation from great white sharks. Using decoy seals, the researchers varied the distance between the decoys to produce different domains of danger. The seals with a greater domain of danger had an increased risk of shark attack.\n\nA radical strategy for avoiding predators which may otherwise kill a large majority of the emerging young of a population is to emerge very rarely, at irregular intervals. This strategy is seen in dramatic form in the periodical cicadas, which emerge at intervals of 13 or 17 years. Predators with a life-cycle of one or a few years are unable to reproduce rapidly enough in response to such an emergence, so predator satiation is a likely evolutionary explanation for the cicadas' unusual life-cycle, though not the only one. Predators may still feast on the emerging cicadas, but are unable to consume more than a fraction of the brief surfeit of prey.\n\nAnimals that live in groups often give alarm calls that give warning of an attack. For example, vervet monkeys give different calls depending on the nature of the attack: for an eagle, a disyllabic cough; for a leopard or other cat, a loud bark; for a python or other snake, a \"chutter\". The monkeys hearing these calls respond defensively, but differently in each case: to the eagle call, they look up and run into cover; to the leopard call, they run up into the trees; to the snake call, they stand on two legs and look around for snakes, and on seeing the snake, they sometimes mob it. Similar calls are found in other species of monkey, while birds also give different calls that elicit different responses.\n\nIn the improved vigilance effect, groups are able to detect predators sooner than solitary individuals. For many predators, success depends on surprise. If the prey is alerted early in an attack, they have an improved chance of escape. For example, wood pigeon flocks are preyed upon by goshawks. Goshawks are less successful when attacking larger flocks of wood pigeons than they are when attacking smaller flocks. This is because the larger the flock size, the more likely it is that one bird will notice the hawk sooner and fly away. Once one pigeon flies off in alarm, the rest of the pigeons follow. Wild ostriches in Tsavo National Park in Kenya feed either alone or in groups of up to four birds. They are subject to predation by lions. As the ostrich group size increases, the frequency at which each individual raises its head to look for predators decreases. Because ostriches are able to run at speeds that exceed those of lions for great distances, lions try to attack an ostrich when its head is down. By grouping, the ostriches present the lions with greater difficulty in determining how long the ostriches' heads stay down. Thus, although individual vigilance decreases, the overall vigilance of the group increases.\n\nIndividuals living in large groups may be safer from attack because the predator may be confused by the large group size. As the group moves, the predator has greater difficulty targeting an individual prey animal. The zebra has been suggested by the zoologist Martin Stevens and his colleagues as an example of this. When stationary, a single zebra stands out because of its large size. To reduce the risk of attack, zebras often travel in herds. The striped patterns of all the zebras in the herd may confuse the predator, making it harder for the predator to focus in on an individual zebra. Furthermore, when moving rapidly, the zebra stripes create a confusing, flickering motion dazzle effect in the eye of the predator.\n\nDefensive structures such as spines may be used both to ward off attack as already mentioned, and if need be to fight back against a predator. Methods of fighting back include chemical defences, mobbing, defensive regurgitation, and suicidal altruism.\n\nMany prey animals, and to defend against seed predation also seeds of plants, make use of poisonous chemicals for self-defence. These may be concentrated in surface structures such as spines or glands, giving an attacker a taste of the chemicals before it actually bites or swallows the prey animal: many toxins are bitter-tasting. A last-ditch defence is for the animal's flesh itself to be toxic, as in the puffer fish, danaid butterflies and burnet moths. Many insects acquire toxins from their food plants; \"Danaus\" caterpillars accumulate toxic cardenolides from milkweeds (Asclepiadaceae). \n\nSome prey animals are able to eject noxious materials to deter predators actively. The bombardier beetle has specialized glands on the tip of its abdomen that allows it to direct a toxic spray towards predators. The spray is generated explosively through oxidation of hydroquinones and is sprayed at a temperature of 100 °C. Armoured crickets similarly release blood at their joints when threatened (autohaemorrhaging). Several species of grasshopper including \"Poecilocerus pictus\", \"Parasanaa donovani\", \"Aularches miliaris\", and \"Tegra novaehollandiae\" secrete noxious liquids when threatened, sometimes ejecting these forcefully. Spitting cobras accurately squirt venom from their fangs at the eyes of potential predators, striking their target eight times out of ten, and causing severe pain. Termite soldiers in the Nasutitermitinae have a fontanellar gun, a gland on the front of their head which can secrete and shoot an accurate jet of resinous terpenes \"many centimeters\". The material is sticky and toxic to other insects. One of the terpenes in the secretion, pinene, functions as an alarm pheromone. Seeds deter predation with combinations of toxic non-protein amino acids, cyanogenic glycosides, protease and amylase inhibitors, and phytohemaglutinins.\n\nA few vertebrate species such as the Texas horned lizard are able to shoot squirts of blood from their eyes, by rapidly increasing the blood pressure within the eye sockets, if threatened. Because an individual may lose up to 53% of blood in a single squirt, this is only used against persistent predators like foxes, wolves and coyotes (Canidae), as a last defence. Canids often drop horned lizards after being squirted, and attempt to wipe or shake the blood out of their mouths, suggesting that the fluid has a foul taste; they choose other lizards if given the choice, suggesting a learned aversion towards horned lizards as prey.\n\nThe slime glands along the body of the hagfish secrete enormous amounts of mucus when it is provoked or stressed. The gelatinous slime has dramatic effects on the flow and viscosity of water, rapidly clogging the gills of any fish that attempt to capture hagfish; predators typically release the hagfish within seconds \"(pictured above)\". Common predators of hagfish include seabirds, pinnipeds and cetaceans, but few fish, suggesting that predatory fish avoid hagfish as prey.\n\nIn communal defence, prey groups actively defend themselves by grouping together, and sometimes by attacking or mobbing a predator, rather than allowing themselves to be passive victims of predation. Mobbing is the harassing of a predator by many prey animals. Mobbing is usually done to protect the young in social colonies. For example, red colobus monkeys exhibit mobbing when threatened by chimpanzees, a common predator. The male red colobus monkeys group together and place themselves between predators and the group's females and juveniles. The males jump together and actively bite the chimpanzees. Fieldfares are birds which may nest either solitarily or in colonies. Within colonies, fieldfares mob and defecate on approaching predators, shown experimentally to reduce predation levels.\n\nSome birds and insects use defensive regurgitation to ward off predators. The northern fulmar vomits a bright orange, oily substance called stomach oil when threatened. The stomach oil is made from their aquatic diets. It causes the predator's feathers to mat, leading to the loss of flying ability and the loss of water repellency. This is especially dangerous for aquatic birds because their water repellent feathers protect them from hypothermia when diving for food.\n\nEuropean roller chicks vomit a bright orange, foul smelling liquid when they sense danger. This repels prospective predators and may alert their parents to danger: they respond by delaying their return.\n\nNumerous insects utilize defensive regurgitation. The eastern tent caterpillar regurgitates a droplet of digestive fluid to repel attacking ants. Similarly, larvae of the noctuid moth regurgitate when disturbed by ants. The vomit of noctuid moths has repellent and irritant properties that help to deter predator attacks.\n\nAn unusual type of predator deterrence is observed in the Malaysian exploding ant. Social hymenoptera rely on altruism to protect the entire colony, so the self-destructive acts benefit all individuals in the colony. When a worker ant's leg is grasped, it suicidally expels the contents of its hypertrophied submandibular glands, expelling corrosive irritant compounds and adhesives onto the predator. These prevent predation and serve as a signal to other enemy ants to stop predation of the rest of the colony.\n\nThe normal reaction of a prey animal to an attacking predator is to flee by any available means, whether flying, gliding, falling, swimming, running, jumping, burrowing or rolling, according to the animal's capabilities. Escape paths are often erratic, making it difficult for the predator to predict which way the prey will go next: for example, birds such as snipe, ptarmigan and black-headed gulls evade fast raptors such as peregrine falcons with zigzagging or jinking flight. In the tropical rain forests of Southeast Asia in particular, many vertebrates escape predators by falling and gliding. Among the insects, many moths turn sharply, fall, or perform a powered dive in response to the sonar clicks of bats. Among fish, the stickleback follows a zigzagging path, often doubling back erratically, when chased by a fish-eating merganser duck.\n\nSome animals are capable of autotomy (self-amputation), shedding one of their own appendages in a last-ditch attempt to elude a predator's grasp or to distract the predator and thereby allow escape. The lost body part may be regenerated later. Certain sea slugs discard stinging papillae; arthropods such as crabs can sacrifice a claw, which can be regrown over several successive moults; among vertebrates, many geckos and other lizards shed their tails when attacked: the tail goes on writhing for a while, distracting the predator, and giving the lizard time to escape; a smaller tail slowly regrows.\n\nAristotle recorded observations (around 350 BC) of the antipredator behaviour of cephalopods in his \"History of Animals\", including the use of ink as a distraction, camouflage, and signalling.\n\nIn 1940, Hugh Cott wrote a compendious study of camouflage, mimicry, and aposematism, \"Adaptive Coloration in Animals\".\n\n\n\n"}
{"id": "563239", "url": "https://en.wikipedia.org/wiki?curid=563239", "title": "Biogenic substance", "text": "Biogenic substance\n\nA biogenic substance is a product made by or of life forms. The term encompasses constituents, secretions, and metabolites of plants or animals. In context of molecular biology, biogenic substances are referred to as biomolecules. \n\n\nAn abiogenic substance or process does not result from the present or past activity of living organisms. Abiogenic products may, e.g., be minerals, other inorganic compounds, as well as simple organic compounds (e.g. extraterrestrial methane, see also abiogenesis).\n\n"}
{"id": "8553751", "url": "https://en.wikipedia.org/wiki?curid=8553751", "title": "Biological organisation", "text": "Biological organisation\n\nBiological organization is the hierarchy of complex biological structures and systems that define life using a reductionistic approach. The traditional hierarchy, as detailed below, extends from atoms to biospheres. The higher levels of this scheme are often referred to as an ecological organization concept, or as the field, hierarchical ecology.\n\nEach level in the hierarchy represents an increase in organizational complexity, with each \"object\" being primarily composed of the previous level's basic unit. The basic principle behind the organization is the concept of \"emergence\"—the properties and functions found at a hierarchical level are not present and irrelevant at the lower levels.\n\nThe biological organization of life is a fundamental premise for numerous areas of scientific research, particularly in the medical sciences. Without this necessary degree of organization, it would be much more difficult—and likely impossible—to apply the study of the effects of various physical and chemical phenomena to diseases and physiology (body function). For example, fields such as cognitive and behavioral neuroscience could not exist if the brain was not composed of specific types of cells, and the basic concepts of pharmacology could not exist if it was not known that a change at the cellular level can affect an entire organism. These applications extend into the ecological levels as well. For example, DDT's direct insecticidal effect occurs at the subcellular level, but affects higher levels up to and including multiple ecosystems. Theoretically, a change in one atom could change the entire biosphere.\n\nThe simple standard biological organization scheme, from the lowest level to the highest level, is as follows:\n\nMore complex schemes incorporate many more levels. For example, a molecule can be viewed as a grouping of elements, and an atom can be further divided into subatomic particles (these levels are outside the scope of biological organization). Each level can also be broken down into its own hierarchy, and specific types of these biological objects can have their own hierarchical scheme. For example, genomes can be further subdivided into a hierarchy of genes.\n\nEach level in the hierarchy can be described by its lower levels. For example, the organism may be described at any of its component levels, including the atomic, molecular, cellular, histological (tissue), organ and organ system levels. Furthermore, at every level of the hierarchy, new functions necessary for the control of life appear. These new roles are not functions that the lower level components are capable of and are thus referred to as \"emergent properties\".\n\nEvery organism is organised, though not necessarily to the same degree. An organism can not be organised at the histological (tissue) level if it is not composed of tissues in the first place.\n\nEmpirically, a large proportion of the (complex) biological systems we observe in nature exhibit hierarchic structure. On theoretical grounds we could expect complex systems to be hierarchies in a world in which complexity had to evolve from simplicity. System hierarchies analysis performed in the 1950s, laid the empirical foundations for a field that would be, from the 1980s, hierarchical ecology.\n\nThe theoretical foundations are summarized by thermodynamics.\nWhen biological systems are modeled as physical systems, in its most general abstraction, they are thermodynamic open systems that exhibit self-organised behavior, and the set/subset relations between dissipative structures can be characterized in a hierarchy.\n\nA simpler and more direct way to explain the fundamentals of the \"hierarchical organization of life\", was introduced in Ecology by Odum and others as the \"Simon's hierarchical principle\"; Simon emphasized that hierarchy \"emerges almost inevitably through a wide variety of evolutionary processes, for the simple reason that hierarchical structures are stable\".\n\nTo motivate this deep idea, he offered his \"parable\" about imaginary watchmakers.\n\n\n"}
{"id": "42739", "url": "https://en.wikipedia.org/wiki?curid=42739", "title": "Bubble fusion", "text": "Bubble fusion\n\nBubble fusion is the non-technical name for a nuclear fusion reaction hypothesized to occur inside extraordinarily large collapsing gas bubbles created in a liquid during acoustic cavitation. The more technical name is sonofusion.\n\nThe term was coined in 2002 with the release of a report by Rusi Taleyarkhan and collaborators that claimed to have observed evidence of sonofusion. The claim was quickly surrounded by controversy, including allegations ranging from experimental error to academic fraud. Subsequent publications claiming independent verification of sonofusion were also highly controversial.\n\nEventually, an investigation by Purdue University found that Taleyarkhan had engaged in falsification of independent verification, and had included a student as an author on a paper when he had not participated in the research. He was subsequently stripped of his professorship. One of his funders, the Office of Naval Research reviewed the report by Purdue and barred him from federal funding for 28 months.\n\nUS patent 4,333,796, filed by Hugh Flynn in 1978, appears to be the earliest documented reference to a sonofusion-type reaction.\n\nIn the March 8, 2002 issue of the peer-reviewed journal \"Science\", Rusi P. Taleyarkhan and colleagues at the Oak Ridge National Laboratory (ORNL) reported that acoustic cavitation experiments conducted with deuterated acetone () showed measurements of tritium and neutron output consistent with the occurrence of fusion. The neutron emission was also reported to be coincident with the sonoluminescence pulse, a key indicator that its source was fusion caused by the heat and pressure inside the collapsing bubbles.\n\nThe results were so startling, that the Oak Ridge National Laboratory asked two independent researchers, D. Shapira and M. J. Saltmarsh, to repeat the experiment using more sophisticated neutron detection equipment. They reported that the neutron release was consistent with random coincidence. A rebuttal by Taleyarkhan and the other authors of the original report argued that the Shapira and Saltmarsh report failed to account for significant differences in experimental setup, including over an inch of shielding between the neutron detector and the sonoluminescing acetone. According to Taleyarkhan \"et al.\", when properly considering those differences, the results were consistent with fusion.\n\nAs early as 2002, while experimental work was still in progress, Aaron Galonsky of Michigan State University, in a letter to the journal \"Science\"\nexpressed doubts about the claim made by the Taleyarkhan team. In Galonsky's opinion, the observed neutrons were too high in energy to be from a deuterium-deuterium (d-d) fusion reaction. In their response (published on the same page), the Taleyarkhan team provided detailed counter-arguments and concluded that the energy was \"reasonably close\" to that which was expected from a fusion reaction.\n\nIn February 2005 the documentary series \"Horizon\" commissioned two leading sonoluminescence researchers, Seth Putterman and Kenneth S. Suslick, to reproduce Taleyarkhan's work. Using similar acoustic parameters, deuterated acetone, similar bubble nucleation, and a much more sophisticated neutron detection device, the researchers could find no evidence of a fusion reaction.\n\nIn 2004, new reports of bubble fusion were published by the Taleyarkhan group, claiming that the results of previous experiments had been replicated under more stringent experimental conditions. These results differed from the original results in that fusion was claimed to occur over longer times than previously reported. The original report only claimed neutron emission from the initial bubble collapse following bubble nucleation, whereas this report claimed neutron emission many acoustic cycles later.\n\nIn July 2005, two of Taleyarkhan's students at Purdue University published evidence confirming the previous result. They used the same acoustic chamber, the same deuterated acetone fluid and a similar bubble nucleation system. In this report, no neutron-sonoluminescence coincidence was attempted. An article in \"Nature\" raised issues about the validity of the research and complaints from his Purdue colleagues (see full analysis elsewhere in this page). Charges of misconduct were raised, and Purdue University opened an investigation. It concluded in 2008 that Taleyarkhan's name should have appeared in the author list because of his deep involvement in many steps of the research, that he added one author that had not really participated in the paper just to overcome the criticism of one reviewer, and that this was part of an attempt of \"an effort to falsify the scientific record by assertion of independent confirmation\". The investigation did not address the validity of the experimental results.\n\nIn January 2006, a paper published in the journal \"Physical Review Letters\" by Taleyarkhan in collaboration with researchers from Rensselaer Polytechnic Institute reported statistically significant evidence of fusion.\n\nIn November 2006, in the midst of accusations concerning Taleyarkhan's research standards, two different scientists visited the meta-stable fluids research lab at Purdue University to measure neutrons, using Taleyarkhan's equipment. Dr. Edward R. Forringer and undergraduates David Robbins and Jonathan Martin of LeTourneau University presented two papers at the American Nuclear Society Winter Meeting that reported replication of neutron emission. Their experimental setup was similar to previous experiments in that it used a mixture of deuterated acetone, deuterated benzene, tetrachloroethylene and uranyl nitrate. Notably, however, it operated without an external neutron source and used two types of neutron detectors. They claimed a liquid scintillation detector measured neutron levels at 8 standard deviations above the background level, while plastic detectors measured levels at 3.8 standard deviations above the background. When the same experiment was performed with non-deuterated control liquid, the measurements were within one standard deviation of background, indicating that the neutron production had only occurred during cavitation of the deuterated liquid. William M. Bugg, emeritus physics professor at the University of Tennessee also traveled to Taleyarkhan's lab to repeat the experiment with his equipment. He also reported neutron emission, using plastic neutron detectors. Taleyarkhan claimed these visits counted as independent replications by experts, but Forringer later recognized that he was not an expert, and Bugg later said that Taleyarkhan performed the experiments and he had only watched.\n\nIn March 2006, \"Nature\" published a special report that called into question the validity of the results of the Purdue experiments. The report quotes Brian Naranjo of the University of California, Los Angeles to the effect that neutron energy spectrum reported in the 2006 paper by Taleyarkhan, et al. was statistically inconsistent with neutrons produced by the proposed fusion reaction and instead highly consistent with neutrons produced by the radioactive decay of Californium 252, an isotope commonly used as a laboratory neutron source .\n\nThe response of Taleyarkhan \"et al.\", published in \"Physical Review Letters\", attempts to refute Naranjo's hypothesis as to the cause of the neutrons detected.\n\nTsoukalas, head of the School of Nuclear Engineering at Purdue, and several of his colleagues at Purdue, had convinced Taleyarkhan to move to Purdue and attempt a joint replication. In the 2006 \"Nature\" report they detail several troubling issues when trying to collaborate with Taleyarkhan. He reported positive results from certain set of raw data, but his colleagues had also examined that set and it only contained negative results. He never showed his colleagues the raw data corresponding to the positive results, despite several requests. He moved the equipment from a shared laboratory to his own laboratory, thus impeding review by his colleagues, and he didn't give any advance warning or explanation for the move. Taleyarkhan convinced his colleagues that they shouldn't publish a paper with their negative results. Taleyarkhan then insisted that the university's press release present his experiment as \"peer-reviewed\" and \"independent\", when the co-authors were working in his laboratory under his supervision, and his peers in the faculty were not allowed to review the data. In summary, Taleyarkhan's colleagues at Purdue said he placed obstacles to peer review of his experiments, and they had serious doubts about the validity of the research.\n\n\"Nature\" also revealed that the process of anonymous peer-review had not been followed, and that the journal \"Nuclear Engineering and Design\" was not independent from the authors. Taleyarkhan was co-editor of the journal, and the paper was only peer-reviewed by his co-editor, with Taleyarkhan's knowledge.\n\nIn 2002 Taleyarkhan filed a patent application on behalf of the United States Department of Energy, while working in Oak Ridge. \"Nature\" reported that the patent had been rejected in 2005 by the US Patent Office. The examiner called the experiment a variation of discredited cold fusion, found that there was \"no reputable evidence of record to support any allegations or claims that the invention is capable of operating as indicated\", and found that there was not enough detail for others to replicate the invention. The field of fusion suffered from many flawed claims, thus the examiner asked for additional proof that the radiation was generated from fusion and not from other sources. An appeal was not filed because the Department of Energy had dropped the claim in December 2005.\n\nDoubts among Purdue University's Nuclear Engineering faculty as to whether the positive results reported from sonofusion experiments conducted there were truthful prompted the university to initiate a review of the research, conducted by Purdue's Office of the Vice President for Research. In a March 9, 2006 article entitled \"Evidence for bubble fusion called into question\", \"Nature\" interviewed several of Taleyarkhan's colleagues who suspected something was amiss.\n\nOn February 7, 2007, the Purdue University administration determined that \"the evidence does not support the allegations of research misconduct and that no further investigation of the allegations is warranted\". Their report also stated that \"vigorous, open debate of the scientific merits of this new technology is the most appropriate focus going forward.\" In order to verify that the investigation was properly conducted, House Representative Brad Miller requested full copies of its documents and reports by March 30, 2007. His congressional report concluded that \"Purdue deviated from its own procedures in investigating this case and did not conduct a thorough investigation\"; in response, Purdue announced that it would re-open its investigation.\n\nIn June 2008, a multi-institutional team including Taleyarkhan published a paper in Nuclear Engineering and Design to \"clear up misconceptions generated by a webposting of UCLA which served as the basis for the \"Nature\" article of March 2006\", according to a press release.\n\nOn July 18, 2008, Purdue University announced that a committee with members from five institutions had investigated 12 allegations of research misconduct against Rusi Taleyarkhan. It concluded that two allegations were founded—that Taleyarkhan had claimed independent confirmation of his work when in reality the apparent confirmations were done by Taleyarkhan's former students and was not as \"independent\" as Taleyarkhan implied, and that Taleyarkhan had included a colleague's name on one of his papers who had not actually been involved in the research (\"the sole apparent motivation for the addition of Mr. Butt was a desire to overcome a reviewer's criticism\", the report concluded).\n\nTaleyarkhan's appeal of the report's conclusions was rejected. He said the two allegations of misconduct were trivial administrative issues and had nothing to do with the discovery of bubble nuclear fusion or the underlying science, and that \"all allegations of fraud and fabrication have been dismissed as invalid and without merit — thereby supporting the underlying science and experimental data as being on solid ground\". A researcher questioned by the LA Times said that the report had not clarified whether bubble fusion was real or not, but that the low quality of the papers and the doubts cast by the report had destroyed Taleyarkhan's credibility with the scientific community.\n\nOn August 27, 2008 he was stripped of his named Arden Bement Jr. Professorship, and forbidden to be a thesis advisor for graduate students for at least the next 3 years.\n\nDespite the findings against him, Taleyarkhan received a $185,000 grant from the National Science Foundation between September 2008 and August 2009 to investigate bubble fusion. In 2009 the Office of Naval Research debarred him for 28 months, until September 2011, from receiving U.S. Federal Funding. During that period his name was listed in the 'Excluded Parties List' to prevent him from receiving further grants from any government agency.\n\n\n"}
{"id": "10019499", "url": "https://en.wikipedia.org/wiki?curid=10019499", "title": "Burned area emergency response", "text": "Burned area emergency response\n\nBurned area emergency response (BAER) is an emergency risk management reaction to post wildfire conditions that pose risks to human life and property or could further destabilize or degrade the burned lands. Even though wildfires are natural events, the presence of people and man-made structures in and adjacent to the burned area frequently requires continued emergency risk management actions. High severity wildfires pose a continuing flood, debris flow and mudflow risk to people living within and downstream from a burned watershed as well as a potential loss of desirable watershed values.\n\nThe burned area emergency response risk management process begins during or shortly after wildfire containment with risk assessments evaluating the effects of the wildfire against values needing protection. These risk assessments can range from simple to complex. An organized interdisciplinary team of subject matter experts (e.g., hydrologists, soil scientists, botanists, cultural resource specialists, engineers, etc.) used among other assessment tools hydrological modeling and soil burn severity mapping to assess potential flooding and vegetation recovery after the Cerro Grande Fire in 2000.\n\nA BAER plan is developed based on the risk assessments and burned area land management objectives. The BAER Plan identifies the most effective treatments to address the identified risks. Plan implementation timeframes are dictated primarily by anticipated future events (e.g., next significant rainstorm) which also influence treatment options.\n\nBurned area emergency response has mostly concentrated on risk reduction treatments with varying degrees of success. Risk avoidance, transfer and retention treatments are integral in the burned area emergency response risk management process.\n\nRisk reduction treatments are designed to protect human life and safety and reduce flood severity, soil erosion and prevent the establishment of non-native plants. On 10 wildfires studied in Colorado, rainfall amount and intensity followed by bare mineral soil explained 63% of soil erosion variation. Research has shown that the risk of flooding, debris flows and mudflows are significantly increased with increasing rainfall intensities and burn severity and that some risk reduction treatments help for low but not high intensity rainfall events.\n\nMulches, erosion cloth and seeding retard overland flow and protect soil from rain drop impact and increase soil moisture holding capacity. Landscape structures (e.g., log erosion barriers, contour trenches, straw wattles) trap sediment and prevent slope rilling. Strip tillage and chemicals break up or reduce hydrophobic soils and improve infiltration. Wood and straw mulch reduced erosion rates by 60 to 80%, contour-felled log erosion barriers 50 to 70%, hydromulch 19% and post fire seeding had little effect the first year when rainfall events were small and intensities low.\n\nIn stream flood control treatments slow, delay, redistribute, or redirect water, mud and debris. Straw bale check dams, silt screens and debris retention basins slow water flow and trap sediment. Riparian vegetation stabilizes streambanks. Roads and culverts are armored and debris removed as needed. Water diversion implements protect facilities and property.\n\nThe chance of introducing new invasive plants to the burned area is reduced by restricting access or thoroughly cleaning all equipment, people and animals of seeds before entering a burned area. Research has shown that non-native plant cover is positively associated with post-wildfire seeded grass cover. Even though post-wildfire seeding operations require seed mix purity standards and the number of contaminated seeds may be small on a percentage based, that the application of very large amounts of seed (thousands of pounds) ensures that a significant number of non-native plant seeds will be distributed.\n\nAvoidance treatments remove values at risk from risk prone areas. Frequently homes and other values are located on alluvial fans at the base of watersheds. The presence of the alluvial fans indicates a history of significant flooding, debris flows and mudflows with potential personal and property damage potential. Mobile property is temporally or permanently relocated. Evacuation planning and early warning systems are frequently used to protect people at risk. Flood peaks increase more rapidly with increases in rainfall intensity above a threshold value for the maximum 30 min intensity of approximately 10 mm per hour. That this rainfall intensity could be used to set threshold limits in rain gauges that are part of an early warning flood system after wildfire.\n\nOften it is not feasible to avoid or reduce risks. Flood insurance is a means of transferring risk to another party for values with insurable value.\n\nAccepting the risk is an option when values at risk are small and inevitable or when the risks cannot be reduced, avoided or transferred (i.e., infrequent catastrophic events).\n\n\n"}
{"id": "4024", "url": "https://en.wikipedia.org/wiki?curid=4024", "title": "Butterfly effect", "text": "Butterfly effect\n\nIn chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.\n\nThe term, coined by Edward Lorenz, is derived from the metaphorical example of the details of a tornado (the exact time of formation, the exact path taken) being influenced by minor perturbations such as the flapping of the wings of a distant butterfly several weeks earlier. Lorenz discovered the effect when he observed that runs of his weather model with initial condition data that was rounded in a seemingly inconsequential manner would fail to reproduce the results of runs with the unrounded initial condition data. A very small change in initial conditions had created a significantly different outcome.\n\nThough Lorenz gave a name to the phenomenon, the idea that small causes may have large effects in general and in weather specifically was earlier recognized by French mathematician and engineer Henri Poincaré and American mathematician and philosopher Norbert Wiener. Edward Lorenz's work placed the concept of \"instability\" of the earth's atmosphere onto a quantitative base and linked the concept of instability to the properties of large classes of dynamic systems which are undergoing nonlinear dynamics and deterministic chaos.\n\nThe butterfly effect can also be demonstrated by very simple systems.\n\nIn \"The Vocation of Man\" (1800), Johann Gottlieb Fichte says that \"you could not remove a single grain of sand from its place without thereby ... changing something throughout all parts of the immeasurable whole\".\n\nChaos theory and the sensitive dependence on initial conditions were described in the literature in a particular case of the three-body problem by Henri Poincaré in 1890. He later proposed that such phenomena could be common, for example, in meteorology.\n\nIn 1898, Jacques Hadamard noted general divergence of trajectories in spaces of negative curvature. Pierre Duhem discussed the possible general significance of this in 1908.\n\nThe idea that one butterfly could eventually have a far-reaching ripple effect on subsequent historic events made its earliest known appearance in \"A Sound of Thunder\", a 1952 short story by Ray Bradbury about time travel.\n\nIn 1961, Lorenz was running a numerical computer model to redo a weather prediction from the middle of the previous run as a shortcut. He entered the initial condition 0.506 from the printout instead of entering the full precision 0.506127 value. The result was a completely different weather scenario.\n\nLorenz wrote:\nIn 1963 Lorenz published a theoretical study of this effect in a highly cited, seminal paper called \"Deterministic Nonperiodic Flow\" (the calculations were performed on a Royal McBee LGP-30 computer). Elsewhere he stated: Following suggestions from colleagues, in later speeches and papers Lorenz used the more poetic butterfly. According to Lorenz, when he failed to provide a title for a talk he was to present at the 139th meeting of the American Association for the Advancement of Science in 1972, Philip Merilees concocted \"Does the flap of a butterfly’s wings in Brazil set off a tornado in Texas?\" as a title. Although a butterfly flapping its wings has remained constant in the expression of this concept, the location of the butterfly, the consequences, and the location of the consequences have varied widely.\n\nThe phrase refers to the idea that a butterfly's wings might create tiny changes in the atmosphere that may ultimately alter the path of a tornado or delay, accelerate or even prevent the occurrence of a tornado in another location. The butterfly does not power or directly create the tornado, but the term is intended to imply that the flap of the butterfly's wings can \"cause\" the tornado: in the sense that the flap of the wings is a part of the initial conditions; one set of conditions leads to a tornado while the other set of conditions doesn't. The flapping wing represents a small change in the initial condition of the system, which cascades to large-scale alterations of events (compare: domino effect). Had the butterfly not flapped its wings, the trajectory of the system might have been vastly different—but it's also equally possible that the set of conditions without the butterfly flapping its wings is the set that leads to a tornado.\n\nThe butterfly effect presents an obvious challenge to prediction, since initial conditions for a system such as the weather can never be known to complete accuracy. This problem motivated the development of ensemble forecasting, in which a number of forecasts are made from perturbed initial conditions.\n\nSome scientists have since argued that the weather system is not as sensitive to initial conditions as previously believed. David Orrell argues that the major contributor to weather forecast error is model error, with sensitivity to initial conditions playing a relatively small role. Stephen Wolfram also notes that the Lorenz equations are highly simplified and do not contain terms that represent viscous effects; he believes that these terms would tend to damp out small perturbations.\n\nRecurrence, the approximate return of a system towards its initial conditions, together with sensitive dependence on initial conditions, are the two main ingredients for chaotic motion. They have the practical consequence of making complex systems, such as the weather, difficult to predict past a certain time range (approximately a week in the case of weather) since it is impossible to measure the starting atmospheric conditions completely accurately.\n\nA dynamical system displays sensitive dependence on initial conditions if points arbitrarily close together separate over time at an exponential rate. The definition is not topological, but essentially metrical.\n\nIf \"M\" is the state space for the map formula_1, then formula_1 displays sensitive dependence to initial conditions if for any x in \"M\" and any δ > 0, there are y in \"M\", with distance \"d\"(. , .) such that formula_3 and such that\n\nfor some positive parameter \"a\". The definition does not require that all points from a neighborhood separate from the base point \"x\", but it requires one positive Lyapunov exponent.\n\nThe simplest mathematical framework exhibiting sensitive dependence on initial conditions is provided by a particular parametrization of the logistic map:\n\nwhich, unlike most chaotic maps, has a closed-form solution:\n\nwhere the initial condition parameter formula_7 is given by formula_8. For rational formula_7, after a finite number of iterations formula_10 maps into a periodic sequence. But almost all formula_7 are irrational, and, for irrational formula_7, formula_10 never repeats itself – it is non-periodic. This solution equation clearly demonstrates the two key features of chaos – stretching and folding: the factor 2 shows the exponential growth of stretching, which results in sensitive dependence on initial conditions (the butterfly effect), while the squared sine function keeps formula_10 folded within the range [0, 1].\n\nThe butterfly effect is most familiar in terms of weather; it can easily be demonstrated in standard weather prediction models, for example. The climate scientists James Annan and William Connolley explain that chaos is important in the development of weather prediction methods; models are sensitive to initial conditions. They add the caveat: \"Of course the existence of an unknown butterfly flapping its wings has no direct bearing on weather forecasts, since it will take far too long for such a small perturbation to grow to a significant size, and we have many more immediate uncertainties to worry about. So the direct impact of this phenomenon on weather prediction is often somewhat overstated.\"\n\nThe potential for sensitive dependence on initial conditions (the butterfly effect) has been studied in a number of cases in semiclassical and quantum physics including atoms in strong fields and the anisotropic Kepler problem. Some authors have argued that extreme (exponential) dependence on initial conditions is not expected in pure quantum treatments; however, the sensitive dependence on initial conditions demonstrated in classical motion is included in the semiclassical treatments developed by Martin Gutzwiller and Delos and co-workers.\n\nOther authors suggest that the butterfly effect can be observed in quantum systems. Karkuszewski et al. consider the time evolution of quantum systems which have slightly different Hamiltonians. They investigate the level of sensitivity of quantum systems to small changes in their given Hamiltonians. Poulin et al. presented a quantum algorithm to measure fidelity decay, which \"measures the rate at which identical initial states diverge when subjected to slightly different dynamics\". They consider fidelity decay to be \"the closest quantum analog to the (purely classical) butterfly effect\". Whereas the classical butterfly effect considers the effect of a small change in the position and/or velocity of an object in a given Hamiltonian system, the quantum butterfly effect considers the effect of a small change in the Hamiltonian system with a given initial position and velocity. This quantum butterfly effect has been demonstrated experimentally. Quantum and semiclassical treatments of system sensitivity to initial conditions are known as quantum chaos.\n\nThe journalist Peter Dizikes, writing in \"The Boston Globe\" in 2008, notes that popular culture likes the idea of the butterfly effect, but gets it wrong. Whereas Lorenz suggested correctly with his butterfly metaphor that predictability \"is inherently limited\", popular culture supposes that each event can be explained by finding the small reasons that caused it. Dizikes explains: \"It speaks to our larger expectation that the world should be comprehensible – that everything happens for a reason, and that we can pinpoint all those reasons, however small they may be. But nature itself defies this expectation.\"\n\n\n\n"}
{"id": "6111", "url": "https://en.wikipedia.org/wiki?curid=6111", "title": "Chemical vapor deposition", "text": "Chemical vapor deposition\n\nChemical vapor deposition (CVD) is a deposition method used to produce high quality, high-performance, solid materials, typically under vacuum. The process is often used in the semiconductor industry to produce thin films.\n\nIn typical CVD, the wafer (substrate) is exposed to one or more volatile precursors, which react and/or decompose on the substrate surface to produce the desired deposit. Frequently, volatile by-products are also produced, which are removed by gas flow through the reaction chamber.\n\nMicrofabrication processes widely use CVD to deposit materials in various forms, including: monocrystalline, polycrystalline, amorphous, and epitaxial. These materials include: silicon (dioxide, carbide, nitride, oxynitride), carbon (fiber, nanofibers, nanotubes, diamond and graphene), fluorocarbons, filaments, tungsten, titanium nitride and various high-k dielectrics.\n\nCVD is practiced in a variety of formats. These processes generally differ in the means by which chemical reactions are initiated.\n\nMost modern CVD is either LPCVD or UHVCVD.\n\nCVD is commonly used to deposit conformal films and augment substrate surfaces in ways that more traditional surface modification techniques are not capable of. CVD is extremely useful in the process of atomic layer deposition at depositing extremely thin layers of material. A variety of applications for such films exist. Gallium arsenide is used in some integrated circuits (ICs) and photovoltaic devices. Amorphous polysilicon is used in photovoltaic devices. Certain carbides and nitrides confer wear-resistance. Polymerization by CVD, perhaps the most versatile of all applications, allows for super-thin coatings which possess some very desirable qualities, such as lubricity, hydrophobicity and weather-resistance to name a few. CVD of metal-organic frameworks, a class of crystalline nanoporous materials, has recently been demonstrated. Applications for these films are anticipated in gas sensing and low-k dielectrics\nCVD techniques are adventageous for membrane coatings as well, such as those in desalination or water treatment, as these coatings can be sufficiently uniform (conformal) and thin that they do not clog membrane pores.\n\nPolycrystalline silicon is deposited from trichlorosilane (SiHCl) or silane (SiH), using the following reactions:\n\nThis reaction is usually performed in LPCVD systems, with either pure silane feedstock, or a solution of silane with 70–80% nitrogen. Temperatures between 600 and 650 °C and pressures between 25 and 150 Pa yield a growth rate between 10 and 20 nm per minute. An alternative process uses a hydrogen-based solution. The hydrogen reduces the growth rate, but the temperature is raised to 850 or even 1050 °C to compensate. Polysilicon may be grown directly with doping, if gases such as phosphine, arsine or diborane are added to the CVD chamber. Diborane increases the growth rate, but arsine and phosphine decrease it.\n\nSilicon dioxide (usually called simply \"oxide\" in the semiconductor industry) may be deposited by several different processes. Common source gases include silane and oxygen, dichlorosilane (SiClH) and nitrous oxide (NO), or tetraethylorthosilicate (TEOS; Si(OCH)). The reactions are as follows:\n\nThe choice of source gas depends on the thermal stability of the substrate; for instance, aluminium is sensitive to high temperature. Silane deposits between 300 and 500 °C, dichlorosilane at around 900 °C, and TEOS between 650 and 750 °C, resulting in a layer of \"low- temperature oxide\" (LTO). However, silane produces a lower-quality oxide than the other methods (lower dielectric strength, for instance), and it deposits nonconformally. Any of these reactions may be used in LPCVD, but the silane reaction is also done in APCVD. CVD oxide invariably has lower quality than thermal oxide, but thermal oxidation can only be used in the earliest stages of IC manufacturing.\n\nOxide may also be grown with impurities (alloying or \"doping\"). This may have two purposes. During further process steps that occur at high temperature, the impurities may diffuse from the oxide into adjacent layers (most notably silicon) and dope them. Oxides containing 5–15% impurities by mass are often used for this purpose. In addition, silicon dioxide alloyed with phosphorus pentoxide (\"P-glass\") can be used to smooth out uneven surfaces. P-glass softens and reflows at temperatures above 1000 °C. This process requires a phosphorus concentration of at least 6%, but concentrations above 8% can corrode aluminium. Phosphorus is deposited from phosphine gas and oxygen:\n\nGlasses containing both boron and phosphorus (borophosphosilicate glass, BPSG) undergo viscous flow at lower temperatures; around 850 °C is achievable with glasses containing around 5 weight % of both constituents, but stability in air can be difficult to achieve. Phosphorus oxide in high concentrations interacts with ambient moisture to produce phosphoric acid. Crystals of BPO can also precipitate from the flowing glass on cooling; these crystals are not readily etched in the standard reactive plasmas used to pattern oxides, and will result in circuit defects in integrated circuit manufacturing.\n\nBesides these intentional impurities, CVD oxide may contain byproducts of the deposition. TEOS produces a relatively pure oxide, whereas silane introduces hydrogen impurities, and dichlorosilane introduces chlorine.\n\nLower temperature deposition of silicon dioxide and doped glasses from TEOS using ozone rather than oxygen has also been explored (350 to 500 °C). Ozone glasses have excellent conformality but tend to be hygroscopic – that is, they absorb water from the air due to the incorporation of silanol (Si-OH) in the glass. Infrared spectroscopy and mechanical strain as a function of temperature are valuable diagnostic tools for diagnosing such problems.\n\nSilicon nitride is often used as an insulator and chemical barrier in manufacturing ICs. The following two reactions deposit silicon nitride from the gas phase:\n\nSilicon nitride deposited by LPCVD contains up to 8% hydrogen. It also experiences strong tensile stress, which may crack films thicker than 200 nm. However, it has higher resistivity and dielectric strength than most insulators commonly available in microfabrication (10 Ω·cm and 10 MV/cm, respectively).\n\nAnother two reactions may be used in plasma to deposit SiNH:\n\nThese films have much less tensile stress, but worse electrical properties (resistivity 10 to 10 Ω·cm, and dielectric strength 1 to 5 MV/cm).\n\nCVD for tungsten is achieved from tungsten hexafluoride (WF), which may be deposited in two ways:\n\nOther metals, notably aluminium and copper, can be deposited by CVD. , commercially cost-effective CVD for copper did not exist, although volatile sources exist, such as Cu(hfac). Copper is typically deposited by electroplating. Aluminum can be deposited from triisobutylaluminium (TIBAL) and related organoaluminium compounds.\n\nCVD for molybdenum, tantalum, titanium, nickel is widely used. These metals can form useful silicides when deposited onto silicon. Mo, Ta and Ti are deposited by LPCVD, from their pentachlorides. Nickel, molybdenum, and tungsten can be deposited at low temperatures from their carbonyl precursors. In general, for an arbitrary metal \"M\", the chloride deposition reaction is as follows:\n\nwhereas the carbonyl decomposition reaction can happen spontaneously under thermal treatment or acoustic cavitation and is as follows:\n\nthe decomposition of metal carbonyls is often violently precipitated by moisture or air, where oxygen reacts with the metal precursor to form metal or metal oxide along with carbon dioxide.\n\nNiobium(V) oxide layers can be produced by the thermal decomposition of niobium(V) ethoxide with the loss of diethyl ether according to the equation:\n\nMany variations of CVD can be utilized to synthesize graphene. Although many advancements have been made, the processes listed below are not commercially viable yet.\nThe most popular carbon source used to produce graphene is methane gas. Less popular choices include petroleum asphalt, notable for being inexpensive but more difficult to work with.\nThe use of catalyst is viable in changing the physical process of graphene production. Notable examples include iron nanoparticles, nickel foam, and gallium vapor. These catalysts can either be used in situ during graphene buildup, or situated at some distance away at the deposition area. Some catalysts require another step to remove them from the sample material.\n\nThe direct growth of high-quality, large single-crystalline domains of graphene on a dielectric substrate is of vital importance for applications in electronics and optoelectronics. Combining the advantages of both catalytic CVD and the ultra-flat dielectric substrate, gaseous catalyst-assisted CVD paves the way for synthesizing high-quality graphene for device applications while avoiding the transfer process.\nPhysical conditions such as surrounding pressure, temperature, carrier gas, and chamber material play a big role in production of graphene.\n\nMost systems use LPCVD with pressures ranging from 1 to 1500 Pa. However, some still use APCVD. Low pressures are used more commonly as they help prevent unwanted reactions and produce more uniform thickness of deposition on the substrate.\n\nOn the other hand, temperatures used range from 800–1050 °C. High temperatures translate to an increase of the rate of reaction. Caution has to be exercised as high temperatures do pose higher danger levels in addition to greater energy costs.\nHydrogen gas and inert gases such as argon are flowed into the system. These gases act as a carrier, enhancing surface reaction and improving reaction rate, thereby increasing deposition of graphene onto the substrate.\nStandard quartz tubing and chambers are used in CVD of graphene. Quartz is chosen because it has a very high melting point and is chemically inert. In other words, quartz does not interfere with any physical or chemical reactions regardless of the conditions.\nRaman spectroscopy, X-ray spectroscopy, transmission electron microscopy (TEM), and scanning electron microscopy (SEM) are used to examine and characterize the graphene samples.\n\nRaman spectroscopy is used to characterize and identify the graphene particles; X-ray spectroscopy is used to characterize chemical states; TEM is used to provide fine details regarding the internal composition of graphene; SEM is used to examine the surface and topography.\n\nSometimes, atomic force microscopy (AFM) is used to measure local properties such as friction and magnetism.\n\nCold wall CVD technique can be used to study the underlying surface science involved in graphene nucleation and growth as it allows unprecedented control of process parameters like gas flow rates, temperature and pressure as demonstrated in a recent study. The study was carried out in a home-built vertical cold wall system utilizing resistive heating by passing direct current through the substrate. It provided conclusive insight into a typical surface-mediated nucleation and growth mechanism involved in two-dimensional materials grown using catalytic CVD under conditions sought out in the semiconductor industry.\n\nIn spite of graphene's exciting electronic and thermal properties, it is unsuitable as a transistor for future digital devices, due to the absence of a bandgap between the conduction and valence bands. This makes it impossible to switch between on and off states with respect to electron flow. Scaling things down, graphene nanoribbons of less than 10 nm in width do exhibit electronic bandgaps and are therefore potential candidates for digital devices. Precise control over their dimensions, and hence electronic properties, however, represents a challenging goal, and the ribbons typically possess rough edges that are detrimental to their performance.\n\nCVD can be used to produce a synthetic diamond by creating the circumstances necessary for carbon atoms in a gas to settle on a substrate in crystalline form. CVD of diamonds has received much attention in the materials sciences because it allows many new applications that had previously been considered too expensive. CVD diamond growth typically occurs under low pressure (1–27 kPa; 0.145–3.926 psi; 7.5–203 Torr) and involves feeding varying amounts of gases into a chamber, energizing them and providing conditions for diamond growth on the substrate. The gases always include a carbon source, and typically include hydrogen as well, though the amounts used vary greatly depending on the type of diamond being grown. Energy sources include hot filament, microwave power, and arc discharges, among others. The energy source is intended to generate a plasma in which the gases are broken down and more complex chemistries occur. The actual chemical process for diamond growth is still under study and is complicated by the very wide variety of diamond growth processes used.\n\nUsing CVD, films of diamond can be grown over large areas of substrate with control over the properties of the diamond produced. In the past, when high pressure high temperature (HPHT) techniques were used to produce a diamond, the result was typically very small free standing diamonds of varying sizes. With CVD diamond growth areas of greater than fifteen centimeters (six inches) diameter have been achieved and much larger areas are likely to be successfully coated with diamond in the future. Improving this process is key to enabling several important applications.\n\nThe growth of diamond directly on a substrate allows the addition of many of diamond's important qualities to other materials. Since diamond has the highest thermal conductivity of any bulk material, layering diamond onto high heat producing electronics (such as optics and transistors) allows the diamond to be used as a heat sink. Diamond films are being grown on valve rings, cutting tools, and other objects that benefit from diamond's hardness and exceedingly low wear rate. In each case the diamond growth must be carefully done to achieve the necessary adhesion onto the substrate. Diamond's very high scratch resistance and thermal conductivity, combined with a lower coefficient of thermal expansion than Pyrex glass, a coefficient of friction close to that of Teflon (polytetrafluoroethylene) and strong lipophilicity would make it a nearly ideal non-stick coating for cookware if large substrate areas could be coated economically.\n\nCVD growth allows one to control the properties of the diamond produced. In the area of diamond growth, the word \"diamond\" is used as a description of any material primarily made up of sp3-bonded carbon, and there are many different types of diamond included in this. By regulating the processing parameters—especially the gases introduced, but also including the pressure the system is operated under, the temperature of the diamond, and the method of generating plasma—many different materials that can be considered diamond can be made. Single crystal diamond can be made containing various dopants. Polycrystalline diamond consisting of grain sizes from several nanometers to several micrometers can be grown. Some polycrystalline diamond grains are surrounded by thin, non-diamond carbon, while others are not. These different factors affect the diamond's hardness, smoothness, conductivity, optical properties and more.\n\nCommercially, mercury cadmium telluride is of continuing interest for detection of infrared radiation. Consisting of an alloy of CdTe and HgTe, this material can be prepared from the dimethyl derivatives of the respective elements.\n\n\n"}
{"id": "12850901", "url": "https://en.wikipedia.org/wiki?curid=12850901", "title": "Climate risk", "text": "Climate risk\n\nClimate risk means a risk resulting from climate change and affecting natural and human systems and regions.\n\nIn the course of increasing global temperature and extreme weather phenomena\nthe Intergovernmental Panel on Climate Change (IPCC) has been founded by the United Nations Environment Programme (UNEP) and the World Meteorological Organization (WMO) for a better understanding of climate change and meeting concerns of these observations. Its main aim is evaluating climate risks and exploring strategies for the prevention of these risks.\n\nAs per current projections of IPCC the following future effects have to be expected:\n\nWhile affecting all economic sectors, the effect on single continents will differ. Beside these direct physical climate risks there are also indirect derived ones:\n\nDirect risks of climate change are expected especially for branches, which strongly depend on natural resources like agriculture, fishing, forestry, health care, real estate and tourism. For example, storms and flooding damage buildings and infrastructure, whereas hot summers with less precipitation cause crop failure.\nThe governmental endeavours to reduce climate costs have direct effects on economy. For example, the targets regarding emissions within the Kyoto-Protocol shall be realised by implementing emissions trading. By this instrument the value of emissions can be quantified monetarily, approximating the value of avoiding hazardous substances. This value shall be internalized by companies and considered in investment decisions. By considering emission costs the prices for i.e. energy and transport can increase and therefore change consumer demand. The insecurity of legislation leads to indefinite adjournation of projects and investments.\n\nSimilar to the tobacco industry, industries producing excessive greenhouse gases are exposed to the risk of an increasing number of lawsuits, if damages can be traced back to emissions, i.e. for floodings, crop failure, etc.\n\nIf companies do not take measures to reduce climate risks they are competitively disadvantaged. This might lead to increasing production costs caused by obsolete technologies and therefore to decreasing profits.\n\nProduction shortfalls can result from direct or indirect climate risks. I.e. hurricanes damaging oil production facilities can lead to a scarcity of oil and increasing prices. Also the price for energy will rise, because heatwaves cause water scarcity and therefore the supply for cooling water of power plants becomes short.\n\nCompanies who are publicly criticized for their environmental policy or high emission rates, might lose customers, because of negative reputation. This risk is currently subordinate.\n\nBesides climate risks also opportunities can derive from climate change for some branches and innovative companies, i.e. for the automobile and renewable energy sectors. Especially energy-intensive sectors can reduce energy costs by using more efficient technologies, which necessarily have to be developed in near future.\n\n\n\n"}
{"id": "14194971", "url": "https://en.wikipedia.org/wiki?curid=14194971", "title": "Crushed stone", "text": "Crushed stone\n\nCrushed stone or angular rock is a form of construction aggregate, typically produced by mining a suitable rock deposit and breaking the removed rock down to the desired size using crushers. It is distinct from gravel which is produced by natural processes of weathering and erosion, and typically has a more rounded shape.\n\nAngular crushed stone is the key material for macadam road construction which depends on the interlocking of the individual stones' angular faces for its strength. Crushed natural stone is also used similarly without a binder for riprap, railroad track ballast, and filter stone. It may be used with a binder in a composite material such as concrete, tarmac, or asphalt concrete.\n\nCrushed stone is one of the most accessible natural resources, and is a major basic raw material used by construction, agriculture, and other industries. Despite the low value of its basic products, the crushed stone industry is a major contributor to and an indicator of the economic well-being of a nation.\nThe demand for crushed stone is determined mostly by the level of construction activity, and, therefore, the demand for construction materials.\n\nStone resources of the world are very large. High-purity limestone and dolomite suitable for specialty uses are limited in many geographic areas. Crushed stone substitutes for roadbuilding include sand and gravel, and slag. Substitutes for crushed stone used as construction aggregates include sand and gravel, iron and steel slag, sintered or expanded clay or shale, and perlite or vermiculite.\n\nCrushed stone is a high-volume, low-value commodity. The industry is highly competitive and is characterized by many operations serving local or regional markets. Production costs are determined mainly by the cost of labor, equipment, energy, and water, in addition to the costs of compliance with environmental and safety regulations. These costs vary depending on geographic location, the nature of the deposit, and the number and type of products produced. Crushed stone has one of the lowest average by weight values of all mineral commodities. The average unit price increased from US$1.58 per metric ton, f.o.b. plant, in 1970 to US$4.39 in 1990. However, the unit price in constant 1982 dollars fluctuated between US$3.48 and US$3.91 per metric ton for the same period. Increased productivity achieved through increased use of automation and more efficient equipment was mainly responsible for maintaining the prices at this level.\n\nTransportation is a major factor in the delivered price of crushed stone. The cost of moving crushed stone from the plant to the market often equals or exceeds the sale price of the product at the plant. Because of the high cost of transportation and the large quantities of bulk material that have to be shipped, crushed stone is usually marketed locally. The high cost of transportation is responsible for the wide dispersion of quarries, usually located near highly populated areas. However, increasing land values combined with local environmental concerns are moving crushed stone quarries farther from the end-use locations, increasing the price of delivered material. Economies of scale, which might be realized if fewer, larger operations served larger marketing areas, would probably not offset the increased transportation costs.\n\nAccording to the United States Geological Survey, 1.72 billion tonnes of crushed stone worth $13.8 billion was sold or used in 2006, of which 1.44 billion tonnes was used as construction aggregate, 74.9 million tonnes used for cement manufacture, and 18.1 million tonnes used to make lime. Crushed marble sold or used totaled 11.8 million tonnes, the majority of which was ground very fine and used as calcium carbonate.\n\nIn 2006, 9.40 million tonnes of crushed stone (almost all limestone or dolomite) was used for soil treatment, primarily to reduce soil acidity. Soils tend to become acidic from heavy use of nitrogen-containing fertilizers, unless a soil conditioner is used. Using aglime or agricultural lime, a finely-ground limestone or dolomite, to change the soil from acidic to nearly neutral particularly benefits crops by maximizing availability of plant nutrients, and also by reducing aluminum or manganese toxicity, promoting soil microbe activity, and improving the soil structure.\n\nIn 2006, 5.29 million tonnes of crushed stone (mostly limestone or dolomite) was used as a flux in blast furnaces and in certain steel furnaces to react with gangue minerals (i.e. silica and silicate impurities) to produce liquid slag that floats and can be poured off from the much denser molten metal (i.e., iron). The slag cools to become a stone-like material that is commonly crushed and recycled as construction aggregate.\n\nIn addition, 4.53 million tonnes of crushed stone was used for fillers and extenders (including asphalt fillers or extenders), 2.71 million tonnes for sulfur oxide removal-mine dusting-acid water treatment, and 1.45 million tonnes sold or used for poultry grit or mineral food.\n\nCrushed stone is recycled primarily as construction aggregate or concrete.\nCrushed stone or 'road metal' is used in landscape design and gardening for gardens, parks, and municipal and private projects as a mulch, walkway, path, and driveway pavement, and cell infill for modular permeable paving units. As a mineral mulch its benefits include erosion control, water conservation, weed suppression, and aesthetic qualities. It is often seen used in rock gardens and cactus gardens.\n\n\n"}
{"id": "53365898", "url": "https://en.wikipedia.org/wiki?curid=53365898", "title": "Earliest known life forms", "text": "Earliest known life forms\n\nThe earliest known life forms on Earth are putative fossilized microorganisms found in hydrothermal vent precipitates. The earliest time that life forms first appeared on Earth is unknown. They may have lived earlier than 3.77 billion years ago, possibly as early as 4.28 billion years ago, or nearly 4.5 billion years ago according to some; in any regards, not long after the oceans formed 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. The earliest \"direct\" evidence of life on Earth are microfossils of microorganisms permineralized in 3.465-billion-year-old Australian Apex chert rocks.\n\nA life form, or lifeform, is an entity or being that is living.\n\nCurrently, Earth remains the only place in the universe known to harbor life forms.\n\nMore than 99% of all species of life forms, amounting to over five billion species, that ever lived on Earth are estimated to be extinct.\n\nSome estimates on the number of Earth's current species of life forms range from 10 million to 14 million, of which about 1.2 million have been documented and over 86 percent have not yet been described. However, a May 2016 scientific report estimates that 1 trillion species are currently on Earth, with only one-thousandth of one percent described. The total number of DNA base pairs on Earth is estimated at 5.0 x 10 with a weight of 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 trillion tons of carbon. In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.\n\nThe Earth's biosphere includes soil, hydrothermal vents, and rock up to or deeper underground, the deepest parts of the ocean, and at least high into the atmosphere. Under certain test conditions, life forms have been observed to thrive in the near-weightlessness of space and to survive in the vacuum of outer space. Life forms appear to thrive in the Mariana Trench, the deepest spot in the Earth's oceans, reaching a depth of . Other researchers reported related studies that life forms thrive inside rocks up to below the sea floor under of ocean, off the coast of the northwestern United States, as well as beneath the seabed off Japan. In August 2014, scientists confirmed the existence of life forms living below the ice of Antarctica.\n\nAccording to one researcher, \"You can find microbes everywhere — [they are] extremely adaptable to conditions, and survive wherever they are.\"\n\nFossil evidence informs most studies of the origin of life. The age of the Earth is about 4.54 billion years; the earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago. There is evidence that life began much earlier.\n\nIn 2017, fossilized microorganisms, or microfossils, were announced to have been discovered in hydrothermal vent precipitates in the Nuvvuagittuq Belt of Quebec, Canada that may be as old as 4.28 billion years old, the oldest record of life on Earth, suggesting \"an almost instantaneous emergence of life\" (in a geological time-scale sense), after ocean formation 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. Nonetheless, life may have started even earlier, at nearly 4.5 billion years ago, as claimed by some researchers.\n\n\"Remains of life\" have been found in 4.1 billion-year-old rocks in Western Australia.\n\nEvidence of biogenic graphite, and possibly stromatolites, were discovered in 3.7 billion-year-old metasedimentary rocks in southwestern Greenland.\n\nIn May 2017, evidence of life on land may have been found in 3.48 billion-year-old geyserite which is often found around hot springs and geysers, and other related mineral deposits, uncovered in the Pilbara Craton of Western Australia. This complements the November 2013 publication that microbial mat fossils had been found in 3.48 billion-year-old sandstone in Western Australia.\n\nIn November 2017, a study by the University of Edinburgh suggested that life on Earth may have originated from biological particles carried by streams of space dust.\n\nA December 2017 report stated that 3.465-billion-year-old Australian Apex chert rocks once contained microorganisms, the earliest \"direct\" evidence of life on Earth.\n\nIn January 2018, a study found that 4.5 billion-year-old meteorites found on Earth contained liquid water along with prebiotic complex organic substances that may be ingredients for life.\n\nAccording to biologist Stephen Blair Hedges, \"If life arose relatively quickly on Earth … then it could be common in the universe.\"\n\n"}
{"id": "4396171", "url": "https://en.wikipedia.org/wiki?curid=4396171", "title": "Earth's rotation", "text": "Earth's rotation\n\nEarth's rotation is the rotation of Planet Earth around its own axis. Earth rotates eastward, in prograde motion. As viewed from the north pole star Polaris, Earth turns counter clockwise.\n\nThe North Pole, also known as the Geographic North Pole or Terrestrial North Pole, is the point in the Northern Hemisphere where Earth's axis of rotation meets its surface. This point is distinct from Earth's North Magnetic Pole. The South Pole is the other point where Earth's axis of rotation intersects its surface, in Antarctica.\n\nEarth rotates once in about 24 hours with respect to the Sun, but once every 23 hours, 56 minutes, and 4 seconds with respect to the stars (see below). Earth's rotation is slowing slightly with time; thus, a day was shorter in the past. This is due to the tidal effects the Moon has on Earth's rotation. Atomic clocks show that a modern day is longer by about 1.7 milliseconds than a century ago, slowly increasing the rate at which UTC is adjusted by leap seconds. Analysis of historical astronomical records shows a slowing trend of about 2.3 milliseconds per century since the 8th century BCE.\n\nAmong the ancient Greeks, several of the Pythagorean school believed in the rotation of the earth rather than the apparent diurnal rotation of the heavens. Perhaps the first was Philolaus (470–385 BCE), though his system was complicated, including a counter-earth rotating daily about a central fire.\n\nA more conventional picture was that supported by Hicetas, Heraclides and Ecphantus in the fourth century BCE who assumed that the earth rotated but did not suggest that the earth revolved about the sun. In the third century BCE, Aristarchus of Samos suggested the sun's central place.\n\nHowever, Aristotle in the fourth century BCE criticized the ideas of Philolaus as being based on theory rather than observation. He established the idea of a sphere of fixed stars that rotated about the earth. This was accepted by most of those who came after, in particular Claudius Ptolemy (2nd century CE), who thought the earth would be devastated by gales if it rotated.\n\nIn 499 CE, the Indian astronomer Aryabhata wrote that the spherical earth rotates about its axis daily, and that the apparent movement of the stars is a relative motion caused by the rotation of the Earth. He provided the following analogy: \"Just as a man in a boat going in one direction sees the stationary things on the bank as moving in the opposite direction, in the same way to a man at Lanka the fixed stars appear to be going westward.\"\n\nIn the 10th century, some Muslim astronomers accepted that the Earth rotates around its axis. According to al-Biruni, Abu Sa'id al-Sijzi (d. circa 1020) invented an astrolabe called \"al-zūraqī\" based on the idea believed by some of his contemporaries \"that the motion we see is due to the Earth's movement and not to that of the sky.\" The prevalence of this view is further confirmed by a reference from the 13th century which states: \"According to the geometers [or engineers] (\"muhandisīn\"), the earth is in constant circular motion, and what appears to be the motion of the heavens is actually due to the motion of the earth and not the stars.\" Treatises were written to discuss its possibility, either as refutations or expressing doubts about Ptolemy's arguments against it. At the Maragha and Samarkand observatories, the Earth's rotation was discussed by Tusi (b. 1201) and Qushji (b. 1403); the arguments and evidence they used resemble those used by Copernicus.\n\nIn medieval Europe, Thomas Aquinas accepted Aristotle's view and so, reluctantly, did John Buridan and Nicole Oresme in the fourteenth century. Not until Nicolaus Copernicus in 1543 adopted a heliocentric world system did the contemporary understanding of earth's rotation begin to be established. Copernicus pointed out that if the movement of the earth is violent, then the movement of the stars must be very much more so. He acknowledged the contribution of the Pythagoreans and pointed to examples of relative motion. For Copernicus this was the first step in establishing the simpler pattern of planets circling a central sun.\n\nTycho Brahe, who produced accurate observations on which Kepler based his laws, used Copernicus's work as the basis of a system assuming a stationary earth. In 1600, William Gilbert strongly supported the earth's rotation in his treatise on the earth's magnetism and thereby influenced many of his contemporaries. Those like Gilbert who did not openly support or reject the motion of the earth about the sun are often called \"semi-Copernicans\". A century after Copernicus, Riccioli disputed the model of a rotating earth due to the lack of then-observable eastward deflections in falling bodies; such deflections would later be called the Coriolis effect. However, the contributions of Kepler, Galileo and Newton gathered support for the theory of the rotation of the Earth.\n\nEarth's rotation implies that the Equator bulges and the geographical poles are flattened. In his \"Principia\", Newton predicted this flattening would occur in the ratio of 1:230, and pointed to the pendulum measurements taken by Richer in 1673 as corroboration of the change in gravity, but initial measurements of meridian lengths by Picard and Cassini at the end of the 17th century suggested the opposite. However, measurements by Maupertuis and the French Geodesic Mission in the 1730s established the oblateness of Earth, thus confirming the positions of both Newton and Copernicus.\n\nIn the Earth's rotating frame of reference, a freely moving body follows an apparent path that deviates from the one it would follow in a fixed frame of reference. Because of the Coriolis effect, falling bodies veer slightly eastward from the vertical plumb line below their point of release, and projectiles veer right in the Northern Hemisphere (and left in the Southern) from the direction in which they are shot. The Coriolis effect is mainly observable at a meteorological scale, where it is responsible for the opposite directions of cyclone rotation in the Northern and Southern hemispheres (anticlockwise and clockwise, respectively).\n\nHooke, following a suggestion from Newton in 1679, tried unsuccessfully to verify the predicted eastward deviation of a body dropped from a height of , but definitive results were obtained later, in the late 18th and early 19th century, by Giovanni Battista Guglielmini in Bologna, Johann Friedrich Benzenberg in Hamburg and Ferdinand Reich in Freiberg, using taller towers and carefully released weights. A ball dropped from a height of departed by from the vertical compared with a calculated value of .\n\nThe most celebrated test of Earth's rotation is the Foucault pendulum first built by physicist Léon Foucault in 1851, which consisted of a lead-filled brass sphere suspended from the top of the Panthéon in Paris. Because of the Earth's rotation under the swinging pendulum, the pendulum's plane of oscillation appears to rotate at a rate depending on latitude. At the latitude of Paris the predicted and observed shift was about clockwise per hour. Foucault pendulums now swing in museums around the world.\n\nEarth's rotation period relative to the Sun (solar noon to solar noon) is its \"true solar day\" or \"apparent solar day\". It depends on the Earth's orbital motion and is thus affected by changes in the eccentricity and inclination of Earth's orbit. Both vary over thousands of years, so the annual variation of the true solar day also varies. Generally, it is longer than the mean solar day during two periods of the year and shorter during another two. The true solar day tends to be longer near perihelion when the Sun apparently moves along the ecliptic through a greater angle than usual, taking about longer to do so. Conversely, it is about shorter near aphelion. It is about longer near a solstice when the projection of the Sun's apparent motion along the ecliptic onto the celestial equator causes the Sun to move through a greater angle than usual. Conversely, near an equinox the projection onto the equator is shorter by about . Currently, the perihelion and solstice effects combine to lengthen the true solar day near by solar seconds, but the solstice effect is partially cancelled by the aphelion effect near when it is only longer. The effects of the equinoxes shorten it near and by and , respectively.\n\nThe average of the true solar day during the course of an entire year is the \"mean solar day\", which contains solar seconds. Currently, each of these seconds is slightly longer than an SI second because Earth's mean solar day is now slightly longer than it was during the 19th century due to tidal friction. The average length of the mean solar day since the introduction of the leap second in 1972 has been about 0 to 2 ms longer than 86,400 SI seconds. Random fluctuations due to core-mantle coupling have an amplitude of about 5 ms. The mean solar second between 1750 and 1892 was chosen in 1895 by Simon Newcomb as the independent unit of time in his Tables of the Sun. These tables were used to calculate the world's ephemerides between 1900 and 1983, so this second became known as the ephemeris second. In 1967 the SI second was made equal to the ephemeris second.\n\nThe apparent solar time is a measure of the Earth's rotation and the difference between it and the mean solar time is known as the equation of time.\n\nEarth's rotation period relative to the fixed stars, called its \"stellar day\" by the International Earth Rotation and Reference Systems Service (IERS), is seconds of mean solar time (UT1) , mean solar days). Earth's rotation period relative to the precessing mean vernal equinox, named \"sidereal day\", is seconds of mean solar time (UT1) , mean solar days). Thus, the sidereal day is shorter than the stellar day by about .\n\nBoth the stellar day and the sidereal day are shorter than the mean solar day by about . The mean solar day in SI seconds is available from the IERS for the periods and .\n\nRecently (1999–2010) the average annual length of the mean solar day in excess of 86,400 SI seconds has varied between and , which must be added to both the stellar and sidereal days given in mean solar time above to obtain their lengths in SI seconds (see Fluctuations in the length of day).\n\nThe angular speed of Earth's rotation in inertial space is radians per SI second (mean solar second). Multiplying by (180°/π radians) × (86,400 seconds/mean solar day) yields 360.9856°/mean solar day, indicating that Earth rotates more than 360° relative to the fixed stars in one solar day. Earth's movement along its nearly circular orbit while it is rotating once around its axis requires that Earth rotate slightly more than once relative to the fixed stars before the mean Sun can pass overhead again, even though it rotates only once (360°) relative to the mean Sun. Multiplying the value in rad/s by Earth's equatorial radius of (WGS84 ellipsoid) (factors of 2π radians needed by both cancel) yields an equatorial speed of , or . Some sources state that Earth's equatorial speed is slightly less, or . This is obtained by dividing Earth's equatorial circumference by . However, the use of only one circumference unwittingly implies only one rotation in inertial space, so the corresponding time unit must be a sidereal hour. This is confirmed by multiplying by the number of sidereal days in one mean solar day, , which yields the equatorial speed in mean solar hours given above of .\n\nThe tangential speed of Earth's rotation at a point on Earth can be approximated by multiplying the speed at the equator by the cosine of the latitude. For example, the Kennedy Space Center is located at latitude 28.59° N, which yields a speed of: cos 28.59° × = \n\nThe Earth's rotation axis moves with respect to the fixed stars (inertial space); the components of this motion are precession and nutation. It also moves with respect to the Earth's crust; this is called polar motion.\n\nPrecession is a rotation of the Earth's rotation axis, caused primarily by external torques from the gravity of the Sun, Moon and other bodies. The polar motion is primarily due to free core nutation and the Chandler wobble.\n\nOver millions of years, the Earth's rotation slowed significantly by tidal acceleration through gravitational interactions with the Moon. In this process, angular momentum is slowly transferred to the Moon at a rate proportional to formula_1, where formula_2 is the orbital radius of the Moon. This process gradually increased the length of day to its current value and resulted in the Moon being tidally locked with the Earth.\n\nThis gradual rotational deceleration is empirically documented with estimates of day lengths obtained from observations of tidal rhythmites and stromatolites; a compilation of these measurements found the length of day to increase steadily from about 21 hours at 600Myr ago to the current 24 hour value. By counting the microscopic lamina that form at higher tides, tidal frequencies (and thus day lengths) can be estimated, much like counting tree rings, though these estimates can be increasingly unreliable at older ages.\n\nThe current rate of tidal deceleration is anomalously high, implying the Earth's rotational velocity must have decreased more slowly in the past. Empirical data tentatively shows a sharp increase in rotational deceleration about 600Myr ago. Some models suggest that the Earth maintained a constant day length of 21 hours throughout much of the Precambrian. This day length corresponds to the semidiurnal resonant period of the thermally-driven atmospheric tide; at this day length, the decelerative lunar torque could have been canceled by an accelerative torque from the atmospheric tide, resulting in no net torque and a constant rotational period. This stabilizing effect could have been broken by a sudden change in global temperature. Recent computational simulations support this hypothesis and suggest the Marinoan or Sturtian glaciations broke this stable configuration about 600Myr ago, citing the resemblance of simulated results and existing paleorotational data.\n\nAdditionally, some large-scale events, such as the 2004 Indian Ocean earthquake, have caused the length of a day to shorten by 3 microseconds by affecting the Earth's moment of inertia. Post-glacial rebound, ongoing since the last Ice age, is also changing the distribution of the Earth's mass thus affecting the moment of inertia of the Earth and, by the conservation of angular momentum, the Earth's rotation period.\n\nThe primary monitoring of the Earth's rotation is performed with very-long-baseline interferometry coordinated with the Global Positioning System, satellite laser ranging, and other satellite techniques. This provides an absolute reference for the determination of universal time, precession, and nutation.\n\nThere are recorded observations of solar and lunar eclipses by Babylonian and Chinese astronomers beginning in the 8th century BCE, as well as from the medieval Islamic world and elsewhere. These observations can be used to determine changes in the Earth's rotation over the last 27 centuries, since the length of the day is a critical parameter in the calculation of the place and time of eclipses. A change in day length of milliseconds per century shows up as a change of hours and thousands of kilometers in eclipse observations. The ancient data are consistent with a shorter day, meaning the Earth was turning faster throughout the past.\n\nThe Earth's original rotation was a vestige of the original angular momentum of the cloud of dust, rocks, and gas that coalesced to form the Solar System. This primordial cloud was composed of hydrogen and helium produced in the Big Bang, as well as heavier elements ejected by supernovas. As this interstellar dust is heterogeneous, any asymmetry during gravitational accretion resulted in the angular momentum of the eventual planet.\n\nHowever, if the giant-impact hypothesis for the origin of the Moon is correct, this primordial rotation rate would have been reset by the Theia impact 4.5 billion years ago. Regardless of the speed and tilt of the Earth's rotation before the impact, it would have experienced a day some five hours long after the impact. Tidal effects would then have slowed this rate to its modern value.\n\n"}
{"id": "31450053", "url": "https://en.wikipedia.org/wiki?curid=31450053", "title": "Energy accidents", "text": "Energy accidents\n\nEnergy resources bring with them great social and economic promise, providing financial growth for communities and energy services for local economies. However, the infrastructure which delivers energy services can break down in an energy accident, sometimes causing much damage, and energy fatalities can occur, and with many systems often deaths will happen even when the systems are working as intended.\n\nHistorically, coal mining has been the most dangerous energy activity and the list of historical coal mining disasters is a long one. Underground mining hazards include suffocation, gas poisoning, roof collapse and gas explosions. Open cut mining hazards are principally mine wall failures and vehicle collisions. In the US alone, more than 100,000 coal miners have been killed in accidents over the past century, with more than 3,200 dying in 1907 alone.\n\nAccording to Benjamin K. Sovacool, 279 \"major\" energy accidents occurred from 1907 to 2007 and they caused 182,156 deaths with $41 billion in property damages, with these figures not including deaths from smaller accidents.\n\nHowever, by far the greatest energy fatalities that result from energy generation by humanity, is the creation of air pollution. The most lethal of which, particulate matter, which is primarily generated from the burning of fossil fuels and biomass is (counting outdoor air pollution effects only) estimated to cause 2.1 million deaths annually.\n\nAccording to Benjamin K. Sovacool, while responsible for less than 1 percent of the total number of energy accidents, hydroelectric facilities claimed 94 percent of reported immediate fatalities. Results on immediate fatalities are dominated by one disaster in which Typhoon Nina in 1975 washed out the Shimantan Dam (Henan Province, China) and 171,000 people perished. While the other major accident that involved greater than 1000 immediate deaths followed the rupture of the NNPC petroleum pipeline in 1998 and the resulting explosion. The other singular accident described by Sovacool is the \"predicted\" latent death toll of greater than 1000, as a result of the 1986 steam explosion at the Chernobyl nuclear reactor in the Ukraine. With approximately 4000 deaths in total, to eventually result in the decades ahead due to the radio-isotope pollution released.\n\nIn the oil and gas industry, the need for improved safety culture and training within companies is evidenced by the finding that workers new to a company are more likely to be involved in fatalities.\n\nCoal mining accidents resulted in 5,938 immediate deaths in 2005, and 4746 immediate deaths in 2006 in China alone according to the World Wildlife Fund. Coal mining is the most dangerous occupation in China, the death rate for every 100 tons of coal mined is 100 times that of the death rate in the US and 30 times that achieved in South Africa. Moreover, 600,000 Chinese coal miners, as of 2004, were suffering from Coalworker's pneumoconiosis (known as \"black lung\") a disease of the lungs caused by long-continued inhalation of coal dust. And the figure increases by 70,000 miners every year in China.\n\nHistorically, coal mining has been a very dangerous activity and the list of historical coal mining disasters is a long one. In the US alone, more than 100,000 coal miners were killed in accidents over the past century, with more than 3,200 dying in 1907 alone. In the decades following this peak, an annual death toll of 1,500 miner fatalities occurred every year in the US until approximately the 1970s. Coal mining fatalities in the US between 1990 and 2012 have continued to decline, with fewer than 100 each year. (See more Coal mining disasters in the United States)\n\nIn the United States, in the 2000s, after three decades of regulation on the Environmental impact of the coal industry, including regulations in the 1970s and 1990s from the Clean Air Act, an act created to cut down on pollution related deaths from fossil fuel usage, US coal fired power plants were estimated, in the 2000s, to continue to cause between 10,000 and 30,000 latent, or air pollution related deaths per year, due to the emissions of sulfur dioxide, nitrogen oxides and directly emitted particulate matter that result when coal is burnt.\n\nAccording to the World Health Organization in 2012, urban outdoor air pollution, from the burning of fossil fuels and biomass is estimated to cause 3 million deaths worldwide per year and indoor air pollution from biomass and fossil fuel burning is estimated to cause approximately 4.3 million premature deaths. In 2013 a team of researchers estimated the number of premature deaths caused by particulate matter in outdoor air pollution as 2.1 million, occurring annually.\n\nBenjamin Sovacool says that while hydroelectric plants were responsible for the most fatalities, nuclear power plants rank first in terms of their economic cost, accounting for 41 percent of all property damage. Oil and hydroelectric follow at around 25 percent each, followed by natural gas at 9 percent and coal at 2 percent. Excluding Chernobyl and the Shimantan Dam, the three other most expensive accidents involved the Exxon Valdez oil spill (Alaska), The Prestige oil spill (Spain), and the Three Mile Island nuclear accident (Pennsylvania). However analysis presented in the international Journal, \"Human and Ecological Risk Assessment\" found that coal, oil, Liquid petroleum gas and hydro accidents have cost more than nuclear power accidents.\n\nModern-day U.S. regulatory agencies frequently implement regulations on conventional pollution if one life or more is predicted saved per $6 million to $8 million of economic costs incurred.\n\n\n\n"}
{"id": "33442335", "url": "https://en.wikipedia.org/wiki?curid=33442335", "title": "Erminio Sipari", "text": "Erminio Sipari\n\nErminio Sipari (1 December 1879 – 28 January 1968) was an Italian politician and naturalist, author of studies on the preservation of nature and founder of Parco Nazionale d'Abruzzo, which he chaired from 1922 to 1933.\n\nSipari was born in Alvito, Lazio. His family, well known in southern Italy, had many properties in Terra di Lavoro, Abruzzo and Apulia. His father was the brother of Luisa Sipari, the mother of Benedetto Croce, and his mother, Cristina Cappelli, came from a noble family. Erminio studied in Rome and graduated in civil engineering at the University of Turin, after which he specialized in electrical engineering in Liège. Back in Italy in 1905, he established his own engineering firm, based in Rome and Pescasseroli.\n\nIn 1913, when Sipari was first elected in the Italian Parliament, he took the opportunity to draw attention to the real threat of extinction of important species, such as the Abruzzo chamois and the Marsican brown bear, and proposed the creation of a national park in the area of Marsica. In 1921 he was appointed as Undersecretary of State to the Italian Navy. In the same year, he founded the \"Ente Autonomo of the Parco Nazionale d'Abruzzo\", who leased 5 square kilometres of land to be protected in the district of Opi. In September 1922 Sipari opened the Park in Pescasseroli. After, the park was recognized by Italian law (January 1923).\n\nAs its president, Sipari argued that the Abruzzo National Park must protect fauna and flora, but also allow the birth of tourism. He was able to create and manage a park based on sustainable development. He wrote numerous articles on the birth of the National Park and also a comprehensive report, called \"Relazione Sipari\" (1926), which at the time was considered the main promoter of nature conservation in Italy.\n\n"}
{"id": "3230875", "url": "https://en.wikipedia.org/wiki?curid=3230875", "title": "Evolutionary Principle", "text": "Evolutionary Principle\n\nThe Evolutionary Principle is a largely psychological doctrine which roughly states that when a species is removed from the habitat in which it evolved, or that habitat changes significantly within a brief period (evolutionarily speaking), the species will develop maladaptive or outright pathological behavior. The Evolutionary Principle is important in neo-tribalist and anarcho-primitivist thinking.\n"}
{"id": "4329772", "url": "https://en.wikipedia.org/wiki?curid=4329772", "title": "Exertion", "text": "Exertion\n\nExertion is the physical or perceived use of energy. Exertion traditionally connotes a strenuous or costly \"effort,\"resulting in generation of force, initiation of motion, or in the performance of work. It often relates to muscularactivity and can be quantified, empirically and by measurable metabolic response.\n\nIn physics, \"exertion\" is the expenditure of energy against, or inductive of, inertia as described by Isaac Newton's third law of motion. In physics, force exerted equivocates work done. The ability to do work can be either positive or negative depending on the direction of exertion relative to gravity. For example, a force exerted upwards, like lifting an object, creates positive work done on that object.\n\nExertion often results in force generated, a contributing dynamic of general motion. In mechanics it describes the use of force against a body in the direction of its motion (see vector).\n\nExertion, physiologically, can be described by the initiation of exercise, or, intensive and exhaustive physical activity that causes cardiovascular stress or a sympathetic nervous response. This can be continuous or intermittent exertion.\n\nExertion requires, of the body, modified oxygen uptake, increased heart rate, and autonomic monitoring of blood lactate concentrations. Mediators of physical exertion include cardio-respiratory and musculoskeletal strength, as well as metabolic capability. This often correlates to an output of force followed by a refractory period of recovery. Exertion is limited by cumulative load and repetitive motions.\n\nMuscular energy reserves, or stores for biomechanical exertion, stem from metabolic, immediate production of ATP and increased O2 consumption. Muscular exertion generated depends on the muscle length and the velocity at which it is able to shorten, or contract.\n\nPerceived exertion can be explained as subjective, perceived experience that mediates response to somatic sensations and mechanisms. A rating of perceived exertion, as measured by the \"RPE-scale\", or Borg scale, is a quantitative measure of physical exertion.\n\nOften in health, exertion of oneself resulting in cardiovascular stress showed reduced physiological responses, like cortisol levels and mood, to stressors. Therefore, biological exertion is effective in mediating psychological exertion, responsive to environmental stress.\n\nOverexertion causes more than 3.5 million injuries a year. An overexertion injury can include sprains or strains, the stretching and tear of ligaments, tendons, or muscles caused by a load that exceeds the human ability to perform the work. Overexertion, besides causing acute injury, implies physical exertion beyond the person's capacity which leads to symptoms such as dizziness, irregular breathing and heart rate, and fatigue. Preventative measures can be taken based on biomechanical knowledge to limit possible overexertion injuries.\n\n\n"}
{"id": "12393", "url": "https://en.wikipedia.org/wiki?curid=12393", "title": "Gaia philosophy", "text": "Gaia philosophy\n\nGaia philosophy (named after Gaia, Greek goddess of the Earth) is a broadly inclusive term for related concepts that living organisms on a planet will affect the nature of their environment in order to make the environment more suitable for life. This set of theories holds that all organisms on a life-giving planet regulate the biosphere in such a way as to promote its habitability. Gaia concept draws a connection between the survivability of a species (hence its evolutionary course) and its usefulness to the survival of other species.\n\nWhile there were a number of precursors to Gaia theory, the first scientific form of this idea was proposed as the Gaia hypothesis by James Lovelock, a UK chemist, in 1970. The Gaia hypothesis deals with the concept of biological homeostasis, and claims the resident life forms of a host planet coupled with their environment have acted and act like a single, self-regulating system. This system includes the near-surface rocks, the soil, and the atmosphere. Today many scientists consider such ideas to be unsupported by, or at odds with, the available evidence (see recent criticism). These theories are however significant in green politics.\n\nThere are some mystical, scientific and religious predecessors to the Gaia philosophy, which had a Gaia-like conceptual basis. Many religious mythologies had a view of Earth as being a whole that is greater than the sum of its parts (e.g. some Native American religions and various forms of shamanism).\n\nLewis Thomas believed that Earth should be viewed as a single cell; he derived this view from Johannes Kepler's view of Earth as a single round organism.\n\nIsaac Newton wrote of the earth, \"Thus this Earth resembles a great animall or rather inanimate vegetable, draws in æthereall breath for its dayly refreshment & vitall ferment & transpires again with gross exhalations, And according to the condition of all other things living ought to have its times of beginning youth old age & perishing.\"\n\nPierre Teilhard de Chardin, a paleontologist and geologist, believed that evolution unfolded from cell to organism to planet to solar system and ultimately the whole universe, as we humans see it from our limited perspective. Teilhard later influenced Thomas Berry and many Catholic humanist thinkers of the 20th century.\n\nBuckminster Fuller is generally credited with making the idea respectable in Western scientific circles in the 20th century. Building to some degree on his observations and artifacts, e.g. the Dymaxion map of the Earth he created, others began to ask if there was a way to make the Gaia theory scientifically sound.\n\nOberon Zell-Ravenheart in 1970 in an article in \"Green Egg\" Magazine, independently articulated the Gaia Thesis.\n\nNone of these ideas are considered scientific hypotheses; by definition a scientific hypothesis must make testable predictions. As the above claims are not testable, they are outside the bounds of current science.\n\nThese are conjectures and perhaps can only be considered as social and maybe political philosophy; they may have implications for theology, or \"thealogy\" as Zell-Ravenheart and Isaac Bonewits put it.\n\nAccording to James Kirchner there is a spectrum of Gaia hypotheses, ranging from the undeniable to radical. At one end is the undeniable statement that the organisms on the Earth have radically altered its composition. A stronger position is that the Earth's biosphere effectively acts as if it is a self-organizing system which works in such a way as to keep its systems in some kind of equilibrium that is conducive to life. Today many scientists consider that such a view (and any stronger views) are unlikely to be correct. An even stronger claim is that all lifeforms are part of a single planetary being, called Gaia. In this view, the atmosphere, the seas, the terrestrial crust would be the result of interventions carried out by Gaia, through the coevolving diversity of living organisms.\n\nThe most extreme form of Gaia theory is that the entire Earth is a single unified organism with a highly intelligent mind that arose as an emergent property of the whole biosphere. In this view, the Earth's biosphere is \"consciously\" manipulating the climate in order to make conditions more conducive to life. Scientists contend that there is no evidence at all to support this last point of view, and it has come about because many people do not understand the concept of homeostasis. Many non-scientists instinctively and incorrectly see homeostasis as a process that requires conscious control .\n\nThe more speculative versions of Gaia, including versions in which it is believed that the Earth is actually conscious, sentient, and highly intelligent, are usually considered outside the bounds of what is usually considered science.\n\nBuckminster Fuller has been credited as the first to incorporate scientific ideas into a Gaia theory, which he did with his Dymaxion map of the Earth.\n\nThe first scientifically rigorous theory was the Gaia hypothesis by James Lovelock, a UK chemist.\n\nA variant of this hypothesis was developed by Lynn Margulis, a microbiologist, in 1979.\nHer version is sometimes called the \"Gaia Theory\" (note uppercase-T). Her model is more limited in scope than the one that Lovelock proposed.\n\nWhether this sort of system is present on Earth is still open to debate. Some relatively simple homeostatic mechanisms are generally accepted. For example, when atmospheric carbon dioxide levels rise, plants are able to grow better and thus remove more carbon dioxide from the atmosphere. Other biological effects and feedbacks exist, but the extent to which these mechanisms have stabilized and modified the Earth's overall climate is largely not known.\n\nThe Gaia hypothesis is sometimes viewed from significantly different philosophical perspectives. Some environmentalists view it as an almost conscious process, in which the Earth's ecosystem is literally viewed as a single unified organism. Some evolutionary biologists, on the other hand, view it as an undirected emergent property of the ecosystem: as each individual species pursues its own self-interest, their combined actions tend to have counterbalancing effects on environmental change. Proponents of this view sometimes point to examples of life's actions in the past that have resulted in dramatic change rather than stable equilibrium, such as the conversion of the Earth's atmosphere from a reducing environment to an oxygen-rich one.\n\nDepending on how strongly the case is stated, the hypothesis conflicts with mainstream neo-Darwinism. Most biologists would accept Daisyworld-style homeostasis as possible, but would certainly not accept the idea that this equates to the whole biosphere acting as one organism.\n\nA very small number of scientists, and a much larger number of environmental activists, claim that Earth's biosphere is \"consciously\" manipulating the climate in order to make conditions more conducive to life. Scientists contend that there is no evidence to support this belief.\n\nA social science view of Gaia theory is the role of humans as a keystone species who may be able to accomplish global homeostasis. Whilst a few social scientists who draw inspiration from 'organic' views of society have embraced Gaia philosophy as a way to explain the human-nature interconnections, most professional social scientists are more involved in reflecting upon the way Gaia philosophy is used and engaged with within sub-sections of society. Alan Marshall, in the Department of Social Sciences at Mahidol University, for example, reflects upon the way Gaia philosophy has been used and advocated by environmentalists, spiritualists, managers, economists, and scientists and engineers (see The Unity of Nature, 2002, Imperial College Press: London and Singapore). Social Scientists themselves in the 1960s gave up on systems ideas of society since they were interpreted as supporting conservatism and traditionalism.\n\nSome radical political environmentalists who accept some form of the Gaia theory call themselves Gaians. They actively seek to restore the Earth's homeostasis — whenever they see it out of balance, e.g. to prevent manmade climate change, primate extinction, or rainforest loss. In effect, they seek to cooperate to become the \"system consciously manipulating to make conditions more conducive to life\". Such activity defines the homeostasis, but for leverage it relies on deep investigation of the homeorhetic balances, if only to find places to intervene in a system which is changing in undesirable ways.\n\nTony Bondhus brings up the point in his book, \"Society of Conceivia\", that if Gaia is alive, then societies are living things as well. This suggests that our understanding of Gaia can be used to create a better society and to design a better political system.\n\nOther intellectuals in the environmental movement, like Edward Goldsmith, have used Gaia in the completely opposite way; to stake a claim about how Gaia's focus on natural balance and resistance and resilience, should be emulated to design a conservative political system (as explored in Alan Marshall's 2002 book \"The Unity of Nature\", (Imperial College Press: London).\n\nGaians do not passively ask \"what is going on\", but rather, \"what to do next\", e.g. in terraforming or climate engineering or even on a small scale, such as gardening. Changes can be planned, agreed upon by many people, being very deliberate, as in urban ecology and especially industrial ecology. \"See arcology for more on this 'active' view.\"\n\nGaians argue that it is a human duty to act as such - committing themselves in particular to the Precautionary Principle. Such views began to influence the Green Parties, Greenpeace, and a few more radical wings of the environmental movement such as the Gaia Liberation Front and the Earth Liberation Front. These views dominate some such groups, e.g. the Bioneers. Some refer to this political activity as a separate and radical branch of the ecology movement, one that takes the axioms of the science of ecology in general, and Gaia theory in particular, and raises them to a kind of theory of personal conduct or moral code.\n\nThe ecologist and theologian Anne Primavesi is the author of two books dealing with the Gaia hypothesis and theology.\n\nRosemary Radford Ruether, the American feminist scholar and theologian, wrote a book called \"Gaia and God: An Ecofeminist Theology of Earth Healing\".\n\nA book edited by Allan Hunt Badiner called Dharma Gaia explores the ground where Buddhism and ecology meet through writings by the Dalai Lama, Gary Snyder, Thich Nhat Hanh, Allen Ginsberg, Joanna Macy, Robert Aitken, and 25 other Buddhists and ecologists.\n\nMany new age authors have written books which mix New Age teachings with Gaia philosophy. This is known as New Age Gaian. Often referred to as Gaianism, or the Gaian Religion, this spiritual aspect of the philosophy is very broad and inclusive, making it adaptable to other religions: Taoism, Neo-Paganism, Pantheism, Judeo-Christian Religions, and many others.\n\nThe question of \"what is an organism\", and at what scale is it rational to speak about organisms vs. biospheres, gives rise to a semantic debate. We are all ecologies in the sense that our (human) bodies contain gut bacteria, parasite species, etc., and to them our body is not organism but rather more of a microclimate or biome. Applying that thinking to whole planets:\n\nThe argument is that these symbiotic organisms, being unable to survive apart from each other and their climate and local conditions, form an organism in their own right, under a wider conception of the term organism than is conventionally used. It is a matter for often heated debate whether this is a valid usage of the term, but ultimately it appears to be a semantic dispute. In this sense of the word organism, it is argued under the theory that the entire biomass of the Earth is a single organism (as Johannes Kepler thought).\n\nUnfortunately, many supporters of the various Gaia theories do not state exactly where they sit on this spectrum; this makes discussion and criticism difficult.\n\nMuch effort on behalf of those analyzing the theory currently is an attempt to clarify what these different hypotheses are, and whether they are proposals to 'test' or 'manipulate' outcomes. Both Lovelock's and Margulis's understanding of Gaia are considered scientific hypotheses, and like all scientific theories are constantly put to the test.\n\nMore speculative versions of Gaia, including all versions in which it is held that the Earth is actually conscious, are currently held to be outside the bounds of science, and are not supported by either Lovelock or Margulis.\n\nOne of the most problematic issues with referring to Gaia as an organism is its apparent failure to meet the biological criterion of being able to reproduce. Richard Dawkins has asserted that the planet is not the offspring of any parents and is unable to reproduce.\n\n\n\n"}
{"id": "48107455", "url": "https://en.wikipedia.org/wiki?curid=48107455", "title": "George-ericksenite", "text": "George-ericksenite\n\nGeorge-ericksenite is a mineral with the chemical formula NaCaMg(IO)(CrO)(HO). It is vitreous, pale yellow to bright lemon yellow, brittle, and features a prismatic to acicular crystal habit along [001] and somewhat flattened crystal habit on {110}. It was first encountered in 1984 at the Pinch Mineralogical Museum. One specimen of dietzeite from Oficina Chacabuco, Chile had bright lemon-yellow micronodules on it. These crystals produced an X-ray powder diffraction pattern that did not match any XRD data listed for inorganic compounds. The X-ray diffraction pattern and powder mount were set aside until 1994. By then, the entire mineral collection from the Pinch Mineralogical Museum had been purchased by the Canadian Museum of Nature. The specimen was then retrieved and studied further. This study was successful and the new mineral george-ericksenite was discovered. The mineral was named for George E. Ericksen who was a research economic geologist with the U.S. Geological Survey for fifty years. The mineral and name have been approved by Commission on New Minerals and Mineral Names (IMA). The specimen, polished thin section, and the actual crystal used for the structure determination are kept in the Display Series of the National Mineral Collection of Canada at the Canadian Museum of Nature, Ottawa, Ontario.\n\nGeorge-ericksonite is commonly found as isolated bright lemon-yellow micronodules of crystals that are concentrated on the surface of one part of the mineral specimen. However, in some cases the micronodules occur as groupings instead of isolated occurrences. The average size of these micronodules is approximately 0.2 mm and each consists of numerous individual crystals in random orientation.\n\nThis examination was carried out by attaching two acicular crystals to the surface of a disk with epoxy and then examining them with a CAMECA SX-50 electron microprobe. One of the crystals had the (100) surface facing up, and the other crystal had a growth face of the form (110) facing up. The microprobe was operating in wavelength-dispersive mode at 15 kV and ran various currents from 20 nA to 0.5 nA. The CAMECA SX-50 has three spectrometers and the samples were examined in the sequence (Na, Cl, I), then (Mg, S, Ca). When the crystal was exposed to the electron beam for the first 200 seconds, the counts per second on each element varied greatly which indicates that the crystals are extremely unstable in the electron beam. The counts per second for each element were also dependent on the surface of the crystal [(100) or (110)] analyzed. Over shorter counting times (<10 s) at 15 kV and 5 nA, there is a gain in IO and a drop in NaO relative to ideal values. However, a significant orientation effect exists for SO and CaO values for the (100) and (110) surfaces on either side of the ideal values. With increasing exposure to the electron beam, NaO increases and all other oxides decrease. This behavior is also more rapid on the (100) surface than on the (110) crystal face. The (100) surface is overall more reactive to the electron beam than the (110) surface, but both surfaces seem to approach equilibrium with the beam and give similar oxides weight percentages after 200 seconds.\n\nEven at low currents and short counting times george-ericksenite is extremely unstable under the electron beam. After examination, the crystal faces are stained brown from the reaction with I and the decrease in analyzed IO with increasing time. The crystallographic orientation of the material analyzed has a large impact on the analytical values at any given time. The overall quantitative behavior of george-ericksenite in the electron beam is consistent with the chemical composition derived (ideally IO 59.13, CrO 9.92. SO 1.51, MgO 2.38, CaO 3.31, NaO 10.98, HO 12.77 weight %).\n\nA 0.046 X 0.059 X 0.060 mm crystal was mounted on a Siemans \"P\"4 four-circle diffractometer. The crystal was aligned using 42 reflections automatically centered following measurement from a rotation photograph. The orientation matrix and unit-cell dimensions were determined from the setting angles of least-squares refinement. 3872 reflections were recorded out to 60 2θ with a fixed scan speed of 1.33° 2θ/min. Corrections for absorption by Gaussian quadrature integration were applied. Corrections for Lorentz, polarization, and background effects were also applied as well as reduction of intensities to structure factors.\n\nThe SHELXTL PC Plus system of programs were used for the calculations. The \"R\" and \"Rw\" indices are of the conventional form. The structure was solved by direct methods. The structure is centrosymmetric as indicated by the E statistics. Systematic absences also indicate the presence of a \"c\" glide for the C-centered cell. The result was placing george-ericksenite in the C2/c space group. The structure was refined by a combination of least-squares refinement and difference-Fourier synthesis to an \"R\" index of 3.5% and \"Rw\" equal to 3.5%. Site occupancies were determined by the basis of site-scattering refinement and crystal-chemical criteria.\n\nThere is one chromium (Cr) site that is symmetrically distinct and is tetrahedrally coordinated by four oxygen (O) atoms. The average length of the bonds is 1.61 Å which indicates that the Cr cation is hexavalent. The average bond length at the Cr site is less than would be expected for complete occupancy by Cr. This difference can be accounted for by partial substitution of sulfur (S) atoms.\n\nThere are three iodine (I) sites that are coordinated by three oxygen (O) atoms arranged in a triangle to one side of the cation. The distances of the bonds between the I and O atoms is 1.81 Å. This results in the IO group forming a triangular pyramid with the I site at the top of the pyramid. At each I sites there are also three additional ligands that causes the iodine atoms to have a distorted octahedral coordination. This also causes the I atom to occupy off centered positions within each octahedron. The long bonds between the atoms at the I sites contributes significant bond valence to the bonded anions.\n\nThere are three sodium (Na) sites that are each unique. Each site has a different type of coordination. The Na1 site is encompassed by two O atoms and four HO groups in a distorted octahedral arrangement with a Na1-Φ distance (where Φ=unspecified ligand) of 2.41 Å. The Na2 site is surrounded by five O atoms and 2 HO groups in an augmented octahedral arrangement with a Na2-Φ distance of 2.54 Å. The Na3 site is surrounded by five O atoms and three HO atoms in a triangular dodecahedral arrangement with a Na3-Φ distance of 2.64 Å.\n\nThere is only one magnesium (Mg) site which is coordinated by six O atoms in an octahedral arrangement with a Mg-O distance of 2.09 Å. This bond length is in accord with this site being entirely occupied by Mg with not substitution.\n\nThe one calcium (Ca) site is coordinated by six O atoms and two HO groups in a square-antiprismatic arrangement with a Ca-Φ distance of 2.50 Å. This bond length is in accord with this site being entirely occupied by Ca with not substitution.\n\nGeorge-ericksenite features a structural arrangement that is composed of slabs of polyhedra orthogonal to [100]. These slabs feature the same composition as the mineral itself and are a half of a unit thick in the [100] direction. These are connected to adjacent slabs solely by hydrogen bonding. The edges of each slab are bounded by near-planar layers of anions. The slabs themselves are composed of three planar layers of cations. There are also three planar layers of cations parallel to the edges of the slabs. This indicates that each slab consists of three layers of polyhedra. The \"c\"-glide symmetry relates the top and bottom of the slab which means the slab may be broken into two unique sheets of polyhedra.\n\nThere is a prominent zigzag pattern of chains of Na polyhedra extending in the \"c\" direction on the outer layer of the slab. The Na1 octahedron shares an edge with the Na2 augmented octahedron which shares a face with the Na3 triangular dodecahedron. This forms a linear trimer that extends in the [011] direction. This trimer is then links by edge-sharing between the Na3 and a1 polyhedra to another trimer extending in the [0-11] direction. This motif continues to form a [NaΦ] zigzag chain extending in the \"c\" direction. In each embayment of this chain the polyhedra are accented by two (IO) groups. Identical chains run parallel to the \"c\" axis that are linked only by one weak I-O bond.\n\nThe inner layer of the slab is composed of one Mg octahedron that shares corners with two Cr tetrahedra. This forms a [MTΦ] cluster. The other two anions of the Mg octahedron link by corner-sharing to two (IO) groups. These [Mg(CrO)(IO)O] clusters link together two (CaΦ) polyhedra. This forms chains parallel to the \"b\" axis. Weak I-O bonds link these chains to form the central layer of the slab.\n\nThe only other chromate-iodate mineral is dietzeite, Ca(IO)(CrO)(HO). Dietzeite and george-ericksenite have no structural relationship. Fuenzalidaite and carlosruizite are sulphate-iodate minerals found in the Chilean nitrate fields and contain small amounts of Cr substituting for S. They are also sheet structures, but the sheets are vastly different in terms of connectivity than in george-ericksenite.\n"}
{"id": "4277031", "url": "https://en.wikipedia.org/wiki?curid=4277031", "title": "Index of gardening articles", "text": "Index of gardening articles\n\nThis is a list of gardening topics.\n\nAesthetics\n- African Violet Society of America\n- Allotment\n- Aquascaping\n- Arboretum\n- Architectural theory\n\nBonsai\n- Botanical gardens\n\nCalifornia native plants\n- Chelsea Flower Show\n- Community garden\n- Companion planting\n- Compost\n- Composting\n- Conservation\n\nDesign\n\nEnglish garden\n- Environmental design\n\nFlowerbed\n- Fountains\n- French intensive gardening\n- French landscape garden\n\nGarden\n- Garden designer\n- Gardener\n- Gardening\n- Garden buildings\n- List of gardens in fiction\n- Garden tool\n- Garden Gnome Liberation Front\n- Garden à la française\n- Gardens of the French Renaissance\n- Gardens of Versailles\n- \n- Giardino all'italiana \n- Grandi Giardini Italiani\n- Growbag\n- Guerrilla gardening\n\nHistory of gardening\n- History of gardens\n- History of landscape architecture\n- Hedge\n- Herbaceous border\n- Home economics\n- Horticulture\n\nInvasive species\n- Italian Renaissance garden\n\nJapanese garden\n- Japanese rock garden\n\n- \n- \n- List of notable historical gardens\n- Land Arts of the American West\n- Landscape architecture\n- Landscape design\n- Landscape detailing\n- Landscape garden\n- Landscape manager\n- Landscape products\n- Lawn\n- Lawnmower\n- List of botanical gardens in Italy\n- List of botanical gardens in the United States\n- List of Chinese gardens\n- List of garden and horticulture books\n- List of garden features\n- List of garden plants\n- List of gardens in Italy\n- List of invasive species in North America\n- List of organic gardening and farming topics\n- List of professional gardeners\n- Local food\n\nNative plant gardening\n- Natural landscaping\n- Nature and Culture\n- Never Ending Gardens\n- No-dig gardening\n\nOrganic gardening\n- Arboreta\n\nPatio garden\n- Pergola\n- Parterre\n- Permaculture\n- Plant community\n- Planting design\n- Flower pot\n- Pruning\n\nRain gardens\n- Raised bed gardening\n- Remarkable Gardens of France\n- Rock garden\n- Roof garden\n- Roman garden\n\nSculpture garden\n- Sheet mulching\n- Shrub\n- Spanish garden\n- Spanish gardens\n- Square foot gardening\n- Statuary\n- Sustainable art\n- Sustainable design\n- Sustainability\n\nterrace\n- Topiary\n- Tree\n- Tropical garden\n\nVegetable farming\n\nWater garden\n- water feature\n- Wildlife corridor\n- Wildlife garden\n\nXeriscaping\n\nZen garden\n\n"}
{"id": "53646845", "url": "https://en.wikipedia.org/wiki?curid=53646845", "title": "Liquid slugging", "text": "Liquid slugging\n\nLiquid slugging is the phenomenon of liquid entering the cylinder of a reciprocating compressor, a common cause of failure. Under normal conditions, the intake and output of a compressor cylinder is entirely vapor or gas, when a liquid accumulates at the suction port liquid slugging can occur. As more of the practically incompressible liquid enters, strain is placed upon the system leading to a variety of failures.\n"}
{"id": "660678", "url": "https://en.wikipedia.org/wiki?curid=660678", "title": "List of glaciers", "text": "List of glaciers\n\nA glacier ( ) or () is a persistent body of dense ice that is constantly moving under its own weight; it forms where the accumulation of snow exceeds its ablation (melting and sublimation) over many years, often centuries. Glaciers slowly deform and flow due to stresses induced by their weight, creating crevasses, seracs, and other distinguishing features. Because glacial mass is affected by long-term climate changes, e.g., precipitation, mean temperature, and cloud cover, glacial mass changes are considered among the most sensitive indicators of climate change.\n\nAfrica, specifically East Africa, has contained glacial regions, possibly as far back as the last glacier maximum 10 to 15 thousand years ago. Seasonal snow does exist on the highest peaks of East Africa as well as in the Drakensberg Range of South Africa, the Stormberg Mountains, and the Atlas Mountains in Morocco. Currently, the only remaining glaciers on the continent exist on Mount Kilimanjaro, Mount Kenya, and the Rwenzori.\n\nThere are many glaciers in the Antarctic. This set of lists does not include ice sheets, ice caps or ice fields, such as the Antarctic ice sheet, but includes glacial features that are defined by their flow, rather than general bodies of ice. The lists include outlet glaciers, valley glaciers, cirque glaciers, tidewater glaciers and ice streams. Ice streams are a type of glacier and many of them have \"glacier\" in their name, e.g. Pine Island Glacier. Ice shelves are listed separately in the List of Antarctic ice shelves. For the purposes of these lists, the Antarctic is defined as any latitude further south than 60° (the continental limit according to the Antarctic Treaty System).\n\nThere are also glaciers in the subantarctic. This includes one snow field (Murray Snowfield). Snow fields are not glaciers in the strict sense of the word, but they are commonly found at the accumulation zone or head of a glacier. For the purposes of this list, Antarctica is defined as any latitude further south than 60° (the continental limit according to the Antarctic Treaty).\n\nThe majority of Europe's glaciers are found in the Alps, Caucasus and the Scandinavian Mountains (mostly Norway) as well as in Iceland. Iceland has the largest glacier in Europe, Vatnajökull glacier, that covers between 8,100-8,300 km² in area and 3,100 km³ in volume. Norway alone has more than 2500 glaciers (including very small ones) covering an estimated 1% of mainland Norway's surface area. Several of mainland Europe's biggest glaciers are found here including; Jostedalsbreen(the largest in mainland Europe at 487 km), Vestre Svartisen(221 km), Søndre Folgefonna(168 km) and Østre Svartisen(148 km). The two Svartisen glaciers used to be one connected entity during the Little Ice Age but has since separated.\n\n\nThere are a number of glaciers existing in North America, currently or in recent centuries. In the United States, these glaciers are located in nine states, all in the Rocky Mountains or further west. The southernmost named glacier among them is the Lilliput Glacier in Tulare County, east of the Central Valley of California.\n\nMexico has about two dozen glaciers, all of which are located on Pico de Orizaba (Citlaltépetl), Popocatépetl and Iztaccíhuatl, the three tallest mountains in the country.\n\n\nGlaciers in South America develop exclusively on the Andes and are subject of the Andes various climatic regimes namely the Tropical Andes, Dry Andes and the Wet Andes. Apart from this there is a wide range of latitudes on which glaciers develop from 5000 m in the Altiplano mountains and volcanoes to reaching sealevel as tidewater glaciers from San Rafael Lagoon (45° S) and southwards. South America hosts two large ice fields, the Northern and Southern Patagonian Ice Fields, of which the second is the largest contiguous body of glaciers in extrapolar regions.\n\nThe glaciers of Chile cover 2.7% (20,188 km) of the land area of the country, excluding Antártica Chilena, and have a considerable impact on its landscape and water supply. By surface 80% of South America’s glaciers lie in Chile. Glaciers develop in the Andes of Chile from 27˚S southwards and in a very few places north of 18°30'S in the extreme north of the country: in between they are absent because of extreme aridity, though rock glaciers formed from permafrost are common. The largest glaciers of Chile are the Northern and Southern Patagonian Ice Fields. From a latitude of 47° S and south some glaciers reach sea level.\n\nApart from height and latitude, the settings of Chilean glaciers depend on precipitation patterns; in this sense two different regions exist: the Dry Andes and the Wet Andes.\n\nNo glaciers remain on the Australia mainland or Tasmania. A few, like the Heard Island glaciers are located in the territory of Heard Island and McDonald Islands in the southern Indian Ocean.\n\nNew Guinea has the Puncak Jaya glacier.\n\nNew Zealand contains many glaciers, mostly located near the Main Divide of the Southern Alps in the South Island. They are classed as mid-latitude mountain glaciers. There are eighteen small glaciers in the North Island on Mount Ruapehu.\n\nAn inventory of South Island glaciers compiled in the 1980s indicated there were about 3,155 glaciers with an area of at least one hectare (2.5 acres). Approximately one sixth of these glaciers covered more than 10 hectares. These include:\n\n\nThe following is the list of longest glaciers in the non-polar regions, generally regarded as between 60 degrees north and 60 degrees south latitude, though some definitions expand it slightly.\n\n"}
{"id": "24694990", "url": "https://en.wikipedia.org/wiki?curid=24694990", "title": "List of invasive species in Europe", "text": "List of invasive species in Europe\n\nThis is a list of invasive species in Europe. A species is regarded as invasive if it has become introduced to a location, area, or region where it did not previously occur naturally (i.e., is not a native species) and becomes capable of establishing a breeding population in the new location. An invasive species will be one that thrives in its new environment and negatively influences the ecology and biodiversity of that ecosystem.\n\nThe term invasive species refers to a subset of those species defined as introduced species. If a species has been introduced but remains local, and is not problematic to agriculture or to the local biodiversity, then it cannot be considered to be invasive, and does not belong on this list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "15911800", "url": "https://en.wikipedia.org/wiki?curid=15911800", "title": "List of reserves for waterbirds and migratory birds in Switzerland", "text": "List of reserves for waterbirds and migratory birds in Switzerland\n\nThis is a list of reserves for waterbirds and migratory birds in Switzerland. The nature reserves on this inventory are protected by Swiss federal legislation (\"Federal Inventory of Water and Migratory Birds Reserves of National and International Importance\").\n\n\n"}
{"id": "16964856", "url": "https://en.wikipedia.org/wiki?curid=16964856", "title": "List of types of limestone", "text": "List of types of limestone\n\nThis is a list of types of limestone arranged according to location. It includes both formal stratigraphic unit names and less formal designations.\n\n\n\n\n\n\n\n\n\nEngland:\nScotland:\nWales:\n\n\n\nThis section is a list of generic types of limestone\n\n\n"}
{"id": "16554664", "url": "https://en.wikipedia.org/wiki?curid=16554664", "title": "Living systems", "text": "Living systems\n\nLiving systems are open self-organizing life forms that interact with their environment. These systems are maintained by flows of information, energy and matter.\n\nSome scientists have proposed in the last few decades that a general living systems theory is required to explain the nature of life. Such a general theory, arising out of the ecological and biological sciences, attempts to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into components, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment.\n\nLiving systems theory is a general theory about the existence of all living systems, their structure, interaction, behavior and development. This work is created by James Grier Miller, which was intended to formalize the concept of life. According to Miller's original conception as spelled out in his magnum opus \"Living Systems\", a \"living system\" must contain each of twenty \"critical subsystems\", which are defined by their functions and visible in numerous systems, from simple cells to organisms, countries, and societies. In \"Living Systems\" Miller provides a detailed look at a number of systems in order of increasing size, and identifies his subsystems in each. \nMiller considers living systems as a subset of all systems. Below the level of living systems, he defines space and time, matter and energy, information and entropy, levels of organization, and physical and conceptual factors, and above living systems ecological, planetary and solar systems, galaxies, etc.\n\nLiving systems according to Parent (1996) are by definition \"open self-organizing systems that have the special characteristics of life and interact with their environment. This takes place by means of information and material-energy exchanges. Living systems can be as simple as a single cell or as complex as a supranational organization such as the European Union. Regardless of their complexity, they each depend upon the same essential twenty subsystems (or processes) in order to survive and to continue the propagation of their species or types beyond a single generation\".\n\nMiller said that systems exist at eight \"nested\" hierarchical levels: cell, organ, organism, group, organization, community, society, and supranational system. At each level, a system invariably comprises twenty critical subsystems, which process matter–energy or information except for the first two, which process both matter–energy and information: reproducer and boundary.\n\nThe processors of matter–energy are: \n\nThe processors of information are:\n\nJames Grier Miller in 1978 wrote a 1,102-page volume to present his living systems theory. He constructed a general theory of living systems by focusing on concrete systems—nonrandom accumulations of matter–energy in physical space–time organized into interacting, interrelated subsystems or components. Slightly revising the original model a dozen years later, he distinguished eight \"nested\" hierarchical levels in such complex structures. Each level is \"nested\" in the sense that each higher level contains the next lower level in a nested fashion.\n\nHis central thesis is that the systems in existence at all eight levels are open systems composed of twenty critical subsystems that process inputs, throughputs, and outputs of various forms of matter–energy and information. Two of these subsystems—reproducer and boundary—process both matter–energy and information. Eight of them process only matter–energy. The other ten process information only. \nAll nature is a continuum. The endless complexity of life is organized into patterns which repeat themselves—theme and variations—at each level of system. These similarities and differences are proper concerns for science. From the ceaseless streaming of protoplasm to the many-vectored activities of supranational systems, there are continuous flows through living systems as they maintain their highly organized steady states.\nSeppänen (1998) says that Miller applied general systems theory on a broad scale to describe all aspects of living systems.\n\nMiller's theory posits that the mutual interrelationship of the components of a system extends across the hierarchical levels. Examples: Cells and organs of a living system thrive on the food the organism obtains from its suprasystem; the member countries of a supranational system reap the benefits accrued from the communal activities to which each one contributes. Miller says that his eclectic theory \"ties together past discoveries from many disciplines and provides an outline into which new findings can be fitted\".\n\nMiller says the concepts of space, time, matter, energy, and information are essential to his theory because the living systems exist in space and are made of matter and energy organized by information. Miller's theory of living systems employs two sorts of spaces: physical or geographical space, and conceptual or abstracted spaces. Time is the fundamental \"fourth dimension\" of the physical space–time continuum/spiral. Matter is anything that has mass and occupies physical space. Mass and energy are equivalent as one can be converted into the other. Information refers to the degrees of freedom that exist in a given situation to choose among signals, symbols, messages, or patterns to be transmitted.\n\nOther relevant concepts are system, structure, process, type, level, echelon, suprasystem, subsystem, transmissions, and steady state. A system can be conceptual, concrete or abstracted. The structure of a system is the arrangement of the subsystems and their components in three-dimensional space at any point of time. Process, which can be reversible or irreversible, refers to change over time of matter–energy or information in a system. Type defines living systems with similar characteristics. Level is the position in a hierarchy of systems. Many complex living systems, at various levels, are organized into two or more echelons. The suprasystem of any living system is the next higher system in which it is a subsystem or component. The totality of all the structures in a system which carry out a particular process is a subsystem. Transmissions are inputs and outputs in concrete systems. Because living systems are open systems, with continually altering fluxes of matter–energy and information, many of their equilibria are dynamic—situations identified as steady states or flux equilibria.\n\nMiller identifies the comparable matter–energy and information processing critical subsystems. Elaborating on the eight hierarchical levels, he defines society, which constitutes the seventh hierarchy, as \"a large, living, concrete system with [community] and lower am levels of living systems as subsystems and components\". Society may include small, primitive, totipotential communities; ancient city–states, and kingdoms; as well as modern nation–states and empires that are not supranational systems. Miller provides general descriptions of each of the subsystems that fit all eight levels.\n\nA supranational system, in Miller's view, \"is composed of two or more societies, some or all of whose processes are under the control of a decider that is superordinate to their highest echelons\". However, he contends that no supranational system with all its twenty subsystems under control of its decider exists today. The absence of a supranational decider precludes the existence of a concrete supranational system. Miller says that studying a supranational system is problematical because its subsystems\n...tend to consist of few components besides the decoder. These systems do little matter-energy processing. The power of component societies [nations] today is almost always greater than the power of supranational deciders. Traditionally, theory at this level has been based upon intuition and study of history rather than data collection. Some quantitative research is now being done, and construction of global-system models and simulations is currently burgeoning.\n\nAt the supranational system level, Miller's emphasis is on international organizations, associations, and groups comprising representatives of societies (nation–states). Miller identifies the subsystems at this level to suit this emphasis. Thus, for example, the reproducer is \"any multipurpose supranational system which creates a single purpose supranational organization\" (p. 914); and the boundary is the \"supranational forces, usually located on or near supranational borders, which defend, guard, or police them\" (p. 914).\n\nNot just those specialized in international communication, but all communication science scholars could pay particular attention to the major contributions of living systems theory (LST) to social systems approaches that Bailey has pointed out:\n\nBailey says that LST, perhaps the \"most integrative\" social systems theory, has made many more contributions that may be easily overlooked, such as: providing a detailed analysis of types of systems; making a distinction between concrete and abstracted systems; discussion of physical space and time; placing emphasis on information processing; providing an analysis of entropy; recognition of totipotential systems, and partipotential systems; providing an innovative approach to the structure–process issue; and introducing the concept of joint subsystem—a subsystem that belongs to two systems simultaneously; of dispersal—lateral, outward, upward, and downward; of inclusion—inclusion of something from the environment that is not part of the system; of artifact—an animal-made or human-made inclusion; of adjustment process, which combats stress in a system; and of critical subsystems, which carry out processes that all living systems need to survive.\n\nLST's analysis of the twenty interacting subsystems, Bailey adds, clearly distinguishing between matter–energy-processing and information-processing, as well as LST's analysis of the eight interrelated system levels, enables us to understand how social systems are linked to biological systems. LST also analyzes the irregularities or \"organizational pathologies\" of systems functioning (e.g., system stress and strain, feedback irregularities, information–input overload). It explicates the role of entropy in social research while it equates negentropy with information and order. It emphasizes both structure and process, as well as their interrelations.\n\nIt omits the analysis of subjective phenomena, and it overemphasizes concrete Q-analysis (correlation of objects) to the virtual exclusion of R-analysis (correlation of variables). By asserting that societies (ranging from totipotential communities to nation-states and non-supranational systems) have greater control over their subsystem components than supranational systems have, it dodges the issue of transnational power over the contemporary social systems. Miller's supranational system bears no resemblance to the modern world-system that Immanuel Wallerstein (1974) described, although both of them were looking at the same living (dissipative) structure.\n\n\n\n"}
{"id": "23708860", "url": "https://en.wikipedia.org/wiki?curid=23708860", "title": "Magmatic water", "text": "Magmatic water\n\nMagmatic water or juvenile water is water that exists within, and in equilibrium with, a magma or water-rich volatile fluids that are derived from a magma. This magmatic water is released to the atmosphere during a volcanic eruption. Magmatic water may also be released as hydrothermal fluids during the late stages of magmatic crystallization or solidification within the Earth's crust. The crystallization of hydroxyl bearing amphibole and mica minerals acts to contain part of the magmatic water within a solidified igneous rock. Ultimate sources of this magmatic water includes water and hydrous minerals in rocks melted during subduction as well as primordial water brought up from the deep mantle.\n\nWater has limited solubility in silicate melts ranging from almost 0% at surface pressure to 10% at 1100 °C and 5 kbar of pressure for a granitic melt. Solubility is lower for more mafic magmas. As the temperature and pressure drop during emplacement and cooling of the magma a separate aqueous phase will exsolve. This aqueous phase will be enriched in other volatile and silicate incompatible species such as the metals: copper, lead, zinc, silver and gold; alkalis and alkaline earths and others, including: lithium, beryllium, boron, rubidium; and volatiles: fluorine, chlorine and carbon dioxide.\n\nWater in silicate melts at the high temperature and pressure conditions within the crust exists as a supercritical fluid rather than in a gaseous state (the critical point for water is at 374 °C and 218 bar).\n\nStable isotope studies of oxygen and hydrogen in igneous rocks indicate that the oxygen-18, δO, content is approximately 6 - 8 ‰ higher than standard mean ocean water (SMOW) while the deuterium, δH, content is 40 to 80 ‰ lower than SMOW. Water in equilibrium with igneous melts should bear the same isotopic signature for oxygen-18 and deuterium. Isotope data on hydrothermal solutions spatially associated with igneous intrusions should reflect this isotopic signature. However, isotopic studies of hydrothermal waters indicate that most bear the isotopic signature of meteoric water. Any magmatic waters in these hydrothermal solutions must have been swamped by the circulating meteoric groundwaters of the environment.\n\nFluid inclusions are microscopic bubbles of aqueous solutions which were trapped within crystals during crystallization and are considered as relic samples of the mineralizing waters. Analyses of the isotopic content of these trapped bubbles show a wide range of δO and δH content. All examined show an enrichment in O and depletion in H relative to SMOW and meteoric waters. Fluid inclusion data from a number of ore deposits plot directly on the magmatic water \"region\" of an δO vs δH plot.\n\n"}
{"id": "6748280", "url": "https://en.wikipedia.org/wiki?curid=6748280", "title": "Material", "text": "Material\n\nA material is a chemical substance or mixture of substances that constitute an object. Materials can be pure or impure, a singular composite or a complex mix, living or non-living matter, whether natural or man-made, either concrete or abstract. Materials can be classified based on different properties such as physical and chemical properties (see List of materials properties), geological, biological, choreographical, or philosophical properties. In the physical sense, materials are studied in the field of materials science.\n\nIn industry, materials are inputs to production or manufacturing processes. They may either be raw material, that is, unprocessed, or processed before being used in more advanced production processes, either by distillation or synthesis (synthetic materials).\n\nTypes of materials include:\n\nMaterials are classified according to many different criteria including their physical and chemical characteristics as well as their intended applications whether it is thermal, optical, electrical, magnetic, or combined. As their methods of usage dictate their physical appearance, they can be designed, tailored, and/or prepared in many forms such as powders, thin or thick films, and plates and could be introduced/studied in a single or multi layers. End products could be pure materials or doped ones with most useful compounds are those with controlled added impurities.The dopants could be added chemically or mixed and implanted physically. In case the impurities were added chemically, the dopants/co-dopants on substitutional/interstitial sites should be optimized and investigated thoroughly as well as any stresses instigated by their presence within the structure; whereas in the case of the physical mixing, the influence of the degree of heterogeneity of the prepared hybrid composites ought to be studied.The different physical and chemical preparation techniques can be used solely or combined including solid state synthesis, hydrothermal, sol-gel, precipitations and coprecipitations, spin coating, physical vapor deposition, and spray pyrolysis. Types of impurities along with their amounts are usually dictated by types of matrices to be added to, and their ability to maximize the desired products’ usefulness. Among the most commonly used characterization techniques are X-ray diffraction (XRD) either single crystal or powder, scanning electron microscopy (SEM), energy dispersive X-ray spectroscopy (EDS), X-ray fluorescence (XRF), differential scanning calorimetry (DSC), UV-Vis absorption Spectroscopy, Fourier transform infra-red (FTIR), and Photoluminescence spectrometry. In addition, it is usually considered of extreme importance to find theoretical models that can confirm and/or predict the experimental findings and assist in discussion, assignment, and the explanation of results and outcomes. Also, vision and room for future modification and development should always be pinpointed. Hence, one can classify the material as a smart one if its presence can serve multi purposes within the final product.\n\n"}
{"id": "36688650", "url": "https://en.wikipedia.org/wiki?curid=36688650", "title": "Moisture expansion", "text": "Moisture expansion\n\nMoisture expansion is the tendency of matter to change in volume in response to a change in moisture content. The macroscopic effect is similar to that of thermal expansion but the microscopic causes are very different. Moisture expansion is caused by hygroscopy.\n"}
{"id": "19555", "url": "https://en.wikipedia.org/wiki?curid=19555", "title": "Molecule", "text": "Molecule\n\nA molecule is an electrically neutral group of two or more atoms held together by chemical bonds. Molecules are distinguished from ions by their lack of electrical charge. However, in quantum physics, organic chemistry, and biochemistry, the term \"molecule\" is often used less strictly, also being applied to polyatomic ions.\n\nIn the kinetic theory of gases, the term \"molecule\" is often used for any gaseous particle regardless of its composition. According to this definition, noble gas atoms are considered molecules as they are monatomic molecules.\n\nA molecule may be homonuclear, that is, it consists of atoms of one chemical element, as with oxygen (O); or it may be heteronuclear, a chemical compound composed of more than one element, as with water (HO). Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are generally not considered single molecules.\n\nMolecules as components of matter are common in organic substances (and therefore biochemistry). They also make up most of the oceans and atmosphere. However, the majority of familiar solid substances on Earth, including most of the minerals that make up the crust, mantle, and core of the Earth, contain many chemical bonds, but are \"not\" made of identifiable molecules. Also, no typical molecule can be defined for ionic crystals (salts) and covalent crystals (network solids), although these are often composed of repeating unit cells that extend either in a plane (such as in graphene) or three-dimensionally (such as in diamond, quartz, or sodium chloride). The theme of repeated unit-cellular-structure also holds for most condensed phases with metallic bonding, which means that solid metals are also not made of molecules. In glasses (solids that exist in a vitreous disordered state), atoms may also be held together by chemical bonds with no presence of any definable molecule, nor any of the regularity of repeating units that characterizes crystals.\n\nThe science of molecules is called \"molecular chemistry\" or \"molecular physics\", depending on whether the focus is on chemistry or physics. Molecular chemistry deals with the laws governing the interaction between molecules that results in the formation and breakage of chemical bonds, while molecular physics deals with the laws governing their structure and properties. In practice, however, this distinction is vague. In molecular sciences, a molecule consists of a stable system (bound state) composed of two or more atoms. Polyatomic ions may sometimes be usefully thought of as electrically charged molecules. The term \"unstable molecule\" is used for very reactive species, i.e., short-lived assemblies (resonances) of electrons and nuclei, such as radicals, molecular ions, Rydberg molecules, transition states, van der Waals complexes, or systems of colliding atoms as in Bose–Einstein condensate.\n\nAccording to Merriam-Webster and the Online Etymology Dictionary, the word \"molecule\" derives from the Latin \"moles\" or small unit of mass.\n\nThe definition of the molecule has evolved as knowledge of the structure of molecules has increased. Earlier definitions were less precise, defining molecules as the smallest particles of pure chemical substances that still retain their composition and chemical properties. This definition often breaks down since many substances in ordinary experience, such as rocks, salts, and metals, are composed of large crystalline networks of chemically bonded atoms or ions, but are not made of discrete molecules.\n\nMolecules are held together by either covalent bonding or ionic bonding. Several types of non-metal elements exist only as molecules in the environment. For example, hydrogen only exists as hydrogen molecule. A molecule of a compound is made out of two or more elements.\n\nA covalent bond is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are termed \"shared pairs\" or \"bonding pairs\", and the stable balance of attractive and repulsive forces between atoms, when they share electrons, is termed \"covalent bonding\".\n\nIonic bonding is a type of chemical bond that involves the electrostatic attraction between oppositely charged ions, and is the primary interaction occurring in ionic compounds. The ions are atoms that have lost one or more electrons (termed cations) and atoms that have gained one or more electrons (termed anions). This transfer of electrons is termed \"electrovalence\" in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complicated nature, e.g. molecular ions like NH or SO. Basically, an ionic bond is the transfer of electrons from a metal to a non-metal for both atoms to obtain a full valence shell.\n\nMost molecules are far too small to be seen with the naked eye, but there are exceptions. DNA, a macromolecule, can reach macroscopic sizes, as can molecules of many polymers. Molecules commonly used as building blocks for organic synthesis have a dimension of a few angstroms (Å) to several dozen Å, or around one billionth of a meter. Single molecules cannot usually be observed by light (as noted above), but small molecules and even the outlines of individual atoms may be traced in some circumstances by use of an atomic force microscope. Some of the largest molecules are macromolecules or supermolecules.\n\nThe smallest molecule is the diatomic hydrogen (H), with a bond length of 0.74 Å.\n\nEffective molecular radius is the size a molecule displays in solution.\nThe table of permselectivity for different substances contains examples.\n\nThe chemical formula for a molecule uses one line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, and \"plus\" (+) and \"minus\" (−) signs. These are limited to one typographic line of symbols, which may include subscripts and superscripts.\n\nA compound's empirical formula is a very simple type of chemical formula. It is the simplest integer ratio of the chemical elements that constitute it. For example, water is always composed of a 2:1 ratio of hydrogen to oxygen atoms, and ethyl alcohol or ethanol is always composed of carbon, hydrogen, and oxygen in a 2:6:1 ratio. However, this does not determine the kind of molecule uniquely – dimethyl ether has the same ratios as ethanol, for instance. Molecules with the same atoms in different arrangements are called isomers. Also carbohydrates, for example, have the same ratio (carbon:hydrogen:oxygen= 1:2:1) (and thus the same empirical formula) but different total numbers of atoms in the molecule.\n\nThe molecular formula reflects the exact number of atoms that compose the molecule and so characterizes different molecules. However different isomers can have the same atomic composition while being different molecules.\n\nThe empirical formula is often the same as the molecular formula but not always. For example, the molecule acetylene has molecular formula CH, but the simplest integer ratio of elements is CH.\n\nThe molecular mass can be calculated from the chemical formula and is expressed in conventional atomic mass units equal to 1/12 of the mass of a neutral carbon-12 (C isotope) atom. For network solids, the term formula unit is used in stoichiometric calculations.\n\nFor molecules with a complicated 3-dimensional structure, especially involving atoms bonded to four different substituents, a simple molecular formula or even semi-structural chemical formula may not be enough to completely specify the molecule. In this case, a graphical type of formula called a structural formula may be needed. Structural formulas may in turn be represented with a one-dimensional chemical name, but such chemical nomenclature requires many words and terms which are not part of chemical formulas.\n\nMolecules have fixed equilibrium geometries—bond lengths and angles— about which they continuously oscillate through vibrational and rotational motions. A pure substance is composed of molecules with the same average geometrical structure. The chemical formula and the structure of a molecule are the two important factors that determine its properties, particularly its reactivity. Isomers share a chemical formula but normally have very different properties because of their different structures. Stereoisomers, a particular type of isomer, may have very similar physico-chemical properties and at the same time different biochemical activities.\n\nMolecular spectroscopy deals with the response (spectrum) of molecules interacting with probing signals of known energy (or frequency, according to Planck's formula). Molecules have quantized energy levels that can be analyzed by detecting the molecule's energy exchange through absorbance or emission.\nSpectroscopy does not generally refer to diffraction studies where particles such as neutrons, electrons, or high energy X-rays interact with a regular arrangement of molecules (as in a crystal).\n\nMicrowave spectroscopy commonly measures changes in the rotation of molecules, and can be used to identify molecules in outer space. Infrared spectroscopy measures changes in vibration of molecules, including stretching, bending or twisting motions. It is commonly used to identify the kinds of bonds or functional groups in molecules. Changes in the arrangements of electrons yield absorption or emission lines in ultraviolet, visible or near infrared light, and result in colour. Nuclear resonance spectroscopy actually measures the environment of particular nuclei in the molecule, and can be used to characterise the numbers of atoms in different positions in a molecule.\n\nThe study of molecules by molecular physics and theoretical chemistry is largely based on quantum mechanics and is essential for the understanding of the chemical bond. The simplest of molecules is the hydrogen molecule-ion, H, and the simplest of all the chemical bonds is the one-electron bond. H is composed of two positively charged protons and one negatively charged electron, which means that the Schrödinger equation for the system can be solved more easily due to the lack of electron–electron repulsion. With the development of fast digital computers, approximate solutions for more complicated molecules became possible and are one of the main aspects of computational chemistry.\n\nWhen trying to define rigorously whether an arrangement of atoms is \"sufficiently stable\" to be considered a molecule, IUPAC suggests that it \"must correspond to a depression on the potential energy surface that is deep enough to confine at least one vibrational state\". This definition does not depend on the nature of the interaction between the atoms, but only on the strength of the interaction. In fact, it includes weakly bound species that would not traditionally be considered molecules, such as the helium dimer, He, which has one vibrational bound state and is so loosely bound that it is only likely to be observed at very low temperatures.\n\nWhether or not an arrangement of atoms is \"sufficiently stable\" to be considered a molecule is inherently an operational definition. Philosophically, therefore, a molecule is not a fundamental entity (in contrast, for instance, to an elementary particle); rather, the concept of a molecule is the chemist's way of making a useful statement about the strengths of atomic-scale interactions in the world that we observe.\n\n\n"}
{"id": "440959", "url": "https://en.wikipedia.org/wiki?curid=440959", "title": "Mpemba effect", "text": "Mpemba effect\n\nThe Mpemba effect is a process in which hot water can freeze faster than cold water. The phenomenon is temperature-dependent. There is disagreement about the parameters required to produce the effect and about its theoretical basis.\nThe Mpemba effect is named after Erasto Batholomeo Mpemba (b.1950) who discovered it in 1963. There were preceding ancient accounts of similar phenomena, but lacking sufficient detail to attempt verification.\n\nThe phenomenon, when taken to mean \"hot water freezes faster than cold\", is difficult to reproduce or confirm because this statement is ill-defined. Monwhea Jeng proposes as a more precise wording:\n\nThere exists a set of initial parameters, and a pair of temperatures, such that given two bodies of water identical in these parameters, and differing only in initial uniform temperatures, the hot one will freeze sooner.\n\nHowever, even with this definition it is not clear whether \"freezing\" refers to the point at which water forms a visible surface layer of ice; the point at which the entire volume of water becomes a solid block of ice; or when the water reaches . A quantity of water can be at and not be ice; after enough heat has been removed to reach more heat must be removed before the water changes to solid state (ice), so water can be liquid or solid at .\n\nWith the above definition there are simple ways in which the effect might be observed: For example, if the hotter temperature melts the frost on a cooling surface and thus increases the thermal conductivity between the cooling surface and the water container. On the other hand, there may be many circumstances in which the effect is not observed.\n\nVarious effects of heat on the freezing of water were described by ancient scientists such as Aristotle: \"The fact that the water has previously been warmed contributes to its freezing quickly: for so it cools sooner. Hence many people, when they want to cool water quickly, begin by putting it in the sun. So the inhabitants of Pontus when they encamp on the ice to fish (they cut a hole in the ice and then fish) pour warm water round their reeds that it may freeze the quicker, for they use the ice like lead to fix the reeds.\" Aristotle's explanation involved \"antiperistasis\", \"the supposed increase in the intensity of a quality as a result of being surrounded by its contrary quality.\"\n\nEarly modern scientists such as Francis Bacon noted that, \"slightly tepid water freezes more easily than that which is utterly cold.\" In the original Latin, \"aqua parum tepida facilius conglacietur quam omnino frigida.\"\n\nRené Descartes wrote in his \"Discourse on the Method\", \"One can see by experience that water that has been kept on a fire for a long time freezes faster than other, the reason being that those of its particles that are least able to stop bending evaporate while the water is being heated.\" This relates to Descartes' vortex theory.\n\nThe Scottish scientist Joseph Black investigated a special case of this phenomenon comparing previously-boiled with unboiled water; the previously-boiled water froze more quickly. Evaporation was controlled for. He discussed the influence of stirring on the results of the experiment, noting that stirring the unboiled water led to it freezing at the same time as the previously-boiled water, and also noted that stirring the very-cold unboiled water led to immediate freezing. Joseph Black then discussed Fahrenheit's description of supercooling of water (although the term supercooling had not then been coined), arguing, in modern terms, that the previously-boiled water could not be as readily supercooled.\n\nThe effect is named after Tanzanian Erasto Mpemba. He described it in 1963 in Form 3 of Magamba Secondary School, Tanganyika, when freezing ice cream mix that was hot in cookery classes and noticing that it froze before the cold mix. He later became a student at Mkwawa Secondary (formerly High) School in Iringa. The headmaster invited Dr. Denis G. Osborne from the University College in Dar es Salaam to give a lecture on physics. After the lecture, Erasto Mpemba asked him the question, \"If you take two similar containers with equal volumes of water, one at and the other at , and put them into a freezer, the one that started at freezes first. Why?\", only to be ridiculed by his classmates and teacher. After initial consternation, Osborne experimented on the issue back at his workplace and confirmed Mpemba's finding. They published the results together in 1969, while Mpemba was studying at the College of African Wildlife Management.\n\nMpemba and Osborne describe placing samples of water in beakers in the ice box of a domestic refrigerator on a sheet of polystyrene foam. They showed the time for \"freezing to start\" was longest with an initial temperature of and that it was much less at around . They ruled out loss of liquid volume by evaporation as a significant factor and the effect of dissolved air. In their setup most heat loss was found to be from the liquid surface.\nDavid Auerbach describes an effect that he observed in samples in glass beakers placed into a liquid cooling bath. In all cases the water supercooled, reaching a temperature of typically before spontaneously freezing. Considerable random variation was observed in the time required for spontaneous freezing to start and in some cases this resulted in the water which started off hotter (partially) freezing first.\n\nJames Brownridge, a radiation safety officer at the State University of New York, has said that he believes that supercooling is involved. Several molecular dynamics simulations have also supported that changes in hydrogen bonding during supercooling takes a major role in the process.\n\nIn 2016, Burridge and Linden defined the criterion as the time to reach , carried out experiments and reviewed published work to date. They noted that the large difference originally claimed had not been replicated, and that studies showing a small effect could be influenced by variations in the positioning of thermometers. They say, \"We conclude, somewhat sadly, that there is no evidence to support meaningful observations of the Mpemba effect\".\n\nHowever, in 2017, two research groups independently and simultaneously found theoretical evidence of the Mpemba effect and also predicted a new \"inverse\" Mpemba effect in which heating a cooled, far-from-equilibrium system takes less time than another system that is initially closer to equilibrium. Lu and Raz yield a general criterion based on Markovian statistical mechanics, predicting the appearance of the inverse Mpemba effect in the Ising model and diffusion dynamics. Lasanta and co-workers predict also the direct and inverse Mpemba effects for a granular gas in a far-from-equilibrium initial state. In this last work, it is suggested that a very generic mechanism leading to both Mpemba effects is due to a particle velocity distribution function that significantly deviates from the Maxwell-Boltzmann distribution.\n\nThe following explanations have been proposed:\n\n\nA reviewer for \"Physics World\" writes, \"Even if the Mpemba effect is real — if hot water can sometimes freeze more quickly than cold — it is not clear whether the explanation would be trivial or illuminating.\" He pointed out that investigations of the phenomenon need to control a large number of initial parameters (including type and initial temperature of the water, dissolved gas and other impurities, and size, shape and material of the container, and temperature of the refrigerator) and need to settle on a particular method of establishing the time of freezing, all of which might affect the presence or absence of the Mpemba effect. The required vast multidimensional array of experiments might explain why the effect is not yet understood.\n\n\"New Scientist\" recommends starting the experiment with containers at to maximize the effect. In a related study, it was found that freezer temperature also affects the probability of observing the Mpemba phenomenon as well as container temperature.\n\nIn 2012, the Royal Society of Chemistry held a competition calling for papers offering explanations to the Mpemba effect. More than 22,000 people entered and Erasto Mpemba himself announced Nikola Bregović as the winner. Bregović suggests two reasons for the effect — a colder sample gets supercooled rather than frozen, and enhanced convection in the warmer sample speeds up cooling by maintaining the heat gradient on the container walls.\n\nTao and co-workers proposed yet another possible explanation in 2016. On the basis of results from vibrational spectroscopy and modeling with density functional theory-optimized water clusters, they suggest that the reason might lie in the vast diversity and peculiar occurrence of different hydrogen bonds. Their key argument is that the number of strong hydrogen bonds increases as temperature is elevated. The existence of the small strongly-bonded clusters facilitates in turn the nucleation of hexagonal ice when warm water is rapidly cooled down.\n\nOther phenomena in which large effects may be achieved faster than small effects are\n\n\nNotes\n\n"}
{"id": "38393", "url": "https://en.wikipedia.org/wiki?curid=38393", "title": "Natural rubber", "text": "Natural rubber\n\nNatural rubber, also called India rubber or caoutchouc, as initially produced, consists of polymers of the organic compound isoprene, with minor impurities of other organic compounds, plus water. Malaysia and Indonesia are two of the leading rubber producers. Forms of polyisoprene that are used as natural rubbers are classified as elastomers.\n\nCurrently, rubber is harvested mainly in the form of the latex from the rubber tree or others. The latex is a sticky, milky colloid drawn off by making incisions in the bark and collecting the fluid in vessels in a process called \"tapping\". The latex then is refined into rubber ready for commercial processing. In major areas, latex is allowed to coagulate in the collection cup. The coagulated lumps are collected and processed into dry forms for marketing.\n\nNatural rubber is used extensively in many applications and products, either alone or in combination with other materials. In most of its useful forms, it has a large stretch ratio and high resilience, and is extremely waterproof.\n\nThe major commercial source of natural rubber latex is the Pará rubber tree (\"Hevea brasiliensis\"), a member of the spurge family, \"Euphorbiaceae\". This species is preferred because it grows well under cultivation. A properly managed tree responds to wounding by producing more latex for several years.\n\nCongo rubber, formerly a major source of rubber, came from vines in the genus \"Landolphia\" (\"L. kirkii\", \"L. heudelotis\", and \"L. owariensis\").\n\nDandelion milk contains latex. The latex exhibits the same quality as the natural rubber from rubber trees. In the wild types of dandelion, latex content is low and varies greatly. In Nazi Germany, research projects tried to use dandelions as a base for rubber production, but failed. In 2013, by inhibiting one key enzyme and using modern cultivation methods and optimization techniques, scientists in the Fraunhofer Institute for Molecular Biology and Applied Ecology (IME) in Germany developed a cultivar that is suitable for commercial production of natural rubber. In collaboration with Continental Tires, IME began a pilot facility.\n\nMany other plants produce forms of latex rich in isoprene polymers, though not all produce usable forms of polymer as easily as the Pará. Some of them require more elaborate processing to produce anything like usable rubber, and most are more difficult to tap. Some produce other desirable materials, for example gutta-percha (\"Palaquium gutta\") and chicle from \"Manilkara\" species. Others that have been commercially exploited, or at least showed promise as rubber sources, include the rubber fig (\"Ficus elastica\"), Panama rubber tree (\"Castilla elastica\"), various spurges (\"Euphorbia\" spp.), lettuce (\"Lactuca\" species), the related \"Scorzonera tau-saghyz\", various \"Taraxacum\" species, including common dandelion (\"Taraxacum officinale\") and Russian dandelion (\"Taraxacum kok-saghyz\"), and perhaps most importantly for its hypoallergenic properties, guayule (\"Parthenium argentatum\"). The term gum rubber is sometimes applied to the tree-obtained version of natural rubber in order to distinguish it from the synthetic version.\n\nThe first use of rubber was by the indigenous cultures of Mesoamerica. The earliest archeological evidence of the use of natural latex from the \"Hevea\" tree comes from the Olmec culture, in which rubber was first used for making balls for the Mesoamerican ballgame. Rubber was later used by the Maya and Aztec cultures – in addition to making balls Aztecs used rubber for other purposes such as making containers and to make textiles waterproof by impregnating them with the latex sap.\n\nThe Pará rubber tree is indigenous to South America. Charles Marie de La Condamine is credited with introducing samples of rubber to the \"Académie Royale des Sciences\" of France in 1736. In 1751, he presented a paper by François Fresneau to the Académie (published in 1755) that described many of rubber's properties. This has been referred to as the first scientific paper on rubber. In England, Joseph Priestley, in 1770, observed that a piece of the material was extremely good for rubbing off pencil marks on paper, hence the name \"rubber\". It slowly made its way around England. In 1764 François Fresnau discovered that turpentine was a rubber solvent. Giovanni Fabbroni is credited with the discovery of naphtha as a rubber solvent in 1779.\n\nSouth America remained the main source of the limited amounts of latex rubber used during much of the 19th century. The trade was heavily protected and exporting seeds from Brazil was a capital offense, although no law prohibited it. Nevertheless, in 1876, Henry Wickham smuggled 70,000 Pará rubber tree seeds from Brazil and delivered them to Kew Gardens, England. Only 2,400 of these germinated. Seedlings were then sent to India, British Ceylon (Sri Lanka), Dutch East Indies (Indonesia), Singapore, and British Malaya. Malaya (now Peninsular Malaysia) was later to become the biggest producer of rubber.\n\nIn the early 1900s, the Congo Free State in Africa was also a significant source of natural rubber latex, mostly gathered by forced labor. King Leopold II's colonial state brutally enforced production quotas. Tactics to enforce the rubber quotas included removing the hands of victims to prove they had been killed. Soldiers often came back from raids with baskets full of chopped-off hands. Villages that resisted were razed to encourage better compliance locally. See Atrocities in the Congo Free State for more information on the rubber trade in the Congo Free State in the late 1800s and early 1900s. Liberia and Nigeria started production.\n\nIn India, commercial cultivation was introduced by British planters, although the experimental efforts to grow rubber on a commercial scale were initiated as early as 1873 at the Calcutta Botanical Gardens. The first commercial \"Hevea\" plantations were established at Thattekadu in Kerala in 1902. In later years the plantation expanded to Karnataka, Tamil Nadu and the Andaman and Nicobar Islands of India. India today is the world's 3rd largest producer and 4th largest consumer.\n\nIn Singapore and Malaya, commercial production was heavily promoted by Sir Henry Nicholas Ridley, who served as the first Scientific Director of the Singapore Botanic Gardens from 1888 to 1911. He distributed rubber seeds to many planters and developed the first technique for tapping trees for latex without causing serious harm to the tree. Because of his fervent promotion of this crop, he is popularly remembered by the nickname \"Mad Ridley\".\n\nCharles Goodyear developed vulcanization in 1839, although Mesoamericans used stabilized rubber for balls and other objects as early as 1600 BC.\n\nBefore World War II significant uses included door and window profiles, hoses, belts, gaskets, matting, flooring and dampeners (antivibration mounts) for the automotive industry. The use of rubber in car tires (initially solid rather than pneumatic) in particular consumed a significant amount of rubber. Gloves (medical, household and industrial) and toy balloons were large consumers of rubber, although the type of rubber used is concentrated latex. Significant tonnage of rubber was used as adhesives in many manufacturing industries and products, although the two most noticeable were the paper and the carpet industries. Rubber was commonly used to make rubber bands and pencil erasers.\n\nRubber produced as a fiber, sometimes called 'elastic', had significant value to the textile industry because of its excellent elongation and recovery properties. For these purposes, manufactured rubber fiber was made as either an extruded round fiber or rectangular fibers cut into strips from extruded film. Because of its low dye acceptance, feel and appearance, the rubber fiber was either covered by yarn of another fiber or directly woven with other yarns into the fabric. Rubber yarns were used in foundation garments. While rubber is still used in textile manufacturing, its low tenacity limits its use in lightweight garments because latex lacks resistance to oxidizing agents and is damaged by aging, sunlight, oil and perspiration. The textile industry turned to neoprene (polymer of chloroprene), a type of synthetic rubber, as well as another more commonly used elastomer fiber, spandex (also known as elastane), because of their superiority to rubber in both strength and durability.\n\nRubber exhibits unique physical and chemical properties. Rubber's stress–strain behavior exhibits the Mullins effect and the Payne effect and is often modeled as hyperelastic. Rubber strain crystallizes.\n\nDue to the presence of weakened allylic C-H bonds in each repeat unit, natural rubber is susceptible to vulcanisation as well as being sensitive to ozone cracking.\n\nThe two main solvents for rubber are turpentine and naphtha (petroleum). Because rubber does not dissolve easily, the material is finely divided by shredding prior to its immersion.\n\nAn ammonia solution can be used to prevent the coagulation of raw latex.\n\nRubber begins to melt at approximately .\n\nOn a microscopic scale, relaxed rubber is a disorganized cluster of erratically changing wrinkled chains. In stretched rubber, the chains are almost linear. The restoring force is due to the preponderance of wrinkled conformations over more linear ones. For the quantitative treatment see ideal chain, for more examples see entropic force.\n\nCooling below the glass transition temperature permits local conformational changes but a reordering is practically impossible because of the larger energy barrier for the concerted movement of longer chains. \"Frozen\" rubber's elasticity is low and strain results from small changes of bond lengths and angles: this caused the \"Challenger\" disaster, when the American Space Shuttle's flattened o-rings failed to relax to fill a widening gap. The glass transition is fast and reversible: the force resumes on heating.\n\nThe parallel chains of stretched rubber are susceptible to crystallization. This takes some time because turns of twisted chains have to move out of the way of the growing crystallites. Crystallization has occurred, for example, when, after days, an inflated toy balloon is found withered at a relatively large remaining volume. Where it is touched, it shrinks because the temperature of the hand is enough to melt the crystals.\n\nVulcanization of rubber creates di- and polysulfide bonds between chains, which limits the degrees of freedom and results in chains that tighten more quickly for a given strain, thereby increasing the elastic force constant and making the rubber harder and less extensible.\n\nRaw rubber storage depots and rubber processing can produce malodour that is serious enough to become a source of complaints and protest to those living in the vicinity.\n\nMicrobial impurities originate during the processing of block rubber. These impurities break down during storage or thermal degradation and produce volatile organic compounds. Examination of these compounds using gas chromatography/mass spectrometry (GC/MS) and gas chromatography (GC) indicates that they contain sulphur, ammonia, alkenes, ketones, esters, hydrogen sulphite, nitrogen, and low molecular weight fatty acids (C2-C5).\n\nWhen latex concentrate is produced from rubber, sulphuric acid is used for coagulation. This produces malodourous hydrogen sulphide.\n\nThe industry can mitigate these bad odours with scrubber systems.\n\nLatex is the polymer cis-1,4-polyisoprene – with a molecular weight of 100,000 to 1,000,000 daltons. Typically, a small percentage (up to 5% of dry mass) of other materials, such as proteins, fatty acids, resins, and inorganic materials (salts) are found in natural rubber. Polyisoprene can also be created synthetically, producing what is sometimes referred to as \"synthetic natural rubber\", but the synthetic and natural routes are different. Some natural rubber sources, such as gutta-percha, are composed of trans-1,4-polyisoprene, a structural isomer that has similar properties.\n\nNatural rubber is an elastomer and a thermoplastic. Once the rubber is vulcanized, it is a thermoset. Most rubber in everyday use is vulcanized to a point where it shares properties of both; i.e., if it is heated and cooled, it is degraded but not destroyed.\n\nThe final properties of a rubber item depend not just on the polymer, but also on modifiers and fillers, such as carbon black, factice, whiting and others.\n\nRubber particles are formed in the cytoplasm of specialized latex-producing cells called laticifers within rubber plants. Rubber particles are surrounded by a single phospholipid membrane with hydrophobic tails pointed inward. The membrane allows biosynthetic proteins to be sequestered at the surface of the growing rubber particle, which allows new monomeric units to be added from outside the biomembrane, but within the lacticifer. The rubber particle is an enzymatically active entity that contains three layers of material, the rubber particle, a biomembrane and free monomeric units. The biomembrane is held tightly to the rubber core due to the high negative charge along the double bonds of the rubber polymer backbone. Free monomeric units and conjugated proteins make up the outer layer. The rubber precursor is isopentenyl pyrophosphate (an allylic compound), which elongates by Mg-dependent condensation by the action of rubber transferase. The monomer adds to the pyrophosphate end of the growing polymer. The process displaces the terminal high-energy pyrophosphate. The reaction produces a cis polymer. The initiation step is catalyzed by prenyltransferase, which converts three monomers of isopentenyl pyrophosphate into farnesyl pyrophosphate. The farnesyl pyrophosphate can bind to rubber transferase to elongate a new rubber polymer.\n\nThe required isopentenyl pyrophosphate is obtained from the mevalonate pathway, which derives from acetyl-CoA in the cytosol. In plants, isoprene pyrophosphate can also be obtained from the 1-deox-D-xyulose-5-phosphate/2-C-methyl-D-erythritol-4-phosphate pathway within plasmids. The relative ratio of the farnesyl pyrophosphate initiator unit and isoprenyl pyrophosphate elongation monomer determines the rate of new particle synthesis versus elongation of existing particles. Though rubber is known to be produced by only one enzyme, extracts of latex host numerous small molecular weight proteins with unknown function. The proteins possibly serve as cofactors, as the synthetic rate decreases with complete removal.\n\nClose to 28 million tons of rubber were produced in 2013, of which approximately 44% was natural. Since the bulk is synthetic, which is derived from petroleum, the price of natural rubber is determined, to a large extent, by the prevailing global price of crude oil. Asia was the main source of natural rubber, accounting for about 94% of output in 2005. The three largest producers, Thailand, Indonesia (2.4 million tons) and Malaysia, together account for around 72% of all natural rubber production. Natural rubber is not cultivated widely in its native continent of South America due to the existence of South American leaf blight, and other natural predators.\n\nRubber latex is extracted from rubber trees. The economic life period of rubber trees in plantations is around 32 years — up to 7 years of immature phase and about 25 years of productive phase.\n\nThe soil requirement is well-drained, weathered soil consisting of laterite, lateritic types, sedimentary types, nonlateritic red or alluvial soils.\n\nThe climatic conditions for optimum growth of rubber trees are:\n\nMany high-yielding clones have been developed for commercial planting. These clones yield more than 2,000 kg of dry rubber per hectare per year, under ideal conditions.\n\nIn places such as Kerala and Sri Lanka where coconuts are in abundance, the half shell of coconut was used as the latex collection container. Glazed pottery or aluminium or plastic cups became more common in Kerala and other countries. The cups are supported by a wire that encircles the tree. This wire incorporates a spring so it can stretch as the tree grows. The latex is led into the cup by a galvanised \"spout\" knocked into the bark. Tapping normally takes place early in the morning, when the internal pressure of the tree is highest. A good tapper can tap a tree every 20 seconds on a standard half-spiral system, and a common daily \"task\" size is between 450 and 650 trees. Trees are usually tapped on alternate or third days, although many variations in timing, length and number of cuts are used. \"Tappers would make a slash in the bark with a small hatchet. These slanting cuts allowed latex to flow from ducts located on the exterior or the inner layer of bark (cambium) of the tree. Since the cambium controls the growth of the tree, growth stops if it is cut. Thus, rubber tapping demanded accuracy, so that the incisions would not be too many given the size of the tree, or too deep, which could stunt its growth or kill it.\"\n\nIt is usual to tap a pannel at least twice, sometimes three times, during the tree's life. The economic life of the tree depends on how well the tapping is carried out, as the critical factor is bark consumption. A standard in Malaysia for alternate daily tapping is 25 cm (vertical) bark consumption per year. The latex-containing tubes in the bark ascend in a spiral to the right. For this reason, tapping cuts usually ascend to the left to cut more tubes.\n\nThe trees drip latex for about four hours, stopping as latex coagulates naturally on the tapping cut, thus blocking the latex tubes in the bark. Tappers usually rest and have a meal after finishing their tapping work, then start collecting the liquid \"field latex\" at about midday.\n\nThe four types of field coagula are \"cuplump\", \"treelace\", \"smallholders' lump\" and \"earth scrap\". Each has significantly different properties. Some trees continue to drip after the collection leading to a small amount of \"cup lump\" that is collected at the next tapping. The latex that coagulates on the cut is also collected as \"tree lace\". Tree lace and cup lump together account for 10–20% of the dry rubber produced. Latex that drips onto the ground, \"earth scrap\", is also collected periodically for processing of low-grade product.\n\nCup lump is the coagulated material found in the collection cup when the tapper next visits the tree to tap it again. It arises from latex clinging to the walls of the cup after the latex was last poured into the bucket, and from late-dripping latex exuded before the latex-carrying vessels of the tree become blocked. It is of higher purity and of greater value than the other three types.\n\nTree lace is the coagulum strip that the tapper peels off the previous cut before making a new cut. It usually has higher copper and manganese contents than cup lump. Both copper and manganese are pro-oxidants and can damage the physical properties of the dry rubber.\n\nSmallholders' lump is produced by smallholders who collect rubber from trees far from the nearest factory. Many Indonesian smallholders, who farm paddies in remote areas, tap dispersed trees on their way to work in the paddy fields and collect the latex (or the coagulated latex) on their way home. As it is often impossible to preserve the latex sufficiently to get it to a factory that processes latex in time for it to be used to make high quality products, and as the latex would anyway have coagulated by the time it reached the factory, the smallholder will coagulate it by any means available, in any container available. Some smallholders use small containers, buckets etc., but often the latex is coagulated in holes in the ground, which are usually lined with plastic sheeting. Acidic materials and fermented fruit juices are used to coagulate the latex — a form of assisted biological coagulation. Little care is taken to exclude twigs, leaves, and even bark from the lumps that are formed, which may also include tree lace.\n\nEarth scrap is material that gathers around the base of the tree. It arises from latex overflowing from the cut and running down the bark, from rain flooding a collection cup containing latex, and from spillage from tappers' buckets during collection. It contains soil and other contaminants, and has variable rubber content, depending on the amount of contaminants. Earth scrap is collected by field workers two or three times a year and may be cleaned in a scrap-washer to recover the rubber, or sold to a contractor who cleans it and recovers the rubber. It is of low quality.\n\nLatex coagulates in the cups if kept for long and must be collected before this happens. The collected latex, \"field latex\", is transferred into coagulation tanks for the preparation of dry rubber or transferred into air-tight containers with sieving for ammoniation. Ammoniation preserves the latex in a colloidal state for longer periods of time.\n\nLatex is generally processed into either latex concentrate for manufacture of dipped goods or coagulated under controlled, clean conditions using formic acid. The coagulated latex can then be processed into the higher-grade, technically specified block rubbers such as SVR 3L or SVR CV or used to produce Ribbed Smoke Sheet grades.\n\nNaturally coagulated rubber (cup lump) is used in the manufacture of TSR10 and TSR20 grade rubbers. Processing for these grades is a size reduction and cleaning process to remove contamination and prepare the material for the final stage of drying.\n\nThe dried material is then baled and palletized for storage and shipment.\n\nNatural rubber is often vulcanized, a process by which the rubber is heated and sulfur, peroxide or bisphenol are added to improve resistance and elasticity and to prevent it from perishing. Before World War II, carbon black was often used as an additive to rubber to improve its strength, especially in vehicle tires.\n\nNatural rubber latex is shipped from factories in south-west Asia, South America, and West and Center Africa to destinations around the world. As the cost of natural rubber has risen significantly and rubber products are dense, the shipping methods offering the lowest cost per unit weight are preferred. Depending on destination, warehouse availability, and transportation conditions, some methods are preferred by certain buyers. In international trade, latex rubber is mostly shipped in 20-foot ocean containers. Inside the container, smaller containers are used to store the latex.\n\nUncured rubber is used for cements; for adhesive, insulating, and friction tapes; and for crepe rubber used in insulating blankets and footwear. Vulcanized rubber has many more applications. Resistance to abrasion makes softer kinds of rubber valuable for the treads of vehicle tires and conveyor belts, and makes hard rubber valuable for pump housings and piping used in the handling of abrasive sludge.\n\nThe flexibility of rubber is appealing in hoses, tires and rollers for devices ranging from domestic clothes wringers to printing presses; its elasticity makes it suitable for various kinds of shock absorbers and for specialized machinery mountings designed to reduce vibration. Its relative gas impermeability makes it useful in the manufacture of articles such as air hoses, balloons, balls and cushions. The resistance of rubber to water and to the action of most fluid chemicals has led to its use in rainwear, diving gear, and chemical and medicinal tubing, and as a lining for storage tanks, processing equipment and railroad tank cars. Because of their electrical resistance, soft rubber goods are used as insulation and for protective gloves, shoes and blankets; hard rubber is used for articles such as telephone housings, parts for radio sets, meters and other electrical instruments. The coefficient of friction of rubber, which is high on dry surfaces and low on wet surfaces, leads to its use for power-transmission belting and for water-lubricated bearings in deep-well pumps. Indian rubber balls or lacrosse balls are made of rubber.\n\nAround 25 million tonnes of rubber are produced each year, of which 30 percent is natural. The remainder is synthetic rubber derived from petrochemical sources. The top end of latex production results in latex products such as surgeons' gloves, condoms, balloons and other relatively high-value products. The mid-range which comes from the technically specified natural rubber materials ends up largely in tires but also in conveyor belts, marine products, windshield wipers and miscellaneous goods. Natural rubber offers good elasticity, while synthetic materials tend to offer better resistance to environmental factors such as oils, temperature, chemicals and ultraviolet light. \"Cured rubber\" is rubber that has been compounded and subjected to the vulcanisation process to create cross-links within the rubber matrix.\n\nSome people have a serious latex allergy, and exposure to natural latex rubber products such as latex gloves can cause anaphylactic shock. The antigenic proteins found in \"Hevea\" latex may be deliberately reduced (though not eliminated) through processing.\n\nLatex from non-\"Hevea\" sources, such as Guayule, can be used without allergic reaction by persons with an allergy to \"Hevea\" latex.\n\nSome allergic reactions are not to the latex itself, but from residues of chemicals used to accelerate the cross-linking process. Although this may be confused with an allergy to latex, it is distinct from it, typically taking the form of Type IV hypersensitivity in the presence of traces of specific processing chemicals.\n\nNatural rubber is susceptible to degradation by a wide range of bacteria.\nThe bacteria \"Streptomyces coelicolor\", \"Pseudomonas citronellolis\", and \"Nocardia\" spp. are capable of degrading vulcanized natural rubber.\n\n\n"}
{"id": "21830", "url": "https://en.wikipedia.org/wiki?curid=21830", "title": "Nature", "text": "Nature\n\nNature, in the broadest sense, is the natural, physical, or material world or universe. \"Nature\" can refer to the phenomena of the physical world, and also to life in general. The study of nature is a large, if not the only, part of science. Although humans are part of nature, human activity is often understood as a separate category from other natural phenomena.\n\nThe word \"nature\" is derived from the Latin word \"natura\", or \"essential qualities, innate disposition\", and in ancient times, literally meant \"birth\". \"Natura\" is a Latin translation of the Greek word \"physis\" (φύσις), which originally related to the intrinsic characteristics that plants, animals, and other features of the world develop of their own accord. The concept of nature as a whole, the physical universe, is one of several expansions of the original notion; it began with certain core applications of the word φύσις by pre-Socratic philosophers, and has steadily gained currency ever since. This usage continued during the advent of modern scientific method in the last several centuries.\n\nWithin the various uses of the word today, \"nature\" often refers to geology and wildlife. Nature can refer to the general realm of living plants and animals, and in some cases to the processes associated with inanimate objects—the way that particular types of things exist and change of their own accord, such as the weather and geology of the Earth. It is often taken to mean the \"natural environment\" or wilderness—wild animals, rocks, forest, and in general those things that have not been substantially altered by human intervention, or which persist despite human intervention. For example, manufactured objects and human interaction generally are not considered part of nature, unless qualified as, for example, \"human nature\" or \"the whole of nature\". This more traditional concept of natural things which can still be found today implies a distinction between the natural and the artificial, with the artificial being understood as that which has been brought into being by a human consciousness or a human mind. Depending on the particular context, the term \"natural\" might also be distinguished from the or the supernatural.\n\nEarth is the only planet known to support life, and its natural features are the subject of many fields of scientific research. Within the solar system, it is third closest to the sun; it is the largest terrestrial planet and the fifth largest overall. Its most prominent climatic features are its two large polar regions, two relatively narrow temperate zones, and a wide equatorial tropical to subtropical region. Precipitation varies widely with location, from several metres of water per year to less than a millimetre. 71 percent of the Earth's surface is covered by salt-water oceans. The remainder consists of continents and islands, with most of the inhabited land in the Northern Hemisphere.\n\nEarth has evolved through geological and biological processes that have left traces of the original conditions. The outer surface is divided into several gradually migrating tectonic plates. The interior remains active, with a thick layer of plastic mantle and an iron-filled core that generates a magnetic field. This iron core is composed of a solid inner phase, and a fluid outer phase. Convective motion in the core generates electric currents through dynamo action, and these, in turn, generate the geomagnetic field.\n\nThe atmospheric conditions have been significantly altered from the original conditions by the presence of life-forms, which create an ecological balance that stabilizes the surface conditions. Despite the wide regional variations in climate by latitude and other geographic factors, the long-term average global climate is quite stable during interglacial periods, and variations of a degree or two of average global temperature have historically had major effects on the ecological balance, and on the actual geography of the Earth.\n\nGeology is the science and study of the solid and liquid matter that constitutes the Earth. The field of geology encompasses the study of the composition, structure, physical properties, dynamics, and history of Earth materials, and the processes by which they are formed, moved, and changed. The field is a major academic discipline, and is also important for mineral and hydrocarbon extraction, knowledge about and mitigation of natural hazards, some Geotechnical engineering fields, and understanding past climates and environments.\n\nThe geology of an area evolves through time as rock units are deposited and inserted and deformational processes change their shapes and locations.\n\nRock units are first emplaced either by deposition onto the surface or intrude into the overlying rock. Deposition can occur when sediments settle onto the surface of the Earth and later lithify into sedimentary rock, or when as volcanic material such as volcanic ash or lava flows, blanket the surface. Igneous intrusions such as batholiths, laccoliths, dikes, and sills, push upwards into the overlying rock, and crystallize as they intrude.\n\nAfter the initial sequence of rocks has been deposited, the rock units can be deformed and/or metamorphosed. Deformation typically occurs as a result of horizontal shortening, horizontal extension, or side-to-side (strike-slip) motion. These structural regimes broadly relate to convergent boundaries, divergent boundaries, and transform boundaries, respectively, between tectonic plates.\n\nEarth is estimated to have formed 4.54 billion years ago from the solar nebula, along with the Sun and other planets. The moon formed roughly 20 million years later. Initially molten, the outer layer of the Earth cooled, resulting in the solid crust. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, most or all of which came from ice delivered by comets, produced the oceans and other water sources. The highly energetic chemistry is believed to have produced a self-replicating molecule around 4 billion years ago.\n\nContinents formed, then broke up and reformed as the surface of Earth reshaped over hundreds of millions of years, occasionally combining to make a supercontinent. Roughly 750 million years ago, the earliest known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia which broke apart about 540 million years ago, then finally Pangaea, which broke apart about 180 million years ago.\n\nDuring the Neoproterozoic era, freezing temperatures covered much of the Earth in glaciers and ice sheets. This hypothesis has been termed the \"Snowball Earth\", and it is of particular interest as it precedes the Cambrian explosion in which multicellular life forms began to proliferate about 530–540 million years ago.\n\nSince the Cambrian explosion there have been five distinctly identifiable mass extinctions. The last mass extinction occurred some 66 million years ago, when a meteorite collision probably triggered the extinction of the non-avian dinosaurs and other large reptiles, but spared small animals such as mammals. Over the past 66 million years, mammalian life diversified.\n\nSeveral million years ago, a species of small African ape gained the ability to stand upright. The subsequent advent of human life, and the development of agriculture and further civilization allowed humans to affect the Earth more rapidly than any previous life form, affecting both the nature and quantity of other organisms as well as global climate. By comparison, the Great Oxygenation Event, produced by the proliferation of algae during the Siderian period, required about 300 million years to culminate.\n\nThe present era is classified as part of a mass extinction event, the Holocene extinction event, the fastest ever to have occurred. Some, such as E. O. Wilson of Harvard University, predict that human destruction of the biosphere could cause the extinction of one-half of all species in the next 100 years. The extent of the current extinction event is still being researched, debated and calculated by biologists.\nThe Earth's atmosphere is a key factor in sustaining the ecosystem. The thin layer of gases that envelops the Earth is held in place by gravity. Air is mostly nitrogen, oxygen, water vapor, with much smaller amounts of carbon dioxide, argon, etc. The atmospheric pressure declines steadily with altitude. The ozone layer plays an important role in depleting the amount of ultraviolet (UV) radiation that reaches the surface. As DNA is readily damaged by UV light, this serves to protect life at the surface. The atmosphere also retains heat during the night, thereby reducing the daily temperature extremes.\n\nTerrestrial weather occurs almost exclusively in the lower part of the atmosphere, and serves as a convective system for redistributing heat. Ocean currents are another important factor in determining climate, particularly the major underwater thermohaline circulation which distributes heat energy from the equatorial oceans to the polar regions. These currents help to moderate the differences in temperature between winter and summer in the temperate zones. Also, without the redistributions of heat energy by the ocean currents and atmosphere, the tropics would be much hotter, and the polar regions much colder.\n\nWeather can have both beneficial and harmful effects. Extremes in weather, such as tornadoes or hurricanes and cyclones, can expend large amounts of energy along their paths, and produce devastation. Surface vegetation has evolved a dependence on the seasonal variation of the weather, and sudden changes lasting only a few years can have a dramatic effect, both on the vegetation and on the animals which depend on its growth for their food.\n\nClimate is a measure of the long-term trends in the weather. Various factors are known to influence the climate, including ocean currents, surface albedo, greenhouse gases, variations in the solar luminosity, and changes to the Earth's orbit. Based on historical records, the Earth is known to have undergone drastic climate changes in the past, including ice ages.\n\nThe climate of a region depends on a number of factors, especially latitude. A latitudinal band of the surface with similar climatic attributes forms a climate region. There are a number of such regions, ranging from the tropical climate at the equator to the polar climate in the northern and southern extremes. Weather is also influenced by the seasons, which result from the Earth's axis being tilted relative to its orbital plane. Thus, at any given time during the summer or winter, one part of the Earth is more directly exposed to the rays of the sun. This exposure alternates as the Earth revolves in its orbit. At any given time, regardless of season, the northern and southern hemispheres experience opposite seasons.\n\nWeather is a chaotic system that is readily modified by small changes to the environment, so accurate weather forecasting is limited to only a few days. Overall, two things are happening worldwide: (1) temperature is increasing on the average; and (2) regional climates have been undergoing noticeable changes.\n\nWater is a chemical substance that is composed of hydrogen and oxygen and is vital for all known forms of life. In typical usage, \"water\" refers only to its liquid form or state, but the substance also has a solid state, ice, and a gaseous state, water vapor, or steam. Water covers 71% of the Earth's surface. On Earth, it is found mostly in oceans and other large bodies of water, with 1.6% of water below ground in aquifers and 0.001% in the air as vapor, clouds, and precipitation. Oceans hold 97% of surface water, glaciers, and polar ice caps 2.4%, and other land surface water such as rivers, lakes, and ponds 0.6%. Additionally, a minute amount of the Earth's water is contained within biological bodies and manufactured products.\n\nAn ocean is a major body of saline water, and a principal component of the hydrosphere. Approximately 71% of the Earth's surface (an area of some 361 million square kilometers) is covered by ocean, a continuous body of water that is customarily divided into several principal oceans and smaller seas. More than half of this area is over deep. Average oceanic salinity is around 35 parts per thousand (ppt) (3.5%), and nearly all seawater has a salinity in the range of 30 to 38 ppt. Though generally recognized as several 'separate' oceans, these waters comprise one global, interconnected body of salt water often referred to as the World Ocean or global ocean. This concept of a global ocean as a continuous body of water with relatively free interchange among its parts is of fundamental importance to oceanography.\n\nThe major oceanic divisions are defined in part by the continents, various archipelagos, and other criteria: these divisions are (in descending order of size) the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Southern Ocean, and the Arctic Ocean. Smaller regions of the oceans are called seas, gulfs, bays and other names. There are also salt lakes, which are smaller bodies of landlocked saltwater that are not interconnected with the World Ocean. Two notable examples of salt lakes are the Aral Sea and the Great Salt Lake.\n\nA lake (from Latin \"lacus\") is a terrain feature (or physical feature), a body of liquid on the surface of a world that is localized to the bottom of basin (another type of landform or terrain feature; that is, it is not global) and moves slowly if it moves at all. On Earth, a body of water is considered a lake when it is inland, not part of the ocean, is larger and deeper than a pond, and is fed by a river. The only world other than Earth known to harbor lakes is Titan, Saturn's largest moon, which has lakes of ethane, most likely mixed with methane. It is not known if Titan's lakes are fed by rivers, though Titan's surface is carved by numerous river beds. Natural lakes on Earth are generally found in mountainous areas, rift zones, and areas with ongoing or recent glaciation. Other lakes are found in endorheic basins or along the courses of mature rivers. In some parts of the world, there are many lakes because of chaotic drainage patterns left over from the last Ice Age. All lakes are temporary over geologic time scales, as they will slowly fill in with sediments or spill out of the basin containing them.\n\nA pond is a body of standing water, either natural or man-made, that is usually smaller than a lake. A wide variety of man-made bodies of water are classified as ponds, including water gardens designed for aesthetic ornamentation, fish ponds designed for commercial fish breeding, and solar ponds designed to store thermal energy. Ponds and lakes are distinguished from streams via current speed. While currents in streams are easily observed, ponds and lakes possess thermally driven micro-currents and moderate wind driven currents. These features distinguish a pond from many other aquatic terrain features, such as stream pools and tide pools.\n\nA river is a natural watercourse, usually freshwater, flowing toward an ocean, a lake, a sea or another river. In a few cases, a river simply flows into the ground or dries up completely before reaching another body of water. Small rivers may also be called by several other names, including stream, creek, brook, rivulet, and rill; there is no general rule that defines what can be called a river. Many names for small rivers are specific to geographic location; one example is \"Burn\" in Scotland and North-east England. Sometimes a river is said to be larger than a creek, but this is not always the case, due to vagueness in the language. A river is part of the hydrological cycle. Water within a river is generally collected from precipitation through surface runoff, groundwater recharge, springs, and the release of stored water in natural ice and snowpacks (i.e., from glaciers).\n\nA stream is a flowing body of water with a current, confined within a bed and stream banks. In the United States, a stream is classified as a watercourse less than wide. Streams are important as conduits in the water cycle, instruments in groundwater recharge, and they serve as corridors for fish and wildlife migration. The biological habitat in the immediate vicinity of a stream is called a riparian zone. Given the status of the ongoing Holocene extinction, streams play an important corridor role in connecting fragmented habitats and thus in conserving biodiversity. The study of streams and waterways in general involves many branches of inter-disciplinary natural science and engineering, including hydrology, fluvial geomorphology, aquatic ecology, fish biology, riparian ecology, and others.\n\nEcosystems are composed of a variety of abiotic and biotic components that function in an interrelated way. The structure and composition is determined by various environmental factors that are interrelated. Variations of these factors will initiate dynamic modifications to the ecosystem. Some of the more important components are: soil, atmosphere, radiation from the sun, water, and living organisms.\n\nCentral to the ecosystem concept is the idea that living organisms interact with every other element in their local environment. Eugene Odum, a founder of ecology, stated: \"Any unit that includes all of the organisms (ie: the \"community\") in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e.: exchange of materials between living and nonliving parts) within the system is an ecosystem.\" Within the ecosystem, species are connected and dependent upon one another in the food chain, and exchange energy and matter between themselves as well as with their environment. The human ecosystem concept is based on the human/nature dichotomy and the idea that all species are ecologically dependent on each other, as well as with the abiotic constituents of their biotope.\n\nA smaller unit of size is called a microecosystem. For example, a microsystem can be a stone and all the life under it. A \"macroecosystem\" might involve a whole ecoregion, with its drainage basin.\n\nWilderness is generally defined as areas that have not been significantly modified by human activity. Wilderness areas can be found in preserves, estates, farms, conservation preserves, ranches, , national parks, and even in urban areas along rivers, gulches, or otherwise undeveloped areas. Wilderness areas and protected parks are considered important for the survival of certain species, ecological studies, conservation, and solitude. Some nature writers believe wilderness areas are vital for the human spirit and creativity, and some ecologists consider wilderness areas to be an integral part of the Earth's self-sustaining natural ecosystem (the biosphere). They may also preserve historic genetic traits and that they provide habitat for wild flora and fauna that may be difficult or impossible to recreate in zoos, arboretums, or laboratories.\n\nAlthough there is no universal agreement on the definition of life, scientists generally accept that the biological manifestation of life is characterized by organization, metabolism, growth, adaptation, response to stimuli, and reproduction. Life may also be said to be simply the characteristic state of organisms.\n\nProperties common to terrestrial organisms (plants, animals, fungi, protists, archaea, and bacteria) are that they are cellular, carbon-and-water-based with complex organization, having a metabolism, a capacity to grow, respond to stimuli, and reproduce. An entity with these properties is generally considered life. However, not every definition of life considers all of these properties to be essential. Human-made analogs of life may also be considered to be life.\n\nThe biosphere is the part of Earth's outer shell – including land, surface rocks, water, air and the atmosphere – within which life occurs, and which biotic processes in turn alter or transform. From the broadest geophysiological point of view, the biosphere is the global ecological system integrating all living beings and their relationships, including their interaction with the elements of the lithosphere (rocks), hydrosphere (water), and atmosphere (air). The entire Earth contains over 75 billion tons (150 \"trillion\" pounds or about 6.8×10 kilograms) of biomass (life), which lives within various environments within the biosphere.\n\nOver nine-tenths of the total biomass on Earth is plant life, on which animal life depends very heavily for its existence. More than 2 million species of plant and animal life have been identified to date, and estimates of the actual number of existing species range from several million to well over 50 million. The number of individual species of life is constantly in some degree of flux, with new species appearing and others ceasing to exist on a continual basis. The total number of species is in rapid decline.\n\nThe origin of life on Earth is not well understood, but it is known to have occurred at least 3.5 billion years ago, during the hadean or archean eons on a primordial Earth that had a substantially different environment than is found at present. These life forms possessed the basic traits of self-replication and inheritable traits. Once life had appeared, the process of evolution by natural selection resulted in the development of ever-more diverse life forms.\n\nSpecies that were unable to adapt to the changing environment and competition from other life forms became extinct. However, the fossil record retains evidence of many of these older species. Current fossil and DNA evidence shows that all existing species can trace a continual ancestry back to the first primitive life forms.\n\nWhen basic forms of plant life developed the process of photosynthesis the sun's energy could be harvested to create conditions which allowed for more complex life forms. The resultant oxygen accumulated in the atmosphere and gave rise to the ozone layer. The incorporation of smaller cells within larger ones resulted in the development of yet more complex cells called eukaryotes. Cells within colonies became increasingly specialized, resulting in true multicellular organisms. With the ozone layer absorbing harmful ultraviolet radiation, life colonized the surface of Earth.\n\nThe first form of life to develop on the Earth were microbes, and they remained the only form of life until about a billion years ago when multi-cellular organisms began to appear. Microorganisms are single-celled organisms that are generally microscopic, and smaller than the human eye can see. They include Bacteria, Fungi, Archaea, and Protista.\n\nThese life forms are found in almost every location on the Earth where there is liquid water, including in the Earth's interior.\nTheir reproduction is both rapid and profuse. The combination of a high mutation rate and a horizontal gene transfer ability makes them highly adaptable, and able to survive in new environments, including outer space. They form an essential part of the planetary ecosystem. However, some microorganisms are pathogenic and can post health risk to other organisms.\n\nOriginally Aristotle divided all living things between plants, which generally do not move fast enough for humans to notice, and animals. In Linnaeus' system, these became the kingdoms Vegetabilia (later Plantae) and Animalia. Since then, it has become clear that the Plantae as originally defined included several unrelated groups, and the fungi and several groups of algae were removed to new kingdoms. However, these are still often considered plants in many contexts. Bacterial life is sometimes included in flora, and some classifications use the term \"bacterial flora\" separately from \"plant flora\".\n\nAmong the many ways of classifying plants are by regional floras, which, depending on the purpose of study, can also include \"fossil flora\", remnants<br> of plant life from a previous era. People in many regions and countries take great pride in their individual arrays of characteristic flora, which can vary widely across the globe due to differences in climate and terrain.\n\nRegional floras commonly are divided into categories such as \"native flora\" and \"agricultural and garden flora\", the lastly mentioned of which are intentionally grown and cultivated. Some types of \"native flora\" actually have been introduced centuries ago by people migrating from one region or continent to another, and become an integral part of the native, or natural flora of the place to which they were introduced. This is an example of how human interaction with nature can blur the boundary of what is considered nature.\n\nAnother category of plant has historically been carved out for \"weeds\". Though the term has fallen into disfavor among botanists as a formal way to categorize \"useless\" plants, the informal use of the word \"weeds\" to describe those plants that are deemed worthy of elimination is illustrative of the general tendency of people and societies to seek to alter or shape the course of nature. Similarly, animals are often categorized in ways such as \"domestic\", \"farm animals\", \"wild animals\", \"pests\", etc. according to their relationship to human life.\n\nAnimals as a category have several characteristics that generally set them apart from other living things. Animals are eukaryotic and usually multicellular (although see Myxozoa), which separates them from bacteria, archaea, and most protists. They are heterotrophic, generally digesting food in an internal chamber, which separates them from plants and algae. They are also distinguished from plants, algae, and fungi by lacking cell walls.\n\nWith a few exceptions—most notably the two phyla consisting of sponges and placozoans—animals have bodies that are differentiated into tissues. These include muscles, which are able to contract and control locomotion, and a nervous system, which sends and processes signals. There is also typically an internal digestive chamber. The eukaryotic cells possessed by all animals are surrounded by a characteristic extracellular matrix composed of collagen and elastic glycoproteins. This may be calcified to form structures like shells, bones, and spicules, a framework upon which cells can move about and be reorganized during development and maturation, and which supports the complex anatomy required for mobility.\n\nAlthough humans comprise only a minuscule proportion of the total living biomass on Earth, the human effect on nature is disproportionately large. Because of the extent of human influence, the boundaries between what humans regard as nature and \"made environments\" is not clear cut except at the extremes. Even at the extremes, the amount of natural environment that is free of discernible human influence is diminishing at an increasingly rapid pace.\n\nThe development of technology by the human race has allowed the greater exploitation of natural resources and has helped to alleviate some of the risk from natural hazards. In spite of this progress, however, the fate of human civilization remains closely linked to changes in the environment. There exists a highly complex feedback loop between the use of advanced technology and changes to the environment that are only slowly becoming understood. Man-made threats to the Earth's natural environment include pollution, deforestation, and disasters such as oil spills. Humans have contributed to the extinction of many plants and animals.\n\nHumans employ nature for both leisure and economic activities. The acquisition of natural resources for industrial use remains a sizable component of the world's economic system. Some activities, such as hunting and fishing, are used for both sustenance and leisure, often by different people. Agriculture was first adopted around the 9th millennium BCE. Ranging from food production to energy, nature influences economic wealth.\n\nAlthough early humans gathered uncultivated plant materials for food and employed the medicinal properties of vegetation for healing, most modern human use of plants is through agriculture. The clearance of large tracts of land for crop growth has led to a significant reduction in the amount available of forestation and wetlands, resulting in the loss of habitat for many plant and animal species as well as increased erosion.\n\nBeauty in nature has historically been a prevalent theme in art and books, filling large sections of libraries and bookstores. That nature has been depicted and celebrated by so much art, photography, poetry, and other literature shows the strength with which many people associate nature and beauty. Reasons why this association exists, and what the association consists of, are studied by the branch of philosophy called aesthetics. Beyond certain basic characteristics that many philosophers agree about to explain what is seen as beautiful, the opinions are virtually endless. Nature and wildness have been important subjects in various eras of world history. An early tradition of landscape art began in China during the Tang Dynasty (618–907). The tradition of representing nature \"as it is\" became one of the aims of Chinese painting and was a significant influence in Asian art.\n\nAlthough natural wonders are celebrated in the Psalms and the Book of Job, wilderness portrayals in art became more prevalent in the 1800s, especially in the works of the Romantic movement. British artists John Constable and J. M. W. Turner turned their attention to capturing the beauty of the natural world in their paintings. Before that, paintings had been primarily of religious scenes or of human beings. William Wordsworth's poetry described the wonder of the natural world, which had formerly been viewed as a threatening place. Increasingly the valuing of nature became an aspect of Western culture. This artistic movement also coincided with the Transcendentalist movement in the Western world. A common classical idea of beautiful art involves the word mimesis, the imitation of nature. Also in the realm of ideas about beauty in nature is that the perfect is implied through perfect mathematical forms and more generally by patterns in nature. As David Rothenburg writes, \"The beautiful is the root of science and the goal of art, the highest possibility that humanity can ever hope to see\".\n\nSome fields of science see nature as matter in motion, obeying certain laws of nature which science seeks to understand. For this reason the most fundamental science is generally understood to be \"physics\" – the name for which is still recognizable as meaning that it is the study of nature.\n\nMatter is commonly defined as the substance of which physical objects are composed. It constitutes the observable universe. The visible components of the universe are now believed to compose only 4.9 percent of the total mass. The remainder is believed to consist of 26.8 percent cold dark matter and 68.3 percent dark energy. The exact arrangement of these components is still unknown and is under intensive investigation by physicists.\n\nThe behavior of matter and energy throughout the observable universe appears to follow well-defined physical laws. These laws have been employed to produce cosmological models that successfully explain the structure and the evolution of the universe we can observe. The mathematical expressions of the laws of physics employ a set of twenty physical constants that appear to be static across the observable universe. The values of these constants have been carefully measured, but the reason for their specific values remains a mystery.\n\nOuter space, also simply called \"space\", refers to the relatively empty regions of the universe outside the atmospheres of celestial bodies. \"Outer\" space is used to distinguish it from airspace (and terrestrial locations). There is no discrete boundary between the Earth's atmosphere and space, as the atmosphere gradually attenuates with increasing altitude. Outer space within the Solar System is called interplanetary space, which passes over into interstellar space at what is known as the heliopause.\n\nOuter space is sparsely filled with several dozen types of organic molecules discovered to date by microwave spectroscopy, blackbody radiation left over from the big bang and the origin of the universe, and cosmic rays, which include ionized atomic nuclei and various subatomic particles. There is also some gas, plasma and dust, and small meteors. Additionally, there are signs of human life in outer space today, such as material left over from previous manned and unmanned launches which are a potential hazard to spacecraft. Some of this debris re-enters the atmosphere periodically.\n\nAlthough the Earth is the only body within the solar system known to support life, evidence suggests that in the distant past the planet Mars possessed bodies of liquid water on the surface. For a brief period in Mars' history, it may have also been capable of forming life. At present though, most of the water remaining on Mars is frozen.\nIf life exists at all on Mars, it is most likely to be located underground where liquid water can still exist.\n\nConditions on the other terrestrial planets, Mercury and Venus, appear to be too harsh to support life as we know it. But it has been conjectured that Europa, the fourth-largest moon of Jupiter, may possess a sub-surface ocean of liquid water and could potentially host life.\n\nAstronomers have started to discover extrasolar Earth analogs – planets that lie in the habitable zone of space surrounding a star, and therefore could possibly host life as we know it.\n\nMedia:\nOrganizations:\nPhilosophy:\n\n"}
{"id": "188094", "url": "https://en.wikipedia.org/wiki?curid=188094", "title": "Nikolaas Tinbergen", "text": "Nikolaas Tinbergen\n\nNikolaas \"Niko\" Tinbergen (; ; 15 April 1907 – 21 December 1988) was a Dutch biologist and ornithologist who shared the 1973 Nobel Prize in Physiology or Medicine with Karl von Frisch and Konrad Lorenz for their discoveries concerning organization and elicitation of individual and social behavior patterns in animals. He is regarded as one of the founders of modern ethology, the study of animal behavior.\n\nIn 1951, he published \"The Study of Instinct\", an influential book on animal behaviour.\nIn the 1960s, he collaborated with filmmaker Hugh Falkus on a series of wildlife films, including \"The Riddle of the Rook\" (1972) and \"Signals for Survival\" (1969), which won the Italia prize in that year and the American blue ribbon in 1971.\n\nBorn in The Hague, Netherlands, he was one of five children of Dirk Cornelis Tinbergen and his wife Jeannette van Eek. His brother, Jan Tinbergen, won the first Bank of Sweden Prize in Economic Sciences in Memory of Alfred Nobel in 1969. They are the only siblings to each win a Nobel Prize. Another brother, Luuk Tinbergen was also a noted biologist.\n\nTinbergen's interest in nature manifested itself when he was young. He studied biology at Leiden University and was a prisoner of war during World War II. Tinbergen's experience as a prisoner of the Nazis led to some friction with longtime intellectual collaborator Konrad Lorenz, and it was several years before the two reconciled.\n\nAfter the war, Tinbergen moved to England, where he taught at the University of Oxford and was a fellow first at Merton College, Oxford and later at Wolfson College, Oxford. Several of his graduate students went on to become prominent biologists including Richard Dawkins, Marian Dawkins, Desmond Morris, Iain Douglas-Hamilton, and Tony Sinclair.\n\nIn 1951 Tinbergen's \"The Study of Instinct\" was published. Behavioural ecologists and evolutionary biologists still recognise the contribution this book offered the field of behavioural science studies. \"The Study of Instinct\" summarises Tinbergen's ideas on innate behavioural reactions in animals and the adaptiveness and evolutionary aspects of these behaviours. By behaviour, he means the total movements made by the intact animal; innate behaviour is that which is not changed by the learning process. The major question of the book is the role of internal and external stimuli in controlling the expression of behaviour.\n\nIn particular, he was interested in explaining 'spontaneous' behaviours: those that occurred in their complete form the first time they were performed and that seemed resistant to the effects of learning. He explains how behaviour can be considered a combination of these spontaneous behaviour patterns and as set series of reactions to particular stimuli. Behaviour is a reaction in that to a certain extent it is reliant on external stimuli, however it is also spontaneous since it is also dependent upon internal causal factors.\n\nHis model for how certain behavioural reactions are provoked was based on work by Konrad Lorenz. Lorenz postulated that for each instinctive act there is a specific energy which builds up in a reservoir in the brain. In this model, Lorenz envisioned a reservoir with a spring valve at its base that an appropriate stimulus could act on, much like a weight on a scale pan pulling against a spring and releasing the reservoir of energy, an action which would lead an animal to express the desired behaviour.\n\nTinbergen added complexity to this model, a model now known as Tinbergen's hierarchical model. He suggested that motivational impulses build up in nervous centres in the brain which are held in check by blocks. The blocks are removed by an innate releasing mechanism that allows the energy to flow to the next centre (each centre containing a block that needs to be removed) in a cascade until the behaviour is expressed. Tinbergen's model shows multiple levels of complexity and that related behaviours are grouped.\n\nAn example is in his experiments with foraging honey bees. He showed that honey bees show curiosity for yellow and blue paper models of flowers, and suggested that these were visual stimuli causing the buildup of energy in one specific centre. However, the bees rarely landed on the model flowers unless the proper odour was also applied. In this case, the chemical stimuli of the odour allowed the next link in the chain to be released, encouraging the bee to land. The final step was for the bee to insert its mouthparts into the flower and initiate suckling. Tinbergen envisioned this as concluding the reaction set for honey bee feeding behaviour.\n\nIn 1973 Tinbergen, along with Konrad Lorenz and Karl von Frisch, were awarded the Nobel Prize in Physiology or Medicine \"for their discoveries concerning organization and elicitation of individual and social behaviour patterns\". The award recognised their studies on genetically programmed behaviour patterns, their origins, maturation and their elicitation by key stimuli. In his Nobel Lecture, Tinbergen addressed the somewhat unconventional decision of the Nobel Foundation to award the prize for Physiology or Medicine to three men who had until recently been regarded as \"mere animal watchers\". Tinbergen stated that their revival of the \"watching and wondering\" approach to studying behaviour could indeed contribute to the relief of human suffering.\n\nThe studies performed by the trio on fish, insects and birds laid the foundation for further studies on the importance of specific experiences during critical periods of normal development, as well as the effects of abnormal psychosocial situations in mammals. At the time, these discoveries were stated to have caused \"a breakthrough in the understanding of the mechanisms behind various symptoms of psychiatric disease, such as anguish, compulsive obsession, stereotypic behaviour and catatonic posture\". Tinbergen’s contribution to these studies included the testing of the hypotheses of Lorenz/von Frisch by means of \"comprehensive, careful, and ingenious experiments\" as well as his work on supernormal stimuli. The work of Tinbergen during this time was also regarded as having possible implications for further research in child development and behaviour.\n\nHe also caused some intrigue by dedicating a large part of his acceptance speech to FM Alexander, originator of the Alexander technique, a method which investigates postural reflexes and responses in human beings.\n\nIn 1950 Tinbergen became member of the Royal Netherlands Academy of Arts and Sciences. He was elected a Fellow of the Royal Society (FRS) in 1962. He was also awarded the Godman-Salvin Medal in 1969 by the British Ornithologists' Union, and in 1973 received the Swammderdam Medal and Wilhelm Bölsche Medal (from the Genootschap ter bervordering van Natuur-, Genees- en Heelkunde of the University of Amsterdam and the Kosmos-Gesellschaft der Naturfreunde respectively).\n\nTinbergen described four questions he believed should be asked of any animal behaviour, which were:\n\n\nIn ethology and sociobiology, causation and ontogeny are summarised as the \"proximate mechanisms\", while adaptation and phylogeny are the \"ultimate mechanisms\". They are still considered as the cornerstone of modern ethology, sociobiology and transdisciplinarity in Human Sciences.\n\nA major body of Tinbergen's research focused on what he termed the supernormal stimulus. This was the concept that one could build an artificial object which was a stronger stimulus or releaser for an instinct than the object for which the instinct originally evolved. He constructed plaster eggs to see which a bird preferred to sit on, finding that they would select those that were larger, had more defined markings, or more saturated colour—and a dayglo-bright one with black polka dots would be selected over the bird's own pale, dappled eggs.\n\nTinbergen found that territorial male three-spined stickleback (a small freshwater fish) would attack a wooden fish model more vigorously than a real male if its underside was redder. He constructed cardboard dummy butterflies with more defined markings that male butterflies would try to mate with in preference to real females. The superstimulus, by its exaggerations, clearly delineated what characteristics were eliciting the instinctual response.\n\nAmong the modern works calling attention to Tinbergen's classic work is Deirdre Barrett's 2010 book, \"Supernormal Stimuli\".\n\nTinbergen applied his observational methods to the problems of autistic children. He recommended a \"holding therapy\" in which parents hold their autistic children for long periods of time while attempting to establish eye contact, even when a child resists the embrace. However, his interpretations of autistic behaviour, and the holding therapy that he recommended, lacked scientific support and the therapy is described as controversial and potentially abusive.\n\nSome of the publications of Tinbergen are:\n\nPublications about Tinbergen and his work:\n\nTinbergen was a member of the advisory committee to the Anti-Concorde Project and was also an atheist.\n\nTinbergen married Elisabeth Rutten and they had five children. Later in life he suffered depression and feared he might, like his brother Luuk, commit suicide. He was treated by his friend, whose ideas he had greatly influenced, John Bowlby. Tinbergen died on 21 December 1988, after suffering a stroke at his home in Oxford, England.\n"}
{"id": "4224324", "url": "https://en.wikipedia.org/wiki?curid=4224324", "title": "Origin of water on Earth", "text": "Origin of water on Earth\n\nThe origin of water on Earth, or the reason that there is clearly more liquid water on Earth than on the other rocky planets of the Solar System, is not completely understood. There exist numerous more or less mutually compatible hypotheses as to how water may have accumulated on Earth's surface over the past 4.5 billion years in sufficient quantity to form oceans.\n\nComets, trans-Neptunian objects, or water-rich meteoroids (protoplanets) from the outer reaches of the asteroid belt colliding with Earth may have brought water to the world's oceans. Asteroids may have been primarily responsible based on several studies, including measurements of the ratio of the hydrogen isotopes deuterium and protium, since similar percentage impurities as in carbon-rich chondrites were found in oceanic water, whereas previous measurement of the isotopes' concentrations in comets and trans-Neptunian objects correspond only slightly to water on Earth. In January 2018, researchers reported that two 4.5 billion-year-old meteorites found on Earth contained liquid water alongside a wide diversity of deuterium-poor organic matter.\n\nLarge-enough planetesimals were heated by the decay of aluminium-26. This could cause water to rise to the surface. Recent studies suggest that water with similar deuterium-to-hydrogen ratio was already available at the time of Earth's formation, as evidenced in ancient eucrite meteorites originating from the asteroid Vesta.\n\nThat Earth's water originated purely from comets is implausible, since a result of measurements of the isotope ratios of deuterium to protium (D/H ratio) in the four comets Halley, Hyakutake, Hale–Bopp, and 67P/Churyumov–Gerasimenko, by researchers such as David Jewitt, is approximately double that of oceanic water. What is, however, unclear is whether these comets are representative of those from the Kuiper belt. According to Alessandro Morbidelli, the largest part of today's water comes from protoplanets formed in the outer asteroid belt that plunged towards Earth, as indicated by the D/H proportions in carbon-rich chondrites. The water in carbon-rich chondrites point to a similar D/H ratio as oceanic water. Nevertheless, mechanisms have been proposed to suggest that the D/H-ratio of oceanic water may have increased significantly throughout Earth's history. Such a proposal is consistent with the possibility that a significant amount of the water on Earth was already present during the planet's early evolution.\n\nRecent measurements of the chemical composition of Moon rocks suggest that Earth was born with its water already present. Investigating lunar samples carried to Earth by the Apollo 15 and 17 missions found a deuterium-to-hydrogen ratio that matched the isotopic ratio in carbonaceous chondrites. The ratio is also similar to that found in water on Earth. The findings suggest a common source of water for both objects. This supports a theory that Jupiter temporarily migrated into the inner Solar System, destabilizing the orbits of water-rich carbonaceous chondrites. As a result, some of the bodies could have fallen inwards and become part of the raw material for making Earth and its neighbors. The discovery of water vapor out-gassing from Ceres provides related information on water-ice content of the asteroid belt.\n\nGradual \"dehydration melting\"—leakage of water stored in hydrate minerals of Earth's rocks—could have formed a portion of its water. Water may also have come from volcanism: water vapor in the atmosphere that originated in volcanic eruptions may have condensed to form rain, slowly filling Earth's oceanic basins.\n\nA sizeable quantity of water would have been in the material that formed Earth. Water molecules would have escaped Earth's gravity more easily when it was less massive during its formation. Hydrogen and helium are expected to leak from the atmosphere continually, but the lack of denser noble gases in the modern atmosphere suggests that something disastrous happened to the early atmosphere.\n\nPart of the young planet is theorized to have been disrupted by the impact which created the Moon, which should have caused melting of one or two large areas. Present composition does not match complete melting and it is hard to melt and mix huge rock masses completely. However, a fair fraction of material should have been vaporized by this impact, creating a rock-vapor atmosphere around the young planet. The rock vapor would have condensed within two thousand years, leaving behind hot volatiles which probably resulted in a heavy carbon dioxide atmosphere with hydrogen and water vapor. Liquid water oceans existed despite the surface temperature of because of the atmospheric pressure of the heavy CO atmosphere. As cooling continued, subduction and dissolving in ocean water removed most CO from the atmosphere but levels oscillated wildly as new surface and mantle cycles appeared.\n\nStudy of zircons has found that liquid water must have existed as long ago as 4.404 ± 0.008 Ga, very soon after the formation of Earth. This requires the presence of an atmosphere. The Cool early Earth theory covers a range from about 4.4 Ga to 4.0 Ga.\n\nIn fact, recent studies of zircons (in the fall of 2008) found in Australian Hadean rock hold minerals that point to the existence of plate tectonics as early as 4 billion years ago. If this holds true, the previous beliefs about the Hadean period are far from correct. That is, rather than a hot, molten surface and atmosphere full of carbon dioxide, Earth's surface would be very much like it is today. The action of plate tectonics traps vast amounts of carbon dioxide, thereby reducing greenhouse effects, and leading to a much cooler surface temperature, and the formation of solid rock, and possibly even life.\n\nSome terrestrial water may have had a biochemical origin, during the Great Oxygenation Event, via redox reactions and photosynthesis.\n\nIn the early 1930s, Cornelis van Niel discovered that sulfide-dependent chemoautotrophic bacteria (purple sulfur bacteria) fix carbon and synthesize water as a byproduct of a photosynthetic pathway using hydrogen sulfide and carbon dioxide:\n\nFew modern organisms use this method of photosynthesis, making their water contribution negligible. But on the hydrogen-sulfide-rich and oxygen-poor early Earth, a small but significant portion of Earth's water may have been synthesized biochemically through this pathway.\n\n"}
{"id": "33017018", "url": "https://en.wikipedia.org/wiki?curid=33017018", "title": "Outline of natural science", "text": "Outline of natural science\n\nThe following outline is provided as an overview of and topical guide to natural science:\n\nNatural science – a major branch of science that tries to explain, and predict, nature's phenomena based on empirical evidence. In natural science, hypothesis must be verified scientifically to be regarded as scientific theory. Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into 2 main branches: life science, and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.\n\nNatural science can be described as all of the following:\n\n\n\n\n\n\n\nHistory of natural science\n\n\n\n\n\n"}
{"id": "3759820", "url": "https://en.wikipedia.org/wiki?curid=3759820", "title": "Physis", "text": "Physis\n\nPhysis (Greek: \"phusis\") is a Greek theological, philosophical, and scientific term usually translated into English as \"nature\".\n\nThe term is central to Greek philosophy, and as a consequence to Western philosophy as a whole.\nIn pre-Socratic usage, \"phusis\" contrasted with \"nomos\" \"law, human convention\"\nSince Aristotle, however, the \"physical\" (the subject matter of \"physics\", properly \"natural things\") has more typically been juxtaposed to the \"metaphysical\".\n\nThe word φύσις is a verbal noun based on φύω \"to grow, to appear\" (cognate with English \"to be\"). In Homeric Greek it is used quite literally, of the manner of growth of a particular species of plant. \n\nIn pre-Socratic philosophy, beginning with Heraclitus, \"phusis\" in keeping with its etymology of \"growing, becoming\" is always used in the sense of the \"natural\" \"development\", although the focus might lie either with the origin, or the process, or the end result of the process. There is some evidence that by the 6th century BC, beginning with the Ionian School, the word could also be used \nin the comprehensive sense, as referring to \"\"all\" things\", as it were \"Nature\" in the sense of \"Universe\".\n\nIn the Sophist tradition, the term stood in opposition to \"nomos\" (), \"law\" or \"custom\", in the debate on which parts of human existence are natural, and which are due to convention. \nThe contrast of \"phisis\" vs. \"nomos\" could be applied to any subject, much like the modern contrast of \"nature vs. nurture\".\n\nIn book 10 of \"Laws\", Plato criticizes those who write works \"peri phuseōs\". The criticism is that such authors tend to focus on a purely \"naturalistic\" explanation of the world, ignoring the role of \"intention\" or \"technē\", and thus becoming prone to the error of naive atheism. Plato accuses even Hesiod of this, for the reason that the gods in Hesiod \"grow\" out of primordial entities after the physical universe had been established.\n\n\"Because those who use the term mean to say that nature is the first creative power; but if the soul turns out to be the primeval element, and not fire or air, then in the truest sense and beyond other things the soul may be said to exist \"by\" nature; and this would be true if you proved that the soul is older than the body, but not otherwise.\"\n\nAristotle sought out the definition of \"physis\" to prove that there was more than one definition of \"physis\", and more than one way to interpret nature. \"Though Aristotle retains the ancient sense of \"physis\" as growth, he insists that an adequate definition of \"physis\" requires the different perspectives of the four causes (aitia): material, efficient, formal, and final.\" Aristotle believed that nature itself contained its own source of matter (material), power/motion (efficiency), form, and end (final). A unique feature about Aristotle's definition of \"physis\" was his relationship between art and nature. Aristotle said that \"physis\" (nature) is dependent on techne (art). \"The critical distinction between art and nature concerns their different efficient causes: nature is its own source of motion, whereas techne always requires a source of motion outside itself.\" What Aristotle was trying to bring to light, was that art does not contain within itself its form or source of motion. Consider the process of an acorn becoming an oak tree. This is a natural process that has its own driving force behind it. There is no external force pushing this acorn to its final state, rather it is progressively developing towards one specific end (telos).\nThough φύσις was often used in Hellenistic philosophy, it is used only 14 times in the New Testament (10 of those in the writings of Paul). Its meaning varies throughout Paul's writings. One usage refers to the established or natural order of things, as in \"Romans 2:14\" where Paul writes \"For when Gentiles, who do not have the law, by \"nature\" do what the law requires, they are a law to themselves, even though they do not have the law.\" Another use of φύσις in the sense of \"natural order\" is \"Romans 1:26\" where he writes \"the men likewise gave up \"natural\" relations with women and were consumed with passion for one another\". In \"1 Corinthians 11:14\", Paul asks \"Does not nature itself teach you that if a man wears long hair it is a disgrace for him?\"\n\nThis use of φύσις as referring to a \"natural order\" in \"Romans 1:26\" and \"1 Corinthians 11:14\" may have been influenced by Stoicism. The Greek philosophers, including Aristotle and the Stoics are credited with distinguishing between man-made laws and a natural law of universal validity, but Gerhard Kittel states that the Stoic philosophers were not able to combine the concepts of νόμος (law) and φύσις (nature) to produce the concept of \"natural law\" in the sense that was made possible by Judeo-Christian theology.\n\nAs part of the Pauline theology of salvation by grace, Paul writes in \"Ephesians 2:3\" that \"we all once lived in the passions of our flesh, carrying out the desires of the body and the mind, and were by \"nature\" children of wrath, like the rest of mankind. In the next verse he writes, \"by grace you have been saved.\" \n\nTheologians of the early Christian period differed in the usage of this term. In Antiochene circles, it connoted the humanity or divinity of Christ conceived as a concrete set of characteristics or attributes. In Alexandrine thinking, it meant a concrete individual or independent existent and approximated to hypostasis without being a synonym. While it refers to much the same thing as ousia it is more empirical and descriptive focussing on function while ousia is metaphysical and focuses more on reality. Although found in the context of the Trinitarian debate, it is chiefly important in the Christology of Cyril of Alexandria.\n\nThe Greek adjective \"phusikos\" is represented in various forms in modern English:\nAs \"physics\" \"the study of nature\", as \"physical\" (via Middle Latin \"physicalis\") referring both to physics (the study of nature, the material universe) and to the human body. The term physiology (\"physiologia\") is of 16th-century coinage (Jean Fernel). The term \"physique\", for \"the bodily constitution of a person\", is a 19th-century loan from French. \n\nIn medicine the suffix \"-physis\" occurs in such compounds as \"symphysis\", \"epiphysis\", and a few others, in the sense of \"a growth\". The physis also refers to the \"growth plate\", or site of growth at the end of long bones.\n\n"}
{"id": "169115", "url": "https://en.wikipedia.org/wiki?curid=169115", "title": "Preternatural", "text": "Preternatural\n\nThe preternatural or praeternatural is that which appears outside or beside (Latin \"\") the natural. It is \"suspended between the mundane and the miraculous\".\n\nIn theology, the term is often used to distinguish marvels or deceptive trickery, often attributed to witchcraft or demons, from the purely divine power of the genuinely supernatural to violate the laws of nature. In the early modern period the term was used by scientists to refer to abnormalities and strange phenomena of various kinds that seemed to depart from the norms of nature.\n\nMedieval theologians made a clear distinction between the natural, the preternatural and the supernatural. Thomas Aquinas argued that the supernatural consists in \"God’s unmediated actions\"; the natural is \"what happens always or most of the time\"; and the preternatural is \"what happens rarely, but nonetheless by the agency of created beings...Marvels belong, properly speaking, to the realm of the preternatural.\" Theologians, following Aquinas, argued that only God had the power to disregard the laws of nature that he has created, but that demons could manipulate the laws of nature by a form of trickery, to deceive the unwary into believing they had experienced real miracles. According to historian Lorraine Daston,\n\nAlthough demons, astral intelligences, and other spirits might manipulate natural causes with superhuman dexterity and thereby work marvels, as mere creatures they could never transcend from the preternatural to the supernatural and work genuine miracles.\n\nBy the 16th century, the term \"preternatural\" was increasingly used to refer to demonic activity comparable to the use of magic by human adepts: The Devil, \"being a natural Magician … may perform many acts in ways above our knowledge, though not transcending our natural power.\" According to the philosophy of the time, preternatural phenomena were not contrary to divine law, but used hidden, or occult powers that violated the \"normal\" pattern of natural phenomena.\n\nWith the emergence of early modern science, the concept of the preternatural increasingly came to be used to refer to strange or abnormal phenomena that seemed to violate the normal working of nature, but which were not associated with magic and witchcraft. This was a development of the idea that preternatural phenomena were fake miracles. As Daston puts it, \"To simplify the historical sequence somewhat: first, preternatural phenomena were demonized and thereby incidentally naturalized; then the demons were deleted, leaving only the natural causes.\" The use of the term was especially common in medicine, for example in John Brown's \"A Compleat Treatise of Preternatural Tumours\" (1678), or William Smellie's \"A Collection of Preternatural Cases and Observations in Midwifery\" (1754).\n\nIn the 19th century the term was appropriated in anthropology to refer to folk beliefs about fairies, trolls and other such creatures which were not thought of as demonic, but which were perceived to affect the natural world in unpredictable ways. According to Thorstein Veblen, such preternatural agents were often thought of as forces somewhere between supernatural beings and material processes. \"The preternatural agency is not necessarily conceived to be a personal agent in the full sense, but it is an agency which partakes of the attributes of personality to the extent of somewhat arbitrarily influencing the outcome of any enterprise, and especially of any contest.\"\n\nThe linguistic association between individual agents and unexplained or unfortunate circumstances remains. Many people attribute occurrences that are known to be material processes, such as \"gremlins in the engine\", a \"ghost in the machine\", or attributing motives to objects: \"the clouds are threatening\". The anthropomorphism in our daily life is a combination of the above cultural stems, as well as the manifestation of our pattern-projecting minds.\n\nIn 2011, Penn State Press began publishing a learned journal entitled \"Preternature: Critical and Historical Studies on the Preternatural\". Edited by Kirsten Uszkalo and Richard Raiswell, the journal is dedicated to publishing articles, reviews and short editions of original texts that deal with conceptions and perceptions of the preternatural in any culture and in any historical period. The journal covers \"magics, witchcraft, spiritualism, occultism, prophecy, monstrophy, demonology, and folklore.\"\n\n\n"}
{"id": "4286589", "url": "https://en.wikipedia.org/wiki?curid=4286589", "title": "Quarry-faced stone", "text": "Quarry-faced stone\n\nQuarry-faced stone is stone with a rough, unpolished surface, straight from the quarry. Many residential and public buildings in Jerusalem, Israel are built from quarry-faced Jerusalem stone.\n"}
{"id": "48191844", "url": "https://en.wikipedia.org/wiki?curid=48191844", "title": "Scandiobabingtonite", "text": "Scandiobabingtonite\n\nScandiobabingtonite was first discovered in the Montecatini granite quarry near Baveno, Italy in a pegmatite cavity. Though found in pegmatites, the crystals of scandiobabingtonite are sub-millimeter sized, and are tabular shaped. Scandiobabingtonite was the sixth naturally occurring mineral discovered with the rare earth element scandium, and grows around babingtonite, with which it is isostructural, hence the namesake. It is also referred to as scandian babingtonite. The ideal chemical formula for scandiobabingtonite is Ca(Fe,Mn)ScSiO(OH).\n\nScandiobabingtonite is found in association with orthoclase, quartz, light blue albite, stilbite, fluorite, and mica. When found with these minerals, the scandiobabingtonite crystals are emplanted on the surface of the other minerals. It also occurs as growth around green-black prismatic crystals of babingtonite. The samples of scandiobabingtonite that have been discovered have shown that they start out growing from a seed of babingtonite crystal. This is how scandiobabingtonite gets its chemical structure. The starting seed of babingtonite is still present in the center of the resulting crystal and can be detected with optical and chemical studies. Scandiobabingtonite is a uniquely rare mineral, as it occurs in very small amounts in few locations around the world. It is one of thirteen naturally occurring minerals where scandium is a dominant member. The other scandium minerals are bazzite, cascandite, hetftetjernite, jervisite, juonniite, kolbeckite, kristiansenite, magbasite, oftedalite, pretulite, thortveitite, and titanowodginite. Scandium can also concentrate in other minerals, such as in ferromagnesian minerals, aluminum phosphate minerals, meteoric minerals, and other minerals containing rare earth elements, but it occurs in trace amounts.\n\nScandiobabingtonite is a colorless or lightly gray-green colored transparent mineral with a glassy or vitreous luster. It exhibits a hardness of 6 on the Mohs hardness scale. Scandiobabingtonite occurs as short, prismatic crystals that are slightly elongated on the [001] axis which gives it a tabular or platy shape. Its crystals are characterized by the {010}, {001}, {110}, {1-10}, and {101} faces. Scandiobabingtonite is brittle and shows perfect cleavage along the {001} and {1-10} planes. The measured density is 3.24 g/cm.\n\nScandiobabingtonite is biaxial positive, which means it will refract light along two axes. It exhibits a 2V=64(2)°, strong dispersion with r>v, and displays strong pleochroism with colors ranging from pink (γ') to green(α'). The extinction angle along the (110) is 6°. Z:Φ=-250°, ρ=47°; Y:Φ=146°, ρ=75°; X:Φ=42°, ρ=47°.\n\nScandiobabingtonite is isostructural with babingtonite, and has the same chemical properties as well. It is an inosilicate with 5-periodic single chains. Scandium replaces the Fe in babingtonite in six-fold coordination. The empirical chemical formula for scandiobabingtonite is (Ca,Na)(Fe,Mn)(Sc,Sn,Fe)SiO(OH). Simplified, the formula is Ca(Fe,Mn)ScSiO(OH)\n\nScandiobabingtonite is in the triclinic crystal system, with space group P1. The unit cell dimensions are a=7.536(2) Å, b=11.734(2) Å, c=6.748(2) Å, α=91.70(2)°, β=93.86(2)°, γ=104.53(2)°. These dimensions are almost identical to those of babingtonite. The difference in dimensions is caused by the replacement of iron with scandium in the Fe-centered octahedra. The Fe-O distance measures as 2.048 Å, while the Sc-O distance is 2.092 Å. This equates to a slightly larger octahedra in scandiobabingtonite than babingtonite.\n\nList of Minerals\n"}
{"id": "41816267", "url": "https://en.wikipedia.org/wiki?curid=41816267", "title": "Seven Natural Wonders of Africa", "text": "Seven Natural Wonders of Africa\n\nThe Seven Natural Wonders of Africa was a competition where the seven were selected by voting on February 11, 2013.\n\n"}
{"id": "1335297", "url": "https://en.wikipedia.org/wiki?curid=1335297", "title": "Spaceship Earth", "text": "Spaceship Earth\n\nSpaceship Earth or Spacecraft Earth is a world view encouraging everyone on Earth to act as a harmonious crew working toward the greater good.\n\nThe earliest known use is a passage in Henry George's best known work, \"Progress and Poverty\" (1879).\nFrom book IV, chapter 2: \nIt is a well-provisioned ship, this on which we sail through space. If the bread and beef above decks seem to grow scarce, we but open a hatch and there is a new supply, of which before we never dreamed. And very great command over the services of others comes to those who as the hatches are opened are permitted to say, \"This is mine!\"\n\nGeorge Orwell later paraphrases Henry George in \"The Road to Wigan Pier\":\n\nThe world is a raft sailing through space with, potentially, plenty of provisions for everybody; the idea that we must all cooperate and see to it that everyone does his fair share of the work and gets his fair share of the provisions seems so blatantly obvious that one would say that no one could possibly fail to accept it unless he had some corrupt motive for clinging to the present system.\n\nIn 1965 Adlai Stevenson made a famous speech to the UN in which he said:\n\nWe travel together, passengers on a little space ship, dependent on its vulnerable reserves of air and soil; all committed for our safety to its security and peace; preserved from annihilation only by the care, the work, and, I will say, the love we give our fragile craft. We cannot maintain it half fortunate, half miserable, half confident, half despairing, half slave—to the ancient enemies of man—half free in a liberation of resources undreamed of until this day. No craft, no crew can travel safely with such vast contradictions. On their resolution depends the survival of us all.\n\nThe following year, \"Spaceship Earth\" became the title of a book by a friend of Stevenson's, the internationally influential economist Barbara Ward.\n\nAlso in 1966, Kenneth E. Boulding, who was influenced by reading Henry George, used the phrase in the title of an essay, \"The Economics of the Coming Spaceship Earth\". Boulding described the past open economy of apparently illimitable resources, which he said he was tempted to call the \"cowboy economy\", and continued: \"The closed economy of the future might similarly be called the 'spaceman' economy, in which the earth has become a single spaceship, without unlimited reservoirs of anything, either for extraction or for pollution, and in which, therefore, man must find his place in a cyclical ecological system\".\n\nThe phrase was also popularized by Buckminster Fuller, who published a book in 1968 under the title of \"Operating Manual for Spaceship Earth\". This quotation, referring to fossil fuels, reflects his approach: \n…we can make all of humanity successful through science's world-engulfing industrial evolution provided that we are not so foolish as to continue to exhaust in a split second of astronomical history the orderly energy savings of billions of years' energy conservation aboard our Spaceship Earth. These energy savings have been put into our Spaceship's life-regeneration-guaranteeing bank account for use only in self-starter functions.\n\nUnited Nations Secretary-General U Thant spoke of Spaceship Earth on Earth Day March 21, 1971 at the ceremony of the ringing of the Japanese Peace Bell: \"May there only be peaceful and cheerful Earth Days to come for our beautiful Spaceship Earth as it continues to spin and circle in frigid space with its warm and fragile cargo of animate life.\"\n\nSpaceship Earth is the name given to the 50 m diameter geodesic sphere that greets visitors at the entrance of Walt Disney World's Epcot theme park. Housed within the sphere is a dark ride that serves to explore the history of communications and promote Epcot's founding principles, \"[a] belief and pride in man's ability to shape a world that offers hope to people everywhere.\" A previous incarnation of the ride, narrated by actor Jeremy Irons and revised in 2008, was explicit in its message:\n\nLike a grand and miraculous spaceship, our planet has sailed through the universe of time, and for a brief moment, we have been among its many passengers….We now have the ability and the responsibility to build new bridges of acceptance and co-operation between us, to create a better world for ourselves and our children as we continue our amazing journey aboard Spaceship Earth.\n\nDavid Deutsch has pointed out that the picture of Earth as a friendly \"spaceship\" habitat is difficult to defend even in metaphorical sense. The Earth environment is harsh and survival is constant struggle for life, including whole species extinction. Humans wouldn't be able to live in most of the areas where they are living now without knowledge necessary to build life-support systems such as houses, heating, water supply, etc.\n\nThe term \"Spaceship Earth\" is frequently used on the labels of Emanuel Bronner's products to refer to the Earth.\n\n"}
{"id": "614763", "url": "https://en.wikipedia.org/wiki?curid=614763", "title": "Stark effect", "text": "Stark effect\n\nThe Stark effect is the shifting and splitting of spectral lines of atoms and molecules due to the presence of an external electric field. It is the electric-field analogue of the Zeeman effect, where a spectral line is split into several components due to the presence of the magnetic field. Although initially coined for the static case, it is also used in the wider context to describe effect of time-dependent electric fields. In particular, the Stark effect is responsible for the pressure broadening (Stark broadening) of spectral lines by charged particles in plasmas. For majority of spectral lines, the Stark effect is either linear (proportional to the applied electric field) or quadratic with a high accuracy.\n\nThe Stark effect can be observed both for emission and absorption lines. The latter is sometimes called the inverse Stark effect, but this term is no longer used in the modern literature.\n\nThe effect is named after the German physicist Johannes Stark, who discovered it in 1913. It was independently discovered in the same year by the Italian physicist Antonino Lo Surdo, and in Italy it is thus sometimes called the Stark–Lo Surdo effect. The discovery of this effect contributed importantly to the development of quantum theory and was rewarded with the Nobel Prize in Physics for Johannes Stark in the year 1919.\n\nInspired by the magnetic Zeeman effect, and especially by Lorentz's explanation of it, Woldemar Voigt performed classical mechanical calculations of quasi-elastically bound electrons in an electric field. By using experimental indices of refraction he gave an estimate of the Stark splittings. This estimate was a few orders of magnitude too low. Not deterred by this prediction, Stark undertook measurements on excited states of the hydrogen atom and succeeded in observing splittings.\n\nBy the use of the Bohr–Sommerfeld (\"old\") quantum theory, Paul Epstein and Karl Schwarzschild were independently able to derive equations for the linear and quadratic Stark effect in hydrogen. Four years later, Hendrik Kramers derived formulas for intensities of spectral transitions. Kramers also included the effect of fine structure, which includes corrections for relativistic kinetic energy and coupling between electron spin and orbital motion. The first quantum mechanical treatment (in the framework of Heisenberg's matrix mechanics) was by Wolfgang Pauli. Erwin Schrödinger discussed at length the Stark effect in his third paper on quantum theory (in which he introduced his perturbation theory), once in the manner of the 1916 work of Epstein (but generalized from the old to the new quantum theory) and once by his (first-order) perturbation approach.\nFinally, Epstein reconsidered the linear and quadratic Stark effect from the point of view of the new quantum theory. He derived equations for the line intensities which were a decided improvement over Kramers' results obtained by the old quantum theory.\n\nWhile first-order perturbation effects for the Stark effect in hydrogen are in agreement for the Bohr–Sommerfeld model and the quantum-mechanical theory of the atom, higher-order effects are not. Measurements of the Stark effect under high field strengths confirmed the correctness of the quantum theory over the Bohr model.\n\nAn electric field pointing from left to right, for example, tends to pull nuclei to the right and electrons to the left. In another way of viewing it, if an electronic state has its electron disproportionately to the left, its energy is lowered, while if it has the electron disproportionately to the right, its energy is raised.\n\nOther things being equal, the effect of the electric field is greater for outer electron shells, because the electron is more distant from the nucleus, so it travels farther left and farther right.\n\nThe Stark effect can lead to splitting of degenerate energy levels. For example, in the Bohr model, an electron has the same energy whether it is in the 2s state or any of the 2p states. However, in an electric field, there will be hybrid orbitals (also called quantum superpositions) of the 2s and 2p states where the electron tends to be to the left, which will acquire a lower energy, and other hybrid orbitals where the electron tends to be to the right, which will acquire a higher energy. Therefore, the formerly degenerate energy levels will split into slightly lower and slightly higher energy levels.\n\nThe Stark effect originates from the interaction between a charge distribution (atom or molecule) and an external electric field. Before turning to quantum mechanics we describe the interaction\nclassically and consider a continuous charge distribution ρ(r).\nIf this charge distribution is non-polarizable its interaction energy with an external electrostatic potential \"V\"(r) is \nIf the electric field is of macroscopic origin and the charge distribution is microscopic, it is reasonable to assume that the electric field is uniform over the charge distribution. That is, \"V\" is given by a two-term Taylor expansion,\nwhere we took the origin 0 somewhere within ρ.\nSetting \"V\"(0) as the zero energy, the interaction becomes\nHere we have introduced the dipole moment μ of ρ as an integral over the charge distribution. In case ρ consists of \"N\" point charges \"q\" this definition becomes a sum\n\nElectric-field perturbation applied to a classical hydrogen atom produces a distortion of the electron orbit in a direction perpendicular to the applied field. This effect can be shown without perturbation theory using the relation between the angular momentum and the Laplace–Runge–Lenz vector. Using the Laplace-Runge-Lenz approach, one can see both the transverse distortion and the usual Stark effect. The transverse distortion is not mentioned in most textbooks. This approach can also lead to an \"exactly solvable\" approximate model Hamiltonian for an atom in a strong oscillatory field. “There are few exactly-solvable problems in quantum mechanics, and even fewer with a time-dependent Hamiltonian.”\n\nTurning now to quantum mechanics an atom or a molecule can be thought of as a collection of point charges (electrons and nuclei), so that the second definition of the dipole applies. The interaction of atom or molecule with a uniform external field is described by the operator\nThis operator is used as a perturbation in first- and second-order perturbation theory to account for the first- and second-order Stark effect.\n\nLet the unperturbed atom or molecule be in a \"g\"-fold degenerate state with orthonormal zeroth-order state functions formula_7. (Non-degeneracy is the special case \"g\" = 1). According to perturbation theory the first-order energies are the eigenvalues of the \"g\" x \"g\" matrix with general element \nIf \"g\" = 1 (as is often the case for electronic states of molecules) the first-order energy becomes proportional to the expectation (average) value of the dipole operator formula_9,\n\nBecause a dipole moment is a polar vector, the diagonal elements of the perturbation matrix V vanish for systems with an inversion center (such as atoms). Molecules with an inversion center in a non-degenerate electronic state do not have a (permanent) dipole and hence do not show a linear Stark effect.\n\nIn order to obtain a non-zero matrix V for systems with an inversion center it is necessary that some of the unperturbed functions formula_11 have opposite parity (obtain plus and minus under inversion), because only functions of opposite parity give non-vanishing matrix elements. Degenerate zeroth-order states of opposite parity occur for excited hydrogen-like (one-electron) atoms or Rydberg states. Neglecting fine-structure effects, such a state with the principal quantum number \"n\" is \"n\"-fold degenerate and\nwhere formula_13 is the azimuthal (angular momentum) quantum number. For instance, the excited \"n\" = 4 state contains the following formula_14 states,\nThe one-electron states with even formula_14 are even under parity, while those with odd formula_14 are odd under parity. Hence hydrogen-like atoms with \"n\">1 show first-order Stark effect.\n\nThe first-order Stark effect occurs in rotational transitions of symmetric top molecules (but not for linear and asymmetric molecules). In first approximation a molecule may be seen as a rigid rotor. A symmetric top rigid rotor has the unperturbed eigenstates\nwith 2(2\"J\"+1)-fold degenerate energy for |K| > 0 and (2\"J\"+1)-fold degenerate energy for K=0.\nHere \"D\" is an element of the Wigner D-matrix. The first-order perturbation matrix on basis of the unperturbed rigid rotor function is non-zero and can be diagonalized. This gives shifts and splittings\nin the rotational spectrum. Quantitative analysis of these Stark shift yields the permanent electric dipole moment of the symmetric top molecule.\n\nAs stated, the quadratic Stark effect is described by second-order perturbation theory.\nThe zeroth-order eigenproblem\nis assumed to be solved. The perturbation theory gives\nwith the components of the polarizability tensor α defined by\nThe energy \"E\" gives the quadratic Stark effect.\n\nNeglecting the hyperfine structure (which is often justified — unless extremely weak electric fields are considered), the polarizability tensor of atoms is isotropic,\nFor some molecules this expression is a reasonable approximation, too.\n\nIt is important to note that for the ground state formula_23 is \"always\" positive, i.e., the quadratic Stark shift is always negative.\n\nThe perturbative treatment of the Stark effect has some problems. In the presence of an electric field, states of atoms and molecules that were previously bound (square-integrable), become formally (non-square-integrable) resonances of finite width.\nThese resonances may decay in finite time via field ionization. For low lying states and not too strong fields the decay times are so long, however, that for all practical purposes the system can be regarded as bound. For highly excited states and/or very strong fields ionization may have to be accounted for. (See also the article on the Rydberg atom).\n\nIn a semiconductor heterostructure, where a small bandgap material is sandwiched between two layers of a larger bandgap material, the Stark effect can be dramatically enhanced by bound excitons. This is because the electron and hole which form the exciton are pulled in opposite directions by the applied electric field, but they remain confined in the smaller bandgap material, so the exciton is not merely pulled apart by the field. The quantum-confined Stark effect is widely used for semiconductor-based optical modulators, particularly for optical fiber communications.\n\n\n\n"}
{"id": "3331283", "url": "https://en.wikipedia.org/wiki?curid=3331283", "title": "Suction cup", "text": "Suction cup\n\nA suction cup, also known as a sucker, is a device or object that uses the negative fluid pressure of air or water to adhere to nonporous surfaces, creating a partial vacuum.\n\nSuction cups are peripherial traits of some animals such as octopuses and squids, and have been reproduced artificially for numerous purposes.\n\nThe working face of the suction cup is made of elastic, flexible material and has a curved surface. When the center of the suction cup is pressed against a flat, non-porous surface, the volume of the space between the suction cup and the flat surface is reduced, which causes the air or water between the cup and the surface to be expelled past the rim of the circular cup. The cavity which develops between the cup and the flat surface has little to no air or water in it because most of the fluid has already been forced out of the inside of the cup, causing a lack of pressure. The pressure difference between the atmosphere on the outside of the cup and the low-pressure cavity on the inside of the cup keeps the cup adhered to the surface.\n\nWhen the user ceases to apply physical pressure to the outside of the cup, the elastic substance of which the cup is made tends to resume its original, curved shape. The length of time for which the suction effect can be maintained depends mainly on how long it takes for air or water to leak back into the cavity between the cup and the surface, equalizing the pressure with the surrounding atmosphere. This depends on the porosity and flatness of the surface and the properties of the cup's rim.\n\nThe force required to detach an ideal suction cup by pulling it directly away from the surface is given by the formula:\nwhere:\n\nThis is derived from the definition of pressure, which is:\nFor example, a suction cup of radius 2.0 cm has an area of formula_3(0.020 m) = 0.0013 square meters. Using the force formula (\"F\" = \"AP\"), the result is\n\"F\" = (0.0013 m)(100,000 Pa) = about 130 newtons.\n\nThe above formula relies on several assumptions:\n\nArtificial suction cups are believed to have first been used in the third century, B.C., and were made out of gourds. They were used to suction \"bad blood\" from internal organs to the surface. Hippocrates is believed to have invented this procedure.\n\nThe first modern suction cup patents were issued by the United States Patent and Trademark Office during the 1860s. TC Roche was awarded U.S. Patent No. 52,748 in 1866 for a \"Photographic Developer Dipping Stick\"; the patent discloses a primitive suction cup means for handling photographic plates during developing procedures. In 1868, Orwell Needham patented a more refined suction cup design, U.S. Patent No. 82,629, calling his invention an \"Atmospheric Knob\" purposed for general use as a handle and drawer opening means.\n\nSuction cups have a number of commercial and industrial applications:\n\nOn May 25, 1981, Dan Goodwin, a.k.a. SpiderDan, scaled Sears Tower, the former world's tallest building, with a pair of suction cups. He went on to scale the Renaissance Center in Dallas, the Bonaventure Hotel in Los Angeles, the World Trade Center in New York City, Parque Central Tower in Caracas, the Nippon TV station in Tokyo, and the Millennium Tower in San Francisco.\n\n"}
{"id": "54612063", "url": "https://en.wikipedia.org/wiki?curid=54612063", "title": "The Genius of Birds", "text": "The Genius of Birds\n\nThe Genius of Birds is a 2016 book by nature writer Jennifer Ackerman.\n\n\"The Genius of Birds\" highlights new findings and discoveries in the field of bird intelligence. The book explores birds as thinkers (contrary to the cliché \"bird brain\") in the context of observed behavior in the wild and brings to it the scientific findings from lab and field research.\n\nNew research suggests that some birds, such as those in the family corvidae, can rival primates and even humans in forms of intelligence. Much like humans, birds have enormous brains relative to the rest of their bodies.\n\nAckerman highlights the complex social structures of avian society. They are capable of abstract thinking, problem solving, recognizing faces, gift giving, sharing, grieving, and meaningful communication with humans. Ackerman goes in depth to highlight scientific studies that uncover behavior such as tool usage, speaking in regional accents, navigation, and theory of mind.\n\nThe book is a New York Times Bestseller, and was named one of the 10 best nonfiction books of 2016 by The Wall Street Journal.\n"}
{"id": "31880880", "url": "https://en.wikipedia.org/wiki?curid=31880880", "title": "Theoretical foundations of evolutionary psychology", "text": "Theoretical foundations of evolutionary psychology\n\nThe theoretical foundations of evolutionary psychology are the general and specific scientific theories that explain the ultimate origins of psychological traits in terms of evolution. These theories originated with Charles Darwin's work, including his speculations about the evolutionary origins of social instincts in humans. Modern evolutionary psychology, however, is possible only because of advances in evolutionary theory in the 20th century.\n\nEvolutionary psychologists say that natural selection has provided humans with many psychological adaptations, in much the same way that it generated humans' anatomical and physiological adaptations. As with adaptations in general, psychological adaptations are said to be specialized for the environment in which an organism evolved, the environment of evolutionary adaptedness, or EEA. Sexual selection provides organisms with adaptations related to mating. For male mammals, which have a relatively fast reproduction rate, sexual selection leads to adaptations that help them compete for females. For female mammals, with a relatively slow reproduction rate, sexual selection leads to choosiness, which helps females select higher quality mates. Charles Darwin described both natural selection and sexual selection, but he relied on group selection to explain the evolution of self-sacrificing behavior. Group selection is a weak explanation because in any group the less self-sacrificing animals will be more likely to survive and the group will become less self-sacrificing.\n\nIn 1964, William D. Hamilton proposed inclusive fitness theory, emphasizing a \"gene's-eye\" view of evolution. Hamilton noted that individuals can increase the replication of their genes into the next generation by helping close relatives with whom they share genes survive and reproduce. According to \"Hamilton's rule\", a self-sacrificing behavior can evolve if it helps close relatives so much that it more than compensates for the individual animal's sacrifice. Inclusive fitness theory resolved the issue of how \"altruism\" evolved. Other theories also help explain the evolution of altruistic behavior, including evolutionary game theory, tit-for-tat reciprocity, and generalized reciprocity. These theories not only help explain the development of altruistic behavior but also account for hostility toward cheaters (individuals that take advantage of others' altruism).\n\nSeveral mid-level evolutionary theories inform evolutionary psychology. The r/K selection theory proposes that some species prosper by having many offspring while others follow the strategy of having fewer offspring but investing much more in each one. Humans follow the second strategy. Parental investment theory explains how parents invest more or less in individual offspring based on how successful those offspring are likely to be, and thus how much they might improve the parents' inclusive fitness. According to the Trivers-Willard hypothesis, parents in good conditions tend to invest more in sons (who are best able to take advantage of good conditions), while parents in poor conditions tend to invest more in daughters (who are best able to have successful offspring even in poor conditions). According to life history theory, animals evolve life histories to match their environments, determining details such as age at first reproduction and number of offspring. Dual inheritance theory posits that genes and human culture have interacted, with genes affecting the development of culture and culture, in turn, affecting human evolution on a genetic level (see also the Baldwin effect).\n\nCritics of evolutionary psychology have sometimes challenged its theoretical underpinnings, saying that humans never developed powerful social instincts through natural selection and that the hypotheses of evolutionary psychologists are merely just-so-stories.\n\nEvolutionary psychology primarily uses the theories of natural selection, sexual selection, and inclusive fitness to explain the evolution of psychological adaptations.\n\nEvolutionary psychology is sometimes seen not simply as a subdiscipline of psychology but as a metatheoretical framework in which \"the entire field of psychology can be examined.\"\n\nEvolutionary psychologists consider Charles Darwin's theory of natural selection to be important to an understanding of psychology. Natural selection occurs because individual organisms who are genetically better suited to the current environment leave more descendants, and their genes spread through the population, thus explaining why organisms fit their environments so closely. This process is slow and cumulative, with new traits layered over older traits. The advantages created by natural selection are known as adaptations. Evolutionary psychologists say that animals, just as they evolve physical adaptations, evolve psychological adaptations.\n\nEvolutionary psychologists emphasize that natural selection mostly generates specialized adaptations, which are more efficient than general adaptations. They point out that natural selection operates slowly, and that adaptations are sometimes out of date when the environment changes rapidly. In the case of humans, evolutionary psychologists say that much of human nature was shaped during the stone age and may not match the contemporary environment.\n\nSexual selection favors traits that provide mating advantages, such as the peacock's tail, even if these same traits are usually hindrances. Evolutionary psychologists point out that, unlike natural selection, sexual selection typically leads to the evolution of sex differences. Sex differences typically make reproduction faster for one sex and slower for the other, in which case mates are relatively scarce for the faster sex. Sexual selection favors traits that increase the number of mates for the fast sex and the quality of mates for the slow sex. For mammals, the female has the slower reproduction rate. Males typically evolve either traits to help them fight other males or traits to impress females. Females typically evolve greater abilities to discern the qualities of males, such as choosiness in mating.\n\nInclusive fitness theory, proposed by William D. Hamilton, emphasized a \"gene's-eye\" view of evolution. Hamilton noted that what evolution ultimately selects are genes, not groups or species. From this perspective, individuals can increase the replication of their genes into the next generation not only directly via reproduction, by also indirectly helping close relatives with whom they share genes survive and reproduce. General evolutionary theory, in its modern form, \"is\" essentially inclusive fitness theory.\n\nInclusive fitness theory resolved the issue of how \"altruism\" evolved. The dominant, pre-Hamiltonian view was that altruism evolved via group selection: the notion that altruism evolved for the benefit of the group. The problem with this was that if one organism in a group incurred any fitness costs on itself for the benefit of others in the group, (i.e. acted \"altruistically\"), then that organism would reduce its own ability to survive and/or reproduce, therefore reducing its chances of passing on its altruistic traits.\n\nFurthermore, the organism that benefited from that altruistic act and only acted on behalf of its own fitness would increase its own chance of survival and/or reproduction, thus increasing its chances of passing on its \"selfish\" traits.\nInclusive fitness resolved \"the problem of altruism\" by demonstrating that altruism can evolve via kin selection as expressed in Hamilton's rule:\ncost < relatedness × benefit\nIn other words, altruism can evolve as long as the fitness \"cost\" of the altruistic act on the part of the actor is less than the \"degree of genetic relatedness\" of the recipient times the fitness \"benefit\" to that recipient.\nThis perspective reflects what is referred to as the gene-centered view of evolution and demonstrates that group selection is a very weak selective force.\n\nMiddle-level evolutionary theories are consistent with general evolutionary theory, but focus on certain domains of functioning (Buss, 2011) Specific evolutionary psychology hypotheses may be derivative from a mid-level theory (Buss, 2011). Three very important middle-level evolutionary theories were contributed by Robert Trivers as well as Robert MacArthur and E. O. Wilson\n\n"}
{"id": "40624269", "url": "https://en.wikipedia.org/wiki?curid=40624269", "title": "Trabecular cartilage", "text": "Trabecular cartilage\n\nTrabecular cartilages (trabeculae cranii, sometimes simply trabeculae, prechordal cartilages) are paired, rod-shaped cartilages, which develop in the head of the vertebrate embryo. They are the primordia of the anterior part of the cranial base, and are derived from the cranial neural crest cells.\n\nThe trabecular cartilages generally appear as a paired, rod-shaped cartilages at the ventral side of the forebrain and lateral side of the adenohypophysis in the vertebrate embryo. During development, their anterior ends fuse and form the \"trabecula communis\". Their posterior ends fuse with the caudal-most parachordal cartilages.\n\nMost skeletons are of mesodermal origin in vertebrates. Especially axial skeletal elements, such as the vertebrae, are derived from the paraxial mesoderm (e.g., somites), which is regulated by molecular signals from the notochord. Trabecular cartilages, however, originate from the neural crest, and since they are located anterior to the rostral tip of the notochord, they cannot receive signals from the notochord. Due to these specialisations, and their essential role in cranial development, many comparative morphologists and embryologists have argued their developmental or evolutionary origins. The general theory is that the trabecular cartilage is derived from the neural crest mesenchyme which fills anterior to the mandibular arch (premandibular domain).\n\nAs clearly seen in the lamprey, Cyclostome also has a pair of cartilaginous rods in the embryonic head which is similar to the trabecular cartilages in jawed vertebrates.\nHowever, in 1916, Alexej Nikolajevich Sewertzoff pointed out that the cranial base of the lamprey is exclusively originated from the paraxial mesoderm. Then in 1948, reported the detail of the skeletogenesis of the lamprey, and showed that the “trabecular cartilages” in lamprey appear just beside the notochord, in a similar position to the parachordal cartilages in jawed vertebrates. Recent experimental studies also showed that the cartilages are derived from the head mesoderm. The “trabecular cartilages” in the Cyclostome is no longer considered to be the homologue of the trabecular in the jawed vertebrates: the (true) trabecular cartilages were firstly acquired in the Gnathostome lineage.\n\nThe trabecular cartilages were first described in the grass snake by at 1839. In 1874, Thomas Henry Huxley suggested that the trabecular cartilages are a modified part of the splanchnocranium: they arose as the serial homologues of the pharyngeal arches.\n\nThe vertebrate jaw is generally thought to be the modification of the mandibular arch (1st pharyngeal arch). Since the trabecular cartilages appear anterior to the mandibular arch, if the trabecular cartilages are serial homologues of the pharyngeal arches, ancestral vertebrates should possess more than one pharyngeal arch (so-called \"premandibular arches\") anterior to the mandibular arch. The existence of premandibular arch(es) has been accepted by many comparative embryologists and morphologists (e.g., Edwin Stephen Goodrich, Gavin de Beer). Moreover, reported premandibular arches and the corresponding branchiomeric nerves by the reconstruction of the Osteostracans (e.g., \"Cephalaspis\"; recently this arch was reinterpreted as the mandibular arch)\n\nHowever, the existence of the premandibular arch(es) has been rejected, and the trabecular cartilages are no longer assumed to be one of the pharyngeal arches.\n\n"}
{"id": "1103359", "url": "https://en.wikipedia.org/wiki?curid=1103359", "title": "Ultra-high vacuum", "text": "Ultra-high vacuum\n\nUltra-high vacuum (UHV) is the vacuum regime characterised by pressures lower than about 10 pascal or 100 nanopascals (10 mbar, ~10 torr). UHV conditions are created by pumping the gas out of a UHV chamber. At these low pressures the mean free path of a gas molecule is greater than approximately 40 km, so the gas is in free molecular flow, and gas molecules will collide with the chamber walls many times before colliding with each other. Almost all molecular interactions therefore take place on various surfaces in the chamber.\n\nUHV conditions are integral to scientific research. Surface science experiments often require a chemically clean sample surface with the absence of any unwanted adsorbates. Surface analysis tools such as X-ray photoelectron spectroscopy and low energy ion scattering require UHV conditions for the transmission of electron or ion beams. For the same reason, beam pipes in particle accelerators such as the Large Hadron Collider are kept at UHV.\n\n\nMaintaining UHV conditions requires the use of unusual materials for equipment. Heating of the entire system above 100 °C for many hours (\"baking\") to remove water and other trace gases which adsorb on the surfaces of the chamber is required upon \"cycling\" the equipment to atmosphere. To save time, energy, and integrity of the UHV volume an \"interlock\" is often used. The interlock volume has one door or valve facing the UHV side of the volume, and another door against atmospheric pressure through which samples or workpieces are initially introduced. After sample introduction and assuring that the door against atmosphere is closed, the interlock volume is typically pumped down to a medium-high vacuum. In some cases the workpiece itself is baked out or otherwise pre-cleaned under this medium-high vacuum. The gateway to the UHV chamber is then opened, the workpiece transferred to the UHV by robotic means or by other contrivance if necessary, and the UHV valve re-closed. While the initial workpiece is being processed under UHV, a subsequent sample can be introduced into the interlock volume, pre-cleaned, and so-on and so-forth, saving much time. Although a \"puff\" of gas is generally released into the UHV system when the valve to the interlock volume is opened, the UHV system pumps can generally snatch this gas away before it has time to adsorb onto the UHV surfaces. In a system well designed with suitable interlocks, the UHV components seldom need bakeout and the UHV may improve over time even as workpieces are introduced and removed.\n\nMany common materials are used sparingly if at all due to high vapor pressure, high adsorptivity or absorptivity resulting in subsequent troublesome outgassing, or high permeability in the face of differential pressure (i.e.: \"through-gassing\"):\n\nTechnical limitations: \n\nUltra-high vacuum is necessary for many surface analytic techniques such as:\n\nUHV is necessary for these applications to reduce surface contamination, by reducing the number of molecules reaching the sample over a given time period. At 0.1 mPa (10 Torr), it only takes 1 second to cover a surface with a contaminant, so much lower pressures are needed for long experiments.\n\nUHV is also required for:\nand, while not compulsory, can prove beneficial in applications such as:\n\nTypically, UHV requires:\n\nOutgassing is a problem for UHV systems. Outgassing can occur from two sources: surfaces and bulk materials. Outgassing from bulk materials is minimized by selection of materials with low vapor pressures (such as glass, stainless steel, and ceramics) for everything inside the system. Materials which are not generally considered absorbent can outgas, including most plastics and some metals. For example, vessels lined with a highly gas-permeable material such as palladium (which is a high-capacity hydrogen sponge) create special outgassing problems.\n\nOutgassing from surfaces is a subtler problem. At extremely low pressures, more gas molecules are adsorbed on the walls than are floating in the chamber, so the total surface area inside a chamber is more important than its volume for reaching UHV. Water is a significant source of outgassing because a thin layer of water vapor rapidly adsorbs to everything whenever the chamber is opened to air. Water evaporates from surfaces too slowly to be fully removed at room temperature, but just fast enough to present a continuous level of background contamination. Removal of water and similar gases generally requires baking the UHV system at 200 to 400 °C while vacuum pumps are running. During chamber use, the walls of the chamber may be chilled using liquid nitrogen to reduce outgassing further.\n\nHydrogen and carbon monoxide are the most common background gases in a well-designed, well-baked UHV system. Both Hydrogen and CO diffuse out from the grain boundaries in stainless steel. Helium could diffuse through the steel and glass from the outside air, but this effect is usually negligible due to the low abundance of He in the atmosphere.\n\nThere is no single vacuum pump that can operate all the way from atmospheric pressure to ultra-high vacuum. Instead, a series of different pumps is used, according to the appropriate pressure range for each pump. Pumps commonly used to achieve UHV include:\n\nUHV pressures are measured with an ion gauge, either a hot filament or an inverted magnetron type.\n\nMetal seals, with knife edges on both sides cutting into a soft, copper gasket. This all-metal seal can maintain pressures down to 100 pPa (~10 Torr).\n\nMeasurement of high vacuum is done using a \"nonabsolute gauge\" that measures a pressure-related property of the vacuum, for example, its thermal conductivity. See, for example, Pacey. These gauges must be calibrated. The gauges capable of measuring the lowest pressures are magnetic gauges based upon the pressure dependence of the current in a spontaneous gas discharge in intersecting electric and magnetic fields.\n\nA UHV manipulator allows an object which is inside a vacuum chamber and under vacuum to be mechanically positioned. It may provide rotary\nmotion, linear motion, or a combination of both. The most complex devices give motion in three axes and rotations around two of those axes. To generate the mechanical movement inside the chamber, two basic mechanisms are commonly employed: a mechanical coupling through the vacuum wall (using a vacuum-tight seal around the coupling), or a magnetic coupling that transfers motion from air-side to vacuum-side. Various forms of motion control are available for manipulators, such as knobs, handwheels, motors, stepping motors, piezoelectric motors, and pneumatics.\n\nThe manipulator or sample holder may include features that allow additional control and testing of a sample, such as the ability to apply heat, cooling, voltage, or a magnetic field. Sample heating can be accomplished by electron bombardment or thermal radiation. For electron bombardment, the sample holder is equipped with a filament which emits electrons when biased at a high negative potential. The impact of the\nelectrons bombarding the sample at high energy causes it to heat. For thermal radiation, a filament is mounted close to the sample and resistively heated to high temperature. The infrared energy from the filament heats the sample.\n\n\n"}
{"id": "427992", "url": "https://en.wikipedia.org/wiki?curid=427992", "title": "Water hammer", "text": "Water hammer\n\nWater hammer (or, more generally, fluid hammer, also called hydraulic shock) is a pressure surge or wave caused when a fluid, usually a liquid but sometimes also a gas, in motion is forced to stop or change direction suddenly, a momentum change. A water hammer commonly occurs when a valve closes suddenly at an end of a pipeline system, and a pressure wave propagates in the pipe.\n\nThis pressure wave can cause major problems, from noise and vibration to pipe collapse. It is possible to reduce the effects of the water hammer pulses with accumulators, expansion tanks, surge tanks, blowoff valves, and other features.\n\nRough calculations can be made either using the Zhukovsky (Joukowsky) equation, or more accurate ones using the method of characteristics.\n\nIn the 1st century B.C., Marcus Vitruvius Pollio described the effect of water hammer in lead pipes and stone tubes of the Roman public water supply. Water hammer was exploited before there was even a word for it; in 1772, Englishman John Whitehurst built a hydraulic ram for a home in Cheshire, England. In 1796, French inventor Joseph Michel Montgolfier (1740–1810) built a hydraulic ram for his paper mill in Voiron. In French and Italian, the terms for \"water hammer\" come from the hydraulic ram: \"coup de bélier\" (French) and \"colpo d'ariete\" (Italian) both mean \"blow of the ram\". As the 19th century witnessed the installation of municipal water supplies, water hammer became a concern to civil engineers. Water hammer also interested physiologists who were studying the circulatory system.\n\nAlthough it was prefigured in work by Thomas Young, the theory of water hammer is generally considered to have begun in 1883 with the work of German physiologist Johannes von Kries (1853–1928), who was investigating the pulse in blood vessels. However, his findings went unnoticed by civil engineers. Kries's findings were subsequently derived independently in 1898 by the Russian fluid dynamicist Nikolay Yegorovich Zhukovsky (1847–1921), in 1898 by the American civil engineer Joseph Palmer Frizell (1832–1910), and in 1902 by the Italian engineer Lorenzo Allievi (1856–1941).\n\nWhen a pipe is suddenly closed at the outlet (downstream), the mass of water before the closure is still moving, thereby building up high pressure and a resulting shock wave. In domestic plumbing this is experienced as a loud banging resembling a hammering noise. Water hammer can cause pipelines to break if the pressure is high enough. Air traps or stand pipes (open at the top) are sometimes added as to water systems to absorb the potentially damaging forces caused by the moving water.\n\nIn hydroelectric generating stations, the water traveling along the tunnel or pipeline may be prevented from entering a turbine by closing a valve. For example, if there is 14 km of tunnel of 7.7 m diameter full of water travelling at 3.75 m/s, that represents approximately 8000 megajoules of kinetic energy that must be arrested. This arresting is frequently achieved by a surge shaft open at the top, into which the water flows. As the water rises up the shaft its kinetic energy is converted into potential energy, which causes the water in the tunnel to decelerate. At some HEP stations, such as the Saxon Falls Hydro Power Plant In Michigan, what looks like a water tower is actually one of these devices, known in these cases as a surge drum.\n\nIn the home, a water hammer may occur when a dishwasher, washing machine or toilet shuts off water flow. The result may be heard as a loud bang, repetitive banging (as the shock wave travels back and forth in the plumbing system), or as some shuddering.\n\nOn the other hand, when an upstream valve in a pipe closes, water downstream of the valve attempts to continue flowing creating a vacuum that may cause the pipe to collapse or implode. This problem can be particularly acute if the pipe is on a downhill slope. To prevent this, air and vacuum relief valves or air vents are installed just downstream of the valve to allow air to enter the line to prevent this vacuum from occurring.\n\nOther causes of water hammer are pump failure and check valve slam (due to sudden deceleration, a check valve may slam shut rapidly, depending on the dynamic characteristic of the check valve and the mass of the water between a check valve and tank). To alleviate this situation, it is recommended to install non-slam check valves as they do not rely on gravity or fluid flow for their closure. For vertical pipes, other suggestions include installing new piping that can be designed to include air chambers to alleviate the possible shockwave of water due to excess water flow.\n\nSteam distribution systems may also be vulnerable to a situation similar to water hammer, known as \"steam hammer\". In a steam system, a water hammer most often occurs when some of the steam condenses into water in a horizontal section of the piping. Steam picks up the water, forming a \"slug\", and hurls this at high velocity into a pipe fitting, creating a loud hammering noise and greatly stressing the pipe. This condition is usually caused by a poor condensate drainage strategy.\n\nWhere air filled traps are used, these eventually become depleted of their trapped air over a long period through absorption into the water. This can be cured by shutting off the supply, opening taps at the highest and lowest locations to drain the system (thereby restoring air to the traps), and then closing the taps and re-opening the supply.\n\nOn turbocharged internal combustion engines, a fluid hammer can take place when the throttle is closed while the turbocharger is forcing air into the engine. A pressure relief valve placed before the throttle prevents the air from surging against the throttle body by diverting it elsewhere, thus protecting the turbocharger from pressure damage. This valve can either recirculate the air into the turbocharger's intake (recirculation valve), or it can blow the air into the atmosphere and produce the distinctive hiss-flutter of an aftermarket turbocharger (blowoff valve).\n\nIf a stream of high pressure water impinges on a surface, water hammer can quickly erode and destroy it. In the 2009 Sayano–Shushenskaya hydroelectric power station accident, the lid to a 640 MW turbine was ejected upwards, hitting the ceiling above. During the accident, the rotor was seen flying through the air, still spinning, about 3 meters above the floor. Unrestrained, per second of water began to spray all over the generator hall. The geyser caused the structural failure of steel ceiling joists, precipitating a roof collapse around the failed turbine.\n\nWhen an explosion happens in an enclosed space, water hammer can cause the walls of the container to deform. However, it can also impart momentum to the enclosure if it is free to move. An underwater explosion in the SL-1 nuclear reactor vessel caused the water to accelerate upwards through of air before it struck the vessel head at with a pressure of . This pressure wave caused the steel vessel to jump 9 feet 1 inch (2.77 m) into the air before it dropped into its prior location. It is imperative to perform ongoing preventative maintenance to avoid water hammer as the results of these powerful explosions have resulted in fatalities.\n\nWater hammer has caused accidents and fatalities, but usually damage is limited to breakage of pipes or appendages. An engineer should always assess the risk of a pipeline burst. Pipelines transporting hazardous liquids or gases warrant special care in design, construction, and operation. Hydroelectric power plants especially must be carefully designed and maintained because the water hammer can cause water pipes to fail catastrophically.\n\nThe following characteristics may reduce or eliminate water hammer:\n\nOne of the first to successfully investigate the water hammer problem was the Italian engineer Lorenzo Allievi.\n\nWater hammer can be analyzed by two different approaches—\"rigid column theory\", which ignores compressibility of the fluid and elasticity of the walls of the pipe, or by a full analysis that includes elasticity. When the time it takes a valve to close is long compared to the propagation time for a pressure wave to travel the length of the pipe, then rigid column theory is appropriate; otherwise considering elasticity may be necessary.\nBelow are two approximations for the peak pressure, one that considers elasticity, but assumes the valve closes instantaneously, and a second that neglects elasticity but includes a finite time for the valve to close.\n\nThe pressure profile of the water hammer pulse can be calculated from the Joukowsky equation\n\nSo for a valve closing instantaneously, the maximum magnitude of the water hammer pulse is:\n\nwhere Δ\"P\" is the magnitude of the pressure wave (Pa), \"ρ\" is the density of the fluid (kg m), \"a\" is the speed of sound in the fluid (ms), and Δ\"v\" is the change in the fluid's velocity (ms). The pulse comes about due to Newton's laws of motion and the continuity equation applied to the deceleration of a fluid element.\n\nAs the speed of sound in a fluid is formula_3, the peak pressure depends on the fluid compressibility if the valve is closed abruptly.\n\nwhere\n\nWhen the valve is closed slowly compared to the transit time for a pressure wave to travel the length of the pipe, the elasticity can be neglected, and the phenomenon can be described in terms of inertance or rigid column theory:\n\nAssuming constant deceleration of the water column (\"dv\"/\"dt\" = \"v\"/\"t\"), gives:\n\nwhere:\n\nThe above formula becomes, for water and with imperial unit: P = 0.0135 V L/t.\nFor practical application, a safety factor of about 5 is recommended:\n\nwhere \"P\" is the inlet pressure in psi, \"V\" is the flow velocity in ft/sec, \"t\" is the valve closing time in seconds and \"L\" is the upstream pipe length in feet.\n\nWhen a valve with a volumetric flow rate Q is closed, an excess pressure ΔP is created upstream of the valve, whose value is given by the Joukowsky equation:\n\nIn this expression:\nThe hydraulic impedance \"Z\" of the pipeline determines the magnitude of the water hammer pulse. It is itself defined by:\n\nwith:\n\nThe latter follows from a series of hydraulic concepts: \n\nThus, the equivalent elasticity is the sum of the original elasticities:\n\nAs a result, we see that we can reduce the water hammer by:\n\nThe water hammer effect can be simulated by solving the following partial differential equations.\n\nwhere \"V\" is the fluid velocity inside pipe, \"formula_15\" is the fluid density and \"B\" is the equivalent bulk modulus, \"f\" is the Darcy-Weisbach friction factor.\n\nColumn separation is a phenomenon that can occur during a water-hammer event. If the pressure in a pipeline drops below the vapor pressure of the liquid, cavitation will occur (some of the liquid vaporizes, forming a bubble in the pipeline, keeping the pressure close to the vapor pressure). This is most likely to occur at specific locations such as closed ends, high points or knees (changes in pipe slope). When subcooled liquid flows into the space previously occupied by vapor the area of contact between the vapor and the liquid increases. This causes the vapor to condense into the liquid reducing the pressure in the vapor space. The liquid on either side of the vapor space is then accelerated into this space by the pressure difference. The collision of the two columns of liquid (or of one liquid column if at a closed end) causes a large and nearly instantaneous rise in pressure. This pressure rise can damage hydraulic machinery, individual pipes and supporting structures. Many repetitions of cavity formation and collapse may occur in a single water-hammer event.\n\nMost water hammer software packages use the method of characteristics to solve the differential equations involved. This method works well if the wave speed does not vary in time due to either air or gas entrainment in a pipeline. The Wave Method (WM) is also used in various software packages. WM lets operators analyze large networks efficiently. Many commercial and non commercial packages are available.\n\nSoftware packages vary in complexity, dependent on the processes modeled. The more sophisticated packages may have any of the following features:\n\n\n\n"}
{"id": "19678805", "url": "https://en.wikipedia.org/wiki?curid=19678805", "title": "Windows on Earth", "text": "Windows on Earth\n\nWindows on Earth is a museum exhibit, website, and exploration tool, developed by TERC, Inc. (an educational non-profit organization, previously called Technical Education Research Centers), and the Association of Space Explorers, that enables the public to explore an interactive, virtual view of Earth from space. In addition, the tool has been selected by NASA to help astronauts identify targets for photography from the International Space Station (ISS).\n\nThe program simulates the view of Earth as seen from a window aboard the ISS, in high-resolution, photographically accurate colors and 3D animations. The views include cloud cover, day and night cycles, night time lights, and other features that help make the exhibit realistic and interactive.\n\nWindows on Earth provides the user a view of Earth from an astronaut's viewpoint, with interactive photorealistic views of Earth as if seen from an altitude of 360 km. The program uses GeoFusion's digital Earth visualization system, which renders accurate views of Earth with terrain, satellite imagery, clouds, and other layers. The system is programmed for user interaction, allowing users to \"fly\" anywhere they wish to see, and zoom in or out to see details. The system's imagery is derived from Landsat, and features 3D perspective views. Former astronaut Jay Apt assisted with the color-correction of the images, to help get the most realistic colors as seen from space. The program is updated daily to include accurate cloud cover information.\n\nWindows on Earth was created by people from the Center for Earth and Space Science Education (CESSE) at TERC, a not-for profit math and science education company located in Cambridge, MA, in partnership with the Association of Space Explorers, GeoFusion's, and WorldSat International and with funding from the National Science Foundation, Informal Science Education.\n\nAdditional partners include the Challenger Learning Center and NASA's Johnson Space Center.\n\nThe Windows on Earth museum exhibit can be found in the Smithsonian National Air and Space Museum, Boston's Museum of Science, the Montshire Museum of Science in Vermont, the St. Louis Science Center, and the Connecticut Science Center.\n\nWindows on Earth flew on board the International Space Station (ISS). On October 12, 2008, Richard Garriott launched aboard Soyuz TMA-13 to the ISS as a Spaceflight participant. Garriott remained on board the station for 10 days, conducting educational and scientific programs and experiments. Earth observation was one of his prime tasks on this mission, and he used the Windows on Earth system to help him take pictures of specific targets.\n\nAfter the mission, Richard's photographs, along with ones taken by his astronaut father Owen Garriott, who flew on Skylab 3 (1973) and STS-9 (1983), were made available to the public through Windows on Earth. This provides a unique opportunity for comparing areas of Earth photographed by two generations of space explorers, showing how Earth's surface (and the technology of Earth observation) has changed over 35 years, from 1973 to 2008.\n\nIn May 2012, NASA selected Windows on Earth as the new tool to help astronauts identify targets for photography from the ISS.\n\n\n"}
