{"id": "39270850", "url": "https://en.wikipedia.org/wiki?curid=39270850", "title": "Alfred Russel Wallace centenary", "text": "Alfred Russel Wallace centenary\n\nThe centenary of the death of the naturalist Alfred Russel Wallace on 7 November 1913 was marked in 2013 with events around the world to celebrate his life and work. The commemorations was co-ordinated by the Natural History Museum, London.\n\nEvents between October 2013 and June 2014 were planned by the Natural History Museum and other organisations including the Zoological Society of London, Cardiff University, the University of Alberta, Dorset County Museum, Swansea Museum, Dorset Wildlife Trust, Ness Botanical Gardens (South Wirral), the Royal Society, the Linnean Society, the Harvard Museum of Natural History, the American Museum of Natural History, Hertford Museum and the National Museum of Wales.\n\nThe naturalist, explorer, geographer, anthropologist and biologist Alfred Russel Wallace (born 8 January 1823) died on 7 November 1913. He is principally remembered now for having independently conceived the theory of evolution through natural selection, which prompted Charles Darwin to publish \"On the Origin of Species\". Some of his books such as \"The Malay Archipelago\" remain in print; it is considered one of the best accounts of scientific exploration published during the 19th century. Wallace is also remembered for recognizing the presence of a biogeographical boundary, now known as the Wallace Line, that divides the Indonesian archipelago into two distinct parts: a western portion in which the animals are almost entirely of Asian origin, and an eastern portion where the fauna reflect the influence of Australasia.\n\nThe South Kensington Natural History Museum, London, co-ordinating commemorative events for the Wallace centenary worldwide in the 'Wallace100' project, created a website to celebrate Wallace's centenary. The museum holds the Wallace Collection of memorabilia including letters, Wallace's notebooks and other documents, and 28 drawers of insects and other specimens that he collected on his expeditions to the Malay Archipelago and to South America. The museum describes Wallace as \"Father of biogeography\", as a committed socialist, and as a spiritualist.\n\nThe Royal Societyplanned a two-day discussion meeting in October 2013 for researchers on \"Alfred Russel Wallace and his legacy\", with speakers including George Beccaloni, Steve Jones, Lynne Parenti, Tim Caro and Martin Rees. Cardiff University's School of Earth & Ocean Sciences has planned a lecture series in 2013-2014 as part of the centenary commemoration of Wallace.\n\nHertford Museum held several events including an evening of illustrated talks on 15 January 2014 at Hertford Theatre. Errol Fuller will discuss Wallace and the curious 19th century social phenomenon that guided his life and Dr Sandra Knapp will talk about Wallace’s life and explorations in the Amazon.\n\nThe Linnean Society held a two-day celebration of Wallace's centenary in Bournemouth on 7 and 8 June 2013, together with the Society for the History of Natural History, Bournemouth University and Bournemouth Natural Sciences Society. The event included talks about Wallace, his thoughts on natural selection, his evolutionary insights, and his notebooks and letters. A theatrical performance, 'You Should Ask Wallace', was put on by Theatre na n'Og. On the second day the group visited Wallace's grave and went on a nature walk in Wallace's memory.\n\nThe Royal Botanic Gardens, Kew ran a display of Wallace memorabilia including letters, photographs, artefacts made from plants, and herbarium specimens in 2013. \"Kew\" magazine likewise published an article \"The Wallace Connection\" to mark the centenary.\n\nThe American Museum of Natural History, New York City, planned a talk by naturalist and broadcaster David Attenborough for 12 November 2013, entitled 'Alfred Russel Wallace and the Birds of Paradise'. Birute Galdikas, one of Louis Leakey's 'ape women', will speak about her orangutans at the museum's Wallace conference.\n\nIn 2013 the BBC broadcast a two-part television series, \"Bill Bailey's Jungle hero: Alfred Russel Wallace\", in which comedian Bill Bailey travelled in the footsteps of Wallace in Indonesia to show what the naturalist achieved.\n\n"}
{"id": "563239", "url": "https://en.wikipedia.org/wiki?curid=563239", "title": "Biogenic substance", "text": "Biogenic substance\n\nA biogenic substance is a product made by or of life forms. The term encompasses constituents, secretions, and metabolites of plants or animals. In context of molecular biology, biogenic substances are referred to as biomolecules. \n\n\nAn abiogenic substance or process does not result from the present or past activity of living organisms. Abiogenic products may, e.g., be minerals, other inorganic compounds, as well as simple organic compounds (e.g. extraterrestrial methane, see also abiogenesis).\n\n"}
{"id": "34688121", "url": "https://en.wikipedia.org/wiki?curid=34688121", "title": "Coastal hazards", "text": "Coastal hazards\n\nCoastal Hazards are physical phenomena that expose a coastal area to risk of property damage, loss of life and environmental degradation. Rapid-onset hazards last over periods of minutes to several days and examples include major cyclones accompanied by high winds, waves and surges or tsunamis created by submarine earthquakes and landslides. Slow-onset hazards develop incrementally over longer time periods and examples include erosion and gradual inundation.\n\nSince early civilisation, coastal areas have been attractive settling grounds for human population as they provided abundant marine resources, fertile agricultural land and possibilities for trade and transport. This has led to high population densities and high levels of development in many coastal areas and this trend is continuing into the 21st century. At present, about 1,2 billion people live in coastal areas globally, and this number is predicted to increase to 1,8–5,2 billion by the 2080s due to a combination of population growth and coastal migration. Along with this increase follows major investments in infrastructure and the build environment.\n\nThe characteristics of coastal environments, however, pose some great challenges to human habitation. Coastlines are highly dynamic natural systems that interact with terrestrial, marine and atmospheric processes and undergo continuous change in response to these processes. Over the years, human society has often failed to recognize the hazards related to these dynamics and this has led to major disasters and societal disruption to various degrees. Even today, coastal development is often taking place with little regard to the hazards present in these environments, although climate change is likely to increase the general hazard levels. Societal activities in coastal areas can also pose a hazard to the natural balance of coastal systems, thereby disrupting e.g. sensitive ecosystems and subsequently human livelihood.\n\nCoastal hazard management has become an increasingly important aspect of coastal planning in order to improve the resilience of society to coastal hazards. Possible management options include hard engineering structures, soft protection measures, various accommodation approaches as well as a managed retreat from the coastline. For addressing coastal hazards, it is also important to have early warning systems and emergency management plans in place to be able to address sudden and potential disastrous hazards i.e. major flooding events. Events as the Hurricane Katrina affecting the southern USA in 2005 and the cyclone Nargis affecting Myanmar in 2008 provides clear examples of the importance of timely coastal hazard management.\n\nThere are many different types of environments along the coasts of the United States with very diverse features that affect, influence, and mold the near-shore processes that are involved. Understanding these ecosystems and environments can further advance the mitigating techniques and policy-making efforts against natural and man-made coastal hazards in these vulnerable areas. The five most common types of coastal zones range from the northern ice-pushing, mountainous coastline of Alaska and Maine, the barrier island coasts facing the Atlantic, the steep, cliff-back headlands along the pacific coast, the marginal-sea type coastline of the Gulf region, and the coral reef coasts bordering Southern Florida and Hawaii.\n\nIce-pushing/mountainous coastline\n\nThese coastal regions along the northernmost part of the nation were affected predominantly by, along with the rest of the Pacific Coast, continuous tectonic activity, forming a very long, irregular, ridged, steep and mostly mountainous coastline. These environments are heavily occupied with permafrost and glaciers, which are the two major conditions affecting Alaska's Coastal Development.\n\nBarrier island coastline\n\nBarrier islands are a land form system that consists of fairly narrow strips of sand running parallel to the mainland and play a significant role in mitigating storm surges and oceans swells as natural storm events occur. The morphology of the various types and sizes of barrier islands depend on the wave energy, tidal range, basement controls, and sea level trends. The islands create multiple unique environments of wetland systems including marshes, estuaries, and lagoons.\n\nSteep, cliff-backing abrasion coastline\n\nThe coastline along the western part of the nation consists of very steep, cliffed rock formations generally with vegetative slopes descending down and a fringing beach below. The various sedimentary, metamorphic, and volcanic rock formations assembled along a tectonically disturbed environment, all with altering resistances running perpendicular, cause the ridged, extensive stretch of uplifted cliffs that form the peninsulas, lagoons, and valleys.\n\nMarginal-sea type coastline\n\nThe southern banks of the United States border the Gulf of Mexico, intersecting numerous rivers, forming many inlets bays, and lagoons along its coast, consisting of vast areas of marsh and wetlands. This region of landform is prone to natural disasters yet highly and continuously developed, with man-made structures attaining to water flow and control.\n\nCoral reef coastline\n\nCoral reefs are located off the shores of the southern Florida and Hawaii consisting of rough and complex natural structures along the bottom of the ocean floor with extremely diverse ecosystems, absorbing up to ninety percent of the energy dissipated from wind-generated waves. This process is a significant buffer for the inner-lying coastlines, naturally protecting and minimizing the impact of storm surge and direct wave damage. Because of the highly diverse ecosystems, these coral reefs not only provide for the shoreline protection, but also deliver an abundant amount of services to fisheries and tourism, increasing its economic value.\n\nNatural VS Human disasters\n\nThe population that lives along or near our coastlines are an extremely vulnerable population. There are numerous issues facing our coastlines and there are two main categories that these hazards can be placed under, Natural disasters and Human disasters. Both of these issues cause great damage to our coastlines and discussion is still ongoing regarding what standards or responses need to be met to help both the individuals who want to continue living along the coastline, while keeping them safe and not eroding more coastline away. Natural disasters are disasters that are out of human control and are usually caused by the weather. Disasters that include but are not limited to; storms, tsunamis, typhoons, flooding, tides, waterspouts, nor'easters, and storm surge. Human disasters occur when humans are the main culprit behind why the disaster happened. Some human disasters are but are not limited to; pollution, trawling, and human development. Natural and human disasters continue to harm the coastlines severely and they need to be researched in order to prepare/stop the hazards if possible.\n\nThe populations that live near or along the coast experience many hazards and it affects millions of people. Around ten million people globally feel the effects of coastal problems yearly and most are due to certain natural hazards like coastal flooding with storm surges and typhoons. A major problem related to coastal regions deals with how the entire global environment is changing and in response, the coastal regions are easily affected.\n\nStorms, Flooding and Erosion\n\nStorms are one of the major hazards that are associated to coastal regions. Storms, flooding, and erosion are closely associated and can happen simultaneously. Tropical storms or Hurricanes especially can devastate coastal regions. For example, Florida during Hurricane Andrew occurred in 1992 that caused extreme damage. It was a category five hurricane that caused $26.5 billion in damages and even 23 individuals lost their lives from the storm. Hurricane Katrina also caused havoc along the coast to show the extreme force a hurricane can do in a certain region. The Chennai Floods of 2015, which affected many people, is an example of flooding due to cyclones. People across the whole state of Tamil Nadu felt its impact and even parts of Andhra Pradesh got affected. There was a loss of Rs.900 crore and 280 people died. Many cyclones like this happen across Asia but the media reports only minor hurricanes which hit the United States.\n\nAlmost all storms with high wind and water cause erosion along the coast. Erosion occurs when but not limited to; along shore currents, tides, sea level rise and fall, and high winds. Larger amounts of erosion cause the coastline to erode away at a faster rate and can leave people homeless and leave less land to develop or keep for environmental reasons. Coastal erosion has been increasing over the past few years and it is still on the rise which makes it a major coastline hazard. In the United States, 45 percent of its coast line is along the Atlantic or Gulf coast and the erosion rate per year along the Gulf coast is at six feet a year. The average rate of erosion along the Atlantic is around two to three feet a year. Even with these findings, erosion rates in specific locations vary because of various environmental factors such as major storms that can cause major erosion upwards to 100 feet or more in only one day.\n\nPollution, Trawling and Human Development\n\nPollution, trawling, and human development are major human disasters that affect coastal regions. There are two main categories related to pollution, point source pollution, and nonpoint source pollution. Point source pollution is when there is an exact location such as a pipeline or a body of water that leads into the rivers and oceans. Known dumping into the ocean is also another point source of pollution. Nonpoint source pollution would pertain more to fertilizer runoff, and industrial waste. Examples of pollution that affect the coastal regions are but are not limited to; fertilizer runoff, oil spills, and dumping of hazardous materials into the oceans. More human acts that hurt the coastline are as follows; waste discharge, fishing, dredging, mining, and drilling. Oil spills are one of the most hazardous dangers towards coastal communities. They are hard to contain, difficult to clean up, and devastate everything. The fish, animals such as birds, the water, and especially the coastline near the spill. The most recent oil spill that had everybody concerned with oil spill was the BP oil spill.\n\nTrawling hurts the normal ecosystems in the water around the coastline. It depletes all ecosystems on the ocean floor such as, flounder, shellfish, marsh etc.. It is simply a giant net that is drug across the ocean floor and destroys and catches anything in its path. Human development is one of the major problems when facing coastal hazards. The overall construction of buildings and houses on the coast line takes away the natural occurrences to handle the fluctuation in water and sea level rise. Building houses in pre-flood areas or high risk areas that are extremely vulnerable to flooding are major concerns towards human development in coastal regions. Having houses and buildings in areas that are known to have powerful storms that will create people to be in risk by living there. Also pertaining to barrier islands, where land is at risk for erosion but they still continue to build there anyway. More and more houses today are being taken by the ocean; look at picture above.\n\nCoastal hazards & climate change\n\nThe predicted climate change is adding an extra risk factor to human settlement in coastal areas. Whereas the natural dynamics that shape our coastlines have been relatively stable and predictable over the last centuries, much more rapid change is now expected in processes as sea level rise, ocean temperature and acidity, tropical storm intensity and precipitation/runoff patterns. The world's coastlines will respond to these changes in different ways and at different pace depending on their bio-geophysical characteristics, but generally society will have to recognize that past coastal trends cannot be directly projected into the future. Instead, it is necessary to consider how different coastal environments will respond to the predicted climate change and take the expected future hazards into account in the coastal planning processes.\n\nNational Flood Insurance Program\n\nThe National Flood Insurance Program or NFIP was instituted in 1968 and offers home owners in qualifying communities an opportunity to rebuild and recover after flooding events following the decision by insurance companies to discontinue providing flood insurance. This decision was made on behalf of the private insurers after continually high and widespread flood losses. The goals of this program are to not only better protect individuals from flood, but to reduce property losses, and reduce the total amount disbursed for flood loses by the government. Only communities which have adopted and implemented mitigation policies that are compliant with or exceed federal regulations. The regulatory policies reduce risk to life and property located within floodplains. The NFIP also comprehensively mapped domestic floodplains increasing public awareness of risk. The majority of structures were constructed after the mapping was completed and risk could be assessed. To reduce the cost to these owners, which constitute roughly 25% of the total policies the rates for insurance are subsidized.\n\nCoastal States Organization\n\nThe Coastal States Organization or COS was established in 1970 to represent 35 U.S. sub-federal governments on issues of coastal policies. CSO lobbies Congress on issues pertaining to Coastal Policy allowing states input on federal policy decisions. Funding, support, water quality, coastal hazards, and coastal zone management are the primary issues COS promotes. The strategic goals of COS are to provide information and assistance to members,evaluate and manage coastal needs, and secure long term funding for member states initiatives.\n\nCoastal Zone Management Act\n\nIn 1972 the Coastal Zone Management Act or CZMA works to streamline the policies which states create to a minimum federal standard for environmental protection. CZMA establishes the national policy for the development and implementation of regulatory programs for coastal land usage, which is supposed to be reflected in state legislation such as CAMA. CZMA also provides minimum building requirements to make the insurance provided through the NFIP less expensive for the government to operate by mitigating losses. Congress found that it was necessary to establish the minimum which programs should provide for. Each coastal state is required to have a program with 7 distinct parts: Identifying land uses,Identifying critical coastal areas, Management measures,Technical assistance, Public participation, Administrative coordination, State coastal zone boundary modification.\n\nThe Coastal Area Management Act\n\nThe Coastal Area Management Act or CAMA is policy that was implemented by the state of North Carolina in 1974 to work in-tandem with the CZMA. It creates a cooperative program between the state and local governments. The State government operates in an advisory capacity and reviews decisions made by local government planners. The goal of this legislation was to create a management system capable of preserving the coastal environment, insure the preservation of land and water resources, balance the use of coastal resources and establish guidelines and standards for conservations, economic development, tourism, transportation, and the protection of common law.\n\nDue to the increasing urbanization along the coastlines, planning and management are essential to protecting the ecosystems and environment from depleting. Coastal management is becoming implemented more because of the movement of people to the shore and the hazards that come with the territory. Some of the hazards include movement of barrier islands, sea level rise, hurricanes, nor'easters, earthquakes, flooding, erosion, pollution and human development along the coast. The Coastal Zone Management Act (CZMA) was created in 1972 because of the continued growth along the coast, this act introduced better management practices such as integrated coastal zone management, adaptive management and the use mitigation strategies when planning. According to the Coastal Zone Management Act, the objectives are to remain balanced to \"preserve, protect, develop, and where possible, to restore or enhance the resources of the nation's coastal zone\".\nThe development of the land can strongly affect the sea, for example the engineering of structures versus non-structures and the effects of erosion along the shore.\n\nIntegrated coastal zone management\n\nIntegrated coastal zone management means the integration of all aspects of the coastal zone; this includes environmentally, socially, culturally politically and economically to meet a sustainable balance all around. Sustainability is the goal to allow development yet protect the environment in which we develop. Coastal zones are fragile and do not do well with change so it is important to acquire sustainable development. The integration from all views will entitle a holistic view for the best implementation and management of that country, region and local scales. The five types of integration include integration among sectors, integration between land and water elements of the coastal zone, integration amount levels of government, integration between nations and integration among disciplines are all essential to meet the needs for implementation.\nManagement practices include\nThese four management practices should be based on a bottom-up approach, meaning the approach starts from a local level which is more intimate to the specific environment of that area. After assessment from the local level, the state and federal input can be implemented. The bottom-up approach is key for protecting the local environments because there is a diversity of environments that have specific needs all over the world.\n\nAdaptive management\n\nAdaptive management is another practice of development adaptation with the environment. Resources are the major factor when managing adaptively to a certain environment to accommodate all the needs of development and ecosystems. Strategies used must be flexible by either passive or active adaptive management include these key features:\nTo achieve adaptive management is testing the assumptions to achieve a desired outcome, such as trial and error, find the best known strategy then monitoring it to adapt to the environment, and learning the outcomes of success and failures of a project.\n\nMitigation\n\nThe purpose of mitigation is not only to minimize the loss of property damage, but minimize environmental damages due to development. To avoid impacts by not taking or limiting actions, to reduce or rectify impacts by rehabilitation or restoring the affected environments or instituting long-term maintenance operations and compensating for impacts by replacing or providing substitute environments for resources\nStructural mitigation is the current solution to eroding beaches and movement of sand is the use of engineered structures along the coast have been short lived and are only an illusion of safety to the public that result in long term damage of the coastline. Structural management deals with the use of the following: groins which are man-made solution to longshore current movements up and down the coast. The use of groins are efficient to some extent yet cause erosion and sand build up further down the beaches. Bulkheads are man-made structures that help protect the homes built along the coast and other bodies of water that actually induce erosion in the long run. Jetties are structures built to protect sand movement into the inlets where boats for fishing and recreation move through.\nThe use of nonstructural mitigation is the practice of using organic and soft structures for solutions to protect against coastal hazards. These include: artificial dunes, which are used to create dunes that have been either developed on or eroded. There needs to be at least two lines of dunes before any development can occur. Beach Nourishment is a major source of nonstructural mitigation to ensure that beaches are present for the communities and for the protection of the coastline. Vegetation is a key factor when protecting from erosion, specifically for to help stabilize dune erosion.\n\n\n"}
{"id": "44262036", "url": "https://en.wikipedia.org/wiki?curid=44262036", "title": "Coleridge's theory of life", "text": "Coleridge's theory of life\n\nRomanticism grew largely out of an attempt to understand not just inert nature, but also vital nature. Romantic works in the realm of art and Romantic medicine were a response to the general failure of the application of method of inertial science to reveal the foundational laws and operant principles of vital nature. German romantic science and medicine sought to understand the nature of the life principle identified by John Hunter as distinct from matter itself via Johan Friedrich Blumenbach's \"Bildungstrieb\" and Romantic medicine's \"Lebenskraft\", as well as Röschlaub's development of the Brunonian system of medicine system of John Brown, in his \"excitation theory\" of life (German:\"Erregbarkeit theorie\"), working also with Schelling's \"Naturphilosophie\", the work of Goethe regarding morphology, and the first dynamic conception of physiology of Richard Saumarez. But it is in Samuel Taylor Coleridge that we find the question of life and vital nature most intensely and comprehensively examined, particularly in his Hints towards the Formation of a more Comprehensive Theory of Life (1818), providing the foundation for Romantic philosophy, science and medicine. The work is key to understanding the relationship of Romantic literature and science.\n\nThe Enlightenment had developed a philosophy and science supported by formidable twin pillars: the first the Cartesian split of mind and matter, the second Newtonian physics, with its conquest of inert nature, both of which focused the mind's gaze on things or objects. For Cartesian philosophy, life existed on the side of matter, not mind; and for the physical sciences, the method that had been so productive for revealing the secrets of inert nature, should be equally productive in examining vital nature. The initial attempt to seek the cause and principle of life in matter was challenged by John Hunter, who held that the principle of life was not to be found nor confined within matter, but existed independently of matter itself, and informed or animated it, that is, he implied, it was the unifying or antecedent cause of the things or what Aristotelean philosophy termed \"natura naturata\" (the outer appearances of nature).\nThis reduction of the question of life to matter, and the corollary, that the method of the inertial sciences was the way to understand the very phenomenon of life, that is, its very nature and essence as a power (\"natura naturans\"), not as manifestations through sense-perceptible appearances (\"natura naturata\"), also reduced the individual to a material-mechanical 'thing' and seemed to render human freedom an untenable concept. It was this that Romanticism challenged, seeking instead to find an approach to the essence of nature as being also vital not simply inert, through a systematic method involving not just physics, but physiology (living functions). For Coleridge, quantitative analysis was anti-realist and needed to be grounded in qualitative analysis ('-ologies') (as was the case with Goethe's approach).\nAt the same time, the Romantics had to deal with the idealistic view that life was a 'somewhat' outside of things, such that the things themselves lost any real existence, a stream coming through Hume and Kant, and also infusing the German natural philosophical stream, German idealism, and in particular \"naturphilosophie\", eventuating scientifically in the doctrine of 'vitalism'. For the Romantics, life is independent of and antecedent to nature, but also infused and suspended in nature, not apart from it, As David Bohm expresses it in more modern terms \"In nature nothing remains constant…everything comes from other things and gives rise to other things. This principle is…at the foundation of the possibility of our understanding nature in a rational way.\"\n\nAnd as Coleridge explained, 'this antecedent unity, or cause and principle of each union, it has since the time of Bacon and Kepler been customary to call a law.\" And as law, \"we derive from it a progressive insight into the necessity and generation of the phenomena of which it is the law.\"\n\nColeridge's was the dominant mind on many issues involving the philosophy and science in his time, as John Stuart Mill acknowledged, along with others since who have studied the history of Romanticism.\n\nFor Coleridge, as for many of his romantic contemporaries, the idea that matter itself can begat life only dealt with the various changes in the arrangement of particles and did not explain life itself as a principle or power that lay behind the material manifestations, \"natura naturans\" or \"the productive power suspended and, as it were, quenched in the product\" Until this were addressed, according to Coleridge, \"we have not yet attained to a science of nature.\"\n\nThis productive power is above sense experience, but not above nature herself, that is, supersensible, but not supernatural, and, thus, not 'occult' as was the case with vitalism. Vitalism failed to distinguish between spirit and nature, and then within nature, between the visible appearances and the invisible, yet very real and not simply hypothesized notion, essence or motivating principle (\"natura naturans\"). Even Newton spoke of things invisible in themselves (though not in their manifestations), such as force, though Comte, the thorough materialist, complained of the use of such terms as the 'force of gravity' as being relics of animism.\n\nMatter was not a 'datum' or thing in and of itself, but rather a product or effect, and for Coleridge, looking at life in its broadest sense, it was the product of a polarity of forces and energies, but derived from a unity which is itself a power, not an abstract or nominal concept, that is Life, and this polar nature of forces within the power of Life is the very law or 'Idea' (in the Platonic sense) of Creation.\n\nFor Coleridge the essence of the universe is motion and motion is driven by a dynamic polarity of forces that is both inherent in the world as potential and acting inherently in all manifestations. This polarity is the very dynamic that acts throughout all of nature, including into the more particular form of 'life biological', as well as of mind and consciousness.\n\nAnd this polarity is dynamic, that is real, though not visible, and not simply logical or abstract. Thus, the polarity results in manifestations that are real, as the opposite powers are not contradictory, but counteracting and inter-penetrating.\n\nThus, then, Life itself is not a thing—a self-subsistent hypostasis—but an act and process...\n\nAnd in that sense Coleridge re-phrases the question \"What is Life?\" to \"What is not Life that really is?\"\n\nThis dynamic polar essence of nature in all its functions and manifestations is a universal law in the order of the law of gravity and other physical laws of inert nature. And, critically, this dynamic polarity of constituent powers of life at all levels is not outside or above nature, but is within nature (\"natura naturans\"), not as a part of the visible product, but as the ulterior natural functions that produce such products or things.\n\nIt is these functions that provided the bridge being sought by Romantic science and medicine, in particular by Andreas Röschlaub and the Brunonian system of medicine, between the inertial science of inert nature (physics) and the vital science of vital nature (physiology) and its therapeutic application or physic (the domain of the physician).\n\nColeridge was influenced by German philosophy, in particular Kant, Fichte and Schelling (Naturphilosophie), as well as the physiology of Blumenbach and the dynamic excitation theory of life of the Brunonian system. He sought a path that was neither the mystical tendency of the earlier vitalists nor the materialistic reductionist approach to natural science, but a dynamic one.\n\nColeridge's challenge was to describe something that was dynamic neither in mystical terms not materialistic ones, but via analogy, drawing from the examples of inertial science. As one writer explains, he uses the examples of electricity, magnetism and gravity not because they are like life, but because they offer a way of understanding powers, forces and energies, which lie at the heart of life. And using these analogies, Coleridge seeks to demonstrate that life is not a material force, but a product of relations amongst forces. Life is not linear and static, but dynamic process of self-regulation and Emergent evolution that results in increasing complexity and individuation. This spiral, upward movement (cf. Goethe's ideas) creates a force for organization that unifies, and is most intense and powerful in that which is most complex and most individual - the self-regulating, enlightened, developed individual mind. But at the same time, this process of life increases interdependence (like the law of comparative advantage in economics) and associational powers of the mind. Thus, he is not talking about an isolated, individual subjective mind, but about the evolution of a higher level of consciousness and thought at the core of the process of life.\n\nAnd the direction of this motion is towards increasing individuation, that is the creation of specific, individual units of things. At the same time, given the dynamic polarity of the world, there must always be an equal and opposite tendency, in this case, that of connection. So, a given of our experience is that man is both an individual, tending in each life and in history generally to greater and greater individualization, and a social creature seeking interaction and connection. It is the dynamic interplay between the individuation and connecting forces that leads to higher and higher individuation.\n\nColeridge makes a further distinction between mathematics and life, the latter being productive or creative, that is, living, and the former ideal. Thus, the mathematical approach that works so well with inert nature, is not suitable for vital nature.\n\nThe counteraction then of the two assumed forces does not depend on their meeting from opposite directions; the power which acts in them is indestructible; it is therefore inexhaustibly re-ebullient; and as something must be the result of these two forces, both alike infinite, and both alike indestructible; and as rest or neutralization cannot be this result; no other conception is possible, but that the product must be a tertium aliquid, [a third thing] or finite generation. Consequently this conception is necessary. Now this tertium aliquid can be no other than an inter-penetration of the counteracting powers, partaking of both… Consequently the 'constituent powers', that have given rise to a body, may then reappear in it as its function: \"a Power, acting in and by its Product or Representative to a predetermined purpose is a Function...the first product of its energy is the thing itself: \"ipsa se posuit et iam facta est ens positum\". Still, however, its productive energy is not exhausted in this product, but overflows, or is effluent, as the specific forces, properties, faculties, of the product. It reappears, in short, as the function of the body...The vital functions are consequents of the \"Vis Vitae Principium Vitale\", and presuppose the Organs, as the Functionaries.\n\nLife, that is, the essential polarity in unity (multeity in unity) in Coleridge’s sense also has a four beat cycle, different from the arid dialectics of abstraction - namely the tension of the polar forces themselves, the charge of their synthesis, the discharge of their product (indifference) and the resting state of this new form (predominance). The product is not a neutralization, but a new form of the essential forces, these forces remaining within, though now as the functions of the form.\n\nTo make it adequate, we must substitute the idea of positive production for that of rest, or mere neutralization. To the fancy alone it is the null-point, or zero, but to the reason it is the punctum saliens, and the power itself in its eminence.\n\nThis dynamic polarity that is Life is expressed at different levels. At its most basic it is Space-Time, with its product - motion. The interplay of both gives us either a line or a circle, and then there are different degrees possible within a given form or “predominance” of forces. Geometry is not conceivable except as the dynamic interplay of space (periphery) and time (point). Space, time and motion are also geometrically represented by width, length (breadth) and depth. And this correspondence is repeated throughout the scale of Life.\n\nMatter, then, is the product of the dynamic forces - repulsion (centrifugal), and attraction (centripetal); it is not itself a productive power. It is also the mass of a given body.\n\nColeridge’s understanding of life is contrasted with the materialist view which is essentially reduced to defining life as that which is the opposite of not-life, or that which resists death, that is, that which is life.\n\nThe problem for Coleridge and the Romantics was that the intellect, 'left to itself' as Bacon stated, was capable of apprehending only the outer forms of nature (natura naturata) and not the inmost, living functions (natura naturans) giving rise to these forms. Thus, effects can only be 'explained' in terms of other effects, not causes. It takes a different capacity to 'see' these living functions, which is an imaginative activity. For Coleridge, there is an innate, primitive or 'primary' imagination that configures invisibly sense-experience into perception, but a rational perception, that is, one raised into consciousness and awareness and then rationally presentable, requires a higher level, what he termed 'secondary imagination', which is able to connect with the thing being experienced, penetrate to its essence in terms of the living dynamics upholding its outer form, and then present the phenomena as and within its natural law, and further, using reason, develop the various principles of its operation.\n\nThis cognitive capacity involved what Coleridge termed the 'inmost sense' or what Goethe termed the Gemüt. It also involved the reactivation of the old Greek noetic capacity, and the ability to 'see' or produce the theory (Greek \"theoria\" from the verb 'to see') dynamic polarities, or natural Laws, the dynamic transcendent (foundational) entities that Plato termed 'Ideas' (\"eidos\").\n\nSince \"natura naturata\" is sustained by \"natura naturans\", and the creative power of \"natura naturans\" is one of a kind with the human mind, itself creative, then there must be a correspondence or connection between the mind and the things we perceive, such that we can overcome the apparent separation between the object and the representation in the mind of the object that came to bedevil Enlightenment thought (Hume, Kant). As one commentator noted \"to speak at all of the unity of intelligence and nature is of course flatly to contradict Descartes.\"\n\nFor Coleridge the power of life lies in every seed as a potential to be unfolded as a result of interaction with the environment (heat, light, air, moisture, etc.), an insight which allowed him to see in the Brunonian system a dynamic polarity in excitation theory.\nColeridge also saw that there was a progressive movement through time and space of life or the law of polarity, from the level of physics (space and time) and the mineral or inert nature (law of gravity, operating through forces of attraction and repulsion), up to man, with his law of resonance in terms of his innate desire to be himself (force of individuation) and to also connect with like-minded (force of connection), as Goethe expressed in his novel \"Elective Affinities\" (\"Wahlverwandschaften\") as well as in his own life's experience.\nEvolution occurred because the original polarity of creation, the very 'Law of Creation', itself gives birth to subsequent polarities, as each pole is itself a unity that can be further polarized (what Wilhelm Reich later termed 'orgonomic functionalism' and what at the biological level constitutes physiology), an insight that would later be taken up by the concept of emergent evolution, including the emergence of mind and consciousness.\n\nAnd that this is so, is also an intimate and shared experience of all humans, as is set out in Reid's Common Sense philosophy. As Coleridge states\n\nThat nature evolves towards a purpose, and that is the unfolding of the human mind and consciousness in all its levels and degrees, is not teleological but a function of the very nature of the law of polarity or creation itself, namely that of increasing individuation of an original unity, what Coleridge termed 'multeity in unity'. As he states, \"without assigning to nature as nature, a conscious purpose\" we must still \"distinguish her agency from a blind and lifeless mechanism.\"\n\nWhile man contains and is subject to the various laws of nature, man as a self-conscious being is also the summa of a process of creation leading to greater mind and consciousness, that is, a creative capacity of imagination. Instead of being a creature of circumstance, man is the creator of them, or at least has that potential.\n\n"}
{"id": "322355", "url": "https://en.wikipedia.org/wiki?curid=322355", "title": "Deep time", "text": "Deep time\n\nDeep time is the concept of geologic time. The modern philosophical concept was developed in the 18th century by Scottish geologist James Hutton (1726–1797). The age of the Earth has been determined to be, after a long and complex history of developments, around 4.55 billion years.\n\nHutton based his view of deep time on a form of geochemistry that had developed in Scotland and Scandinavia from the 1750s onward. As mathematician John Playfair, one of Hutton's friends and colleagues in the Scottish Enlightenment, remarked upon seeing the strata of the angular unconformity at Siccar Point with Hutton and James Hall in June 1788, \"the mind seemed to grow giddy by looking so far into the abyss of time\".\n\nEarly geologists such as Nicolas Steno (1638-1686) and Horace-Bénédict de Saussure (1740-1799) had developed ideas of geological strata forming from water through chemical processes, which Abraham Gottlob Werner (1749–1817) developed into a theory known as Neptunism, envisaging the slow crystallisation of minerals in the ancient oceans of the Earth to form rock. Hutton's innovative 1785 theory, based on Plutonism, visualised an endless cyclical process of rocks forming under the sea, being uplifted and tilted, then eroded to form new strata under the sea. In 1788 the sight of Hutton's Unconformity at Siccar Point convinced Playfair and Hall of this extremely slow cycle, and in that same year Hutton memorably wrote \"we find no vestige of a beginning, no prospect of an end\".\n\nOther scientists such as Georges Cuvier (1769-1832) put forward ideas of past ages, and geologists such as Adam Sedgwick (1785-1873) incorporated Werner's ideas into concepts of catastrophism; Sedgwick inspired his university student Charles Darwin to exclaim \"What a capital hand is Sedgewick [sic] for drawing large cheques upon the Bank of Time!\". In a competing theory, Charles Lyell in his \"Principles of Geology\" (1830–1833) developed Hutton's comprehension of endless deep time as a crucial scientific concept into uniformitarianism. As a young naturalist and geological theorist, Darwin studied the successive volumes of Lyell's book exhaustively during the \"Beagle\" survey voyage in the 1830s, before beginning to theorise about evolution.\n\nPhysicist Gregory Benford addresses the concept in \"Deep Time: How Humanity Communicates Across Millennia\" (1999), as does paleontologist and \"Nature\" editor Henry Gee in \"In Search of Deep Time: Beyond the Fossil Record to a New History of Life\" (2001) Stephen Jay Gould's \"Time's Arrow, Time's Cycle\" (1987) also deals in large part with the evolution of the concept.\n\nJohn McPhee discussed \"deep time\" at length with the layperson in mind in \"Basin and Range\" (1981), parts of which originally appeared in the \"New Yorker\" magazine. In \"Time's Arrow, Time's Cycle\", Gould cited one of the metaphors McPhee used in explaining the concept of deep time:\nConsider the Earth's history as the old measure of the English yard, the distance from the King's nose to the tip of his outstretched hand. One stroke of a nail file on his middle finger erases human history.\nConcepts similar to geologic time were recognized in the 11th century by the Persian geologist and polymath Avicenna (Ibn Sina, 973–1037), and by the Chinese naturalist and polymath Shen Kuo (1031–1095).\n\nThe Roman Catholic theologian Thomas Berry (1914–2009) explored spiritual implications of the concept of deep time. Berry proposes that a deep understanding of the history and functioning of the evolving universe is a necessary inspiration and guide for our own effective functioning as individuals and as a species. This view has greatly influenced the development of deep ecology and ecophilosophy. The experiential nature of the experience of deep time has also greatly influenced the work of Joanna Macy and John Seed.\n\nH.G. Wells and Julian Huxley regarded the difficulties of coping with the concept of deep time as exaggerated:\n\"The use of different scales is simply a matter of practice\", they said in \"The Science of Life\" (1929). \"We very soon get used to maps, though they are constructed on scales down to a hundred-millionth of natural size. . .  to grasp geological time all that is needed is to stick tight to some magnitude which shall be the unit on the new and magnified scale—a million years is probably the most convenient—to grasp its meaning once and for all by an effort of imagination, and then to think of all passage of geological time in terms of this unit.\"\n\n\n"}
{"id": "53365898", "url": "https://en.wikipedia.org/wiki?curid=53365898", "title": "Earliest known life forms", "text": "Earliest known life forms\n\nThe earliest known life forms on Earth are putative fossilized microorganisms found in hydrothermal vent precipitates. The earliest time that life forms first appeared on Earth is unknown. They may have lived earlier than 3.77 billion years ago, possibly as early as 4.28 billion years ago, or nearly 4.5 billion years ago according to some; in any regards, not long after the oceans formed 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. The earliest \"direct\" evidence of life on Earth are microfossils of microorganisms permineralized in 3.465-billion-year-old Australian Apex chert rocks.\n\nA life form, or lifeform, is an entity or being that is living.\n\nCurrently, Earth remains the only place in the universe known to harbor life forms.\n\nMore than 99% of all species of life forms, amounting to over five billion species, that ever lived on Earth are estimated to be extinct.\n\nSome estimates on the number of Earth's current species of life forms range from 10 million to 14 million, of which about 1.2 million have been documented and over 86 percent have not yet been described. However, a May 2016 scientific report estimates that 1 trillion species are currently on Earth, with only one-thousandth of one percent described. The total number of DNA base pairs on Earth is estimated at 5.0 x 10 with a weight of 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 trillion tons of carbon. In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.\n\nThe Earth's biosphere includes soil, hydrothermal vents, and rock up to or deeper underground, the deepest parts of the ocean, and at least high into the atmosphere. Under certain test conditions, life forms have been observed to thrive in the near-weightlessness of space and to survive in the vacuum of outer space. Life forms appear to thrive in the Mariana Trench, the deepest spot in the Earth's oceans, reaching a depth of . Other researchers reported related studies that life forms thrive inside rocks up to below the sea floor under of ocean, off the coast of the northwestern United States, as well as beneath the seabed off Japan. In August 2014, scientists confirmed the existence of life forms living below the ice of Antarctica.\n\nAccording to one researcher, \"You can find microbes everywhere — [they are] extremely adaptable to conditions, and survive wherever they are.\"\n\nFossil evidence informs most studies of the origin of life. The age of the Earth is about 4.54 billion years; the earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago. There is evidence that life began much earlier.\n\nIn 2017, fossilized microorganisms, or microfossils, were announced to have been discovered in hydrothermal vent precipitates in the Nuvvuagittuq Belt of Quebec, Canada that may be as old as 4.28 billion years old, the oldest record of life on Earth, suggesting \"an almost instantaneous emergence of life\" (in a geological time-scale sense), after ocean formation 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. Nonetheless, life may have started even earlier, at nearly 4.5 billion years ago, as claimed by some researchers.\n\n\"Remains of life\" have been found in 4.1 billion-year-old rocks in Western Australia.\n\nEvidence of biogenic graphite, and possibly stromatolites, were discovered in 3.7 billion-year-old metasedimentary rocks in southwestern Greenland.\n\nIn May 2017, evidence of life on land may have been found in 3.48 billion-year-old geyserite which is often found around hot springs and geysers, and other related mineral deposits, uncovered in the Pilbara Craton of Western Australia. This complements the November 2013 publication that microbial mat fossils had been found in 3.48 billion-year-old sandstone in Western Australia.\n\nIn November 2017, a study by the University of Edinburgh suggested that life on Earth may have originated from biological particles carried by streams of space dust.\n\nA December 2017 report stated that 3.465-billion-year-old Australian Apex chert rocks once contained microorganisms, the earliest \"direct\" evidence of life on Earth.\n\nIn January 2018, a study found that 4.5 billion-year-old meteorites found on Earth contained liquid water along with prebiotic complex organic substances that may be ingredients for life.\n\nAccording to biologist Stephen Blair Hedges, \"If life arose relatively quickly on Earth … then it could be common in the universe.\"\n\n"}
{"id": "29247528", "url": "https://en.wikipedia.org/wiki?curid=29247528", "title": "Earth's shadow", "text": "Earth's shadow\n\nEarth's shadow or Earth shadow is the shadow that Earth itself casts onto its atmosphere and into outer space, toward the antisolar point. During twilight (both early dusk and late dawn), the shadow's visible fringe (sometimes called the dark segment or twilight wedge) appears in a clear sky as a dark and diffused band low above the horizon.\n\nEarth's shadow cast onto the atmosphere can be viewed during the \"civil\" stage of twilight, assuming the sky is clear and the horizon is relatively unobstructed. The shadow's fringe appears as a dark bluish to purplish band that stretches over 180° of the horizon opposite the Sun, i.e. in the eastern sky at dusk and in the western sky at dawn. Before sunrise, Earth's shadow appears to recede as the Sun rises; after sunset, the shadow appears to rise as the Sun sets.\n\nEarth's shadow is best seen when the horizon is low, such as over the sea, and when the sky conditions are clear. In addition, the higher the observer's elevation is to view the horizon, the sharper the shadow appears.\n\nA related phenomenon in the same part of the sky is the Belt of Venus, or anti-twilight arch, a pinkish band visible above the bluish shade of Earth's shadow, named after the planet Venus which, when visible, is typically located in this region of the sky.\nNo defined line divides the Earth's shadow and the Belt of Venus; one colored band blends into the other in the sky.\n\nThe Belt of Venus is quite a different phenomenon from the afterglow, which appears in the geometrically opposite part of the sky.\n\nWhen the Sun is near the horizon around sunset or sunrise, the sunlight appears reddish. This is because the light rays are penetrating an especially thick layer of the atmosphere, which works as a filter, scattering all but the longer (redder) wavelengths. \n\nFrom the observer's perspective, the red sunlight directly illuminates small particles in the lower atmosphere in the sky opposite of the Sun. The red light is backscattered to the observer, which is the reason why the Belt of Venus appears pink. \n\nThe lower the setting Sun descends, the less defined the boundary between Earth's shadow and the Belt of Venus appears. This is because the setting Sun now illuminates a thinner part of the upper atmosphere. There the red light is not scattered because fewer particles are present, and the eye only sees the \"normal\" (usual) blue sky, which is due to Rayleigh scattering from air molecules. Eventually, both Earth's shadow and the Belt of Venus dissolve into the darkness of the night sky.\n\nEarth's shadow is as curved as the planet is, and its umbra extends into outer space. (The antumbra, however, extends indefinitely.) When the Sun, Earth, and the Moon are aligned perfectly (or nearly so), with Earth between the Sun and the Moon, Earth's shadow falls onto the lunar surface facing the night side of the planet, such that the shadow gradually darkens the full Moon, causing a lunar eclipse. \n\nEven during a total lunar eclipse, a small amount of sunlight however still reaches the Moon. This indirect sunlight has been refracted as it passed through Earth's atmosphere. The air molecules and particulates in Earth's atmosphere scatter the shorter wavelengths of this sunlight; thus, the longer wavelengths of reddish light reaches the Moon, in the same way that light at sunset or sunrise appears reddish. This weak red illumination gives the eclipsed Moon a dimly reddish or copper color.\n\n\n"}
{"id": "9951602", "url": "https://en.wikipedia.org/wiki?curid=9951602", "title": "Earth mass", "text": "Earth mass\n\nEarth mass (, where ⊕ is the standard astronomical symbol for planet Earth) is the unit of mass equal to that of Earth. \nThe current best estimate for Earth mass is , with a standard uncertainty of \nIt is equivalent to an average density of .\n\nThe Earth mass is a standard unit of mass in astronomy that is used to indicate the masses of other planets, including rocky terrestrial planets and exoplanets. One Solar mass is close to 333,000 Earth masses.\nThe Earth mass excludes the mass of the Moon. The mass of the Moon is about 1.2% of that of the Earth, so that the mass of the Earth+Moon system is close to .\n\nMost of the mass is accounted for by iron and oxygen (c. 32% each), magnesium and silicon (c. 15% each), calcium, aluminium and nickel (c. 1.5% each).\n\nPrecise measurement of the Earth mass is difficult, as it is equivalent to measuring the gravitational constant, which is the fundamental physical constant known with least accuracy, due to the relative weakness of the gravitational force.\nThe mass of the Earth was measured accurately in the Schiehallion experiment in the 1770s, and within 1% of the modern value in the Cavendish experiment of 1798.\n\nThe mass of Earth is estimated to be:\nwhich can be expressed in terms of solar mass as:\n\nThe ratio of Earth mass to lunar mass has been measured to great accuracy. The current best estimate is:\n\nThe \"G\" product for the Earth is called the geocentric gravitational constant and equals . It is determined using laser ranging data from Earth-orbiting satellites, such as LAGEOS-1. The \"G\" product can also be calculated by observing the motion of the Moon or the period of a pendulum at various elevations. These methods are less precise than observations of artificial satellites.\n\nThe relative uncertainty of the geocentric gravitational constant is just , however, (the mass of the Earth in kilograms) can be found out only by dividing the \"G\" product by \"G\", and \"G\" is known only to a relative uncertainty of \nFor this reason and others, astronomers prefer to use the un-reduced \"G\" product, or mass ratios\n(masses expressed in units of Earth mass or Solar mass) rather than mass in kilograms when referencing and comparing planetary objects.\n\nEarth's density varies considerably, between less than in the upper crust to as much as in the inner core.\nThe Earth's core accounts for 15% of Earth's volume but more than 30% of the mass, the mantle for 84% of the volume and close to 70% of the mass, while the crust accounts for less than 1% of the mass.\nAbout 90% of the mass of the Earth is composed of the iron–nickel alloy (95% iron) in the core (30%), and the silicon dioxides (c. 33%) and magnesium oxide (c. 27%) in the mantle and crust. \nMinor contributions are from iron(II) oxide (5%), aluminium oxide (3%) and calcium oxide (2%), besides numerous trace elements (in elementary terms: iron and oxygen c. 32% each, magnesium and silicon c. 15% each, calcium, aluminium and nickel c. 1.5% each). \nCarbon accounts for 0.03%, water for 0.02%, and the atmosphere for about one part per million.\n\nThe mass of Earth is measured indirectly by determining other quantities such as Earth's density, gravity, or gravitational constant.\nThe first measurement in the 1770s Schiehallion experiment resulted in a value about 20% too low.\nThe Cavendish experiment of 1798 found the correct value within 1%.\nUncertainty was reduced to about 0.2% by the 1890s, \nto 0.1% by 1930,\nand to 0.01% (10) by the 2000s.\nThe figure of the Earth has been known to better than four significant digits since the 1960s (WGS66), so that since that time, the uncertainty of the Earth mass is determined essentially by the uncertainty in measuring the gravitational constant.\n\nBefore the direct measurement of the gravitational constant,\nestimates of the Earth mass were limited to estimating Earth's mean density from observation of the crust and estimates on Earth's volume. Estimates on the volume of the earth in the 17th century were based on a circumference estimate of 60 miles to the degree of latitude, corresponding to a radius of about 5,500 km, resulting in an estimated volume of about one third smaller than the correct value.\nThe average density of the Earth was not accurately known. Earth was assumed to consist either mostly of water (Neptunism) or mostly of igneous rock (Plutonism), both suggesting average densities too low by several orders of magnitude,\nconsistent with a total mass of the order of .\nIsaac Newton estimated, without access to reliable measurement, that the density of Earth would be five or six times as great as the density of water, which is surprisingly accurate (the modern value is 5.515).\nNewton under-estimated the Earth's volume by about 30%, so that his estimate would be roughly equivalent to .\nIn the 18th century, knowledge of Newton's law of gravitation permitted indirect estimates on the mean density of the Earth, \nvia estimates of (what in modern terminology is known as) the gravitational constant.\nEarly estimates on the mean density of the Earth were made by observing the slight deflection of a pendulum near a mountain, as in the Schiehallion experiment. Newton considered the experiment in \"Principia\", but pessimistically concluded that the effect would be too small to be measurable.\n\nAn expedition from 1737 to 1740 by Pierre Bouguer and Charles Marie de La Condamine attempted to determine the density of Earth by measuring the period of a pendulum (and therefore the strength of gravity) as a function of elevation. The experiments were carried out in Ecuador and Peru, on Pichincha Volcano and mount Chimborazo.\nBouguer wrote in a 1749 paper that they had been able to detect a deflection of 8 seconds of arc,\nThe accuracy was not enough for a definite estimate on the mean density of the Earth, but Bouguer stated that it was at least sufficient to prove that the Earth was not hollow.\n\nThat a further attempt should be made on the experiment was proposed to the Royal Society in 1772 by Nevil Maskelyne, Astronomer Royal. He suggested that the experiment would \"do honour to the nation where it was made\" and proposed Whernside in Yorkshire, or the Blencathra-Skiddaw massif in Cumberland as suitable targets. The Royal Society formed the Committee of Attraction to consider the matter, appointing Maskelyne, Joseph Banks and Benjamin Franklin amongst its members. The Committee despatched the astronomer and surveyor Charles Mason to find a suitable mountain.\n\nAfter a lengthy search over the summer of 1773, Mason reported that the best candidate was Schiehallion, a peak in the central Scottish Highlands. The mountain stood in isolation from any nearby hills, which would reduce their gravitational influence, and its symmetrical east–west ridge would simplify the calculations. Its steep northern and southern slopes would allow the experiment to be sited close to its centre of mass, maximising the deflection effect.\nNevil Maskelyne, Charles Hutton and Reuben Burrow performed the experiment, completed by 1776.\nHutton (1778) reported that the mean density of the Earth was estimated at \nformula_4 that of Schiehallion mountain.\nThis corresponds to a mean density about 4 higher than that of water (i.e., about ), about 20% below the modern value, but still significantly larger than the mean density of normal rock, suggesting for the first time that the interior of the Earth might be substantially \ncomposed of metal.\nHutton estimated this metallic portion to occupy some (or 65%) of the diameter of the Earth (modern value 55%). \nWith a value for the mean density of the Earth, Hutton was able to set some values to Jérôme Lalande's planetary tables, which had previously only been able to express the densities of the major solar system objects in relative terms.\n\nThe Henry Cavendish (1798) was the first to attempt to measure the gravitational attraction between two bodies directly in the laboratory.\nEarth's mass could be then found by combining two equations; Newton's second law, and Newton's law of universal gravitation.\n\nIn modern notation, the mass of the Earth is derived from the gravitational constant and the mean Earth radius by\nWhere \"little g\":\n\nCavendish found a mean density of , about 1% below the modern value.\n\nWhile the mass of the Earth is implied by stating the Earth's radius and density, it was not usual to state the absolute mass explicitly \nprior to the introduction of scientific notation using powers of 10 in the later 19th century, \nbecause the absolute numbers would have been too awkward. Ritchie (1850) gives the mass of the Earth's atmosphere as \"11,456,688,186,392,473,000 lbs.\" ( = , modern value is ) and states that \"compared with the weight of the globe this mighty sum dwindles to insignificance\".\n\nAbsolute figures for the mass of the Earth are cited only beginning in the second half of the 19th century, mostly in popular rather than expert literature.\nAn early such figure was given as \"14 quadrillion pounds\" (\"14 Quadrillionen Pfund\") [] in Masius (1859).\n\nBeckett (1871) cites the \"weight of the earth\" as \"5842 quintillion tons\" [].\nThe \"mass of the earth in gravitational measure\" is stated as \"9.81996×6370980\" in \"The New Volumes of the Encyclopaedia Britannica\" (Vol. 25, 1902) with a \"logarithm of earth's mass\" given as \"14.600522\" []. This is the gravitational parameter in m·s (modern value ) and not the absolute mass.\n\nExperiments involving pendulums continued to be performed in the first half of the 19th century. By the second half of the century, these were outperformed by repetitions of the Cavendish experiment, and the modern value of \"G\" (and hence, of the Earth mass) is still derived from high-precision repetitions of the Cavendish experiment.\n\nIn 1821, Francesco Carlini determined a density value of ρ = through measurements made with pendulums in the Milan area. This value was refined in 1827 by Edward Sabine to , and then in 1841 by Carlo Ignazio Giulio to . On the other hand, George Biddell Airy sought to determine ρ by measuring the difference in the period of a pendulum between the surface and the bottom of a mine. \nThe first tests took place in Cornwall between 1826 and 1828. The experiment was a failure due to a fire and a flood. Finally, in 1854, Airy got the value by measurements in a coal mine in Harton, Sunderland. Airy's method assumed that the Earth had a spherical stratification. Later, in 1883, the experiments conducted by Robert von Sterneck (1839 to 1910) at different depths in mines of Saxony and Bohemia provided the average density values ρ between 5.0 and . This led to the concept of isostasy, which limits the ability to accurately measure ρ, by either the deviation from vertical of a plumb line or using pendulums. Despite the little chance of an accurate estimate of the average density of the Earth in this way, Thomas Corwin Mendenhall in 1880 realized a gravimetry experiment in Tokyo and at the top of Mount Fuji. The result was ρ = .\n\nThe uncertainty in the modern value for the Earth's mass has been entirely due to the uncertainty in the gravitational constant \"G\" since at least the 1960s.\n\"G\" is notoriously difficult to measure, and some high-precision measurements during the 1980s to 2010s have yielded mutually exclusive results.\nSagitov (1969) based on the measurement of \"G\" by Heyl and Chrzanowski (1942) cited a value of (relative uncertainty ).\n\nAccuracy has improved only slightly since then. Most modern measurements are repetitions of the Cavendish experiment, with results (within standard uncertainty) ranging between 6.672 and 6.676 ×10  m kgs (relative uncertainty 3×10) in results reported since the 1980s, although the 2014 NIST recommended value is close to 6.674×10  m kgs with a relative uncertainty below 10.\nThe \"Astronomical Almanach Online\" as of 2016 recommends a standard uncertainty of for Earth mass, \n\nEarth's mass is variable, subject to both gain and loss due to the accretion of micrometeorites and cosmic dust \nand the loss of hydrogen and helium gas, respectively.\nThe combined effect is a net loss of material, estimated at (54,000 tons) per year. This amount is of the total earth mass. The annual net loss is essentially due to 100,000 tons lost due to atmospheric escape, and an average of 45,000 tons gained from in-falling dust and meteorites. This is well within the mass uncertainty of 0.01% (), so the estimated value of earth's mass is unaffected by this factor.\n\nMass loss is due to atmospheric escape of gases. About 95,000 tons of hydrogen per year () and 1,600 tons of helium per year are lost through atmospheric escape.\nThe main factor in mass gain is in-falling material, cosmic dust, meteors, etc. are the most significant contributors to Earth's increase in mass. The sum of material is estimated to be 37,000 to 78,000 tons annually.\n\nAdditional changes in mass are due to the mass–energy equivalence principal, although these changes are relatively negligible. An increase in mass has been ascribed to rising temperatures (global warming), estimated at 160 tonnes per years as of 2016.\nAnother 16 tons per year are lost in the form of rotational kinetic energy due to the deceleration of the rotation of Earth's inner core. This energy is transferred to the rotational energy of the solar system, and the trend might also be reversible, as rotation speed has been shown to fluctuate over decades. Mass loss due to nuclear fission is estimated to amount to 16 tons per year.\nAn additional loss due to spacecraft on escape trajectories has been estimated at since the mid-20th century. Earth lost about 3473 tons in the initial 53 years of the space age, but the trend is currently decreasing.\n\n"}
{"id": "40159918", "url": "https://en.wikipedia.org/wiki?curid=40159918", "title": "Ecosystem health", "text": "Ecosystem health\n\nEcosystem health is a metaphor used to describe the condition of an ecosystem. Ecosystem condition can vary as a result of fire, flooding, drought, extinctions, invasive species, climate change, mining, overexploitation in fishing, farming or logging, chemical spills, and a host of other reasons. There is no universally accepted benchmark for a healthy ecosystem, rather the apparent health status of an ecosystem can vary depending upon which health metrics are employed in judging it and which societal aspirations are driving the assessment. Advocates of the health metaphor argue for its simplicity as a communication tool. \"Policy-makers and the public need simple, understandable concepts like health.\" Critics worry that ecosystem health, a \"value-laden construct\", is often \"passed off as science to unsuspecting policy makers and the public.\"\n\nThe health metaphor applied to the environment has been in use at least since the early 1800s and the great American conservationist Aldo Leopold (1887–1948) spoke metaphorically of land health, land sickness, mutilation, and violence when describing land use practices. The term \"ecosystem management\" has been in use at least since the 1950s. The term \"ecosystem health\" has become widespread in the ecological literature, as a general metaphor meaning something good, and as an environmental quality goal in field assessments of rivers, lakes, seas, and forests.\n\nRecently however this metaphor has been subject of quantitative formulation using complex systems concepts such as criticality, meaning that a healthy ecosystem is in some sort of balance between adaptability (randomness) and robustness (order) . Nevertheless the universality of criticality is still under examination and is known as the Criticality Hypothesis, which states that systems in a dynamic regime shifting between order and disorder, attain the highest level of computational capabilities and achieve an optimal trade-off between robustness and flexibility. Recent results in cell and evolutionary biology, neuroscience and computer science have great interest in the criticality hypothesis, emphasizing its role as a viable candidate general law in the realm of adaptive complex systems (see and references therein).\n\nThe term ecosystem health has been employed to embrace some suite of environmental goals deemed desirable. Edward Grumbine's highly cited paper \"What is ecosystem management?\" surveyed ecosystem management and ecosystem health literature and summarized frequently encountered goal statements:\n\nGrumbine describes each of these goals as a \"value statement\" and stresses the role of human values in setting ecosystem management goals.\n\nIt is the last goal mentioned in the survey, accommodating humans, that is most contentious. \"We have observed that when groups of stakeholders work to define … visions, this leads to debate over whether to emphasize ecosystem health or human well-being … Whether the priority is ecosystems or people greatly influences stakeholders' assessment of desirable ecological and social states.\" and, for example, \"For some, wolves are critical to ecosystem health and an essential part of nature, for others they are a symbol of government overreach threatening their livelihoods and cultural values.\"\n\nMeasuring ecosystem health requires extensive goal-driven environmental sampling. For example, a vision for ecosystem health of Lake Superior was developed by a public forum and a series of objectives were prepared for protection of habitat and maintenance of populations of some 70 indigenous fish species. A suite of 80 lake health indicators was developed for the Great Lakes Basin including monitoring native fish species, exotic species, water levels, phosphorus levels, toxic chemicals, phytoplankton, zooplankton, fish tissue contaminants, etc.\n\nSome authors have attempted broad definitions of ecosystem health, such as benchmarking as healthy the historical ecosystem state \"prior to the onset of anthropogenic stress.\" A difficulty is that the historical composition of many human-altered ecosystems is unknown or unknowable. Also, fossil and pollen records indicate that the species that occupy an ecosystem reshuffle through time, so it is difficult to identify one snapshot in time as optimum or \"healthy.\".\n\nA commonly cited broad definition states that a healthy ecosystem has three attributes:\n\nWhile this captures significant ecosystem properties, a generalization is elusive as those properties do not necessarily co-vary in nature. For example, there is not necessarily a clear or consistent relationship between productivity and species richness. Similarly, the relationship between resilience and diversity is complex, and ecosystem stability may depend upon one or a few species rather than overall diversity. And some undesirable ecosystems are highly productive.\n\n\"Resilience is not desirable per se. There can be highly resilient states of ecosystems which are very undesirable from some human perspectives , such as algal-dominated coral reefs.\" Ecological resilience is a \"capacity\" that varies depending upon which properties of the ecosystem are to be studied and depending upon what kinds of disturbances are considered and how they are to be quantified. Approaches to assessing it \"face high uncertainties and still require a considerable amount of empirical and theoretical research.\"\n\nOther authors have sought a numerical index of ecosystem health that would permit quantitative comparisons among ecosystems and within ecosystems over time. One such system employs ratings of the three properties mentioned above: Health = system vigor x system organization x system resilience. Ecologist Glenn Suter argues that such indices employ \"nonsense units,\" the indices have \"no meaning; they cannot be predicted, so they are not applicable to most regulatory problems; they have no diagnostic power; effects of one component are eclipsed by responses of other components, and the reason for a high or low index value is unknown.\"\n\nHealth metrics are determined by stakeholder goals, which drive ecosystem definition. An ecosystem is an abstraction. \"Ecosystems cannot be identified or found in nature. Instead, they must be delimited by an observer. This can be done in many different ways for the same chunk of nature, depending on the specific perspectives of interest.\"\n\nEcosystem definition determines the acceptable range of variability (reference conditions) and determines measurement variables. The latter are used as indicators of ecosystem structure and function, and can be used as indicators of \"health\".\n\nAn indicator is a variable, such as a chemical or biological property, that when measured, is used to infer trends in another (unmeasured) environmental variable or cluster of unmeasured variables (the indicandum). For example, rising mortality rate of canaries in a coal mine is an indicator of rising carbon monoxide levels. Rising chlorophyll-a levels in a lake may signal eutrophication.\n\nEcosystem assessments employ two kinds of indicators, descriptive indicators and normative indicators. \"Indicators can be used descriptively for a scientific purpose or normatively for a political purpose.\"\n\nUsed descriptively, high chlorophyll-a is an indicator of eutrophication, but it may also be used as an ecosystem health indicator. When used as a normative (health) indicator, it indicates a rank on a health scale, a rank that can vary widely depending on societal preferences as to what is desirable. A high chlorophyll-a level in a natural successional wetland might be viewed as healthy whereas a human-impacted wetland with the \"same\" indicator value may be judged unhealthy.\n\nEstimation of ecosystem health has been criticized for intermingling the two types of environmental indicators. A health indicator is a normative indicator, and if conflated with descriptive indicators \"implies that normative values can be measured objectively, which is certainly not true. Thus, implicit values are insinuated to the reader, a situation which has to be avoided.\"\n\nIt can be argued that the very act of selecting indicators of any kind is biased by the observer's perspective but separation of goals from descriptions has been advocated as a step toward transparency: \"A separation of descriptive and normative indicators is essential from the perspective of the philosophy of science … Goals and values cannot be deduced directly from descriptions … a fact that is emphasized repeatedly in the literature of environmental ethics … Hence, we advise always specifying the definition of indicators and propose clearly distinguishing ecological indicators in science from policy indicators used for decision-making processes.\"\n\nAnd integration of multiple, possibly conflicting, normative indicators into a single measure of \"ecosystem health\" is problematic. Using 56 indicators, \"determining environmental status and assessing marine ecosystems health in an integrative way is still one of the grand challenges in marine ecosystems ecology, research and management\"\n\nAnother issue with indicators is validity. Good indicators must have an independently validated high predictive value, that is high sensitivity (high probability of indicating a significant change in the indicandum) and high specificity (low probability of wrongly indicating a change). The reliability of various health metrics has been questioned and \"what combination of measurements should be used to evaluate ecosystems is a matter of current scientific debate.\" Most attempts to identify ecological indicators have been correlative rather than derived from prospective testing of their predictive value and the selection process for many indicators has been based upon weak evidence or has been lacking in evidence.\n\nIn some cases no reliable indicators are known: \"We found no examples of invertebrates successfully used in [forest] monitoring programs. Their richness and abundance ensure that they play significant roles in ecosystem function but thwart focus on a few key species.\" And, \"Reviews of species-based monitoring approaches reveal that no single species, nor even a group of species, accurately reflects entire communities. Understanding the response of a single species may not provide reliable predictions about a group of species even when the group is a few very similar species.\"\n\nA trade-off between human health and the \"health\" of nature has been termed the \"health paradox\" and it illuminates how human values drive perceptions of ecosystem health.\n\nHuman health has benefited by sacrificing the \"health\" of wild ecosystems, such as dismantling and damming of wild valleys, destruction of mosquito-bearing wetlands, diversion of water for irrigation, conversion of wilderness to farmland, timber removal, and extirpation of tigers, whales, ferrets, and wolves.\n\nThere has been an acrimonious schism among conservationists and resource managers over the question of whether to \"ratchet back human domination of the biosphere\" or whether to embrace it. These two perspectives have been characterized as utilitarian vs protectionist.\n\nThe utilitarian view treats human health and well-being as criteria of ecosystem health. For example, destruction of wetlands to control malaria mosquitoes \"resulted in an improvement in ecosystem health.\"\nThe protectionist view treats humans as an invasive species: \"If there was ever a species that qualified as an invasive pest, it is \"Homo sapiens\",\"\n\nProponents of the utilitarian view argue that \"healthy ecosystems are characterized by their capability to sustain healthy human populations,\" and \"healthy ecosystems must be economically viable,\" as it is \"unhealthy\" ecosystems that are likely to result in increases in contamination, infectious diseases, fires, floods, crop failures and fishery collapse.\n\nProtectionists argue that privileging of human health is a conflict of interest as humans have demolished massive numbers of ecosystems to maintain their welfare, also disease and parasitism are historically normal in pre-industrial nature. Diseases and parasites promote ecosystem functioning, driving biodiversity and productivity, and parasites may constitute a significant fraction of ecosystem biomass.\n\nThe very choice of the word \"health\" applied to ecology has been questioned as lacking in neutrality in a BioScience article on responsible use of scientific language: \"Some conservationists fear that these terms could endorse human domination of the planet … and could exacerbate the shifting cognitive baseline whereby humans tend to become accustomed to new and often degraded ecosystems and thus forget the nature of the past.\"\n\nCriticism of ecosystem health largely targets the failure of proponents to explicitly distinguish the normative dimension from the descriptive dimension, and has included the following:\n\nAlternatives have been proposed for the term ecosystem health, including more neutral language such as ecosystem status, ecosystem prognosis, and ecosystem sustainability. Another alternative to the use of a health metaphor is to \"express exactly and clearly the public policy and the management objective\", to employ habitat descriptors and real properties of ecosystems. An example of a policy statement is \"The maintenance of viable natural populations of wildlife and ecological functions always takes precedence over any human use of wildlife.\" An example of a goal is \"Maintain viable populations of all native species in situ.\" An example of a management objective is \"Maintain self-sustaining populations of lake whitefish within the range of abundance observed during 1990-99.\"\n\nKurt Jax presented an ecosystem assessment format that avoids imposing a preconceived notion of normality, that avoids the muddling of normative and descriptive, and that gives serious attention to ecosystem definition. (1) Societal purposes for the ecosystem are negotiated by stakeholders, (2) a functioning ecosystem is defined with emphasis on phenomena relevant to stakeholder goals, (3) benchmark reference conditions and permissible variation of the system are established, (4) measurement variables are chosen for use as indicators, and (5) the time scale and spatial scale of assessment are decided.\n\nEcological health has been used as a medical term in reference to human allergy and multiple chemical sensitivity and as a public health term for programs to modify health risks (diabetes, obesity, smoking, etc.). Human health itself, when viewed in its broadest sense, is viewed as having ecological foundations. It is also an urban planning term in reference to \"green\" cities (composting, recycling), and has been used loosely with regard to various environmental issues, and as the condition of human-disturbed environmental sites. Ecosystem integrity implies a condition of an ecosystem exposed to a minimum of human influence. Ecohealth is the relationship of human health to the environment, including the effect of climate change, wars, food production, urbanization, and ecosystem structure and function. Ecosystem management and ecosystem-based management refer to the sustainable management of ecosystems and in some cases may employ the terms ecosystem health or ecosystem integrity as a goal.\n"}
{"id": "321382", "url": "https://en.wikipedia.org/wiki?curid=321382", "title": "Energy flow (ecology)", "text": "Energy flow (ecology)\n\nIn ecology, energy flow, also called the calorific flow, refers to the flow of energy through a food chain, and is the focus of study in ecological energetics. In an ecosystem, ecologists seek to quantify the relative importance of different component species and feeding relationships.\n\nA general energy flow scenario follows:\n\nThe energy is passed on from trophic level to trophic level and each time about 90% of the energy is lost, with some being lost as heat into the environment (an effect of respiration) and some being lost as incompletely digested food (egesta). Therefore, primary consumers get about 10% of the energy produced by autotrophs, while secondary consumers get 1% and tertiary consumers get 0.1%. This means the top consumer of a food chain receives the least energy, as a lot of the food chain's energy has been lost between trophic levels. This loss of energy at each level limits typical food chains to only four to six links.\n\nEcological energetics appears to have grown out of the Age of Enlightenment and the concerns of the Physiocrats. It began in the works of Sergei Podolinsky in the late 1800s, and subsequently was developed by the Soviet ecologist Vladmir Stanchinsky, the Austro-American Alfred J. Lotka, and American limnologists, Raymond Lindeman and G. Evelyn Hutchinson. It underwent substantial development by Howard T. Odum and was applied by systems ecologists, and radiation ecologists.\n\n\n"}
{"id": "4329772", "url": "https://en.wikipedia.org/wiki?curid=4329772", "title": "Exertion", "text": "Exertion\n\nExertion is the physical or perceived use of energy. Exertion traditionally connotes a strenuous or costly \"effort,\"resulting in generation of force, initiation of motion, or in the performance of work. It often relates to muscularactivity and can be quantified, empirically and by measurable metabolic response.\n\nIn physics, \"exertion\" is the expenditure of energy against, or inductive of, inertia as described by Isaac Newton's third law of motion. In physics, force exerted equivocates work done. The ability to do work can be either positive or negative depending on the direction of exertion relative to gravity. For example, a force exerted upwards, like lifting an object, creates positive work done on that object.\n\nExertion often results in force generated, a contributing dynamic of general motion. In mechanics it describes the use of force against a body in the direction of its motion (see vector).\n\nExertion, physiologically, can be described by the initiation of exercise, or, intensive and exhaustive physical activity that causes cardiovascular stress or a sympathetic nervous response. This can be continuous or intermittent exertion.\n\nExertion requires, of the body, modified oxygen uptake, increased heart rate, and autonomic monitoring of blood lactate concentrations. Mediators of physical exertion include cardio-respiratory and musculoskeletal strength, as well as metabolic capability. This often correlates to an output of force followed by a refractory period of recovery. Exertion is limited by cumulative load and repetitive motions.\n\nMuscular energy reserves, or stores for biomechanical exertion, stem from metabolic, immediate production of ATP and increased O2 consumption. Muscular exertion generated depends on the muscle length and the velocity at which it is able to shorten, or contract.\n\nPerceived exertion can be explained as subjective, perceived experience that mediates response to somatic sensations and mechanisms. A rating of perceived exertion, as measured by the \"RPE-scale\", or Borg scale, is a quantitative measure of physical exertion.\n\nOften in health, exertion of oneself resulting in cardiovascular stress showed reduced physiological responses, like cortisol levels and mood, to stressors. Therefore, biological exertion is effective in mediating psychological exertion, responsive to environmental stress.\n\nOverexertion causes more than 3.5 million injuries a year. An overexertion injury can include sprains or strains, the stretching and tear of ligaments, tendons, or muscles caused by a load that exceeds the human ability to perform the work. Overexertion, besides causing acute injury, implies physical exertion beyond the person's capacity which leads to symptoms such as dizziness, irregular breathing and heart rate, and fatigue. Preventative measures can be taken based on biomechanical knowledge to limit possible overexertion injuries.\n\n\n"}
{"id": "8609949", "url": "https://en.wikipedia.org/wiki?curid=8609949", "title": "Foreline", "text": "Foreline\n\nA foreline is a vacuum line between the pumps of a multistage vacuum system. No longer exclusively used by scientists in research, vacuum systems are used in numerous industries that include food production and the manufacturing of electronic components. A classic foreline is a hose or tube that connects a rotary vane pump to the outlet of an oil diffusion pump, although vacuum technology now uses a vast array of roughing pumps and high vacuum pumps.\n\nAlthough the term has been used for many decades in the high vacuum system industry and in literature concerning vacuum systems, the word foreline has not yet been added to any dictionary.\n"}
{"id": "32974036", "url": "https://en.wikipedia.org/wiki?curid=32974036", "title": "Fowkes hypothesis", "text": "Fowkes hypothesis\n\nThe Fowkes hypothesis (after F. M. Fowkes) is a first order approximation for surface energy. It states the surface energy is the sum of each component's forces:\nγ=γ+γ+γ+...\nwhere γ is the dispersion component, γ is the polar, γ is the dipole and so on.\n\nThe Fowkes hypothesis goes further making the approximation that the interface between an apolar liquid and apolar solid where there are only dispersive interactions acting across the interface can be estimated using the geometric mean of the contributions from each surface i.e.\n\nγ=γ+γ-2(γ x γ)\n\n\n"}
{"id": "48107455", "url": "https://en.wikipedia.org/wiki?curid=48107455", "title": "George-ericksenite", "text": "George-ericksenite\n\nGeorge-ericksenite is a mineral with the chemical formula NaCaMg(IO)(CrO)(HO). It is vitreous, pale yellow to bright lemon yellow, brittle, and features a prismatic to acicular crystal habit along [001] and somewhat flattened crystal habit on {110}. It was first encountered in 1984 at the Pinch Mineralogical Museum. One specimen of dietzeite from Oficina Chacabuco, Chile had bright lemon-yellow micronodules on it. These crystals produced an X-ray powder diffraction pattern that did not match any XRD data listed for inorganic compounds. The X-ray diffraction pattern and powder mount were set aside until 1994. By then, the entire mineral collection from the Pinch Mineralogical Museum had been purchased by the Canadian Museum of Nature. The specimen was then retrieved and studied further. This study was successful and the new mineral george-ericksenite was discovered. The mineral was named for George E. Ericksen who was a research economic geologist with the U.S. Geological Survey for fifty years. The mineral and name have been approved by Commission on New Minerals and Mineral Names (IMA). The specimen, polished thin section, and the actual crystal used for the structure determination are kept in the Display Series of the National Mineral Collection of Canada at the Canadian Museum of Nature, Ottawa, Ontario.\n\nGeorge-ericksonite is commonly found as isolated bright lemon-yellow micronodules of crystals that are concentrated on the surface of one part of the mineral specimen. However, in some cases the micronodules occur as groupings instead of isolated occurrences. The average size of these micronodules is approximately 0.2 mm and each consists of numerous individual crystals in random orientation.\n\nThis examination was carried out by attaching two acicular crystals to the surface of a disk with epoxy and then examining them with a CAMECA SX-50 electron microprobe. One of the crystals had the (100) surface facing up, and the other crystal had a growth face of the form (110) facing up. The microprobe was operating in wavelength-dispersive mode at 15 kV and ran various currents from 20 nA to 0.5 nA. The CAMECA SX-50 has three spectrometers and the samples were examined in the sequence (Na, Cl, I), then (Mg, S, Ca). When the crystal was exposed to the electron beam for the first 200 seconds, the counts per second on each element varied greatly which indicates that the crystals are extremely unstable in the electron beam. The counts per second for each element were also dependent on the surface of the crystal [(100) or (110)] analyzed. Over shorter counting times (<10 s) at 15 kV and 5 nA, there is a gain in IO and a drop in NaO relative to ideal values. However, a significant orientation effect exists for SO and CaO values for the (100) and (110) surfaces on either side of the ideal values. With increasing exposure to the electron beam, NaO increases and all other oxides decrease. This behavior is also more rapid on the (100) surface than on the (110) crystal face. The (100) surface is overall more reactive to the electron beam than the (110) surface, but both surfaces seem to approach equilibrium with the beam and give similar oxides weight percentages after 200 seconds.\n\nEven at low currents and short counting times george-ericksenite is extremely unstable under the electron beam. After examination, the crystal faces are stained brown from the reaction with I and the decrease in analyzed IO with increasing time. The crystallographic orientation of the material analyzed has a large impact on the analytical values at any given time. The overall quantitative behavior of george-ericksenite in the electron beam is consistent with the chemical composition derived (ideally IO 59.13, CrO 9.92. SO 1.51, MgO 2.38, CaO 3.31, NaO 10.98, HO 12.77 weight %).\n\nA 0.046 X 0.059 X 0.060 mm crystal was mounted on a Siemans \"P\"4 four-circle diffractometer. The crystal was aligned using 42 reflections automatically centered following measurement from a rotation photograph. The orientation matrix and unit-cell dimensions were determined from the setting angles of least-squares refinement. 3872 reflections were recorded out to 60 2θ with a fixed scan speed of 1.33° 2θ/min. Corrections for absorption by Gaussian quadrature integration were applied. Corrections for Lorentz, polarization, and background effects were also applied as well as reduction of intensities to structure factors.\n\nThe SHELXTL PC Plus system of programs were used for the calculations. The \"R\" and \"Rw\" indices are of the conventional form. The structure was solved by direct methods. The structure is centrosymmetric as indicated by the E statistics. Systematic absences also indicate the presence of a \"c\" glide for the C-centered cell. The result was placing george-ericksenite in the C2/c space group. The structure was refined by a combination of least-squares refinement and difference-Fourier synthesis to an \"R\" index of 3.5% and \"Rw\" equal to 3.5%. Site occupancies were determined by the basis of site-scattering refinement and crystal-chemical criteria.\n\nThere is one chromium (Cr) site that is symmetrically distinct and is tetrahedrally coordinated by four oxygen (O) atoms. The average length of the bonds is 1.61 Å which indicates that the Cr cation is hexavalent. The average bond length at the Cr site is less than would be expected for complete occupancy by Cr. This difference can be accounted for by partial substitution of sulfur (S) atoms.\n\nThere are three iodine (I) sites that are coordinated by three oxygen (O) atoms arranged in a triangle to one side of the cation. The distances of the bonds between the I and O atoms is 1.81 Å. This results in the IO group forming a triangular pyramid with the I site at the top of the pyramid. At each I sites there are also three additional ligands that causes the iodine atoms to have a distorted octahedral coordination. This also causes the I atom to occupy off centered positions within each octahedron. The long bonds between the atoms at the I sites contributes significant bond valence to the bonded anions.\n\nThere are three sodium (Na) sites that are each unique. Each site has a different type of coordination. The Na1 site is encompassed by two O atoms and four HO groups in a distorted octahedral arrangement with a Na1-Φ distance (where Φ=unspecified ligand) of 2.41 Å. The Na2 site is surrounded by five O atoms and 2 HO groups in an augmented octahedral arrangement with a Na2-Φ distance of 2.54 Å. The Na3 site is surrounded by five O atoms and three HO atoms in a triangular dodecahedral arrangement with a Na3-Φ distance of 2.64 Å.\n\nThere is only one magnesium (Mg) site which is coordinated by six O atoms in an octahedral arrangement with a Mg-O distance of 2.09 Å. This bond length is in accord with this site being entirely occupied by Mg with not substitution.\n\nThe one calcium (Ca) site is coordinated by six O atoms and two HO groups in a square-antiprismatic arrangement with a Ca-Φ distance of 2.50 Å. This bond length is in accord with this site being entirely occupied by Ca with not substitution.\n\nGeorge-ericksenite features a structural arrangement that is composed of slabs of polyhedra orthogonal to [100]. These slabs feature the same composition as the mineral itself and are a half of a unit thick in the [100] direction. These are connected to adjacent slabs solely by hydrogen bonding. The edges of each slab are bounded by near-planar layers of anions. The slabs themselves are composed of three planar layers of cations. There are also three planar layers of cations parallel to the edges of the slabs. This indicates that each slab consists of three layers of polyhedra. The \"c\"-glide symmetry relates the top and bottom of the slab which means the slab may be broken into two unique sheets of polyhedra.\n\nThere is a prominent zigzag pattern of chains of Na polyhedra extending in the \"c\" direction on the outer layer of the slab. The Na1 octahedron shares an edge with the Na2 augmented octahedron which shares a face with the Na3 triangular dodecahedron. This forms a linear trimer that extends in the [011] direction. This trimer is then links by edge-sharing between the Na3 and a1 polyhedra to another trimer extending in the [0-11] direction. This motif continues to form a [NaΦ] zigzag chain extending in the \"c\" direction. In each embayment of this chain the polyhedra are accented by two (IO) groups. Identical chains run parallel to the \"c\" axis that are linked only by one weak I-O bond.\n\nThe inner layer of the slab is composed of one Mg octahedron that shares corners with two Cr tetrahedra. This forms a [MTΦ] cluster. The other two anions of the Mg octahedron link by corner-sharing to two (IO) groups. These [Mg(CrO)(IO)O] clusters link together two (CaΦ) polyhedra. This forms chains parallel to the \"b\" axis. Weak I-O bonds link these chains to form the central layer of the slab.\n\nThe only other chromate-iodate mineral is dietzeite, Ca(IO)(CrO)(HO). Dietzeite and george-ericksenite have no structural relationship. Fuenzalidaite and carlosruizite are sulphate-iodate minerals found in the Chilean nitrate fields and contain small amounts of Cr substituting for S. They are also sheet structures, but the sheets are vastly different in terms of connectivity than in george-ericksenite.\n"}
{"id": "2169038", "url": "https://en.wikipedia.org/wiki?curid=2169038", "title": "Homochirality", "text": "Homochirality\n\nHomochirality is a uniformity of chirality, or handedness. Objects are \"chiral\" when they cannot be superposed on their mirror images. For example, the left and right hands of a human are approximately mirror images of each other but are not their own mirror images, so they are \"chiral\". In biology, 19 of the 20 natural amino acids are homochiral, being -chiral (left-handed), while sugars are -chiral (right-handed). \"Homochirality\" can also refer to \"enantiomerically pure\" substances in which all the constituents are the same enantiomer (a right-handed or left-handed version of an atom or molecule), but some sources discourage this use of the term.\n\nIt is unclear whether homochirality has a purpose, however, it appears to be a form of information storage. One suggestion is that it reduces entropy barriers in the formation of large organized molecules. It has been experimentally verified that amino acids form large aggregates in larger abundance from enantiopure substrates than from racemic ones.\n\nIt is not clear whether homochirality emerged before or after life, and many mechanisms for its origin have been proposed. Some of these models propose three distinct steps: \"mirror-symmetry breaking\" creates a minute enantiomeric imbalance, \"chiral amplification\" builds on this imbalance, and \"chiral transmission\" is the transfer of chirality from one set of molecules to another.\n\nAmino acids are the building blocks of peptides and enzymes while sugar-peptide chains are the backbone of RNA and DNA. In biological organisms, amino acids appear almost exclusively in the left-handed form (-amino acids) and sugars in the right-handed form (R-sugars). Since the enzymes catalyze reactions, they enforce homochirality on a great variety of other chemicals, including hormones, toxins, fragrances and food flavors. Glycine is achiral, as are some other non-proteinogenic amino acids are either achiral (such as dimethylglycine) or of the enantiomeric form.\n\nBiological organisms easily discriminate between molecules with different chiralities. This can affect physiological reactions such as smell and taste. Carvone, a terpenoid found in essential oils, smells like mint in its L-form and caraway in its R-form. Limonene tastes like lemons when right-handed and oranges when left-handed.\n\nHomochirality also affects the response to drugs. Thalidomide, in its left-handed form, cures morning sickness; in its right-handed form, it causes birth defects. Unfortunately, even if a pure left-handed version is administered, some of it can convert to the right-handed form in the patient. Many drugs are available as both a racemic mixture (equal amounts of both chiralities) and an enantiopure drug (only one chirality). Depending on the manufacturing process, enantiopure forms can be more expensive to produce than stereochemical mixtures.\n\nChiral preferences can also be found at a macroscopic level. Snail shells can be right-turning or left-turning helices, but one form or the other is strongly preferred in a given species. In the edible snail \"Helix pomatia\", only one out of 20,000 is left-helical. The coiling of plants can have a preferred chirality and even the chewing motion of cows has a 10% excess in one direction.\n\nKnown mechanisms for the production of non-racemic mixtures from racemic starting materials include: asymmetric physical laws, such as the electroweak interaction; asymmetric environments, such as those caused by circularly polarized light, quartz crystals, or the Earth's rotation; and statistical fluctuations during racemic synthesis. Once established, chirality would be selected for. A small enantiomeric excess can be amplified into a large one by asymmetric autocatalysis, such as in the Soai reaction. In asymmetric autocatalysis, the catalyst is a chiral molecule, which means that a chiral molecule is catalysing its own production. An initial enantiomeric excess, such as can be produced by polarized light, then allows the more abundant enantiomer to outcompete the other.\n\nOne supposition is that the discovery of an enantiomeric imbalance in molecules in the Murchison meteorite supports an extraterrestrial origin of homochirality: there is evidence for the existence of circularly polarized light originating from Mie scattering on aligned interstellar dust particles which may trigger the formation of an enantiomeric excess within chiral material in space. Interstellar and near-stellar magnetic fields can align dust particles in this fashion. Another speculation (the Vester-Ulbricht hypothesis) suggests that fundamental chirality of physical processes such as that of the beta decay (see Parity violation) leads to slightly different half-lives of biologically relevant molecules. Homochirality may also result from spontaneous absolute asymmetric synthesis.\n\nIt is also possible that homochirality is simply a result of the natural autoamplification process of life—that either the formation of life as preferring one chirality or the other was a chance rare event which happened to occur with the chiralities we observe, or that all chiralities of life emerged rapidly but due to catastrophic events and strong competition, the other unobserved chiral preferences were wiped out by the preponderance and metabolic, enantiomeric enrichment from the 'winning' chirality choices. The emergence of chirality consensus as a natural autoamplification process has been associated with the 2nd law of thermodynamics.\n\nIn 1953, Charles Frank proposed a model to demonstrate that homochirality is a consequence of autocatalysis. In his model the and enantiomers of a chiral molecule are autocatalytically produced from an achiral molecule A\n\nwhile suppressing each other through a reaction that he called \"mutual antagonism\"\nIn this model the racemic state is unstable in the sense that the slightest enantiomeric excess will be amplified to a completely homochiral state. This can be shown by computing the reaction rates from the law of mass action:\nwhere formula_2 is the rate constant for the autocatalytic reactions, formula_3 is the rate constant for mutual antagonism reaction, and the concentration of A is kept constant for simplicity. By defining the enantiomeric excess formula_4 as\nwe can compute the rate of change of enatiomeric excess using chain rule from the rate of change of the concentrations of enantiomeres and .\nLinear stability analysis of this equation shows that the racemic state formula_7 is unstable. Starting from almost everywhere in the concentration space, the system evolves to a homochiral state.\n\nIt is generally understood that autocatalysis alone does not yield to homochirality, and the presence of the mutually antagonistic relationship between the two enantiomers is necessary for the instability of the racemic mixture. However, recent studies show that homochirality could be achieved from autocatalysis in the absence of the mutually antagonistic relationship, but the underlying mechanism for symmetry-breaking is different.\n\nThere are several laboratory experiments that demonstrate how a small amount of one enantiomer at the start of a reaction can lead to a large excess of a single enantiomer as the product. For example, the Soai reaction is autocatalytic. If the reaction is started with some of one of the product enantiomers already present, the product acts as an enantioselective catalyst for production of more of that same enantiomer. The initial presence of just 0.2 equivalent one enantiomer can lead to up to 93% enantiomeric excess of the product.\n\nAnother study concerns the proline catalyzed aminoxylation of propionaldehyde by nitrosobenzene. In this system, a small enantiomeric excess of catalyst leads to a large enantiomeric excess of product.\n\nSerine octamer clusters are also contenders. These clusters of 8 serine molecules appear in mass spectrometry with an unusual homochiral preference, however there is no evidence that such clusters exist under non-ionizing conditions and amino acid phase behavior is far more prebiotically relevant. The recent observation that partial sublimation of a 10% enantioenriched sample of leucine results in up to 82% enrichment in the sublimate shows that enantioenrichment of amino acids could occur in space. Partial sublimation processes can take place on the surface of meteors where large variations in temperature exist. This finding may have consequences for the development of the Mars Organic Detector scheduled for launch in 2013 which aims to recover trace amounts of amino acids from the Mars surface exactly by a sublimation technique.\n\nA high asymmetric amplification of the enantiomeric excess of sugars are also present in the amino acid catalyzed asymmetric formation of carbohydrates\n\nOne classic study involves an experiment that takes place in the laboratory. When sodium chlorate is allowed to crystallize from water and the collected crystals examined in a polarimeter, each crystal turns out to be chiral and either the form or the form. In an ordinary experiment the amount of crystals collected equals the amount of crystals (corrected for statistical effects). However, when the sodium chlorate solution is stirred during the crystallization process the crystals are either exclusively or exclusively . In 32 consecutive crystallization experiments 14 experiments deliver -crystals and 18 others -crystals. The explanation for this symmetry breaking is unclear but is related to autocatalysis taking place in the nucleation process.\n\nIn a related experiment, a crystal suspension of a racemic amino acid derivative continuously stirred, results in a 100% crystal phase of one of the enantiomers because the enantiomeric pair is able to equilibrate in solution (compare with dynamic kinetic resolution).\n\nMany strategies in asymmetric synthesis are built on chiral transmission. Especially important is the so-called organocatalysis of organic reactions by proline for example in Mannich reactions.\n\nThere exists no theory elucidating correlations among -amino acids. If one takes, for example, alanine, which has a small methyl group, and phenylalanine, which has a larger benzyl group, a simple question is in what aspect, -alanine resembles -phenylalanine more than -phenylalanine, and what kind of mechanism causes the selection of all -amino acids. Because it might be possible that alanine was and phenylalanine was .\n\nIt was reported in 2004 that excess racemic ,-asparagine (Asn), which spontaneously forms crystals of either isomer during recrystallization, induces asymmetric resolution of a co-existing racemic amino acid such as arginine (Arg), aspartic acid (Asp), glutamine (Gln), histidine (His), leucine (Leu), methionine (Met), phenylalanine (Phe), serine (Ser), valine (Val), tyrosine (Tyr), and tryptophan (Trp). The enantiomeric excess of these amino acids was correlated almost linearly with that of the inducer, i.e., Asn. When recrystallizations from a mixture of 12 ,-amino acids (Ala, Asp, Arg, Glu, Gln, His, Leu, Met, Ser, Val, Phe, and Tyr) and excess ,-Asn were made, all amino acids with the same configuration with Asn were preferentially co-crystallized. It was incidental whether the enrichment took place in - or -Asn, however, once the selection was made, the co-existing amino acid with the same configuration at the α-carbon was preferentially involved because of thermodynamic stability in the crystal formation. The maximal ee was reported to be 100%. Based on these results, it is proposed that a mixture of racemic amino acids causes spontaneous and effective optical resolution, even if asymmetric synthesis of a single amino acid does not occur without an aid of an optically active molecule.\n\nThis is the first study elucidating reasonably the formation of chirality from racemic amino acids with experimental evidences.\n\nThis term was introduced by Kelvin in 1904, the year that he published his Baltimore Lecture of 1884. Kelvin used the term homochirality as a relationship between two molecules, i.e. two molecule are homochiral if they have the same chirality. Recently, however, homochiral has been used in the same sense as enantiomerically pure. This is permitted in some journals (but not encouraged), its meaning changing into the preference of a process or system for a single optical isomer in a pair of isomers in these journals.\n\n\n"}
{"id": "26242611", "url": "https://en.wikipedia.org/wiki?curid=26242611", "title": "Imperial College Faculty of Natural Sciences", "text": "Imperial College Faculty of Natural Sciences\n\nThe Faculty of Natural Sciences is one of the three main faculties of Imperial College London in London, England. It was formed in 2001 from the former Royal College of Science, a constituent college of Imperial College which dated back to 1848, and the faculty largely consists of the original departments of the college. Undergraduate teaching occurs for all departments at the South Kensington campus, with research being split between South Kensington and the new innovation campus at White City.\n\nStudents who study at the departments of the faulty are represented by the Royal College of Science Union, a constituent union of the college union which caters specifically to students on natural science courses. Graduates who obtain an undergraduate degree, either BSc or MSci, from the faculty are awarded the Associateship of the Royal College of Science (ARCS) as an additional degree.\n\nThe origins of the faculty lie in the Royal College of Chemistry, which, after being founded in 1845, moved to a new site in South Kensington in the early 1870s. Incorporated into the Normal School of Science, the college was later renamed the Royal College of Science in 1890, and in 1907 became a constituent college of the newly formed Imperial College of Science and Technology.\n\nIn 2001, Imperial College was restructured to form four new faculties, including the faculties of Physical Sciences and Life Sciences, which took over the role of the Royal College of Science. These faculties were later re-merged over the course of 2005-2006 to form the Faculty of Natural Sciences, which comprises the same departments as the original Royal College of Science.\n\nThe faculty includes five academic departments:\n\n\nImperial College Faculty of Natural Sciences website\n"}
{"id": "41566158", "url": "https://en.wikipedia.org/wiki?curid=41566158", "title": "Janet Lembke", "text": "Janet Lembke\n\nJanet Lembke (2 March 1933 – 3 September 2013), \"née\" Janet Nutt, was an American author, essayist, naturalist, translator and scholar. She was born in Cleveland, Ohio during the Great Depression, graduated in 1953 from Middlebury College, Vermont, with a degree in Classics, and her knowledge of the classical Greek and Latin worldview, from Homer to Virgil, informed her life and work. A Certified Virginia Master Gardener, she lived in Virginia and North Carolina, drawing inspiration from both locales. She was recognized for her creative view of natural cycles, agriculture and of animals, both domestic and wild, with whom we share the natural environment. Referred to as an \"acclaimed Southern naturalist,\" she was equally (as The Chicago Tribune described her) a \"classicist, a noted Oxford University Press translator of the works of Sophocles, Euripides and Aeschylus\". She received a grant from the National Endowment for the Arts to translate Virgil's Georgics, having already translated Euripides' \"Electra\" and \"Hecuba\", and Aeschylus's \"Persians\" and \"Suppliants\".\n\nJanet Lembke's first book was \"Bronze and Iron: Old Latin Poetry from Its Beginnings to 100 B.C.\" (1973), but beyond translations and essays about classics, there were more than a dozen books on nature, works for which the author acquired a base of admirers. Her articles were printed in The New York Times, \"Sierra Magazine\" (The Sierra Club), Oxford American, Audubon, Raleigh News and Observer, Southern Review and other publications. The writing style was eclectic and personal, meditative and detailed, and though she was at least once accused of \"taking poetic license too far\" in her translation of \"Georgics\", readers were often charmed and seduced by her way of weaving scientific fact, history and culture, with personal anecdote, mythological allusion and poetic feeling. \"The author's ability to pull together disparate elements in her writing is impressive, and her passionate connection with the natural world is displayed in line after line,\" wrote The New York Times. Novelist Annie Proulx expressed a similar perception, observing that \"Lembke's writing tacks between three points: the stuff of her late-twentieth-century life; the tangle of creature and plant in every dimension of tide and river flow; and the haunting, connecting wires of mythos that still knot us to the ancient beginnings.\"\n\nAmong Janet Lembke's noted titles were \"Because the Cat Purrs: How We Relate to Other Species and Why It Matters\" (2008); \"Skinny Dipping: And Other Immersions in Water, Myth, and Being Human\" (2004); \"Dangerous Birds\" (1996); \"River Time\" (1997); \"Despicable Species: On Cowbirds, Kudzu, Hornworms, and Other Scourges\" (1999); and \"The Quality of Life: Living Well, Dying Well\" (2004)-- a sober and unflinching account of the death of the author's mother. At the time of her own death at age 80 in Staunton, Virginia, Janet Lembke was working on a memoir, \"I Married an Arsonist\". She had married twice, and had four children and six grandchildren.\n\nThere is a repository of archived materials (\"The Janet Lembke Papers, 1966 - 2008\"), including notes and correspondence by the author, at the Jackson Library of the University of North Carolina in Greensboro, NC.\n\n\n\n\n"}
{"id": "5232171", "url": "https://en.wikipedia.org/wiki?curid=5232171", "title": "Levant bole", "text": "Levant bole\n\nLevant bole is an earthy clay brought from the Levant, and historically used in medicine for the same purposes as Armenian bole. It was indeed so similar to Armenian bole that some believed them both to be the same, or at least mixtures of each other. Levant bole was used in several compositions, particularly diascodium, to give it color.\n\nChambers discusses two other similar boles: \n\n"}
{"id": "41782533", "url": "https://en.wikipedia.org/wiki?curid=41782533", "title": "List of Earth flybys", "text": "List of Earth flybys\n\nList of Earth flybys is a list of cases where spacecraft incidentally performed Earth flybys, typically for a gravity assist to another body.\n"}
{"id": "49659014", "url": "https://en.wikipedia.org/wiki?curid=49659014", "title": "List of cat documentaries, television series and cartoons", "text": "List of cat documentaries, television series and cartoons\n\nList of cat documentaries, television series and cartoons includes \"serious\" documentaries, television series and cartoons, in alphabetical order, related to cats .\n\n\n\n"}
{"id": "31744838", "url": "https://en.wikipedia.org/wiki?curid=31744838", "title": "List of dates predicted for apocalyptic events", "text": "List of dates predicted for apocalyptic events\n\nPredictions of apocalyptic events that would result in the extinction of humanity, a collapse of civilization, or the destruction of the planet have been made since at least the beginning of the Common Era. Most predictions are related to Abrahamic religions, often standing for or similar to the eschatological events described in their scriptures. Christian predictions typically refer to events like the Rapture, Great Tribulation, Last Judgment, and the Second Coming of Christ. End-time events are usually predicted to occur within the lifetime of the person making the prediction, and are usually made using the Bible, and in particular the New Testament, as either the primary or exclusive source for the predictions. Often this takes the form of mathematical calculations, such as trying to calculate the point where it will have been 6000 years since the supposed creation of the Earth by the Abrahamic God, which according to the Talmud marks the deadline for the Messiah to appear. Predictions of the end from natural events have also been theorised by various scientists and scientific groups. While these predictions are generally accepted as plausible within the scientific community, the events and phenomena are not expected to occur for hundreds of thousands or even billions of years from now.\n\nLittle research has been done into why people make apocalyptic predictions. Historically, it has been done for reasons such as diverting attention from actual crises like poverty and war, pushing political agendas, and promoting hatred of certain groups; antisemitism was a popular theme of Christian apocalyptic predictions in medieval times, while French and Lutheran depictions of the apocalypse were known to feature English and Catholic antagonists respectively. According to psychologists, possible explanations for why people believe in modern apocalyptic predictions include mentally reducing the actual danger in the world to a single and definable source, an innate human fascination with fear, personality traits of paranoia and powerlessness and a modern romanticism involved with end-times due to its portrayal in contemporary fiction. The prevalence of Abrahamic religions throughout modern history is said to have created a culture which encourages the embracement of a future that will be drastically different from the present. Such a culture is credited with the rise in popularity of predictions that are more secular in nature, such as the 2012 phenomenon, while maintaining the centuries-old theme that a powerful force will bring the end of humanity.\n\nPolls conducted in 2012 across 20 countries found over 14% of people believe the world will end in their lifetime, with percentages ranging from 6% of people in France to 22% in the US and Turkey. Belief in the apocalypse is most prevalent in people with lower rates of education, lower household incomes, and those under the age of 35. In the UK in 2015, 23% of the general public believed the apocalypse was likely to occur in their lifetime, compared to 10% of experts from the Global Challenges Foundation. The general public believed the likeliest cause would be nuclear war, while experts thought it would be artificial intelligence. Only 3% of Britons thought the end would be caused by the Last Judgement, compared to 16% of Americans. Between one and three percent of people from both countries thought the apocalypse would be caused by zombies or alien invasion.\n\nThis section lists eschatological predictions, mostly by religious individuals or groups. Most predictions are related to Abrahamic religions, with numerous predictions standing for the eschatological events described in their scriptures. Christian predictions typically refer to events like the Rapture, Great Tribulation, Last Judgment, and the Second Coming of Christ.\n\n\n"}
{"id": "18012286", "url": "https://en.wikipedia.org/wiki?curid=18012286", "title": "List of herbaria", "text": "List of herbaria\n\nThis is a list of herbaria, organized first by continent where the herbarium is located, then within each continent by size of the collection. A herbarium (\"plural\" \"herbaria\") is a collection of preserved plant specimens. These specimens may be whole plants or plant parts: these will usually be in a dried form, mounted on a sheet, but depending upon the material may also be kept in alcohol or other preservative. The same term is often used in mycology to describe an equivalent collection of preserved fungi and in phycology to describe a collection of algae.\n\nTo preserve their form and color, plants collected in the field are spread flat on sheets of newsprint and dried, usually in a plant press, between blotters or absorbent paper. The specimens, which are then mounted on sheets of stiff white paper, are labeled with all essential data, such as date and place found, description of the plant, altitude, and special habitat conditions. The sheet is then placed in a protective case. As a precaution against insect attack, the pressed plant is frozen or poisoned and the case disinfected.\n\nMost herbaria utilize a standard system of organizing their specimens into herbarium cases. Specimen sheets are stacked in groups by the species to which they belong and placed into a large lightweight folder that is labelled on the bottom edge. Groups of species folders are then placed together into larger, heavier folders by genus. The genus folders are then sorted by taxonomic family according to the standard system selected for use by the herbarium and placed into pigeonholes in herbarium cabinets. Herbaria are essential for the study of plant taxonomy, the study of geographic distributions, and the stabilizing of nomenclature. Herbaria also preserve an historical record of change in vegetation over time. In some cases, plants become extinct in one area, or may become extinct altogether. In such cases, specimens preserved in an herbarium can represent the only record of the plant's original distribution. Environmental scientists make use of such data to track changes in climate and human impact.\n\n"}
{"id": "18027464", "url": "https://en.wikipedia.org/wiki?curid=18027464", "title": "List of herbaria in North America", "text": "List of herbaria in North America\n\nThis is a list of herbaria in North America, organized first by country or region where the herbarium is located, then within each region by size of the collection. For other continents, see List of herbaria.\n\nThe table below lists herbaria located in Central America and the Caribbean.\n\nAdditional Collection Resources:\n\nListed alphabetically by Herbarium Code. Note that this list includes herbaria that are inactive, meaning that the institutions are not currently adding new materials to their collections. This list also includes herbaria that have been incorporated into other herbaria.\n\nSee also: List of herbaria.\n"}
{"id": "37660", "url": "https://en.wikipedia.org/wiki?curid=37660", "title": "List of parasitic organisms", "text": "List of parasitic organisms\n\nThis is an \"incomplete list\" of organisms that are true parasites upon other organisms.\n\n\nThese can be categorized into three groups; cestodes, nematodes and trematodes. Examples include:\n\n\n\n\n\nMonogeneans are flatworms, generally ectoparasites on fish.\n\n\n\n\n\n"}
{"id": "16092414", "url": "https://en.wikipedia.org/wiki?curid=16092414", "title": "List of raised and transitional bogs of Switzerland", "text": "List of raised and transitional bogs of Switzerland\n\nThis is a list of raised and transitional bogs of Switzerland. It is based on the \"Federal Inventory of Raised and Transitional Bogs of National Importance\". The inventory is part of a 1991 Ordinance of the Swiss Federal Council implementing the Federal Law on the Protection of Nature and Cultural Heritage.\n\n\n"}
{"id": "4065564", "url": "https://en.wikipedia.org/wiki?curid=4065564", "title": "List of the seven natural wonders of Georgia (U.S. state)", "text": "List of the seven natural wonders of Georgia (U.S. state)\n\nThe Seven Natural Wonders of Georgia are considered to be:\n\n\nThe first list of natural wonders was compiled by state librarian Ella May Thornton and published in the \"Atlanta Georgian\" magazine on December 26, 1926. That first list included:\n\n"}
{"id": "60773", "url": "https://en.wikipedia.org/wiki?curid=60773", "title": "List of woods", "text": "List of woods\n\nThis is a list of woods, in particular those most commonly used in the timber and lumber trade.\n\n\n\n\n\n"}
{"id": "1842698", "url": "https://en.wikipedia.org/wiki?curid=1842698", "title": "Loren Eiseley", "text": "Loren Eiseley\n\nLoren Eiseley (September 3, 1907 – July 9, 1977) was an American anthropologist, educator, philosopher, and natural science writer, who taught and published books from the 1950s through the 1970s. He received many honorary degrees and was a fellow of multiple professional societies. At his death, he was Benjamin Franklin Professor of Anthropology and History of Science at the University of Pennsylvania.\n\nHe was a \"scholar and writer of imagination and grace,\" whose reputation and accomplishments extended far beyond the campus where he taught for 30 years. \"Publishers Weekly\" referred to him as \"the modern Thoreau.\" The broad scope of his writing reflected upon such topics as the mind of Sir Francis Bacon, the prehistoric origins of man, and the contributions of Charles Darwin.\n\nEiseley's reputation was established primarily through his books, including \"The Immense Journey\" (1957), \"Darwin's Century\" (1958), \"The Unexpected Universe\" (1969), \"The Night Country\" (1971), and his memoir, \"All the Strange Hours\" (1975). Science author Orville Prescott praised him as a scientist who \"can write with poetic sensibility and with a fine sense of wonder and of reverence before the mysteries of life and nature.\" Naturalist author Mary Ellen Pitts saw his combination of literary and nature writings as his \"quest, not simply for bringing together science and literature... but a continuation of what the 18th and 19th century British naturalists and Thoreau had done.\" In praise of \"The Unexpected Universe\", Ray Bradbury remarked, \"[Eiseley] is every writer's writer, and every human's human... One of us, yet most uncommon...\" \nAccording to his obituary in the \"New York Times\", the feeling and philosophical motivation of the entire body of Eiseley's work was best expressed in one of his essays, \"The Enchanted Glass:\" \"The anthropologist wrote of the need for the contemplative naturalist, a man who, in a less frenzied era, had time to observe, to speculate, and to dream.\" Shortly before his death he received an award from the Boston Museum of Science for his \"outstanding contribution to the public understanding of science\" and another from the U.S. Humane Society for his \"significant contribution for the improvement of life and environment in this country.\"\n\nBorn in Lincoln, Nebraska, Eiseley lived his childhood with a hardworking father and deaf mother who may have suffered from mental illness. Their home was located on the outskirts of town where, as author Naomi Brill writes, it was \"removed from the people and the community from which they felt set apart through poverty and family misfortune.\" His autobiography, \"All the Strange Hours\", begins with his \"childhood experiences as a sickly afterthought, weighed down by the loveless union of his parents.\"\n\nHis father, Clyde, was a hardware salesman who worked long hours for little pay, writes Brill. However, as an amateur Shakespearean actor, he was able to give his son a \"love for beautiful language and writing.\" His mother, Daisey Corey, was a self-taught prairie artist who was considered a beautiful woman. She lost her hearing as a child and sometimes exhibited irrational and destructive behavior. This left Eiseley feeling distant from her and may have contributed to his parents' unhappy marriage.\n\nLiving at the edge of town, however, led to Eiseley's early interest in the natural world, to which he turned when being at home was too difficult. There, he would play in the caves and creek banks nearby. Fortunately, there were others who opened the door to a happier life. His half-brother, Leo, for instance, gave him a copy of \"Robinson Crusoe\", with which he taught himself to read. Thereafter, he managed to find ways to get to the public library and became a voracious reader.\n\nEiseley later attended the Lincoln Public Schools; in high school, he wrote that he wanted to be a nature writer. He would later describe the lands around Lincoln as \"flat and grass-covered and smiling so serenely up at the sun that they seemed forever youthful, untouched by mind or time—a sunlit, timeless prairie over which nothing passed but antelope or wandering bird.\" But, disturbed by his home situation and the illness and recent death of his father, he dropped out of school and worked at menial jobs.\n\nEiseley enrolled in the University of Nebraska, where he wrote for the newly formed journal, \"Prairie Schooner\", and went on archaeology digs for the school's natural history museum, Morrill Hall. In 1927, however, he was diagnosed with tuberculosis and left the university to move to the western desert, believing the drier air would improve his condition. While there, he soon became restless and unhappy, which led him to hoboing around the country by hopping on freight trains (as many did during the Great Depression). Professor of religion, Richard Wentz, writes about this period:\n\nLoren Eiseley had been a drifter in his youth. From the plains of Nebraska he had wandered across the American West. Sometimes sickly, at other times testing his strength with that curious band of roving exiles who searched the land above the rippling railroad ties, he explored his soul as he sought to touch the distant past. He became a naturalist and a bone hunter because something about the landscape had linked his mind to the birth and death of life itself.\n\nEiseley eventually returned to the University of Nebraska and received a B.A. degree in English and a B.S. degree in Geology/Anthropology. While at the university, he served as editor of the literary magazine \"The Prairie Schooner\", and published his poetry and short stories. Undergraduate expeditions to western Nebraska and the southwest to hunt for fossils and human artifacts provided the inspiration for much of his early work. He later noted that he came to anthropology from paleontology, preferring to leave human burial sites undisturbed unless destruction threatened them.\n\nEiseley received his Ph.D. degree from the University of Pennsylvania in 1937 and wrote his dissertation entitled \"Three Indices of Quaternary Time and Their Bearing Upon Pre-History: a Critique\", which launched his academic career. He began teaching at the University of Kansas that same year. During World War II, Eiseley taught anatomy to reservist pre-med students at Kansas.\n\nIn 1944 he left the University of Kansas to assume the role of head of the Department of Sociology and Anthropology at Oberlin College in Ohio. In 1947 he returned to the University of Pennsylvania to head its Anthropology Department. He was elected president of the American Institute of Human Paleontology in 1949. From 1959 to 1961, he was provost at the University of Pennsylvania, and in 1961 the University of Pennsylvania created a special interdisciplinary professorial chair for him.\n\nEiseley was also a fellow of many distinguished professional societies, including the American Association for the Advancement of Science, the United States National Academy of Sciences, the National Institute of Arts and Letters and the American Philosophical Society.\n\nAt the time of his death in 1977, he was Benjamin Franklin Professor of Anthropology and History of Science, and the curator of the Early Man section at the University of Pennsylvania Museum. He had received thirty-six honorary degrees over a period of twenty years, and was the most honored member of the University of Pennsylvania since Benjamin Franklin. In 1976 he won the Bradford Washburn Award of the Boston Museum of Science for his \"outstanding contribution to the public understanding of science\" and the Joseph Wood Krutch Medal from the Humane Society of the United States for his \"significant contribution for the improvement of life and the environment in this country.\"\n\nIn addition to his scientific and academic work, Eiseley began in the mid-1940s to publish the essays which brought him to the attention of a wider audience. Anthropologist Pat Shipman writes,\n\nthe words that flowed from his pen... the images and insights he revealed, the genius of the man as a writer, outweigh his social disability. The words were what kept him in various honored posts; the words were what caused the students to flock to his often aborted courses; the words were what earned him esteemed lectureships and prizes. His contemporaries failed to see the duality of the man, confusing the deep, wise voice of Eiseley's writings with his own personal voice. He was a natural fugitive, a fox at the wood's edge (in his own metaphor)...\n\nEiseley published works in a number of different genres including poetry, autobiography, history of science, biography, and nonfictional essays. In each piece of writing, he consistently used a poetic writing style. Eiseley's style mirrors what he called the concealed essay—a piece of writing that unites the personal dimension with more scientific thoughts. His writing was unique in that it could convey complex ideas about human origin and the relationship between humans and the natural world to a nonscientific audience. Robert G. Franke describes Eiseley's essays as theatrical and dramatic. He also notes the influence his father's hobby as an amateur Shakespearean actor may have had on Eiseley's writing, pointing out that his essays often contain dramatic elements that are usually present in plays.\n\nIn describing Eiseley's writing, Richard Wentz wrote, \"As the works of any naturalist might, Eiseley's essays and poems deal with the flora and fauna of North America. They probe the concept of evolution, which consumed so much of his scholarly attention, examining the bones and shards, the arrowpoints and buried treasures. Every scientific observation leads to reflection.\" \n\nIn an interview on National Public Radio (NPR), author Michael Lind said,\n\nBefore the rise of a self-conscious intelligentsia, most educated people – as well as the unlettered majority – spent most of their time in the countryside or, if they lived in cities, were a few blocks away from farmland or wilderness... At the risk of sounding countercultural, I suspect that thinkers who live in sealed, air-conditioned boxes and work by artificial light (I am one) are as unnatural as apes in cages at zoos. Naturalists like Eiseley in that sense are the most normal human beings to be found among intellectuals, because they spend a lot of time outdoors and know the names of the plants and animals they see... For all of his scientific erudition, Eiseley has a poetic, even cinematic, imagination.\n\nRichard Wentz describes what he feels are the significance and purposes of Eiseley's writings:\"For Loren Eiseley, writing itself becomes a form of contemplation. Contemplation is a kind of human activity in which the mind, spirit and body are directed in solitude toward some other. Scholars and critics have not yet taken the full measure of contemplation as an art that is related to the purpose of all scholarly activity – to see things as they really are... Using narrative, parable and exposition, Eiseley has the uncanny ability to make us feel that we are accompanying him on a journey into the very heart of the universe. Whether he is explicating history or commenting on the ideas of a philosopher, a scientist or a theologian, he takes us with him on a personal visit.\"\n\nHowever, because of Eiseley's intense and poetic writing style and his focus on nature and cosmology, he was not accepted or understood by most of his colleagues. \"You,\" a friend told him, \"are a freak, you know. A God-damned freak, and life is never going to be easy for you. You like scholarship, but the scholars, some of them, anyhow, are not going to like you because you don't stay in the hole where God supposedly put you. You keep sticking your head out and looking around. In a university that's inadvisable.\" \n\nHis first book, \"The Immense Journey\", was a collection of writings about the history of humanity, and it proved to be that rare science book that appealed to a mass audience. It has sold over a million copies and has been published in at least 16 languages. Besides being his first book, \"The Immense Journey\" was also Eiseley's most well known book and established him as a writer with the ability to combine science and humanity in a poetic way. This book was originally published in 1946. Then, it was published again in 1957, a few years after the Piltdown Man hoax discovery.\n\nIn the book Eiseley conveys his sense of wonder at the depth of time and the vastness of the universe. He uses his own experiences, reactions to the paleontological record, and wonderment at the world to address the topic of evolution. More specifically, the text concentrates on human evolution and human ignorance. In \"The Immense Journey\", Eiseley follows the journey from human ignorance at the beginning of life to his own wonderment about the future of mankind. Marston Bates writes,\n\nIt seems to me... that Eiseley is looking at man in a quite hard-headed fashion, because he is willing to sketch problems for which he has no present and sure solution. We are not going to find the answers in human evolution until we have framed the right questions, and the questions are difficult because they involve both body and mind, physique and culture—tools and symbols as well as cerebral configurations.\n\nAuthor Orville Prescott wrote,\n\nConsider the case of Loren Eiseley, author of \"The Immense Journey\", who can sit on a mountain slope beside a prairie-dog town and imagine himself back in the dawn of the Age of mammals eighty million years ago: 'There by a tree root I could almost make him out, that shabby little Paleocene rat, eternal tramp and world wanderer, father of all mankind.' ... his prose is often lyrically beautiful, something that considerable reading in the works of anthropologists had not led me to expect. ... The subjects discussed here include the human ancestral tree, water and its significance to life, the mysteries of cellular life, 'the secret and remote abysses' of the sea, the riddle of why human beings alone among living creatures have brains capable of abstract thought and are far superior to their mere needs for survival, the reasons why Dr. Eiseley is convinced that there are no men or man-like animals on other planets, ...\n\nHe offers an example of Eiseley's style: \"There is no logical reason for the existence of a snowflake any more than there is for evolution. It is an apparition from that mysterious shadow world beyond nature, that final world which contains—if anything contains—the explanation of men and catfish and green leaves.\"\n\nThis book's subtitle is, \"Evolution and the Men Who Discovered It.\" Eiseley documented that animal variation, extinction, and a lengthy history of the earth were observed from the 1600s onward. Scientists groped towards a theory with increasingly detailed observations. They became aware that evolution had occurred without knowing how. Evolution was \"in the air\" and part of the intellectual discourse both before and after \"On the Origin of Species\" was published. The publisher describes it thus:\n\nAt the heart of the account is Charles Darwin, but the story neither begins nor ends with him. Starting with the seventeenth-century notion of the Great Chain of Being, Dr. Eiseley traces the achievements and discoveries of men in many fields of science who paved the way for Darwin; and the book concludes with an extensive discussion of the ways in which Darwin's work has been challenged, improved upon, and occasionally refuted during the past hundred years.\n\nPersons whose contributions are discussed include Sir Thomas Browne, Sir Francis Bacon, Carl Linnaeus, Benoît de Maillet, the Comte de Buffon, Erasmus Darwin, Louis Agassiz, Jean-Baptiste Lamarck, James Hutton, William Smith, Georges Cuvier, Étienne Geoffroy Saint-Hilaire, Sir Charles Lyell, Thomas Robert Malthus, William Wells, Patrick Matthew, Karl von Baer, Robert Chambers, Thomas Henry Huxley, Sir John Richardson, Alexander Humboldt, Gregor Mendel, Hugo De Vries, W. L. Johannsen, Lambert Quételet, and Alfred Russel Wallace. Critics discussed include Fleeming Jenkin, A.W. Bennett, Lord Kelvin, and Adam Sedgwick, both a mentor and a critic.\n\nAccording to naturalist author Mary Ellen Pitts, in the \"seminal\" \"Darwin's Century\", Eiseley was studying the history of evolutionary thinking, and he came to see that \"as a result of scientific studies, nature has become externalized, particularized, mechanized, separated from the human and fragmented, reduced to conflict without consideration of cooperation, confined to reductionist and positivist study.\" The results for humankind, \"as part of the 'biota' – Eiseley's concern as a writer – are far reaching.\" In the book, his unique impact as a thinker and a literary figure emerges as he reexamines science and the way man understands science. She concludes that, for Eiseley, \"Nature emerges as a metonym for a view of the physical world, of the 'biota,' and of humankind that must be reexamined if life is to survive.\"\n\nIn his conclusion, Eiseley quotes Darwin: \"If we choose to let conjecture run wild, then animals, our fellow brethren in pain, disease, suffering and famine—our slaves in the most laborious works, our companions in our amusements—they may partake of our origin in one common ancestor—we may be all melted together.\" Eiseley adds, \"If he had never conceived of natural selection, if he had never written the \"Origin,\" it would still stand as a statement of almost clairvoyant perception.\"\n\nThe book won the Phi Beta Kappa prize for best book in science in 1958.\n\nIn discussing \"The Firmament of Time\", Professor of Zoology Leslie Dunn wrote, \"How can man of 1960, burdened with the knowledge of the world external to him, and with the consciousness that scientific knowledge is attained through continually interfering with nature, 'bear his part' and gain the hope and confidence to live in the new world to which natural science has given birth? ... The answer comes in the eloquent, moving central essay of his new book.\" \"The New Yorker\" wrote, \"Dr. Eiseley describes with zest and admiration the giant steps that have led man, in a scant three hundred years, to grasp the nature of his extraordinary past and to substitute a natural world for a world of divine creation and intervention... An irresistible inducement to partake of the almost forgotten excitements of reflection.\" A review in \"The Chicago Tribune\" added, \"[This book] has a warm feeling for all natural phenomena; it has a rapport with man and his world and his problems; ... it has hope and belief. And it has the beauty of prose that characterizes Eiseley's philosophical moods.\"\n\n\"The Firmament of Time\" was awarded the 1961 John Burroughs Medal for the best publication in the field of Nature Writing.\n\n\nPoet W.H. Auden wrote, \"The main theme of \"The Unexpected Universe\" is Man as the Quest Hero, the wanderer, the voyager, the seeker after adventure, knowledge, power, meaning, and righteousness.\" He quotes from the book:\n\nEvery time we walk along a beach some ancient urge disturbs us so that we find ourselves shedding shoes and garments or scavenging among seaweed and whitened timbers like the homesick refugees of a long war... Mostly the animals understand their roles, but man, by comparison, seems troubled by a message that, it is often said, he cannot quite remember or has gotten wrong... Bereft of instinct, he must search continually for meanings... Man was a reader before he became a writer, a reader of what Coleridge once called the mighty alphabet of the universe.\n\nEvolutionary biologist Theodosius Dobzhansky described Dr. Eiseley as\n\nGregory McNamee of Amazon.com writes, \"In 1910 young Loren Eiseley watched the passage of Halley's Comet with his father. The boy who became a famous naturalist was never again to see the spectacle except in his imagination. That childhood event contributed to the profound sense of time and space that marks \"The Invisible Pyramid\". This collection of essays, first published shortly after Americans landed on the moon, explores inner and outer space, the vastness of the cosmos, and the limits of what can be known. Bringing poetic insight to scientific discipline, Eiseley makes connections between civilizations past and present, multiple universes, humankind, and nature.\n\nEiseley took the occasion of the lunar landing to consider how far humans had to go in understanding their own small corner of the universe, their home planet, much less what he called the 'cosmic prison' of space. Likening humans to the microscopic phagocytes that dwell within our bodies, he grumpily remarks, 'We know only a little more extended reality than the hypothetical creature below us. Above us may lie realms it is beyond our power to grasp.' Science, he suggests, would be better put to examining that which lies immediately before us, although he allows that the quest to explore space is so firmly rooted in Western technological culture that it was unlikely to be abandoned simply because of his urging. Eiseley's opinion continues to be influential among certain environmentalists, and these graceful essays show why that should be so.\n\nBook excerpt:\n\nMan would not be man if his dreams did not exceed his grasp. ... Like John Donne, man lies in a close prison, yet it is dear to him. Like Donne's, his thoughts at times overleap the sun and pace beyond the body. If I term humanity a slime mold organism it is because our present environment suggest it. If I remember the sunflower forest it is because from its hidden reaches man arose. The green world is his sacred center. In moments of sanity he must still seek refuge there. ... If I dream by contrast of the eventual drift of the star voyagers through the dilated time of the universe, it is because I have seen thistledown off to new worlds and am at heart a voyager who, in this modern time, still yearns for the lost country of his birth.\n\nKirkus Reviews wrote,\n\nIn a published essay, University of Pennsylvania alumnus Carl Hoffman wrote,\n\nAn old man who had done almost all of his writing late, late at night, was speaking to a younger man who liked to read in those same dark hours. In a chapter entitled 'One Night's Dying,' Eiseley said to me: 'It is thus that one day and the next are welded together, and that one night's dying becomes tomorrow's birth. I, who do not sleep, can tell you this.' Today, well into my fifties, in the midst of a lifetime of almost compulsive reading, I still regard \"The Night Country\" as my all-time favorite book.\n\n\"In \"All the Strange Hours\",\" states Amazon.com,\n\nEiseley turns his considerable powers of reflection and discovery on his own life to weave a compelling story, related with the modesty, grace, and keen eye for a telling anecdote that distinguish his work. His story begins with his childhood experiences as a sickly afterthought, weighed down by the loveless union of his parents. From there he traces the odyssey that led to his search for early postglacial man—and into inspiriting philosophical territory—culminating in his uneasy achievement of world renown. Eiseley crafts an absorbing self-portrait of a man who has thought deeply about his place in society as well as humanity's place in the natural world.\n\nHis friend and science fiction author Ray Bradbury wrote, \"The book will be read and cherished in the year 2001. It will go to the Moon and Mars with future generations. Loren Eiseley's work changed my life.\" And from the \"Philadelphia Sunday Bulletin\": \"An astonishing breadth of knowledge, infinite capacity for wonder and compassionate interest for everyone and everything in the universe.\n\n\"Darwin and the Mysterious Mr. X\" attempts to solve a mystery: \"Samuel Butler, a master of acrimonious polemic, confronted Charles Darwin with the sorest of all scientific subjects—a dispute about priority. In \"Evolution Old and New\" (1879), Butler accused Darwin of slighting the evolutionary speculations of Buffon, Lamarck, and his own grandfather, Erasmus Darwin.\" The Kirkus Reviews calls it, \"...an essay devoted to resurrecting the name and importance of Edward Blyth, a 19th-century naturalist. Eiseley credits Blyth with the development of the idea, and even the coining of the words \"natural selection,\" which Darwin absorbed and enlarged upon...[and] some thoughts on Darwin's Descent of Man; and a concluding speculation on the meaning of evolution. The last piece is very much Eiseley's poetic from-whence-do-we come/whither-do-we-go vein.\" Many experts on Darwin such as Stephen Jay Gould disagreed with Eisley. Michael Ruse, a philosopher of science, even stated \"If a work like this was handed into me for a course. I would give it a failing grade.\" Howard Gruber wrote that \"Eiseley was wrong on every count, both in the broad picture he painted of the Darwin‐Blyth relationship and in the minutiae he scratched up to support his claims.\"\n\nJust before his death Eiseley asked his wife to destroy the personal notebooks which he had kept since 1953. However, she compromised by disassembling them so they couldn't be used. Later, after great effort, his good friend Kenneth Heuer managed to reassemble most of his notebooks into readable form. \"The Lost Notebooks of Loren Eiseley\" includes a variety of Eiseley's writings including childhood stories, sketches while he was a vagabond, old family pictures, unpublished poems, portions of unfinished novels, and letters to and from literary admirers like W.H. Auden, Howard Nemerov, Lewis Mumford and Ray Bradbury.\n\nIn a review of the book, author Robert Finch writes, \"Like Melville, Eiseley thought of himself, and by extension all mankind, as 'an orphan, a wood child, a changeling,' a cosmic outcast born into a world that afforded him no true home.\" He adds that his \"distinctive gift as a writer was to take powerfully formative personal influences of family and place and fuse them with his intellectual meditations on universal topics such as evolution, human consciousness and the weight of time. ... he found metaphors that released a powerful view of man's fate in the modern world.\" As Kenneth Heuer writes, \"there are countless examples of Eiseley's empathy with life in all its forms, and particularly with its lost outcasts...the love that transcends the boundaries of species was the highest spiritual expression he knew.\n\nFinch adds, \"We are grateful for a life and a sensibility that would be welcome in any age, but never more so than in our increasingly depersonalized world. ... he made a generation of readers 'see the world through his eyes.' In an undated passage, circa 1959, Eiseley wrote, 'Man is alone in the universe...Only in the act of love, in rare and hidden communion with nature, does man escape himself.'\" \"The Lost Notebooks\" contains numerous examples of his \"creative and sympathetic imagination, even when that creation takes place in the solitude of journals never meant for public eyes.\"\n\nFrom other reviews: \"Eiseley has rightly been called 'the modern Thoreau.'\" –\"Publishers Weekly;\" \"[an] extensive and enlightening glimpses ... into the intellectual and emotional workshop of one of the most original and influential American essayists of this century.\" –\"New York Times Book Review;\" \"Eiseley's great genius for the art of the word coupled with a poetic insight into the connection between science and humanism shines through in page after page... This is a book that will be read and quoted and whose pages will grow thin with wear from hands in continued search of new meaning within its words and images.\" –\"Los Angeles Times;\" \"it will enhance any dedicated reader's knowledge of this most remarkable literary naturalist... They provide more than a glimpse into Eiseley's mind and imagination.\" –\"The Bloomsbury Review;\" \"It is a joy, like finding a lost Rembrandt in the attic, to discover that Eiseley left behind a legacy.\" –\"San Francisco Examiner-Chronicle.\"\n\nRichard Wentz, professor of religious studies, noted that \"The Christian Century\" magazine called attention to a study of Loren Eiseley by saying: \"The religious chord did not sound in him, but he vibrated to many of the concerns historically related to religion.\" Wentz adds, \"Although Eiseley may not have considered his writing as an expression of American spiritually, one feels that he was quite mindful of its religious character. As an heir of Emerson and Thoreau, he is at home among the poets and philosophers and among those scientists whose observations also were a form of contemplation of the universe.\"\n\nBut Wentz considered the inherent contradictions in the statements: \"We do not really know what to do with religiousness when it expresses itself outside those enclosures which historians and social scientists have carefully labeled religions. What, after all, does it mean to say, \"the religious chord does not sound in someone,\" but that the person vibrates to the concerns historically related to religion? If the person vibrates to such concerns, the chord is religious whether or not it manages to resound in the temples and prayer houses of the devout.\"\n\nWentz quotes Eiseley, from \"All the Strange Hours\" and \"The Star Thrower\", to indicate that he was, in fact, a religious thinker:\n\nWentz encompasses such quotes in his partial conclusion:\n\nHe was indeed a scientist – a bone hunter, he called himself. Archaeologist, anthropologist and naturalist, he devoted a great deal of time and reflection to the detective work of scientific observation. However, if we are to take seriously his essays, we cannot ignore the evidence of his constant meditation on matters of ultimate order and meaning. Science writer Connie Barlow says Eiseley wrote eloquent books from a perspective that today would be called Religious Naturalism.\nWentz writes, \"Loren Eiseley is very much in the tradition of Henry David Thoreau. He takes the circumstances of whatever \"business\" he is about as the occasion for new questioning, new searching for some sign, some glimpse into the meaning of the unknown that confronts him at every center of existence.\" He quotes Eiseley from \"The Star Thrower\", \"We are, in actuality, students of that greater order known as nature. It is into nature that man vanishes.\" \n\nIn comparing Eiseley with Thoreau, he discusses clear similarities in their life and philosophies. He notes that Eiseley was, like Thoreau, a 'spiritual wanderer through the deserts of the modern world.' However, notes Wentz, \"Thoreau had left the seclusion of Walden Pond in order to pace the fields of history, sorting out the artifacts that people had dropped along the way.\" But \"it was those 'fossil thoughts' and 'mindprints' that Eiseley himself explored in his wanderings. These explorations gave depth, a tragic dimension and catharsis to what he called the 'one great drama that concerns us most, the supreme mystery, man.'\"\n\nEiseley's writing often includes his belief that mankind does not have enough evidence to determine exactly how humans came to be. In \"The Immense Journey\", he writes, \"…many lines of seeming relatives, rather than merely one, lead to man. It is as though we stood at the heart of a maze and no longer remembered how we had come there.\" According to Wentz, Eiseley realized that there is nothing below a certain depth that can truly be explained, and quotes Eiseley as saying that there is \"nothing to explain the necessity of life, nothing to explain the hunger of the elements to become life . . . .\" and that \"the human version of evolutionary events [is] perhaps too simplistic for belief.\" \n\nEiseley talked about the illusions of science in his book, \"The Firmament of Time\":\n\nA scientist writing around the turn of the century remarked that all of the past generations of men have lived and died in a world of illusion. The unconscious irony in his observation consists in the fact that this man assumed the progress of science to have been so great that a clear vision of the world without illusion was, by his own time, possible. It is needless to add that he wrote before Einstein... at a time when Mendel was just about to be rediscovered, and before the advances in the study of radioactivity...\n\nWentz noted Eiseley's belief that science may have become misguided in its goals: \"Loren Eiseley thought that much of the modern scientific enterprise had removed humanity ever farther from its sense of responsibility to the natural world it had left in order to create an artificial world to satisfy its own insatiable appetites.\" Interpreting Eiseley's messages, he adds, \"It would be well, he tells us, to heed the message of the Buddha, who knew that 'one cannot proceed upon the path of human transcendence until one has made interiorly in one's soul a road into the future.' Spaces within stretch as far as those without.\" \n\n\"In essay after essay,\" writes Wentz, \"he writes as a magus, a spiritual master or a shaman who has seen into the very heart of the universe and shares his healing vision with those who live in a world of feeble sight. We must learn to see again, he tells us; we must rediscover the true center of the self in the otherness of nature.\" \n\nLoren Eiseley died July 9, 1977, of cardiac arrest following surgery at the University of Pennsylvania Hospital. He was buried in West Laurel Hill Cemetery in Bala Cynwyd, Pennsylvania. Eiseley's wife, Mabel Langdon Eiseley, died July 27, 1986, and is buried next to him, in the Westlawn section of the cemetery, in Lot 366. The inscription on their headstone reads, \"We loved the earth but could not stay\", which is a line from his poem \"The Little Treasures\".\n\nA library in the Lincoln City Libraries public library system is named after Eiseley.\n\nLoren Eiseley was awarded the Distinguished Nebraskan Award and inducted into the Nebraska Hall of Fame. A bust of his likeness resides in the Nebraska State Capitol.\n\nIn summarizing some of Eiseley's contributions, the editor of \"The Bloomsbury Review\" wrote,\n\nThere can be no question that Loren Eiseley maintains a place of eminence among nature writers. His extended explorations of human life and mind, set against the backdrop of our own and other universes are like those to be found in every book of nature writing currently available... We now routinely expect our nature writers to leap across the chasm between science, natural history, and poetry with grace and ease. Eiseley made the leap at a time when science was science, and literature was, well, literature... His writing delivered science to nonscientists in the lyrical language of earthly metaphor, irony, simile, and narrative, all paced like a good mystery.\n\nOn October 25, 2007, the Governor of Nebraska, Dave Heineman, officially declared that year \"The Centennial Year of Loren Eiseley.\" In a written proclamation, he encouraged all Nebraskans\n\nto read Loren Eisely's writings and to appreciate in those writings the richness and beauty of his language, his ability to depict the long, slow passage of time and the meaning of the past in the present, his portrayal of the relationships among all living things and his concern for the future.\n\n\nContains \"The Immense Journey\", \"The Firmament of Time\", \"The Unexpected Universe\", \"The Invisible Pyramid\", \"The Night Country\", essays from \"The Star Thrower\", and uncollected prose (2016) Library of America.\n\n\n\n\n\n"}
{"id": "1675601", "url": "https://en.wikipedia.org/wiki?curid=1675601", "title": "Manifold vacuum", "text": "Manifold vacuum\n\nManifold vacuum, or engine vacuum in an internal combustion engine is the difference in air pressure between the engine's intake manifold and Earth's atmosphere.\n\nManifold vacuum is an effect of a piston's movement on the induction stroke and the choked flow through a throttle in the intake manifold of an engine. It is a measure of the amount of restriction of airflow through the engine, and hence of the unused power capacity in the engine. In some engines, the manifold vacuum is also used as an auxiliary power source to drive engine accessories and for the crankcase ventilation system.\n\nManifold vacuum should not be confused with venturi vacuum, which is an effect exploited in carburetors to establish a pressure difference roughly proportional to mass airflow and to maintain a somewhat constant air/fuel ratio. It is also used in light airplanes to provide airflow for pneumatic gyroscopic instruments.\n\nThe rate of airflow through an internal combustion engine is an important factor determining the amount of power the engine generates. Most gasoline engines are controlled by limiting that flow with a throttle that restricts intake airflow, while a diesel engine is controlled by the amount of fuel supplied to the cylinder, and so has no \"throttle\" as such. Manifold vacuum is present in all naturally aspirated engines that use throttles (including carbureted and fuel injected gasoline engines using the Otto cycle or the two-stroke cycle; diesel engines do not have throttle plates).\n\nThe mass flow through the engine is the product of the rotation rate of the engine, the displacement of the engine, and the density of the intake stream in the intake manifold. In most applications the rotation rate is set by the application (engine speed in a vehicle or machinery speed in other applications). The displacement is dependent on the engine geometry, which is generally not adjustable while the engine is in use (although a handful of models do have this feature, see variable displacement). Restricting the input flow reduces the density (and hence pressure) in the intake manifold, reducing the amount of power produced. It is also a major source of engine drag (see engine braking), as the engine must pump material from the low-pressure intake manifold into the exhaust manifold (at ambient atmospheric pressure).\n\nWhen the throttle is opened (in a car, the accelerator pedal is depressed), ambient air is free to fill the intake manifold, increasing the pressure (filling the vacuum). A carburetor or fuel injection system adds fuel to the airflow in the correct proportion, providing energy to the engine. When the throttle is opened all the way, the engine's air induction system is exposed to full atmospheric pressure, and maximum airflow through the engine is achieved. In a naturally aspirated engine, output power is limited by the ambient barometric pressure. Superchargers and turbochargers boost manifold pressure above atmospheric pressure.\n\nModern engines use a manifold absolute pressure (abbreviated as \"MAP\") sensor to measure air pressure in the intake manifold. Manifold absolute pressure is one of a multitude of parameters used by the engine control unit (ECU) to optimize engine operation. It is important to differentiate between absolute and gauge pressure when dealing with certain applications, particularly those that experience changes in elevation during normal operation. \n\nMotivated by government regulations mandating reduction of fuel consumption (in the USA) or reduction of carbon dioxide emissions (in Europe), passenger cars and light trucks have been fitted with a variety of technologies (downsized engines; lockup, multi-ratio and overdrive transmissions; variable valve timing, forced induction, diesel engines, et al.) which render manifold vacuum inadequate or unavailable. Electric vacuum pumps are now commonly used for powering pneumatic accessories.\n\nManifold vacuum is caused by a different phenomenon than venturi vacuum, which is present inside carburetors. Venturi vacuum is caused by the venturi effect which, for fixed ambient conditions (air density and temperature), depends on the total mass flow through the carburetor. In engines that use carburetors, the venturi vacuum is approximately proportional to the total mass flow through the engine (and hence the total power output). As ambient pressure (altitude, weather) or temperature change, the carburetor may need to be adjusted to maintain this relationship.\n\nManifold pressure may also be \"ported\". Porting is selecting a location for the pressure tap within the throttle plate's range of motion. Depending on throttle position, a ported pressure tap may be either upstream or downstream of the throttle. As the throttle position changes, a \"ported\" pressure tap is selectively connected to either manifold pressure or ambient pressure. Antique (pre-OBD II) engines often used ported manifold pressure taps for ignition distributors and emission-control components.\n\nMost automobiles use four-stroke Otto cycle engines with multiple cylinders attached to a single inlet manifold. During the induction stroke, the piston descends in the cylinder and the intake valve is open. As the piston descends it effectively increases the volume in the cylinder above it, setting up low pressure. Atmospheric pressure pushes air through the manifold and carburetor or fuel injection system, where it is mixed with fuel. Because multiple cylinders operate at different times in the engine cycle, there is almost constant pressure difference through the inlet manifold from carburetor to engine.\n\nTo control the amount of fuel/air mix entering the engine, a simple butterfly valve (throttle plate) is generally fitted at the start of the intake manifold (just below the carburetor in carbureted engines). The butterfly valve is simply a circular disc fitted on a spindle, fitting inside the pipe work. It is connected to the accelerator pedal of the car, and is set to be fully open when the pedal is fully depressed and fully closed when the pedal is released. The butterfly valve often contains a small \"idle cutout\", a hole that allows small amounts of fuel/air mixture into the engine even when the valve is fully closed, or the carburetor has a separate air bypass with its own idle jet.\n\nIf the engine is operating under light or no load and low or closed throttle, there is high manifold vacuum. As the throttle is opened, the engine speed increases rapidly. The engine speed is limited only by the amount of fuel/air mixture that is available in the manifold. Under full throttle and light load, other effects (such as valve float, turbulence in the cylinders, or ignition timing) limit engine speed so that the manifold pressure can increase—but in practice, parasitic drag on the internal walls of the manifold, plus the restrictive nature of the venturi at the heart of the carburetor, means that a low pressure will always be set up as the engine's internal volume exceeds the amount of the air the manifold is capable of delivering.\n\nIf the engine is operating under heavy load at wide throttle openings (such as accelerating from a stop or pulling the car up a hill) then engine speed is limited by the load and minimal vacuum will be created. Engine speed is low but the butterfly valve is fully open. Since the pistons are descending more slowly than under no load, the pressure differences are less marked and parasitic drag in the induction system is negligible. The engine pulls air into the cylinders at the full ambient pressure.\n\nMore vacuum is created in some situations. On deceleration or when descending a hill, the throttle will be closed and a low gear selected to control speed. The engine will be rotating fast because the road wheels and transmission are moving quickly, but the butterfly valve will be fully closed. The flow of air through the engine is strongly restricted by the throttle, producing a strong vacuum on the engine side of the butterfly valve which will tend to limit the speed of the engine. This phenomenon, known as engine braking, is used to prevent acceleration or even to slow down with minimal or no brake usage (as when descending a long or steep hill). This vacuum braking should not be confused with compression braking (aka a \"Jake brake\"), or with exhaust braking, which are often used on large diesel trucks. Such devices are necessary for engine braking with a diesel as they lack a throttle to restrict the air flow enough to create sufficient vacuum to brake a vehicle.\n\nThis low (or negative) pressure can be put to uses. A pressure gauge measuring the manifold pressure can be fitted to give the driver an indication of how hard the engine is working and it can be used to achieve maximum momentary fuel efficiency by adjusting driving habits: minimizing manifold vacuum increases momentary efficiency. A weak manifold vacuum under closed-throttle conditions shows that the butterfly valve or internal components of the engine (valves or piston rings) are worn, preventing good pumping action by the engine and reducing overall efficiency.\n\nVacuum is often used to drive auxiliary systems on the vehicle. Vacuum-assist brake servos, for example, use atmospheric pressure pressing against the engine manifold vacuum to increase pressure on the brakes. Since braking is nearly always accompanied by the closing of the throttle and associated high manifold vacuum, this system is simple and almost foolproof. Vacuum tanks were installed on trailers to control their integrated braking systems.\n\nPrior to the introduction of Federal Motor Vehicle Safety Standards in the USA by the National Traffic and Motor Vehicle Safety Act of 1966, it was common to use manifold vacuum to drive windscreen wipers with a pneumatic motor. This system was cheap & simple but resulted in the comical yet unsafe effect of wipers which operate at full speed while the engine idles, operate around half speed while cruising, and stop altogether when the driver depresses the pedal fully. Vehicle HVAC systems also used manifold vacuum to drive actuators controlling airflow and temperature.\n\nAnother obsolete accessory is the \"Autovac\" fuel lifter which uses vacuum to raise fuel from the main tank to a small auxiliary tank, from which it flows by gravity to the carburetor. This eliminated the fuel pump which, in early cars, was an unreliable item.\n\nMany diesel engines do not have butterfly valve throttles. The manifold is connected directly to the air intake and the only suction created is that caused by the descending piston with no venturi to increase it, and the engine power is controlled by varying the amount of fuel that is injected into the cylinder by a fuel injection system. This assists in making diesels much more efficient than petrol engines.\n\nIf vacuum is required (vehicles that can be fitted with both petrol and diesel engines often have systems requiring it), a butterfly valve connected to the throttle can be fitted to the manifold. This reduces efficiency and is still not as effective as it is not connected to a venturi. Since low-pressure is only created on the overrun (such as when descending hills with a closed throttle), not over a wide range of situations as in a petrol engine, a vacuum tank is fitted.\n\nMost diesel engines now have a separate vacuum pump (\"exhauster\") fitted to provide vacuum at all times, at all engine speeds.\n\nMany new BMW petrol engines do not use a throttle in normal running, but instead use \"Valvetronic\" variable-lift intake valves to control the amount of air entering the engine. Like a diesel engine, manifold vacuum is practically non-existent in these engines and a different source must be utilised to power the brake servo.\n\n"}
{"id": "15522138", "url": "https://en.wikipedia.org/wiki?curid=15522138", "title": "Marine counterparts of land creatures", "text": "Marine counterparts of land creatures\n\nThe idea that there are specific marine counterparts to land creatures, inherited from the writers on natural history in Antiquity, was firmly believed in Islam and in Medieval Europe, and is exemplified by the creatures represented in the medieval animal encyclopedias called bestiaries and in the parallels drawn in the moralising attributes attached to each. \"The creation was a mathematical diagram drawn in parallel lines,\" T. H. White said a propos the bestiary he translated. \"Things did not only have a moral they often had physical counterparts in other strata. There was a horse in the land and a sea-horse in the sea. For that matter there was probably a Pegasus in heaven\". The idea of perfect analogies in the fauna of land and sea was considered part of the perfect symmetry of the Creator's plan, offered as the \"book of nature\" to mankind, for which a text could be found in \"Job\":\nBut ask the animals, and they will teach you, or the birds of the air, and they will tell you; or speak to the earth, and it will teach you, or let the fish of the sea inform you. Which of all these does not know that the hand of the Lord has done this? In his hand is the life of every creature and the breath of all mankind.\n\nThe idea appears in the Jewish Tannaic sources as well, as brought down in Babylonian Talmud, Chulin 127a. Rashi (Psalms 49:2) traces this to a biblical source – the land is referred to as \"Chaled\", from the weasel (chulda), because the weasel is the only animal on dry land that does not have its counterpart in the sea.\nAll of Creation was considered to reflect the Creator, and Man could learn about the Creator through studying the Creation, an assumption that underlies the \"watchmaker analogy\" offered as a proof of God's existence.\n\nThe correspondence between the realms of earth and sea, extending to its denizens, offers examples of the taste for allegory engendered by Christian and Islamic methods of exegesis, which also encouraged the doctrine of signatures, a \"key\" to the meaning and use of herbs.\n\nThe source text that was most influential in compiling the bestiaries of the 12th and 13th centuries was the \"Physiologus\", one of the most widely read and copied secular texts of the Middle Ages. Written in Greek in Alexandria the 2nd century CE and accumulating further \"exemplary\" beasts in the next three centuries and more, \"Physiologus\" was transmitted in the West in Latin, and eventually translated into many vernacular languages: many manuscripts in various languages survive.\nAelian, \"On the Characteristics of Animals\" (A. F. Scholfield, in Loeb Classical Library, 1958).\n\nChristian writers, trained in anagogical thinking and expecting to find spiritual instruction inherent in the processes of Nature, disregarded the caveat in Pliny's Natural History, where the idea is presented as a \"vulgar opinion\": \n\nHence it is that the vulgar notion may very possibly be true, that whatever is produced in any other department of Nature, is to be found in the sea as well; while, at the same time, many other productions are there to be found which nowhere else exist. That there are to be found in the sea the forms, not only of terrestrial animals, but of inanimate objects even, is easily to be understood by all who will take the trouble to examine the grape-fish, the sword-fish, the sawfish, and the cucumber-fish, which last so strongly resembles the real cucumber both in colour and in smell.\nPliny points out that many more things are found in the sea than on the land, and also mentions the correspondences that may be discovered between many non-living objects of the land and living creatures in the sea. \n\nSaint Augustine of Hippo reasons based on analogy, that since there is a serpent in the grass, there must be an eel in the sea; because there is a Leviathan in the sea, there must be a Behemoth on the land. (\"City of God\"? xi.15?)\nThe reaction to such anagogical thinking set in with the unfolding of critical scientific thought in the 17th century. Sir Thomas Browne devoted a chapter of his \"Pseudodoxia Epidemica\" to dispelling such a belief: Chapter XXIV: \"That all Animals in the land are in their kinde in the Sea.\" During the Enlightenment the ancient conception was given an innovative and rationalized cast by Benoît de Maillet in describing the transformations and metamorphoses undergone by creatures of the sea to render them fit for life on land, a proto-evolutionist concept, though it was based on superficial morphological similarities:\nThere are in the Sea, Fish of almost all the Figures of Land-Animals, and even of Birds. She includes Plants, Flowers, and some Fruits; the Nettle, the Rose, the Pink, the Melon and the Grape, are to be found there.<br>\n<br>\nAs for the Quadrupeds, we not only find in the Sea, Species of the same Figure and Inclinations, and in the Waves living on the same Aliments by which they are nourished on Land, we have also Examples of those Species living equally in the Air and in the Water. Have not the Sea-Apes precisely the same figure with those of the Land?\n\nThough in \"Moby-Dick\" Ishmael, with a nod to Sir Thomas Browne's wording, denies the claim that land animals find their counterparts in the sea,For though some old naturalists have maintained that all creatures of the land are of their kind in the sea; and though taking a broad general view of the thing, this may very well be; yet coming to specialties, where, for example, does the ocean furnish any fish that in disposition answers to the sagacious kindness of the dog? The accursed shark alone can in any generic respect be said to bear comparative analogy to him.\nin discussing dolphins trained to aid scuba divers, a 1967 \"Popular Mechanics\" article could still casually state: \"It's hoped that the marine counterparts of some land animals can be trained to become useful members of the Man-in-the-Sea program.\"\n"}
{"id": "2466610", "url": "https://en.wikipedia.org/wiki?curid=2466610", "title": "Metasystem transition", "text": "Metasystem transition\n\nA metasystem transition is the emergence, through evolution, of a higher level of organization or control.\n\nA metasystem is formed by the integration of a number of initially independent components, such as molecules (as theorized for instance by hypercycles), cells, or individuals, and the emergence of a system steering or controlling their interactions. As such, the collective of components becomes a new, goal-directed individual, capable of acting in a coordinated way. This metasystem is more complex, more intelligent, and more flexible in its actions than the initial component systems. Prime examples are the origin of life, the transition from unicellular to multicellular organisms, the emergence of eusociality or symbolic thought. \n\nThe concept of metasystem transition was introduced by the cybernetician Valentin Turchin in his 1970 book \"The Phenomenon of Science\", and developed among others by Francis Heylighen in the Principia Cybernetica Project. The related notion of evolutionary transition was proposed by the biologists John Maynard Smith and Eörs Szathmáry, in their 1995 book \"The Major Transitions in Evolution\". Another related idea, that systems (\"operators\") evolve to become more complex by successive closures encapsulating components in a larger whole, is proposed in \"the operator theory\", developed by Gerard Jagers op Akkerhuis.\n\nTurchin has applied the concept of metasystem transition in the domain of computing, via the notion of metacompilation or supercompilation. A supercompiler is a compiler program that compiles its own code, thus increasing its own efficiency, producing a remarkable speedup in its execution.\n\nThe following is the classical sequence of metasystem transitions in the history of animal evolution according to Turchin, from the origin of animate life to sapient culture:\n\n\nMany argue that the next human metasystem transition consists of a merger of biological metasystems with technological metasystems, especially information processing technology. Several cumulative major transitions of evolution have transformed life through key innovations in information storage and replication, including RNA, DNA, multicellularity, and also language and culture as inter-human information processing systems. In this sense it can be argued that the carbon-based biosphere has generated a cognitive system (humans) capable of creating technology that will result in a comparable evolutionary transition. \"Digital information has reached a similar magnitude to information in the biosphere... Like previous evolutionary transitions, the potential symbiosis between biological and digital information will reach a critical point where these codes could compete via natural selection. Alternatively, this fusion could create a higher-level superorganism employing a low-conflict division of labor in performing informational tasks... humans already embrace fusions of biology and technology. We spend most of our waking time communicating through digitally mediated channels, ...most transactions on the stock market are executed by automated trading algorithms, and our electric grids are in the hands of artificial intelligence. With one in three marriages in America beginning online, digital algorithms are also taking a role in human pair bonding and reproduction\".\n\n\n"}
{"id": "166380", "url": "https://en.wikipedia.org/wiki?curid=166380", "title": "Natural history", "text": "Natural history\n\nNatural history is a domain of inquiry involving organisms including animals, fungi and plants in their environment; leaning more towards observational than experimental methods of study. A person who studies natural history is called a naturalist or natural historian.\n\nNatural history encompasses scientific research but is not limited to it. It involves the systematic study of any category of natural objects or organisms. So while it dates from studies in the ancient Greco-Roman world and the mediaeval Arabic world, through to European Renaissance naturalists working in near isolation, today's natural history is a cross discipline umbrella of many specialty sciences; e.g., geobiology has a strong multi-disciplinary nature.\n\nThe meaning of the English term \"natural history\" (a calque of the Latin \"historia naturalis\") has narrowed progressively with time; while, by contrast, the meaning of the related term \"nature\" has widened (see also History below).\n\nIn antiquity, \"natural history\" covered essentially anything connected with nature, or which used materials drawn from nature, such as Pliny the Elder's encyclopedia of this title, published circa 77 to 79 AD, which covers astronomy, geography, humans and their technology, medicine, and superstition, as well as animals and plants.\n\nMedieval European academics considered knowledge to have two main divisions: the humanities (primarily what is now known as classics) and divinity, with science studied largely through texts rather than observation or experiment. The study of nature revived in the Renaissance, and quickly became a third branch of academic knowledge, itself divided into descriptive natural history and natural philosophy, the analytical study of nature. In modern terms, natural philosophy roughly corresponded to modern physics and chemistry, while natural history included the biological and geological sciences. The two were strongly associated. During the heyday of the gentleman scientists, many people contributed to both fields, and early papers in both were commonly read at professional science society meetings such as the Royal Society and the French Academy of Sciences – both founded during the seventeenth century.\n\nNatural history had been encouraged by practical motives, such as Linnaeus' aspiration to improve the economic condition of Sweden. Similarly, the Industrial Revolution prompted the development of geology to help find useful mineral deposits.\n\nModern definitions of natural history come from a variety of fields and sources, and many of the modern definitions emphasize a particular aspect of the field, creating a plurality of definitions with a number of common themes among them. For example, while natural history is most often defined as a type of observation and a subject of study, it can also be defined as a body of knowledge, and as a craft or a practice, in which the emphasis is placed more on the observer than on the observed.\n\nDefinitions from biologists often focus on the scientific study of individual organisms in their environment, as seen in this definition by Marston Bates: \"Natural history is the study of animals and Plants – of organisms. ... I like to think, then, of natural history as the study of life at the level of the individual – of what plants and animals do, how they react to each other and their environment, how they are organized into larger groupings like populations and communities\" and this more recent definition by D.S. Wilcove and T. Eisner: \"The close observation of organisms—their origins, their evolution, their behavior, and their relationships with other species\".\n\nThis focus on organisms in their environment is also echoed by H.W. Greene and J.B. Losos: \"Natural history focuses on where organisms are and what they do in their environment, including interactions with other organisms. It encompasses changes in internal states insofar as they pertain to what organisms do\".\n\nSome definitions go further, focusing on direct observation of organisms in their environment, both past and present, such as this one by G.A. Bartholomew: \"A student of natural history, or a naturalist, studies the world by observing plants and animals directly. Because organisms are functionally inseparable from the environment in which they live and because their structure and function cannot be adequately interpreted without knowing some of their evolutionary history, the study of natural history embraces the study of fossils as well as physiographic and other aspects of the physical environment\".\n\nA common thread in many definitions of natural history is the inclusion of a descriptive component, as seen in a recent definition by H.W. Greene: \"Descriptive ecology and ethology\". Several authors have argued for a more expansive view of natural history, including S. Herman, who defines the field as \"the scientific study of plants and animals in their natural environments. It is concerned with levels of organization from the individual organism to the ecosystem, and stresses identification, life history, distribution, abundance, and inter-relationships.\n\nIt often and appropriately includes an esthetic component\", and T. Fleischner, who defines the field even more broadly, as \"A practice of intentional, focused attentiveness and receptivity to the more-than-human world, guided by honesty and accuracy\". These definitions explicitly include the arts in the field of natural history, and are aligned with the broad definition outlined by B. Lopez, who defines the field as the \"Patient interrogation of a landscape\" while referring to the natural history knowledge of the Eskimo (Inuit).\n\nA slightly different framework for natural history, covering a similar range of themes, is also implied in the scope of work encompassed by many leading natural history museums, which often include elements of anthropology, geology, paleontology and astronomy along with botany and zoology, or include both cultural and natural components of the world.\n\nThe plurality of definitions for this field has been recognized as both a weakness and a strength, and a range of definitions have recently been offered by practitioners in a recent collection of views on natural history.\n\nNatural history begins with Aristotle and other ancient philosophers who analyzed the diversity of the natural world. Natural history was understood by Pliny the Elder to cover anything that could be found in the world, including living things, geology, astronomy, technology, art and humanity.\n\n\"De Materia Medica\" was written between 50 and 70 AD by Pedanius Dioscorides, a Roman physician of Greek origin. It was widely read for more than 1,500 years until supplanted in the Renaissance, making it one of the longest-lasting of all natural history books.\n\nFrom the ancient Greeks until the work of Carl Linnaeus and other 18th century naturalists, a major concept of natural history was the \"scala naturae\" or Great Chain of Being, an arrangement of minerals, vegetables, more primitive forms of animals, and more complex life forms on a linear scale of supposedly increasing perfection, culminating in our species.\n\nNatural history was basically static through the Middle Ages in Europe – although in the Arabic and Oriental world it proceeded at a much brisker pace. From the thirteenth century, the work of Aristotle was adapted rather rigidly into Christian philosophy, particularly by Thomas Aquinas, forming the basis for natural theology. During the Renaissance, scholars (herbalists and humanists, particularly) returned to direct observation of plants and animals for natural history, and many began to accumulate large collections of exotic specimens and unusual monsters. Leonhart Fuchs was one of the three founding fathers of botany, along with Otto Brunfels and Hieronymus Bock. Other important contributors to the field were Valerius Cordus, Konrad Gesner (\"Historiae animalium\"), Frederik Ruysch, or Gaspard Bauhin. The rapid increase in the number of known organisms prompted many attempts at classifying and organizing species into taxonomic groups, culminating in the system of the Swedish naturalist Carl Linnaeus.\n\nA significant contribution to English natural history was made by parson-naturalists such as Gilbert White, William Kirby, John George Wood, and John Ray, who wrote about plants, animals, and other aspects of nature. Many of these men wrote about nature to make the natural theology argument for the existence or goodness of God.\n\nIn modern Europe, professional disciplines such as botany, geology, mycology, palaeontology, physiology and zoology were formed. \"Natural history\", formerly the main subject taught by college science professors, was increasingly scorned by scientists of a more specialized manner and relegated to an \"amateur\" activity, rather than a part of science proper. In Victorian Scotland it was believed that the study of natural history contributed to good mental health. Particularly in Britain and the United States, this grew into specialist hobbies such as the study of birds, butterflies, seashells (malacology/conchology), beetles and wildflowers; meanwhile, scientists tried to define a unified discipline of biology (though with only partial success, at least until the modern evolutionary synthesis). Still, the traditions of natural history continue to play a part in the study of biology, especially ecology (the study of natural systems involving living organisms and the inorganic components of the Earth's biosphere that support them), ethology (the scientific study of animal behavior), and evolutionary biology (the study of the relationships between life-forms over very long periods of time), and re-emerges today as integrative organismal biology.\n\nAmateur collectors and natural history entrepreneurs played an important role in building the world's large natural history collections, such as the Natural History Museum, London, and the National Museum of Natural History in Washington D.C.\n\nThree of the greatest English naturalists of the nineteenth century, Henry Walter Bates, Charles Darwin, and Alfred Russel Wallace—who all knew each other—each made natural history travels that took years, collected thousands of specimens, many of them new to science, and by their writings both advanced knowledge of \"remote\" parts of the world—the Amazon basin, the Galápagos Islands, and the Malay archipelago, among others—and in so doing helped to transform biology from a descriptive to a theory based science.\n\nThe understanding of \"Nature\" as \"an organism and not as a mechanism\" can be traced to the writings of Alexander von Humboldt (Prussia, 1769–1859). Humboldt's copious writings and research were seminal influences for Charles Darwin, Simón Bolívar, Henry David Thoreau, Ernst Haeckel, and John Muir.\n\nNatural history museums, which evolved from cabinets of curiosities, played an important role in the emergence of professional biological disciplines and research programs. Particularly in the 19th century, scientists began to use their natural history collections as teaching tools for advanced students and the basis for their own morphological research.\n\nThe term \"natural history\" alone, or sometimes together with archaeology, forms the name of many national, regional and local natural history societies that maintain records for animals (including birds (ornithology), insects (entomology) and mammals (mammalogy)), fungi (mycology), plants (botany) and other organisms. They may also have geological and microscopical sections.\n\nExamples of these societies in Britain include the Natural History Society of Northumbria founded in 1829, London Natural History Society (1858), Birmingham Natural History Society (1859), British Entomological and Natural History Society founded in 1872, Glasgow Natural History Society, Manchester Microscopical and Natural History Society established in 1880, Whitby Naturalists' Club founded in 1913, Scarborough Field Naturalists' Society and the Sorby Natural History Society, Sheffield, founded in 1918. The growth of natural history societies was also spurred due to the growth of British colonies in tropical regions with numerous new species to be discovered. Many civil servants took an interest in their new surroundings, sending specimens back to museums in Britain. (See also: Indian natural history)\n\nSocieties in other countries include the American Society of Naturalists and Polish Copernicus Society of Naturalists.\n\n"}
{"id": "412846", "url": "https://en.wikipedia.org/wiki?curid=412846", "title": "Naturalistic pantheism", "text": "Naturalistic pantheism\n\nNaturalistic pantheism is a kind of pantheism. It has been used in various ways such as to relate God or divinity with concrete things, determinism, or the substance of the Universe. God, from these perspectives, is seen as the aggregate of all unified natural phenomena. The phrase has often been associated with the philosophy of Baruch Spinoza, although academics differ on how it is used.\n\nThe term “pantheism\" is derived from Greek words \"pan\" (Greek: πᾶν) meaning \"all\" and \"theos\" (θεός) meaning God. It was coined by Joseph Raphson in his work \"De spatio reali\", published in 1697. The term was introduced to English by Irish writer John Toland in his 1705 work \"Socinianism Truly Stated, by a pantheist\" that described pantheism as the \"opinion of those who believe in no other eternal being but the universe.\"\n\nThe term \"naturalistic\" derives from the word \"naturalism\", which has several meanings in philosophy and aesthetics. In philosophy the term frequently denotes the view that everything belongs to the world of nature and can be studied with the methods appropriate for studying that world, \"i.e.\" the sciences. It generally implies an absence of belief in supernatural beings.\n\nJoseph Needham, a modern British scholar of Chinese philosophy and science, has identified Taoism as \"a naturalistic pantheism which emphasizes the unity and spontaneity of the operations of Nature.\" This philosophy can be dated to the late 4th century BCE.\n\nThe Hellenistic Greek philosophical school of Stoicism (which started in the early 3rd century BCE) rejected the dualist idea of the separate ideal/conscious and material realms, and identified the substance of God with the entire cosmos and heaven. However, not all philosophers who did so can be classified as naturalistic pantheists.\n\nNaturalistic pantheism was expressed by various thinkers, including Giordano Bruno, who was burned at the stake for his views. However, the 17th century Dutch philosopher Spinoza became particularly known for it.\n\nPossibly drawing upon the ideas of Descartes,\nBaruch Spinoza connected God and Nature through the phrase \"deus sive natura\" (\"God, or Nature\"), making him the father of classical pantheism. He relied upon rationalism rather than the more intuitive approach of some Eastern traditions.\n\nSpinoza's philosophy, sometimes known as Spinozism, has been understood in a number of ways, and caused disagreements such as the Pantheism controversy. However, many scholars have considered it to be a form of naturalistic pantheism. This has included viewing the pantheistic unity as natural. \nOthers focus on the deterministic aspect of naturalism.\nSpinoza inspired a number of other pantheists, with varying degrees of idealism towards nature. However, Spinoza's influence in his own time was limited.\nScholars have considered Spinoza the founder of a line of naturalistic pantheism, though not necessarily the only one.\n\nIn 1705 the Irish writer John Toland endorsed a form of pantheism in which the God-soul is identical with the material universe.\n\nGerman naturalist Ernst Haeckel (1834–1919) proposed a monistic pantheism in which the idea of God is identical with that of nature or substance.\n\nThe World Pantheist Movement, started in 1999, describes Naturalistic Pantheism as including reverence for the universe, realism, strong naturalism, and respect for reason and the scientific method as methods of understanding the world. Paul Harrison considers its position the closest modern equivalent to Toland's.\n\n"}
{"id": "1548703", "url": "https://en.wikipedia.org/wiki?curid=1548703", "title": "Nature worship", "text": "Nature worship\n\nNature worship is any of a variety of religious, spiritual and devotional practices that focus on the worship of the nature spirits considered to be behind the natural phenomena visible throughout nature. A nature deity can be in charge of nature, a place, a biotope, the biosphere, the cosmos, or the universe. Nature worship is often considered the primitive source of modern religious beliefs and can be found in theism, panentheism, pantheism, deism, polytheism, animism, totemism, shamanism, paganism. Common to most forms of nature worship is a spiritual focus on the individual's connection and influence on some aspects of the natural world and reverence towards it.\n"}
{"id": "188094", "url": "https://en.wikipedia.org/wiki?curid=188094", "title": "Nikolaas Tinbergen", "text": "Nikolaas Tinbergen\n\nNikolaas \"Niko\" Tinbergen (; ; 15 April 1907 – 21 December 1988) was a Dutch biologist and ornithologist who shared the 1973 Nobel Prize in Physiology or Medicine with Karl von Frisch and Konrad Lorenz for their discoveries concerning organization and elicitation of individual and social behavior patterns in animals. He is regarded as one of the founders of modern ethology, the study of animal behavior.\n\nIn 1951, he published \"The Study of Instinct\", an influential book on animal behaviour.\nIn the 1960s, he collaborated with filmmaker Hugh Falkus on a series of wildlife films, including \"The Riddle of the Rook\" (1972) and \"Signals for Survival\" (1969), which won the Italia prize in that year and the American blue ribbon in 1971.\n\nBorn in The Hague, Netherlands, he was one of five children of Dirk Cornelis Tinbergen and his wife Jeannette van Eek. His brother, Jan Tinbergen, won the first Bank of Sweden Prize in Economic Sciences in Memory of Alfred Nobel in 1969. They are the only siblings to each win a Nobel Prize. Another brother, Luuk Tinbergen was also a noted biologist.\n\nTinbergen's interest in nature manifested itself when he was young. He studied biology at Leiden University and was a prisoner of war during World War II. Tinbergen's experience as a prisoner of the Nazis led to some friction with longtime intellectual collaborator Konrad Lorenz, and it was several years before the two reconciled.\n\nAfter the war, Tinbergen moved to England, where he taught at the University of Oxford and was a fellow first at Merton College, Oxford and later at Wolfson College, Oxford. Several of his graduate students went on to become prominent biologists including Richard Dawkins, Marian Dawkins, Desmond Morris, Iain Douglas-Hamilton, and Tony Sinclair.\n\nIn 1951 Tinbergen's \"The Study of Instinct\" was published. Behavioural ecologists and evolutionary biologists still recognise the contribution this book offered the field of behavioural science studies. \"The Study of Instinct\" summarises Tinbergen's ideas on innate behavioural reactions in animals and the adaptiveness and evolutionary aspects of these behaviours. By behaviour, he means the total movements made by the intact animal; innate behaviour is that which is not changed by the learning process. The major question of the book is the role of internal and external stimuli in controlling the expression of behaviour.\n\nIn particular, he was interested in explaining 'spontaneous' behaviours: those that occurred in their complete form the first time they were performed and that seemed resistant to the effects of learning. He explains how behaviour can be considered a combination of these spontaneous behaviour patterns and as set series of reactions to particular stimuli. Behaviour is a reaction in that to a certain extent it is reliant on external stimuli, however it is also spontaneous since it is also dependent upon internal causal factors.\n\nHis model for how certain behavioural reactions are provoked was based on work by Konrad Lorenz. Lorenz postulated that for each instinctive act there is a specific energy which builds up in a reservoir in the brain. In this model, Lorenz envisioned a reservoir with a spring valve at its base that an appropriate stimulus could act on, much like a weight on a scale pan pulling against a spring and releasing the reservoir of energy, an action which would lead an animal to express the desired behaviour.\n\nTinbergen added complexity to this model, a model now known as Tinbergen's hierarchical model. He suggested that motivational impulses build up in nervous centres in the brain which are held in check by blocks. The blocks are removed by an innate releasing mechanism that allows the energy to flow to the next centre (each centre containing a block that needs to be removed) in a cascade until the behaviour is expressed. Tinbergen's model shows multiple levels of complexity and that related behaviours are grouped.\n\nAn example is in his experiments with foraging honey bees. He showed that honey bees show curiosity for yellow and blue paper models of flowers, and suggested that these were visual stimuli causing the buildup of energy in one specific centre. However, the bees rarely landed on the model flowers unless the proper odour was also applied. In this case, the chemical stimuli of the odour allowed the next link in the chain to be released, encouraging the bee to land. The final step was for the bee to insert its mouthparts into the flower and initiate suckling. Tinbergen envisioned this as concluding the reaction set for honey bee feeding behaviour.\n\nIn 1973 Tinbergen, along with Konrad Lorenz and Karl von Frisch, were awarded the Nobel Prize in Physiology or Medicine \"for their discoveries concerning organization and elicitation of individual and social behaviour patterns\". The award recognised their studies on genetically programmed behaviour patterns, their origins, maturation and their elicitation by key stimuli. In his Nobel Lecture, Tinbergen addressed the somewhat unconventional decision of the Nobel Foundation to award the prize for Physiology or Medicine to three men who had until recently been regarded as \"mere animal watchers\". Tinbergen stated that their revival of the \"watching and wondering\" approach to studying behaviour could indeed contribute to the relief of human suffering.\n\nThe studies performed by the trio on fish, insects and birds laid the foundation for further studies on the importance of specific experiences during critical periods of normal development, as well as the effects of abnormal psychosocial situations in mammals. At the time, these discoveries were stated to have caused \"a breakthrough in the understanding of the mechanisms behind various symptoms of psychiatric disease, such as anguish, compulsive obsession, stereotypic behaviour and catatonic posture\". Tinbergen’s contribution to these studies included the testing of the hypotheses of Lorenz/von Frisch by means of \"comprehensive, careful, and ingenious experiments\" as well as his work on supernormal stimuli. The work of Tinbergen during this time was also regarded as having possible implications for further research in child development and behaviour.\n\nHe also caused some intrigue by dedicating a large part of his acceptance speech to FM Alexander, originator of the Alexander technique, a method which investigates postural reflexes and responses in human beings.\n\nIn 1950 Tinbergen became member of the Royal Netherlands Academy of Arts and Sciences. He was elected a Fellow of the Royal Society (FRS) in 1962. He was also awarded the Godman-Salvin Medal in 1969 by the British Ornithologists' Union, and in 1973 received the Swammderdam Medal and Wilhelm Bölsche Medal (from the Genootschap ter bervordering van Natuur-, Genees- en Heelkunde of the University of Amsterdam and the Kosmos-Gesellschaft der Naturfreunde respectively).\n\nTinbergen described four questions he believed should be asked of any animal behaviour, which were:\n\n\nIn ethology and sociobiology, causation and ontogeny are summarised as the \"proximate mechanisms\", while adaptation and phylogeny are the \"ultimate mechanisms\". They are still considered as the cornerstone of modern ethology, sociobiology and transdisciplinarity in Human Sciences.\n\nA major body of Tinbergen's research focused on what he termed the supernormal stimulus. This was the concept that one could build an artificial object which was a stronger stimulus or releaser for an instinct than the object for which the instinct originally evolved. He constructed plaster eggs to see which a bird preferred to sit on, finding that they would select those that were larger, had more defined markings, or more saturated colour—and a dayglo-bright one with black polka dots would be selected over the bird's own pale, dappled eggs.\n\nTinbergen found that territorial male three-spined stickleback (a small freshwater fish) would attack a wooden fish model more vigorously than a real male if its underside was redder. He constructed cardboard dummy butterflies with more defined markings that male butterflies would try to mate with in preference to real females. The superstimulus, by its exaggerations, clearly delineated what characteristics were eliciting the instinctual response.\n\nAmong the modern works calling attention to Tinbergen's classic work is Deirdre Barrett's 2010 book, \"Supernormal Stimuli\".\n\nTinbergen applied his observational methods to the problems of autistic children. He recommended a \"holding therapy\" in which parents hold their autistic children for long periods of time while attempting to establish eye contact, even when a child resists the embrace. However, his interpretations of autistic behaviour, and the holding therapy that he recommended, lacked scientific support and the therapy is described as controversial and potentially abusive.\n\nSome of the publications of Tinbergen are:\n\nPublications about Tinbergen and his work:\n\nTinbergen was a member of the advisory committee to the Anti-Concorde Project and was also an atheist.\n\nTinbergen married Elisabeth Rutten and they had five children. Later in life he suffered depression and feared he might, like his brother Luuk, commit suicide. He was treated by his friend, whose ideas he had greatly influenced, John Bowlby. Tinbergen died on 21 December 1988, after suffering a stroke at his home in Oxford, England.\n"}
{"id": "58377746", "url": "https://en.wikipedia.org/wiki?curid=58377746", "title": "Path of the Puma: The Remarkable Resilience of the Mountain Lion", "text": "Path of the Puma: The Remarkable Resilience of the Mountain Lion\n\nPath of the Puma: The Remarkable Resilience of the Mountain Lion, by Jim Williams, is a non-fiction book presenting the research of the author, a wildlife biologist and supervisor for Montana Fish, Wildlife and Parks' Region 1 in Kalispell. Williams also discusses DNA research conducted by others on these animals, and makes the case for coexistence with these big, wild cats\n\nWith a foreword by Doug Chadwick, the book chronicles Williams' journey, from his early work at a Florida marine park, to his conversion to \"a lifelong devotee of the species\". Williams has fitted pumas (also known as cougars and mountain lions) with radio collars and has installed cameras in their caves to track and study them. \"Publishers Weekly\" wrote, \"Montana-based wildlife biologist Jim Williams celebrates wildlife research and conservation of ghost cats from Canada’s Yukon Territory to Tierra del Fuego in Argentina and Chile...\"\n\n\"Path of the Puma\" has received positive reviews. \"Publishers Weekly\" said, \"Williams writes movingly of the challenges these animals face, many due to human encroachment on their habitats. He makes a strong case for the necessity of preserving—or at least peacefully coexisting with—the puma.\" \"Kirkus Reviews\" noted, \"While the book is an undisguised conservationist’s plea ... it is not a harangue. The author’s passion and his firsthand knowledge of his subject make the narrative highly readable. A noteworthy feature of this work is the presence of numerous full-color photographs...\" Kirkus concluded, \"A handsome book that is well-balanced, instructive, and authoritative.\" The Natural History Book Service wrote that the puma's story, \"...is fascinating for the lessons it can afford the protection of all species in times of dire challenge and decline.\"\n\nSusan Waggoner of PressReader notes Williams' \"...enthusiasm for nature and animals jumps off every page... Williams’s writing is expert, friendly, and interesting. Well organized and clearly presented, information emerges from field work examples, such as the tracking studies that showed that pumas feed almost exclusively on wild prey rather than domestic herds.\" Tristan Scott of the Flathead Beacon wrote, \"\"Path of the Puma\" doesn’t sugarcoat the risks of predators living among humans — mountain lions live at the intersection of human landscapes, livelihoods and lifestyles — but it points out the critical role predatory species play in the natural world.\"\n"}
{"id": "1413688", "url": "https://en.wikipedia.org/wiki?curid=1413688", "title": "Primary energy", "text": "Primary energy\n\nPrimary energy (PE) is an energy form found in nature that has not been subjected to any human engineered conversion process. It is energy contained in raw fuels, and other forms of energy received as input to a system. Primary energy can be non-renewable or renewable.\n\nWhere primary energy is used to describe fossil fuels, the embodied energy of the fuel is available as thermal energy and around 70% is typically lost in conversion to electrical or mechanical energy. There is a similar 60-80% conversion loss when solar and wind energy is converted to electricity, but today's UN conventions on energy statistics counts the electricity made from wind and solar as the primary energy itself for these sources. One consequence of this counting method is that the contribution of wind and solar energy is under reported compared to fossil energy sources, and there is hence an international debate on how to count primary energy from wind and solar. \n\nTotal primary energy supply (TPES) is the sum of production and imports subtracting exports and storage changes.\n\nThe concept of primary energy is used in energy statistics in the compilation of energy balances, as well as in the field of energetics. In energetics, a primary energy source (PES) refers to the energy forms required by the energy sector to generate the supply of energy carriers used by human society.\n\nSecondary energy is a carrier of energy, such as electricity. These are produced by conversion from a primary energy source.\n\nThe use of primary energy as a measure ignores conversion efficiency. Thus forms of energy with poor conversion efficiency, particularly the thermal sources, coal, gas and nuclear are overstated, whereas energy sources such as hydroelectricity which are converted efficiently, while a small fraction of primary energy are significantly more important than their total raw energy supply may seem to imply.\n\nPE and TPES are better defined in the context of worldwide energy supply.\n\nPrimary energy sources should not be confused with the energy system components (or conversion processes) through which they are converted into energy carriers.\n\nPrimary energy sources are transformed in energy conversion processes to more convenient forms of energy that can directly be used by society, such as electrical energy, refined fuels, or synthetic fuels such as hydrogen fuel. In the field of energetics, these forms are called energy carriers and correspond to the concept of \"secondary energy\" in energy statistics.\n\nEnergy carriers are energy forms which have been transformed from primary energy sources. Electricity is one of the most common energy carriers, being transformed from various primary energy sources such as coal, oil, natural gas, and wind. Electricity is particularly useful since it has low entropy (is highly ordered) and so can be converted into other forms of energy very efficiently. District heating is another example of secondary energy.\n\nAccording to the laws of thermodynamics, primary energy sources cannot be produced. They must be available to society to enable the production of energy carriers.\n\nConversion efficiency varies. For thermal energy, electricity and mechanical energy production is limited by Carnot's theorem, and generates a lot of waste heat. Other non-thermal conversions can be more efficient. For example, while wind turbines do not capture all of the wind's energy, they have a high conversion efficiency and generate very little waste heat since wind energy is low entropy. In principle solar photovoltaic conversions could be very efficient, but current conversion can only be done well for narrow ranges of wavelength, whereas solar thermal is also subject to Carnot efficiency limits. Hydroelectric power is also very ordered, and converted very efficiently. The amount of usable energy is the exergy of a system.\n\nSite energy is the term used in North America for the amount of end-use energy of all forms consumed at a specified location. This can be a mix of primary energy (such as natural gas burned at the site) and secondary energy (such as electricity). Site energy is measured at the campus, building, or sub-building level and is the basis for energy charges on utility bills.\n\nSource energy, in contrast, is the term used in North America for the amount of primary energy consumed in order to provide a facility’s site energy. It is always greater than the site energy, as it includes all site energy and adds to it the energy lost during transmission, delivery, and conversion. While source or primary energy provides a more complete picture of energy consumption, it cannot be measured directly and must be calculated using conversion factors from site energy measurements. For electricity, a typical value is three units of source energy for one unit of site energy. However, this can vary considerably depending on factors such as the primary energy source or fuel type, the type of power plant, and the transmission infrastructure. One full set of conversion factors is available as technical reference from Energy STAR.\n\nEither site or source energy can be an appropriate metric when comparing or analyzing energy use of different facilities. The U.S Energy Information Administration, for example, uses primary (source) energy for its energy overviews but site energy for its Commercial Building Energy Consumption Survey and Residential Building Energy Consumption Survey. The US Environmental Protection Agency's Energy STAR program recommends using source energy, and the US Department of Energy uses site energy in its definition of a zero net energy building.\n\nEnergy accidents are accidents that occur in systems that provide energy or power. These can result in fatalities, as can the normal running of many systems, for example those deaths due to pollution.\n\nGlobally, coal is responsible for 100,000 deaths per trillion kWh.\n\n\n\n"}
{"id": "4286589", "url": "https://en.wikipedia.org/wiki?curid=4286589", "title": "Quarry-faced stone", "text": "Quarry-faced stone\n\nQuarry-faced stone is stone with a rough, unpolished surface, straight from the quarry. Many residential and public buildings in Jerusalem, Israel are built from quarry-faced Jerusalem stone.\n"}
{"id": "48191844", "url": "https://en.wikipedia.org/wiki?curid=48191844", "title": "Scandiobabingtonite", "text": "Scandiobabingtonite\n\nScandiobabingtonite was first discovered in the Montecatini granite quarry near Baveno, Italy in a pegmatite cavity. Though found in pegmatites, the crystals of scandiobabingtonite are sub-millimeter sized, and are tabular shaped. Scandiobabingtonite was the sixth naturally occurring mineral discovered with the rare earth element scandium, and grows around babingtonite, with which it is isostructural, hence the namesake. It is also referred to as scandian babingtonite. The ideal chemical formula for scandiobabingtonite is Ca(Fe,Mn)ScSiO(OH).\n\nScandiobabingtonite is found in association with orthoclase, quartz, light blue albite, stilbite, fluorite, and mica. When found with these minerals, the scandiobabingtonite crystals are emplanted on the surface of the other minerals. It also occurs as growth around green-black prismatic crystals of babingtonite. The samples of scandiobabingtonite that have been discovered have shown that they start out growing from a seed of babingtonite crystal. This is how scandiobabingtonite gets its chemical structure. The starting seed of babingtonite is still present in the center of the resulting crystal and can be detected with optical and chemical studies. Scandiobabingtonite is a uniquely rare mineral, as it occurs in very small amounts in few locations around the world. It is one of thirteen naturally occurring minerals where scandium is a dominant member. The other scandium minerals are bazzite, cascandite, hetftetjernite, jervisite, juonniite, kolbeckite, kristiansenite, magbasite, oftedalite, pretulite, thortveitite, and titanowodginite. Scandium can also concentrate in other minerals, such as in ferromagnesian minerals, aluminum phosphate minerals, meteoric minerals, and other minerals containing rare earth elements, but it occurs in trace amounts.\n\nScandiobabingtonite is a colorless or lightly gray-green colored transparent mineral with a glassy or vitreous luster. It exhibits a hardness of 6 on the Mohs hardness scale. Scandiobabingtonite occurs as short, prismatic crystals that are slightly elongated on the [001] axis which gives it a tabular or platy shape. Its crystals are characterized by the {010}, {001}, {110}, {1-10}, and {101} faces. Scandiobabingtonite is brittle and shows perfect cleavage along the {001} and {1-10} planes. The measured density is 3.24 g/cm.\n\nScandiobabingtonite is biaxial positive, which means it will refract light along two axes. It exhibits a 2V=64(2)°, strong dispersion with r>v, and displays strong pleochroism with colors ranging from pink (γ') to green(α'). The extinction angle along the (110) is 6°. Z:Φ=-250°, ρ=47°; Y:Φ=146°, ρ=75°; X:Φ=42°, ρ=47°.\n\nScandiobabingtonite is isostructural with babingtonite, and has the same chemical properties as well. It is an inosilicate with 5-periodic single chains. Scandium replaces the Fe in babingtonite in six-fold coordination. The empirical chemical formula for scandiobabingtonite is (Ca,Na)(Fe,Mn)(Sc,Sn,Fe)SiO(OH). Simplified, the formula is Ca(Fe,Mn)ScSiO(OH)\n\nScandiobabingtonite is in the triclinic crystal system, with space group P1. The unit cell dimensions are a=7.536(2) Å, b=11.734(2) Å, c=6.748(2) Å, α=91.70(2)°, β=93.86(2)°, γ=104.53(2)°. These dimensions are almost identical to those of babingtonite. The difference in dimensions is caused by the replacement of iron with scandium in the Fe-centered octahedra. The Fe-O distance measures as 2.048 Å, while the Sc-O distance is 2.092 Å. This equates to a slightly larger octahedra in scandiobabingtonite than babingtonite.\n\nList of Minerals\n"}
{"id": "184101", "url": "https://en.wikipedia.org/wiki?curid=184101", "title": "Singing sand", "text": "Singing sand\n\nSinging sand, also called whistling sand or barking sand, is sand that produces sound. The sound emission may be caused by wind passing over dunes or by walking on the sand.\n\nCertain conditions have to come together to create singing sand:\n\nThe most common frequency emitted seems to be close to 450 Hz.\n\nThere are various theories about the singing sand mechanism. It has been proposed that the sound frequency is controlled by the shear rate. Others have suggested that the frequency of vibration is related to the thickness of the dry surface layer of sand. The sound waves bounce back and forth between the surface of the dune and the surface of the moist layer, creating a resonance that increases the sound's volume. The noise may be generated by friction between the grains or by the compression of air between them.\n\nOther sounds that can be emitted by sand have been described as \"roaring\" or \"booming\".\n\nSinging sand dunes, an example of the phenomenon of singing sand, produce a sound described as roaring, booming, squeaking, or the \"Song of Dunes\". This is a natural sound phenomenon of up to 105 decibels, lasting as long as several minutes, that occurs in about 35 desert locations around the world. The sound is similar to a loud low-pitch rumble. It emanates from crescent-shaped dunes, or barchans. The sound emission accompanies a slumping or avalanching movement of sand, usually triggered by wind passing over the dune or by someone walking near the crest.\n\nExamples of singing sand dunes include California's Kelso Dunes and Eureka Dunes; AuTrain Beach in Northern Michigan; sugar sand beaches and Warren Dunes in southwestern Michigan; Sand Mountain in Nevada; the Booming Dunes in the Namib Desert, Africa; Porth Oer (also known as Whistling Sands) near Aberdaron in Wales; Indiana Dunes in Indiana; Barking Sands in Hawaiʻi; Mingsha Shan in Dunhuang, China; Kotogahama Beach in Odashi, Japan; Singing Beach in Manchester-by-the-Sea, Massachusetts; near Mesaieed in Qatar; and Gebel Naqous, near el-Tor, South Sinai, Egypt.\n\nThe phenomena also inspired a song called \"The Singing Sands of Alamosa\" in Bing Crosby's album Drifting and Dreaming(1947).\n\nOn some beaches around the world, dry sand will make a singing, squeaking, whistling, or screaming sound if a person scuffs or shuffles their feet with sufficient force. The phenomenon is not completely understood scientifically, but it has been found that quartz sand will do this if the grains are very well-rounded and highly spherical. It is believed by some that the sand grains must be of similar size, so the sand must be well sorted by the actions of wind and waves, and that the grains should be close to spherical and have dust-, pollution-, and organic-matter-free surfaces. The \"singing\" sound is then believed to be produced by shear, as each layer of sand grains slides over the layer beneath it. The similarity in size, the uniformity, and the cleanness means that grains move up and down in unison over the layer of grains below them. Even small amounts of pollution on the sand grains reduce the friction enough to silence the sand.\n\nOthers believe that the sound is produced by the friction of grain against grain that have been coated with dried salt, in a way that is analogous to the way that the rosin on the bow produces sounds from a violin string. It has also been speculated that thin layers of gas trapped and released between the grains act as \"percussive cushions\" capable of vibration, and so produce the tones heard.\n\nNot all sands sing, whistle or bark alike. The sounds heard have a wide frequency range that can be different for each patch of sand. Fine sands, where individual grains are barely visible to the naked eye, produce only a poor, weak sounding bark. Medium-sized grains can emit a range of sounds, from a faint squeak or a high-pitched sound, to the best and loudest barks when scuffed enthusiastically.\n\nWater also influences the effect. Wet sands are usually silent because the grains stick together instead of sliding past each other, but small amounts of water can actually raise the pitch of the sounds produced. The most common part of the beach on which to hear singing sand is the dry upper beach above the normal high tide line, but singing has been reported on the lower beach near the low tide line as well.\n\nSinging sand has been reported on 33 beaches in the British Isles, including in the north of Wales and on the little island of Eigg in the Scottish Hebrides. It has also been reported at a number of beaches along North America's Atlantic coast. Singing sands can be found at Souris, on the eastern tip of Prince Edward Island, at the Singing Sands beach in Basin Head Provincial Park; on Singing Beach in Manchester-by-the-Sea, Massachusetts, as well as in the fresh waters of Lake Superior and Lake Michigan and in other places.\n\n\n\n\n\n\n"}
{"id": "2430727", "url": "https://en.wikipedia.org/wiki?curid=2430727", "title": "Subsolar point", "text": "Subsolar point\n\nThe subsolar point on a planet is the point at which its sun is perceived to be directly overhead (at the zenith); that is, where the sun's rays strike the planet exactly perpendicular to its surface. It can also mean the point closest to the sun on an astronomical object, even though the sun might not be visible.\nTo an observer on a planet with an orientation and rotation similar to those of Earth, the subsolar point will appear to move westward, completing one circuit around the globe each day, approximately moving along the equator. However, it will also move north and south between the tropics over the course of a year, so it is spiraling like a helix.\n\nThe subsolar point contacts the Tropic of Cancer on the June solstice and the Tropic of Capricorn on the December solstice. The subsolar point crosses the Equator on the March and September equinoxes. \nWhen the point passes through Hawaii, the only U.S. state in which this happens, it is known as Lahaina Noon.\n\n"}
{"id": "13581828", "url": "https://en.wikipedia.org/wiki?curid=13581828", "title": "Surface conductivity", "text": "Surface conductivity\n\nSurface conductivity is an additional conductivity of an electrolyte in the vicinity of charged surfaces. Close to charged surfaces a layer of counter ions of opposite polarity exists which is attracted by the surface charges. This layer of higher ionic concentration is a part of the interfacial double layer. The concentration of the ions in this layer is higher as compared to the volume conductivity far from the charged surface and leads to a higher conductivity of this layer.\n\nSmoluchowski was the first to recognize the importance of surface conductivity at the beginning of the 20th century.\n\nThere is a detailed description of surface conductivity by Lyklema in \"Fundamentals of Interface and Colloid Science\" \n\nThe Double Layer (DL) has two regions, according to the well established Gouy-Chapman-Stern model, Ref.2. The upper level, which is in contact with the bulk fluid is the diffuse layer. The inner layer that is in contact with interface is the Stern layer.\n\nIt is possible that the lateral motion of ions in both parts of the DL contributes to the surface conductivity.\n\nThe contribution of the Stern layer is less well described. It is often called \"additional surface conductivity\".\n\nThe theory of the surface conductivity of the diffuse part of the DL was developed by Bikerman. He derived a simple equation that links surface conductivity κ with the behaviour of ions at the interface. For symmetrical electrolyte and assuming identical ions diffusion coefficients D=D=D it is given in Ref.2:\n\nwhere\n\nThe parameter \"m\" characterizes the contribution of electro-osmosis to the motion of ions within the DL:\n\nThe Dukhin number is a dimensionless parameter that characterizes the contribution of the surface conductivity to a variety of electrokinetic phenomena, such as, electrophoresis and electroacoustic phenomena.\n\n\nSurface conductivity may refer to the electrical conduction across a solid surface measured by surface probes. Experiments may be done to test this material property as in the n-type surface conductivity of p-type . Additionally, surface conductivity is measured in coupled phenomena such as photoconductivity, for example, for the metal oxide semiconductor ZnO. Surface conductivity differs from bulk conductivity for analogous reasons to the electrolyte solution case, where the charge carriers of holes (+1) and electrons (-1) play the role of ions in solution.\n"}
{"id": "8028338", "url": "https://en.wikipedia.org/wiki?curid=8028338", "title": "Systematic evolution of ligands by exponential enrichment", "text": "Systematic evolution of ligands by exponential enrichment\n\nSystematic evolution of ligands by exponential enrichment (SELEX), also referred to as \"in vitro selection\" or \"in vitro evolution\", is a combinatorial chemistry technique in molecular biology for producing oligonucleotides of either single-stranded DNA or RNA that specifically bind to a target ligand or ligands, which are commonly referred to as aptamers. \nAlthough SELEX has emerged as the most commonly used name for the procedure, some researchers have referred to it as SAAB (selected and amplified binding site) and CASTing (cyclic amplification and selection of targets) SELEX was first introduced in 1990. In 2015 a special issue was published in the Journal of Molecular Evolution in the honor of quarter century of the SELEX discovery.\n\nThe process begins with the synthesis of a very large oligonucleotide library consisting of randomly generated sequences of fixed length flanked by constant 5' and 3' ends that serve as primers. For a randomly generated region of length \"n\", the number of possible sequences in the library is 4 (\"n\" positions with four possibilities (A,T,C,G) at each position). The sequences in the library are exposed to the target ligand - which may be a protein or a small organic compound - and those that do not bind the target are removed, usually by affinity chromatography or target capture on paramagnetic beads. The bound sequences are eluted and amplified by PCR to prepare for subsequent rounds of selection in which the stringency of the elution conditions can be increased to identify the tightest-binding sequences. A caution to consider in this method is that the selection of extremely high, sub-nanomolar binding affinity entities may not in fact improve specificity for the target molecule. Off-target binding to related molecules could have significant clinical effects.\n\nSELEX has been used to develop a number of aptamers that bind targets interesting for both clinical and research purposes. Also towards these ends, a number of nucleotides with chemically modified sugars and bases have been incorporated into SELEX reactions. These modified nucleotides allow for the selection of aptamers with novel binding properties and potentially improved stability.\n\nThe first step of SELEX involves the synthesis of fully or partially randomized oligonucleotide sequences of some length flanked by defined regions which allow PCR amplification of those randomized regions and, in the case of RNA SELEX, in vitro transcription of the randomized sequence. While Ellington and Szostak demonstrated that chemical synthesis is capable of generating ~10 unique sequences for oligonucleotide libraries in their 1990 paper on in vitro selection, they found that amplification of these synthesized oligonucleotides led to significant loss of pool diversity due to PCR bias and defects in synthesized fragments. The oligonucleotide pool is amplified and a sufficient amount of the initial library is added to the reaction so that there are numerous copies of each individual sequence to minimize the loss of potential binding sequences due to stochastic events. Before the library is introduced to target for incubation and selective retention, the sequence library must be converted to single stranded oligonucleotides to achieve structural conformations with target binding properties.\n\nImmediately prior to target introduction, the single stranded oligonucleotide library is often heated and cooled slowly to renature oligonucleotides into thermodynamically stable secondary and tertiary structures. Once prepared, the randomized library is incubated with immobilized target to allow oligonucleotide-target binding. There are several considerations for this target incubation step, including the target immobilization method and strategies for subsequent unbound oligonucleotide separation, incubation time and temperature, incubation buffer conditions, and target versus oligonucleotide concentrations. Examples of target immobilization methods include affinity chromatography columns, nitrocellulose binding assay filters, and paramagnetic beads. Recently, SELEX reactions have been developed where the target is whole cells, which are expanded near complete confluence and incubated with the oligonucleotide library on culture plates. Incubation buffer conditions are altered based on the intended target and desired function of the selected aptamer. For example, in the case of negatively charged small molecules and proteins, high salt buffers are used for charge screening to allow nucleotides to approach the target and increase the chance of a specific binding event. Alternatively, if the desired aptamer function is in vivo protein or whole cell binding for potential therapeutic or diagnostic application, incubation buffer conditions similar to in vivo plasma salt concentrations and homeostatic temperatures are more likely to generate aptamers that can bind in vivo. Another consideration in incubation buffer conditions is non-specific competitors. If there is a high likelihood of non-specific oligonucleotide retention in the reaction conditions, non specific competitors, which are small molecules or polymers other than the SELEX library that have similar physical properties to the library oligonucleotides, can be used to occupy these non-specific binding sites. Varying the relative concentration of target and oligonucleotides can also affect properties of the selected aptamers. If a good binding affinity for the selected aptamer is not a concern, then an excess of target can be used to increase the probability that at least some sequences will bind during incubation and be retained. However, this provides no selective pressure for high binding affinity, which requires the oligonucleotide library to be in excess so that there is competition between unique sequences for available specific binding sites.\n\nOnce the oligonucleotide library has been incubated with target for sufficient time, unbound oligonucleotides are washed away from immobilized target, often using the incubation buffer so that specifically bound oligonucleotides are retained. With unbound sequences washed away, the specifically bound sequences are then eluted by creating denaturing conditions that promote oligonucleotide unfolding or loss of binding conformation including flowing in deionized water, using denaturing solutions containing urea and EDTA, or by applying high heat and physical force. Upon elution of bound sequences, the retained oligonucleotides are reverse-transcribed to DNA in the case of RNA or modified base selections, or simply collected for amplification in the case of DNA SELEX. These DNA templates from eluted sequences are then amplified via PCR and converted to single stranded DNA, RNA, or modified base oligonucleotides, which are used as the initial input for the next round of selection.\n\nOne of the most critical steps in the SELEX procedure is obtaining single stranded DNA (ssDNA) after the PCR amplification step. This will serve as input for the next cycle so it is of vital importance that all the DNA is single stranded and as little as possible is lost. Because of the relative simplicity, one of the most used methods is using biotinylated reverse primers in the amplification step, after which the complementary strands can be bound to a resin followed by elution of the other strand with lye. Another method is asymmetric PCR, where the amplification step is performed with an excess of forward primer and very little reverse primer, which leads to the production of more of the desired strand. A drawback of this method is that the product should be purified from double stranded DNA (dsDNA) and other left-over material from the PCR reaction. Enzymatic degradation of the unwanted strand can be performed by tagging this strand using a phosphate-probed primer, as it is recognized by enzymes such as Lambda exonuclease. These enzymes then selectively degrade the phosphate tagged strand leaving the complementary strand intact. All of these methods recover approximately 50 to 70% of the DNA. For a detailed comparison refer to the article by Svobodová et al. where these, and other, methods are experimentally compared. In classical SELEX, the process of randomized single stranded library generation, target incubation, and binding sequence elution and amplification described above are repeated until the vast majority of the retained pool consists of target binding sequences, though there are modifications and additions to the procedure that are often used, which are discussed below.\n\nIn order to increase the specificity of aptamers selected by a given SELEX procedure, a negative selection, or counter selection, step can be added prior to or immediately following target incubation. To eliminate sequences with affinity for target immobilization matrix components from the pool, negative selection can be used where the library is incubated with target immobilization matrix components and unbound sequences are retained. Negative selection can also be used to eliminate sequences that bind target-like molecules or cells by incubating the oligonucleotide library with small molecule target analogs, undesired cell types, or non-target proteins and retaining the unbound sequences.\n\nTo track the progress of a SELEX reaction, the number of target bound molecules, which is equivalent to the number of oligonucleotides eluted, can be compared to the estimated total input of oligonucleotides following elution at each round. The number of eluted oligonucleotides can be estimated through elution concentration estimations via 260 nm wavelength absorbance or fluorescent labeling of oligonucleotides. As the SELEX reaction approaches completion, the fraction of the oligonucleotide library that binds target approaches 100%, such that the number of eluted molecules approaches the total oligonucleotide input estimate, but may converge at a lower number.\n\nSome SELEX reactions can generate probes that are dependent on primer binding regions for secondary structure formation. There are aptamer applications for which a short sequence, and thus primer truncation, is desirable. An advancement on the original method allows an RNA library to omit the constant primer regions, which can be difficult to remove after the selection process because they stabilize secondary structures that are unstable when formed by the random region alone.\n\nRecently, SELEX has expanded to include the use of chemically modified nucleotides. These chemically modified oligonucleotides offer many potential advantages for selected aptamers including greater stability and nuclease resistance, enhanced binding for select targets, expanded physical properties - like increased hydrophobicity, and more diverse structural conformations.\n\nThe genetic alphabet, and thus possible aptamers, is also expanded using unnatural base pairs the use of these unnatural base pairs was applied to SELEX and high affinity DNA aptamers were generated.\n\nThe technique has been used to evolve aptamers of extremely high binding affinity to a variety of target ligands, including small molecules such as ATP and adenosine and proteins such as prions and vascular endothelial growth factor (VEGF). Moreover, SELEX has been used to select high-affinity aptamers for complex targets such as tumor cells. Clinical uses of the technique are suggested by aptamers that bind tumor markers, GFP-related fluorophores, and a VEGF-binding aptamer trade-named Macugen has been approved by the FDA for treatment of macular degeneration. Additionally, SELEX has been utilized to obtain highly specific catalytic DNA or DNAzymes. Several metal-specific DNAzymes have been reported including the GR-5 DNAzyme (lead-specific), the CA1-3 DNAzymes (copper-specific), the 39E DNAzyme (uranyl-specific) and the NaA43 DNAzyme (sodium-specific).\n\nThese developed aptamers have seen diverse application in therapies for macular degeneration and various research applications including biosensors, fluorescent labeling of proteins and cells, and selective enzyme inhibition.\n\n\n"}
{"id": "103141", "url": "https://en.wikipedia.org/wiki?curid=103141", "title": "Thatching", "text": "Thatching\n\nThatching is the craft of building a roof with dry vegetation such as straw, water reed, sedge (\"Cladium mariscus\"), rushes, heather, or palm branches, layering the vegetation so as to shed water away from the inner roof. Since the bulk of the vegetation stays dry and is densely packed—trapping air—thatching also functions as insulation. It is a very old roofing method and has been used in both tropical and temperate climates. Thatch is still employed by builders in developing countries, usually with low-cost local vegetation. By contrast, in some developed countries it is the choice of some affluent people who desire a rustic look for their home, would like a more ecologically friendly roof, or who have purchased an originally thatched abode.\nThatching methods have traditionally been passed down from generation to generation, and numerous descriptions of the materials and methods used in Europe over the past three centuries survive in archives and early publications.\n\nIn some equatorial countries, thatch is the prevalent local material for roofs, and often walls. There are diverse building techniques from the ancient Hawaiian \"hale\" shelter made from the local ti leaves (\"Cordyline fruticosa\"), lauhala (\"Pandanus tectorius\") or pili grass (\"Heteropogon contortus\").\n\nPalm leaves are also often used. For example, in Na Bure, Fiji, thatchers combine fan palm leave roofs with layered reed walls. Feathered palm leaf roofs are used in Dominica. Alang-alang (\"Imperata cylindrica\") thatched roofs are used in Hawaii and Bali. In Southeast Asia, mangrove nipa palm leaves are used as thatched roof material known as attap dwelling. In Bali, Indonesia, the black fibres of Arenga pinnata called \"ijuk\" is also used as thatched roof materials, usually used in Balinese temple roof and meru towers. Sugar cane leaf roofs are used in Kikuyu tribal homes in Kenya.\n\nWild vegetation such as water reed (\"Phragmites australis\"), bulrush/cat tail (\"Typha\" spp.), broom (\"Cytisus scoparius\"), heather (\"Calluna vulgaris\"), and rushes (\"Juncus\" spp. and \"Schoenoplectus lacustris\") was probably used to cover shelters and primitive dwellings in Europe in the late Palaeolithic period, but so far no direct archaeological evidence for this has been recovered. People probably began to use straw in the Neolithic period when they first grew cereals—but once again, no direct archaeological evidence of straw for thatching in Europe prior to the early medieval period survives.\n\nMany indigenous people of the Americas, such as the former Maya civilization, Mesoamerica, the Inca empire, and the Triple Alliance (Aztec), lived in thatched buildings. It is common to spot thatched buildings in rural areas of the Yucatán Peninsula as well as many settlements in other parts of Latin America, which closely resemble the method of construction from distant ancestors. After the collapse of most extant American societies due to diseases introduced by Europeans, wars, enslavement, and genocide, the first Americans encountered by Europeans lived in structures roofed with bark or skin set in panels that could be added or removed for ventilation, heating, and cooling. Evidence of the many complex buildings with fiber-based roofing material was not rediscovered until the early 2000s. French and British settlers built temporary thatched dwellings with local vegetation as soon as they arrived in New France and New England, but covered more permanent houses with wooden shingles.\n\nIn most of England, thatch remained the only roofing material available to the bulk of the population in the countryside, in many towns and villages, until the late 1800s. Commercial production of Welsh slate began in 1820, and the mobility provided by canals and then railways made other materials readily available. Still, the number of thatched properties actually increased in the UK during the mid-1800s as agriculture expanded, but then declined again at the end of the 19th century because of agricultural recession and rural depopulation. \n\nGradually, thatch became a mark of poverty, and the number of thatched properties gradually declined, as did the number of professional thatchers. Thatch has become much more popular in the UK over the past 30 years, and is now a symbol of wealth rather than poverty. There are approximately 1,000 full-time thatchers at work in the UK, and thatching is becoming popular again because of the renewed interest in preserving historic buildings and using more sustainable building materials.\n\nAlthough thatch is popular in Germany, The Netherlands, Denmark, parts of France, Sicily, Belgium and Ireland, there are more thatched roofs in the United Kingdom than in any other European country. Good quality straw thatch can last for more than 50 years when applied by a skilled thatcher. Traditionally, a new layer of straw was simply applied over the weathered surface, and this \"spar coating\" tradition has created accumulations of thatch over 7’ (2.1 m) thick on very old buildings. The straw is bundled into \"yelms\" before it is taken up to the roof and then is attached using staples, known as \"spars\", made from twisted hazel sticks. Over 250 roofs in Southern England have base coats of thatch that were applied over 500 years ago, providing direct evidence of the types of materials that were used for thatching in the medieval period. Almost all of these roofs are thatched with wheat, rye, or a \"maslin\" mixture of both. Medieval wheat grew to almost tall in very poor soils and produced durable straw for the roof and grain for baking bread.\n\nTechnological change in the farming industry significantly affected the popularity of thatching. The availability of good quality thatching straw declined in England after the introduction of the combine harvester in the late 1930s and 1940s, and the release of short-stemmed wheat varieties. Increasing use of nitrogen fertiliser in the 1960s–70s also weakened straw and reduced its longevity. Since the 1980s, however, there has been a big increase in straw quality as specialist growers have returned to growing older, tall-stemmed, \"heritage\" varieties of wheat such as Squareheads Master (1880), N59 (1959), Rampton Rivet (1937), Victor (1910) and April Bearded (early 1800s)] in low input/organic conditions.\n\nIn the UK it is illegal under the Plant Variety and Seeds Act 1964 (with many amendments) for an individual or organisation to give, trade or sell seed of an older variety of wheat (or any other agricultural crop) to a third party for growing purposes, subject to a significant fine. Because of this legislation, thatchers in the UK can no longer obtain top quality thatching straw grown from traditional, tall-stemmed varieties of wheat.\n\nAll evidence indicates that water reed was rarely used for thatching outside of East Anglia. It has traditionally been a \"one coat\" material applied in a similar way to how it is used in continental Europe. Weathered reed is usually stripped and replaced by a new layer. It takes 4–5 acres of well-managed reed bed to produce enough reed to thatch an average house, and large reed beds have been uncommon in most of England since the Anglo-Saxon period. Over 80% of the water reed used in the UK is now imported from Turkey, Eastern Europe, China and South Africa. Though water reed might last for 50 years or more on a steep roof in a dry climate, modern imported water reed on an average roof in England does not last any longer than good quality wheat straw. The lifespan of a thatched roof also depends on the skill of the thatcher, but other factors must be considered—such as climate, quality of materials, and the roof pitch.\n\nIn areas where palms are abundant, palm leaves are used to thatch walls and roofs. Many species of palm trees are called \"thatch palm\", or have \"thatch\" as part of their common names. In the southeastern United States, Native and pioneer houses were often constructed of palmetto-leaf thatch. The chickees of the Seminole and Miccosukee are still thatched with palmetto leaves.\n\nGood thatch does not require frequent maintenance. In England a ridge normally lasts 8–14 years, and re-ridging is required several times during the lifespan of a thatch. Experts no longer recommend covering thatch with wire netting, as this slows evaporation and reduces longevity. Moss can be a problem if very thick, but is not usually detrimental, and many species of moss are actually protective. , remains the most widely used reference book on the techniques used for thatching. The thickness of a layer of thatch decreases over time as the surface gradually turns to compost and is blown off the roof by wind and rain. Thatched roofs generally needs replacement when the horizontal wooden 'sways' and hair-pin 'spars', also known as 'gads' (twisted hazel staples) that fix each course become visible near the surface. It is not total depth of the thatch within a new layer applied to a new roof that determines its longevity, but rather how much weathering thatch covers the fixings of each overlapping course. “A roof is as good as the amount of correctly laid thatch covering the fixings.”\n\nThatch is not as flammable as many people believe. It burns slowly, \"like a closed book,\" thatchers say. The vast majority of fires are linked to the use of wood burners and faulty chimneys with degraded or poorly installed or maintained flues. Sparks from paper or burned rubbish can ignite dry thatch on the surface around a chimney. Fires can also begin when sparks or flames work their way through a degraded chimney and ignite the surrounding semi-charred thatch. This can be avoided by ensuring that the chimney is in good condition, which may involve stripping thatch immediately surrounding the chimney to the full depth of the stack. This can easily be done without stripping thatch over the entire roof. Insurance premiums on thatched houses are higher than average in part because of the perception that thatched roofs are a fire hazard, but also because a thatch fire can cause extensive smoke damage and a thatched roof is more expensive to replace than a standard tiled or slate roof. Workmen should never use open flame near thatch, and nothing should be burnt that could fly up the chimney and ignite the surface of the thatch. Spark arrestors usually cause more harm than good, as they are easily blocked and reduce air flow. All thatched roofs should have smoke detectors in the roof space. Spray-on fire retardant or pressure impregnated fire retardants can reduce the spread of flame and radiated heat output.\n\nOn new buildings, a solid fire retardant barrier over the rafters can make the thatch sacrificial in case of fire. If fireboards are used, they require a ventilation gap between boarding and thatch so that the roof can breathe, as condensation can be a significant problem in thin, single layer thatch. Condensation is much less of a problem on thick straw roofs, which also provide much better insulation since they do not need to be ventilated.\n\nThe performance of thatch depends on roof shape and design, pitch of roof, position—its geography and topography—the quality of material and the expertise of the thatcher.\n\nThatch has some natural properties that are advantageous to its performance. It is naturally weather-resistant, and when properly maintained does not absorb a lot of water. There should not be a significant increase to roof weight due to water retention. A roof pitch of at least 50 degrees allows precipitation to travel quickly down slope so that it runs off the roof before it can penetrate the structure.\n\nThatch is also a natural insulator, and air pockets within straw thatch insulate a building in both warm and cold weather. A thatched roof ensures that a building is cool in summer and warm in winter.\n\nThatch also has very good resistance to wind damage when applied correctly.\n\nThatching materials range from plains grasses to waterproof leaves found in equatorial regions. It is the most common roofing material in the world, because the materials are readily available.\n\nBecause thatch is lighter, less timber is required in the roof that supports it.\n\nThatch is a versatile material when it comes to covering irregular roof structures. This fact lends itself to the use of second-hand, recycled and natural materials that are not only more sustainable, but need not fit exact standard dimensions to perform well.\n\nThatched houses are harder to insure because of the perceived fire risk, and because thatching is labor-intensive, it is much more expensive to thatch a roof than to cover it with slate or tiles. Birds can damage a roof while they are foraging for grubs, and rodents are attracted by residual grain in straw.\nThatch has fallen out of favor in much of the industrialized world not because of fire, but because thatching has become very expensive and alternative 'hard' materials are cheaper—but this situation is slowly changing. There are about 60,000 thatched roofs in the UK, of which 50–80 suffer a serious fire each year, most of these being completely destroyed. The cost to the Fire Brigade is £1.3m per annum. Many more thatched roofs are being built every year.\n\nNew thatched roofs were forbidden in London in 1212 following a major fire, and existing roofs had to have their surfaces plastered to reduce the risk of fire. The modern Globe Theatre is one of the few thatched buildings in London (others can be found in the suburb of Kingsbury), but the Globe's modern, water reed thatch is purely for decorative purpose and actually lies over a fully waterproofed roof built with modern materials. The Globe Theatre, opened in 1997, was modelled on the Rose, which was destroyed by a fire on a dry June night in 1613 when a burning wad of cloth ejected from a special effects cannon during a performance set light to the surface of the thatch. The original Rose Theatre was actually thatched with cereal straw, a sample of which was recovered by Museum of London archaeologists during the excavation of the site in the 1980s.\n\nSome claim thatch cannot cope with regular snowfall but, as with all roofing materials, this depends on the strength of the underlying roof structure and the pitch of the surface. A law passed in 1640 in Massachusetts outlawed the use of thatched roofs in the colony for this reason. Thatch is lighter than most other roofing materials, typically around , so the roof supporting it does not need to be so heavily constructed, but if snow accumulates on a lightly constructed thatched roof, it could collapse. A thatched roof is usually pitched between 45–55 degrees and under normal circumstances this is sufficient to shed snow and water. In areas of extreme snowfall, such as parts of Japan, the pitch is increased further.\nSome thatched roofs in the UK are extremely old and preserve evidence of traditional materials and methods that have long since been lost. In northern Britain this evidence is often preserved beneath corrugated sheet materials and frequently come to light during the development of smaller rural properties. Historic Scotland have funded several research projects into thatching techniques and these have revealed a wide range of materials including broom, heather, rushes, cereals, bracken, turf and clay and highlighted significant regional variation \n\nMore recent examples include the Moirlanich Longhouse, Killin owned by the National Trust for Scotland (rye, bracken & turf) and Sunnybrae Cottage, Pitlochry owned by Historic Scotland (rye, broom & turf) \n\n\n\n\n"}
{"id": "32840848", "url": "https://en.wikipedia.org/wiki?curid=32840848", "title": "The Cloud (poem)", "text": "The Cloud (poem)\n\n\"The Cloud\" is a major 1820 poem written by Percy Bysshe Shelley. \"The Cloud\" was written during late 1819 or early 1820, and submitted for publication on 12 July 1820. The work was published in the 1820 collection \"Prometheus Unbound, A Lyrical Drama, in Four Acts, With Other Poems\" by Charles and James Ollier in London in August 1820. The work was proof-read by John Gisborne. There were multiple drafts of the poem. The poem consists of six stanzas in anapestic or antidactylus meter, a foot with two unaccented syllables followed by an accented syllable.\n\nThe cloud is a metaphor for the unending cycle of nature: \"I silently laugh at my own cenotaph/ ... I arise and unbuild it again.\" As with the wind and the leaves in \"Ode to the West Wind\", the skylark in \"To a Skylark\", and the plant in \"The Sensitive Plant\", Shelley endows the cloud with sentient traits that personify the forces of nature.\n\nIn \"The Cloud\", Shelley relies on the imagery of transformation or metamorphosis, a cycle of birth, death, and rebirth: \"I change, but I cannot die.\" Mutability or change is a fact of physical nature.\n\nLightning or electricity is the \"pilot\" or guide for the cloud. Lightning is attracted to the \"genii\" in the earth which results in lightning flashes. The genii symbolize the positive charge of the surface of the earth while the cloud possesses a negative charge.\n\nBritish scientist and poet Erasmus Darwin, the grandfather of Charles Darwin, had written about plant life and science in the poem collection \"The Botanic Garden\" (1791) and on \"spontaneous vitality\", that \"microscopic animals are said to remain dead for many days or weeks ... and quickly to recover life and motion\" when water and heat are added, in \"The Temple of Nature\" (1803). Percy Bysshe Shelley had cited Darwin in his Preface to the anonymously published novel \"Frankenstein; or, The Modern Prometheus\" (1818), explaining how the novel was written and its meaning. He argued that imparting life to a corpse \"as not of impossible occurrence\".\n\nThe cloud is a personification and a metaphor for the perpetual cycle of transformation and change in nature. All life and matter are interconnected and undergo unending change and metamorphosis.\n\nA review of the 1820 \"Prometheus Unbound\" collection in the September and October 1821 issues of \"The London Magazine\" noted the originality of \"The Cloud\": \"It is impossible to peruse them without admiring the peculiar property of the author's mind, which can doff in an instant the cumbersome garments of metaphysical speculations, and throw itself naked as it were into the arms of nature and humanity. The beautiful and singularly original poem of 'The Cloud' will evince proofs of our opinion, and show the extreme force and freshness with which the writer can impregnate his poetry.\"\n\nIn the October 1821 issue of \"Quarterly Review\", W.S. Walker argued that \"The Cloud\" is related to \"Prometheus Unbound\" in that they are both absurd and \"galimatias\".\n\nJohn Todhunter wrote in 1880 that \"The Cloud\" and \"To a Skylark\" were \"the two most popular of Shelley's lyrics\".\n\nIn 1889, Francis Thompson asserted that \"The Cloud\" was the \"most typically Shelleyan of all the poems\" because it contained \"the child's faculty of make-believe raised to the nth power\" and that \"He is still at play, save only that his play is such as manhood stops to watch, and his playthings are those which the gods give their children. The universe is his box of toys. He dabbles his fingers in the dayfall. He is gold-dusty with tumbling amidst the stars.\"\n\nOn 20 April 1919, a silent black and white movie was released in the US entitled \"The Cloud\" which was \"a visual poem featuring clouds and landscapes in accompaniment to the words of Shelley's poem 'The Cloud'.\" The film was directed by W.A. Van Scoy and produced by the Post Nature Pictures company.\n\n\n"}
{"id": "44055899", "url": "https://en.wikipedia.org/wiki?curid=44055899", "title": "Triploid block", "text": "Triploid block\n\nTriploid block is a phenomenon describing the formation of nonviable progeny after hybridization of flowering plants that differ in ploidy. The barrier is established in the endosperm, a nutritive tissue supporting embryo growth. This phenomenon usually happens when autopolyploidy occurs in diploid plants. Triploid blocks lead to reproductive isolation. The triploid block effects have been explained as possibly due to genomic imprinting in the endosperm.\n"}
{"id": "26821712", "url": "https://en.wikipedia.org/wiki?curid=26821712", "title": "Tsunamis in lakes", "text": "Tsunamis in lakes\n\nA tsunami is defined as a series of water waves caused by the displacement of a large volume of a body of water; in the case of this article the body of water being investigated will be a lake rather than an ocean. Tsunamis in lakes are becoming increasingly important to investigate as a hazard, due to the increasing popularity for recreational uses, and increasing populations that inhabit the shores of lakes. Tsunamis generated in lakes and reservoirs are of high concern because it is associated with a near field source region which means a decrease in warning times to minutes or hours.\n\nInland tsunami hazards can be generated by many different types of earth movement. Some of these include earthquakes in or around lake systems, landslides, debris flow, rock avalanches, and glacier calving. Volcanogenic processes such as gas and mass flow characteristics are discussed in more detail below.\n\nTsunamis in lakes can be generated by fault displacement beneath or around lake systems. Faulting shifts the ground in a vertical motion through reverse, normal or oblique strike slip faulting processes, this displaces the water above causing a tsunami (Figure 1). The reason strike-slip faulting does not cause tsunamis is because there is no vertical displacement within the fault movement, only lateral movement resulting in no displacement of the water. In an enclosed basin such as a lake, tsunamis are referred to as the initial wave produced by coseismic displacement from an earthquake, and the seiche as the harmonic resonance within the lake.\n\nIn order for a tsunami to be generated certain criteria is required:\n\nThese tsunamis are of high damage potential due to being within a lake, making them of a near field source. This means a vast decrease in warning times, resulting in organised emergency evacuations after the generation of the tsunami being virtually impossible, and due to low lying shores even small waves lead to substantial flooding. Planning and education of residents needs to be done beforehand, so that when an earthquake is felt they know to head to higher ground and what routes to take to get there.\n\nLake Tahoe is an example of a lake that is in danger of having a tsunami due to faulting processes. Lake Tahoe in California and Nevada USA lies within an intermountain basin bounded by faults, with most of these faults at the lake bottom or hidden in glaciofluvial deposits. Lake Tahoe has had many prehistoric eruptions and in studies of the lake bottom sediments, a 10m high scarp has displaced the lake bottom sediments, indicating that the water was displaced by the same magnitude, as well as generating a tsunami. A tsunami and seiche in Lake Tahoe can be treated as shallow-water long waves as the maximum water depth is much smaller than the wavelength. This demonstrates the interesting impact that lakes have on the tsunami wave characteristics, as it is very different from ocean tsunami wave characteristics due to the ocean being deeper, and lakes being relatively shallow in comparison. With ocean tsunami waves amplitudes only increase when the tsunami gets close to shore, in lake tsunami waves are generated and stay in a shallow environment.\n\nThis would have a major impact on the 34,000 permanent residences along the lake, not to mention the impact on tourism in the area. Tsunami run-ups would leave areas near the lake inundated due to permanent ground subsidence attributed to the earthquake, with the highest run-ups and amplitudes being attributed to the seiches rather than the actual tsunami. The reason seiches cause so much damage is due to resonance within the bays reflecting the waves where they combine to make larger standing waves. For more information see seiches. Lake Tahoe also experienced a massive collapse of the western edge of the basin that formed McKinney Bay around 50,000 years ago. Is thought to have generated a tsunami/seiche wave with a height approaching .\n\nSub-aerial mass flows (landslides or rapid mass wasting) happen when a large amount of sediment becomes unstable, this can happen for example from the shaking from an earthquake, or saturation of the sediment initiating a sliding layer. This volume of sediment then flows into the lake giving a sudden large displacement of water. Tsunamis generated by sub aerial mass flows are defined in terms of the first initial wave being the tsunami wave and any tsunamis in terms of sub aerial mass flows are characterised into three zones. A splash zone or wave generation zone, this is the region were landslides and water motion are coupled and it extends as far as the landslide travels. Near field area, were the concern is based on the characteristics of the tsunami wave such as amplitude and wavelength which are crucial for predictive purposes. Far field area, the process is influenced mainly by dispersion characteristics and is not often used when investigating tsunamis in lakes, as most lake tsunamis are related only to near field processes.\n\nA modern example of a landslide into a reservoir lake, overtopping a dam, occurred in Italy with the Vajont Dam disaster in 1963. Evidence exists in paleoseismological evidence and other sedimentary core sample proxies of catastrophic rock failures of landslide-triggered lake tsunamis worldwide, including in Lake Geneva during AD 563.\n\nIn the event of the Alpine fault in New Zealand rupturing in the South Island, it is predicted that there would be shaking of approximately magnitude eight in the lake side towns of Queenstown (Lake Wakatipu) and Wanaka (Lake Wanaka). These could possibly cause sub-aerial mass flows that could generate tsunamis within the lakes, this would have a devastating impact on the 28,224 residents (2013 New Zealand census) who occupy these lake towns, not only in the potential losses of life and property, but the damage to the booming tourism industry would take years to rebuild.\n\nThe Otago Regional Council, responsible for the area, has recognised that in such an event, tsunamis could occur in both lakes.\n\nIn this article the focus is on tsunamis generated in lakes by volcanogenic processes in terms of gas build up causing violent lake over turns, with other processes such as pyroclastic flows not accounted for, as it requires more complex modelling . Lake overturns can be incredibly dangerous and occur when gas trapped at the bottom of the lake is heated by rising magma causing an explosion and lake overturn; an example of this is Lake Kivu.\n\nLake Kivu, one of the African Great Lakes, lies on the border between the Democratic Republic of the Congo and Rwanda, and is part of the East Africa Rift. Being part of the rift means it is affected by volcanic activity beneath the lake. This has led to a buildup of methane and carbon dioxide at the bottom of the lake, which can lead to violent limnic eruptions.\n\nLimnic eruptions (also called \"lake over turns\") are due to volcanic interaction with the water at the bottom of the lake that has high gas concentrations, this leads to heating of the lake and this rapid rise in temperature would spark a methane explosion displacing a large amount of water, followed nearly simultaneously by a release of carbon dioxide. This carbon dioxide would suffocate large numbers of people, with a possible tsunami generated from water displaced by the gas explosion affecting all of the 2 million people who occupy the shores of Lake Kivu. This is incredibly important as the warning times for an event such as a lake overturn is incredibly short in the order of minutes and the event itself may not even be noticed. Education of locals and preparation is crucial in this case and a lot of research in this area has been done in order to try to understand what is happening within the lake, in order to try to reduce the effects when this phenomenon does happen.\n\nA lake turn-over in Lake Kivu occurs from one of two scenarios. Either (1) up to another hundred years of gas accumulation leads to gas saturation in the lake, resulting in a spontaneous outburst of gas originating at the depth at which gas saturation has exceeded 100%, or (2) a volcanic or even seismic event triggers a turn-over. In either case a strong vertical lift of a large body of water results in a plume of gas bubbles and water rising up to and through the water surface. As the bubbling water column draws in fresh gas-laden water, the bubbling water column widens and becomes more energetic as a virtual \"chain reaction\" occurs which would look like a watery volcano. Very large volumes of water are displaced, vertically at first, then horizontally away from the centre at surface and horizontally inwards to the bottom of the bubbling water column, feeding in fresh gas-laden water. The speed of the rising column of water increases until it has the potential to rise 25m or more in the centre above lake level. The water column has the potential to widen to well in excess of a kilometre, in a violent disturbance of the whole lake. The watery volcano may take as much as a day to fully develop while it releases upwards of 400 billion cubic metres of gas (~12tcf). Some of these parameters are uncertain, particularly the time taken to release the gas and the height to which the water column can rise. As a secondary effect, particularly if the water column behaves irregularly with a series of surges, the lake surface will both rise by up to several metres and create a series of tsunamis or waves radiating away from the epicentre of the eruption. Surface waters may simultaneously race away from the epicentre at speeds as high as 20-40m/second, slowing as distances from the centre increase. The size of the waves created is unpredictable. Wave heights will be highest if the water column surges periodically, resulting in wave heights is great as 10-20m. This is caused by the ever-shifting pathway that the vertical column takes to the surface. No reliable model exists to predict this overall turnover behaviour. For tsunami precautions it will be necessary for people to move to high ground, at least 20m above lake level. A worse situation may pertain in the Ruzizi River where a surge in lake level would cause flash-flooding of the steeply sloping river valley dropping 700m to Lake Tanganyika, where it is possible that a wall of water from 20-50m high may race down the gorge. Water is not the only problem for residents of the Kivu basin; the more than 400 billion cubic metres of gas released creates a denser-than-air cloud which may blanket the whole valley to a depth of 300m or more. The presence of this opaque gas cloud, which would suffocate any living creatures with its mixture of carbon dioxide and methane laced with hydrogen sulphide, would cause the majority of casualties. Residents would be advised to climb to at least 400m above the lake level to ensure their safety. Strangely the risk of a gas explosion is not great as the gas cloud is only about 20% methane in carbon dioxide, a mixture that is difficult to ignite.\n\nAt 11:24 PM on 21 July 2014, in a period experiencing an earthquake swarm related to the upcoming eruption of Bárðarbunga, an 800m-wide section gave way on the slopes of the Icelandic volcano Askja. Beginning at 350m over water height, it caused a tsunami 20–30 meters high across the caldera, and potentially larger at localized points of impact. Thanks to the late hour, no tourists were present; however, search and rescue observed a steam cloud rising from the volcano, apparently geothermal steam released by the landslide. Whether geothermal activity played a role in the landslide is uncertain. A total of 30-50 million cubic meters was involved in the landslide, raising the caldera's water level by 1–2 meters.\n\nHazard mitigation for tsunamis in lakes is immensely important in the preservation of life, infrastructure and property. In order for hazard management of tsunamis in lakes to function at full capacity there are four aspects that need to be balanced and interacted with each other, these are:\n\n\nWhen all these aspects are taken into consideration and continually managed and maintained, the vulnerability of an area to a tsunami within the lake decreases. This is not because the hazard itself has decreased but the awareness of the people who would be affected makes them more prepared to deal with the situation when it does occur. This reduces recovery and response times for an area, decreasing the amount of disruption and in turn the effect the disaster has on the community.\n\nInvestigation into the phenomena of tsunamis in lakes for this article was restricted by certain limitations. Internationally there has been a fair amount of research into certain lakes but not all lakes that can be affected by the phenomenon have been covered. This is especially true for New Zealand with the possible occurrence of tsunamis in the major lakes recognised as a hazard, but with no further research completed.\n\n\n"}
