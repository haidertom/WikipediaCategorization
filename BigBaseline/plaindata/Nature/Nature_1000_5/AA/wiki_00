{"id": "4327451", "url": "https://en.wikipedia.org/wiki?curid=4327451", "title": "Adolf Dygasiński", "text": "Adolf Dygasiński\n\nAdolf Dygasiński (March 7, 1839, Niegosławice–June 3, 1902, Grodzisk Mazowiecki) was a Polish novelist, publicist and educator. In Polish literature, he was one of the leading representatives of Naturalism. \nDuring his literary career, Dygasiński wrote forty-two short stories and novels. \nSince 1884 his works were being published in book-form and enjoyed considerable success. \nThey were translated into Russian and German. \nIn 1891, Dygasiński went on a trip to Brazil on a trail of Polish emigrants from Partitioned Poland. \nHe produced a series of letters describing the tragic fate of Polish emigrees in South America. In the following years Dygasiński maintained a position of a tutor and coach for numerous wealthy landowning families. Late in life he settled in Warsaw, where he died on June 6, 1902, and was buried at the local Powązkowski Cemetery.\n\nIn his work Dygasiński often focused on topics of rural life and residents of small towns, highlighting the common fate of both, human and animal communities. Some of his most important work include: \n\n\n\n"}
{"id": "55257518", "url": "https://en.wikipedia.org/wiki?curid=55257518", "title": "Amity-enmity complex", "text": "Amity-enmity complex\n\nThe amity-enmity complex was a term introduced by Sir Arthur Keith. His work, \"A New Theory of Human Evolution\" (1948), posited that humans evolved as differing races, tribes, and cultures, exhibiting patriotism, morality, leadership and nationalism. Those who belong are part of the in-group, and tolerated; all others are classed as out-group, and subject to hostility; 'The code of enmity is a necessary part of the machinery of evolution. He who feels generous towards his enemy... has given up his place in the turmoil of evolutionary competition.' Conscience in humans evolved a duality; to protect and save friends,\nand also to hate and fight enemies. \nKeith's work summarized earlier opinions on human tribalism by Charles Darwin, Alfred Russel Wallace, and Herbert Spencer.\n\n\nThe amity-enmity complex maintains 'tribal spirit' and thus unity, of the community, 'as long as personal contact between its members is possible.' If the community grows beyond this limitation, then disruption, swarming and disintegration occur. Modern mass communication enables communities 'of 100 million' to remain intact.\n\nKeith expressed regret that this phenomenon, which explains so much, had not become common knowledge: \"[W]e eternally experience the misery... of each new manifestation of the complex, then invent some new 'ism' to categorise this behavior as an evil, dealing with a common behavioural trait piecemeal [instead of] finally grasping and understanding the phenomenon.\"\n\nColleges, sports teams, churches, trades unions, female fashions and political parties enable people to exhibit tribal loyalty within large, mass-communicating nations. 'In politics we have to take sides.' But all these 'petty manifestations' are cast aside in time of war.\nBismarck, Abraham Lincoln and Lloyd George are cited as statesmen who knew how to exploit the tribal spirit for political ends.\n\nRobert Ardrey pointed out that similar behavior can be observed in most primates, especially baboons and chimps. \"Nationalism as such is no more than a human expression of the animal drive to maintain and defend a territory... the mentality of the single Germanic tribe under Hitler differed in no way from that of early man or late baboon.\"\n\nThe amity-enmity complex is a serious obstacle to world peace and world government, and may even lead to nuclear holocaust: \"How can we get along without war?... if we fail to get along without war, the future will be as lacking in human problems as it will be remarkably lacking in men.\"\n\nDesmond Morris makes a prescriptive point: \"We must try to step outside our groups and look down on human battlefields with the unbiased eye of a hovering Martian.\" And he warns that \"the truly violent species all appear to have exterminated themselves, a lesson we should not overlook.\" The inherited aggression of the amity-enmity rivalry between communities is rationalized under a \"persistent cloak of ideology... a matter of ideals, moral principles, social philosophies or religious beliefs... [O]nly an immense amount of intellectual restraint will save the situation.\"\n\nAfter World War Two, a debate about the place of instinct and learning (the nature-versus-nurture debate) has occurred. According to Steven Pinker, the \"bitter lessons of lynchings, world wars, and the Holocaust\" have caused \"prevailing theories of mind\" to be \"refashioned to make racism and sexism as untenable as possible. The doctrine of the blank slate became entrenched in intellectual life.\"\n\nPinker makes the point that \"conflicts of interest are inherent to the human condition.\" Man is a product of nature, as much as malarial mosquitoes; both \"are doing exactly what evolution designed them to do, even if the outcome makes people suffer... [We] cannot call their behavior pathological... [T]he belief that violence is an aberration is dangerous.\"\n\n"}
{"id": "36939819", "url": "https://en.wikipedia.org/wiki?curid=36939819", "title": "Analog observation", "text": "Analog observation\n\nAnalog observation is, in contrast to naturalistic observation, a research tool by which a subject is observed in an artificial setting. Typically, types of settings in which analog observation is utilized include clinical offices or research laboratories, but, by definition, analog observations can be made in any artificial environment, even if the environment is one which the subject is likely to encounter naturally.\n\nAnalog observation is typically divided into two iteration of application: The first iteration primarily studies the effect of manipulation of variables in the subject's environment, including setting and events, on the subject's behavior. The second iteration primarily seeks to observe the subject's behavior in quasi-experimental social situations.\n\n"}
{"id": "25287133", "url": "https://en.wikipedia.org/wiki?curid=25287133", "title": "Anywhere on Earth", "text": "Anywhere on Earth\n\nAnywhere on Earth (AoE) is a calendar designation which indicates that a period expires when the date passes everywhere on Earth. The last place on Earth where any date exists is on Howland and Baker islands, in the time zone (the West side of the International Date Line), and so is the last spot on the globe for any day to exist. Therefore, the day ends AoE when it ends on Howland Island.\n\nThe convention originated in IEEE 802.16 balloting procedures. At this point, many IEEE 802 ballot deadlines are established as the end of day using \"AoE\", for \"Anywhere on Earth\" as a designation. This means that the deadline has not passed if, anywhere on Earth, the deadline date has not yet passed.\n\nNote that the day's end AoE occurs at noon Coordinated Universal Time (UTC) of the following day, Howland and Baker islands being halfway around the world from the prime meridian that is the base reference longitude for UTC.\n\nThus, in standard notation this is:\n\n"}
{"id": "31167381", "url": "https://en.wikipedia.org/wiki?curid=31167381", "title": "Asperity (geotechnical engineering)", "text": "Asperity (geotechnical engineering)\n\nIn Geotechnical engineering the term asperity is mostly used for unevenness (\"roughness\") of the surface of a discontinuity, grain, or particle with heights in the range from approximately 0.1 mm to many decimetre. Smaller unevenness is normally considered to be a \"material\" property (often denoted by \"material friction\" or \"basic material friction\").\n\nAn often used definition for \"asperities\" in geotechnical engineering:\nUnevenness of a surface are \"asperities\" if these cause dilation if two blocks with in between a discontinuity with matching \"asperities\" on the two opposing surfaces (i.e. a \"fitting discontinuity\") move relative to each other, under low stress levels that do not cause breaking of the \"asperities\".\n\nMaterials science recognizes asperities ranging from the sub-visual (normally less than 0.1 mm) to the atomic scale.\n\n"}
{"id": "36136905", "url": "https://en.wikipedia.org/wiki?curid=36136905", "title": "Avalon explosion", "text": "Avalon explosion\n\nThe Avalon explosion, named from the Precambrian fauna of the Avalon Peninsula, is a proposed evolutionary radiation in the history of the Animalia, about 575 million years ago, some 33 million years earlier than the Cambrian explosion.\n\nTrace fossils of these Avalon organisms have been found worldwide, and represent the earliest known complex multicellular organisms. The Avalon explosion produced the Ediacaran biota. The biota largely disappeared contemporaneously with the rapid increase in biodiversity known as the Cambrian explosion.\n\nThe Avalon explosion appears similar to the Cambrian explosion in the rapid increase in diversity of morphologies in a relatively small time frame, followed by diversification within the established body plans, a pattern similar to that observed in other evolutionary events.\n\nThe Avalon explosion was proposed in 2008 by Virginia Tech paleontologists through the analysis of the morphological space change in several Ediacaran assemblages. The discovery suggests that in the early evolution of animals, there may have been more than one explosive event. The original analysis has been the subject of dispute in the literature.\n"}
{"id": "19468941", "url": "https://en.wikipedia.org/wiki?curid=19468941", "title": "Balance of nature", "text": "Balance of nature\n\nThe balance of nature is a theory that proposes that ecological systems are usually in a stable equilibrium or homeostasis, which is to say that a small change in some particular parameter (the size of a particular population, for example) will be corrected by some negative feedback that will bring the parameter back to its original \"point of balance\" with the rest of the system. It may apply where populations depend on each other, for example in predator/prey systems, or relationships between herbivores and their food source. It is also sometimes applied to the relationship between the Earth's ecosystem, the composition of the atmosphere, and the world's weather.\n\nThe Gaia hypothesis is a balance of nature-based theory that suggests that the Earth and its ecology may act as co-ordinated systems in order to maintain the balance of nature.\n\nThe theory that nature is permanently in balance has been largely discredited by scientists working in ecology, as it has been found that chaotic changes in population levels are common, but nevertheless the idea continues to be popular in the general public. During the later half of the twentieth century the theory was superseded by catastrophe theory and chaos theory.\n\nThe concept that nature maintains its condition is of ancient provenance; Herodotus commented on the wonderful relationship between predator and prey species, which remained in a steady proportion to one another, with predators never excessively consuming their prey populations. The \"balance of nature\" concept once ruled ecological research, as well as once governing the management of natural resources. This led to a doctrine popular among some conservationists that nature was best left to its own devices, and that human intervention into it was by definition unacceptable. The validity of a \"balance of nature\" was already questioned in the early 1900s, but the general abandonment of the theory by scientists working in ecology only happened in the last quarter of that century when studies showed that it did not match what could be observed among plant and animal populations.\n\nPredator-prey populations tend to show chaotic behavior within limits, where the sizes of populations change in a way that may appear random, but is in fact obeying deterministic laws based only on the relationship between a population and its food source illustrated by the Lotka–Volterra equation. An experimental example of this was shown in an eight-year study on small Baltic Sea creatures such as plankton, which were isolated from the rest of the ocean. Each member of the food web was shown to take turns multiplying and declining, even though the scientists kept the outside conditions constant. An article in the journal \"Nature\" stated; \"Advanced mathematical techniques proved the indisputable presence of chaos in this food web ... short-term prediction is possible, but long-term prediction is not.\"\n\nAlthough some conservationist organizations argue that human activity is incompatible with a balanced ecosystem, there are numerous examples in history showing that several modern day habitats originate from human activity: some of Latin America's rain forests owe their existence to humans planting and transplanting them, while the abundance of grazing animals in the Serengeti plain of Africa is thought by some ecologists to be partly due to human-set fires that created savanna habitats.\n\nPossibly one of the best examples of an ecosystem fundamentally modified by human activity can be observed as a consequence of the Australian Aboriginal practice of \"Fire-stick farming\". The legacy of this practice over long periods has resulted in forests being converted to grasslands capable of sustaining larger populations of faunal prey, particularly in the northern and western regions of the continent. So thorough has been the effect of these deliberate regular burnings that many plant and tree species from affected regions have now completely adapted to the annual fire regime in that they require the passage of a fire before their seeds will even germinate. One school in Los Angeles states, \" “We have let our kids go to the forest area of the playground. However, five years later, we found that none of the flowers were growing, the natural damp soil had been hardened, and all of the beautiful grass had been plucked,”.\n\nDespite being discredited among ecologists, the theory is widely held to be true by the general public, with one authority calling it an \"enduring myth\". At least in Midwestern America, the \"balance of nature\" idea was shown to be widely held by both science majors and the general student population. In a study at the University of Patras, educational sciences students were asked to reason about the future of ecosystems which suffered human-driven disturbances. Subjects agreed that it was very likely for the ecosystems to fully recover their initial state, referring to either a 'recovery process' which restores the initial 'balance', or specific 'recovery mechanisms' as an ecosystem's inherent characteristic. In a 2017 study, Ampatzidis and Ergazaki discuss the learning objectives and design criteria that a learning environment for non-biology major students should meet to support them challenge the \"balance of nature\" idea.\n\n"}
{"id": "41821574", "url": "https://en.wikipedia.org/wiki?curid=41821574", "title": "Binsey Poplars", "text": "Binsey Poplars\n\n‘Binsey Poplars’ is a poem by Gerard Manley Hopkins (1844–1889), written in 1879. The poem was inspired by the felling of a row of poplar trees near the village of Binsey, northwest of Oxford, England, and overlooking Port Meadow on the bank of the River Thames. The replacements for these trees, running from Binsey north to Godstow, lasted until 2004, when replanting began again.\n\nThe Bodleian Library of Oxford University holds a draft manuscript of the poem, handwritten by Hopkins, acquired in 2013.\n\nThe text of the poem is as follows:\n\n\n"}
{"id": "11003420", "url": "https://en.wikipedia.org/wiki?curid=11003420", "title": "Brown energy", "text": "Brown energy\n\nBrown energy may refer to:\n\n"}
{"id": "25508508", "url": "https://en.wikipedia.org/wiki?curid=25508508", "title": "Building Safer Communities. Risk Governance, Spatial Planning and Responses to Natural Hazards", "text": "Building Safer Communities. Risk Governance, Spatial Planning and Responses to Natural Hazards\n\nBuilding Safer Communities. Risk Governance, Spatial Planning and Responses to Natural Hazards is a 2009 book edited by Urbano Fra Paleo, published by IOS Press.\n\nThis textbook examines the central principles of enhanced risk governance, whose implementation might help to mitigate the increasing losses caused by natural hazards. It promotes the adoption of proactive, preventive approaches in public policies, particularly through land use planning, by influencing on the occupation of hazard-prone areas.\nIt serves both as a comprehensive introduction to the formulation and implementation at the strategic level of policies that address risk, and as an advancement in the integration of current practices, including emergency management, environmental management, community development and spatial planning. \nThe authors study and construe solutions that review integrated strategies of the various levels of government considering:\n\nUrbano Fra Paleo is a geographer, and an Associate Professor of Human Geography at the University of Santiago de Compostela, Spain.\n\n"}
{"id": "10019499", "url": "https://en.wikipedia.org/wiki?curid=10019499", "title": "Burned area emergency response", "text": "Burned area emergency response\n\nBurned area emergency response (BAER) is an emergency risk management reaction to post wildfire conditions that pose risks to human life and property or could further destabilize or degrade the burned lands. Even though wildfires are natural events, the presence of people and man-made structures in and adjacent to the burned area frequently requires continued emergency risk management actions. High severity wildfires pose a continuing flood, debris flow and mudflow risk to people living within and downstream from a burned watershed as well as a potential loss of desirable watershed values.\n\nThe burned area emergency response risk management process begins during or shortly after wildfire containment with risk assessments evaluating the effects of the wildfire against values needing protection. These risk assessments can range from simple to complex. An organized interdisciplinary team of subject matter experts (e.g., hydrologists, soil scientists, botanists, cultural resource specialists, engineers, etc.) used among other assessment tools hydrological modeling and soil burn severity mapping to assess potential flooding and vegetation recovery after the Cerro Grande Fire in 2000.\n\nA BAER plan is developed based on the risk assessments and burned area land management objectives. The BAER Plan identifies the most effective treatments to address the identified risks. Plan implementation timeframes are dictated primarily by anticipated future events (e.g., next significant rainstorm) which also influence treatment options.\n\nBurned area emergency response has mostly concentrated on risk reduction treatments with varying degrees of success. Risk avoidance, transfer and retention treatments are integral in the burned area emergency response risk management process.\n\nRisk reduction treatments are designed to protect human life and safety and reduce flood severity, soil erosion and prevent the establishment of non-native plants. On 10 wildfires studied in Colorado, rainfall amount and intensity followed by bare mineral soil explained 63% of soil erosion variation. Research has shown that the risk of flooding, debris flows and mudflows are significantly increased with increasing rainfall intensities and burn severity and that some risk reduction treatments help for low but not high intensity rainfall events.\n\nMulches, erosion cloth and seeding retard overland flow and protect soil from rain drop impact and increase soil moisture holding capacity. Landscape structures (e.g., log erosion barriers, contour trenches, straw wattles) trap sediment and prevent slope rilling. Strip tillage and chemicals break up or reduce hydrophobic soils and improve infiltration. Wood and straw mulch reduced erosion rates by 60 to 80%, contour-felled log erosion barriers 50 to 70%, hydromulch 19% and post fire seeding had little effect the first year when rainfall events were small and intensities low.\n\nIn stream flood control treatments slow, delay, redistribute, or redirect water, mud and debris. Straw bale check dams, silt screens and debris retention basins slow water flow and trap sediment. Riparian vegetation stabilizes streambanks. Roads and culverts are armored and debris removed as needed. Water diversion implements protect facilities and property.\n\nThe chance of introducing new invasive plants to the burned area is reduced by restricting access or thoroughly cleaning all equipment, people and animals of seeds before entering a burned area. Research has shown that non-native plant cover is positively associated with post-wildfire seeded grass cover. Even though post-wildfire seeding operations require seed mix purity standards and the number of contaminated seeds may be small on a percentage based, that the application of very large amounts of seed (thousands of pounds) ensures that a significant number of non-native plant seeds will be distributed.\n\nAvoidance treatments remove values at risk from risk prone areas. Frequently homes and other values are located on alluvial fans at the base of watersheds. The presence of the alluvial fans indicates a history of significant flooding, debris flows and mudflows with potential personal and property damage potential. Mobile property is temporally or permanently relocated. Evacuation planning and early warning systems are frequently used to protect people at risk. Flood peaks increase more rapidly with increases in rainfall intensity above a threshold value for the maximum 30 min intensity of approximately 10 mm per hour. That this rainfall intensity could be used to set threshold limits in rain gauges that are part of an early warning flood system after wildfire.\n\nOften it is not feasible to avoid or reduce risks. Flood insurance is a means of transferring risk to another party for values with insurable value.\n\nAccepting the risk is an option when values at risk are small and inevitable or when the risks cannot be reduced, avoided or transferred (i.e., infrequent catastrophic events).\n\n\n"}
{"id": "1867894", "url": "https://en.wikipedia.org/wiki?curid=1867894", "title": "Certified wood", "text": "Certified wood\n\nCertified wood and paper products come from responsibly managed forests – as defined by a particular standard. With third-party forest certification, an independent organization develops standards of good forest management, and independent auditors issue certificates to forest operations that comply with those standards.\n\nForest certification programs typically require that forest management practices conform to existing laws. Other basic requirements or characteristics of forest certification programs include:\n\nBasic requirements of credible forest certification programs include:\n\nToday there are more than 50 certification programs worldwide addressing the many types of forests and tenures around the world. The two largest international forest certification programs are the Forest Stewardship Council (FSC) and the Programme for the Endorsement of Forest Certification (PEFC).\n\nThe PEFC is the largest certification framework in terms of forest area, with approximately two-thirds of the total certified area. The FSC program is the fastest growing.\n\nThird-party forest certification was pioneered in the early 1990s by the FSC, a collaboration between environmental NGOs, forest product companies and social interests. Competing systems quickly emerged throughout the world. Some commentators, including Jared Diamond, have suggested that many competing standards were set up by logging companies specifically aiming to confuse consumers with less rigorously enforced but similarly named competing standards.\n\nIn the United States and Canada, there are a number of forest certification programs. Three of these programs are endorsed by the PEFC. They are the American Tree Farm System (ATFS), the Canadian Standards Association’s Sustainable Forest Management Standard and the Sustainable Forestry Initiative (SFI) Program. ATFS is applicable only in the United States; the Canadian Standards Association SFM Standard is applicable only in Canada. SFI is applicable to both the United States and Canada. The FSC, program is applied throughout North America. SFI is the world’s largest regional forest certification program in terms of total certified forest area[1].\n\nThe National Association of State Foresters in the USA passed a resolution in 2008 that supports \"all\" of the forest certification systems used in the USA and recognized the value of their differences: “... the ATFS, FSC, and SFI systems include the fundamental elements of credibility and make positive contributions to forest sustainability... No certification program can credibly claim to be ‘best’, and no certification program that promotes itself as the only certification option can maintain credibility. Forest ecosystems are complex and a simplistic ‘one size fits all’ approach to certification cannot address all sustainability needs.”.\n\nThe Canadian Council of Forest Ministers issued a statement in 2008 on forest certification standards in Canada, which said: “In Canada, each jurisdiction’s forest laws, policies and administrative requirements comprise an framework that fully characterizes what sustainable forest management (SFM) means in that jurisdiction, and what actions may take place on public and/or private forest land. Governments in Canada support third-party forest certification as a tool to demonstrate the rigor of Canada’s forest management laws, and to document the country’s world-class sustainable forest management record. The forest management standards of the Canadian Standards Association (CSA), the FSC and the Sustainable Forestry Initiative (SFI) are all used in Canada. Governments in Canada accept that these standards demonstrate, and promote the sustainability of forest management practices in Canada.” \n\nChain of Custody certification tracks the certified material through the production process – from the forest to the consumer, including all successive stages of processing, transformation, manufacturing and distribution. It also provides evidence that certified material in a certified product originates from certified forests.\n\nThe United Nations reports that between January 2009 and May 2010, the total number of PEFC and FSC chain-of-custody certificates issued worldwide increased by 88% for a total of 23,717 certificates (this does not include SFI certificates).\n\nForest certification is a voluntary process. About 10% of the world’s forest under at least one certification program. Customers that choose to buy certified products are supporting land managers, land owners and forest product companies that have made a commitment to meeting the standards of forest certification.\n\nThird-party forest certification is a useful tool for those seeking to purchase paper and wood products that come from forests that are well-managed and use materials that are legally harvested. Incorporating third-party certification into forest product buying practices can be a centerpiece for responsible wood and paper purchasing policies that include factors such as the protection of sensitive forest values, thoughtful material selection and efficient use of products.\n\nThe 2009-2010 United Nations Market Review reported that companies that produced or traded in certified forest products often had a market advantage during the 2008-2009 recession because, in a buyers’ market, buyers could be more selective in choosing their sources of supply. The report cites four demand drivers for certification:\n\nThe World Resources Institute, in partnership with the Environmental Investigation Agency, released a fact sheet designed to answer some of the frequently asked questions about the Lacey Act, which was amended in 2008 to ban commerce in illegally sourced plants and their products—including timber, wood, and paper products. The fact sheet says forest certification is a very good approach for demonstrating due care by showing government and customers that a company has taken proactive steps to eliminate illegal wood or plant material from its supply chain. Certification does not relieve importers of the requirement to submit appropriate import declaration information to U.S. government agencies.\n\n"}
{"id": "32703814", "url": "https://en.wikipedia.org/wiki?curid=32703814", "title": "Chirality", "text": "Chirality\n\nChirality is a property of asymmetry important in several branches of science. The word \"chirality\" is derived from the Greek (\"kheir\"), \"hand,\" a familiar chiral object.\n\nAn object or a system is \"chiral\" if it is distinguishable from its mirror image; that is, it cannot be superposed onto it. Conversely, a mirror image of an \"achiral\" object, such as a sphere, cannot be distinguished from the object. A chiral object and its mirror image are called \"enantiomorphs\" (Greek, \"opposite forms\") or, when referring to molecules, \"enantiomers\". A non-chiral object is called \"achiral\" (sometimes also \"amphichiral\") and can be superposed on its mirror image. If the object is non-chiral and is imagined as being colored blue and its mirror image is imagined as colored yellow, then by a series of rotations and translations the two can be superposed, producing green, with none of the original colors remaining.\n\nThe term was first used by Lord Kelvin in 1893 in the second Robert Boyle Lecture at the Oxford University Junior Scientific Club which was published in 1894:\n\nHuman hands are perhaps the most universally recognized example of chirality. The left hand is a non-superimposable mirror image of the right hand; no matter how the two hands are oriented, it is impossible for all the major features of both hands to coincide across all axes. This difference in symmetry becomes obvious if someone attempts to shake the right hand of a person using their left hand, or if a left-handed glove is placed on a right hand. In mathematics, \"chirality\" is the property of a figure that is not identical to its mirror image.\n\nIn mathematics, a figure is chiral (and said to have chirality) if it cannot be mapped to its mirror image by rotations and translations alone. For example, a right shoe is different from a left shoe, and clockwise is different from anticlockwise. See for a full mathematical definition.\n\nA chiral object and its mirror image are said to be enantiomorphs. The word \"enantiomorph\" stems from the Greek (enantios) 'opposite' + (morphe) 'form'. A non-chiral figure is called achiral or amphichiral.\n\nThe helix (and by extension a spun string, a screw, a propeller, etc.) and Möbius strip are chiral two-dimensional objects in three-dimensional ambient space. The J, L, S and Z-shaped \"tetrominoes\" of the popular video game Tetris also exhibit chirality, but only in a two-dimensional space.\n\nMany other familiar objects exhibit the same chiral symmetry of the human body, such as gloves, glasses (where two lenses differ in prescription), and shoes. A similar notion of chirality is considered in knot theory, as explained below.\n\nSome chiral three-dimensional objects, such as the helix, can be assigned a right or left handedness, according to the right-hand rule.\n\nIn geometry a figure is achiral if and only if its symmetry group contains at least one \"orientation-reversing\" isometry.\nIn two dimensions, every figure that possesses an axis of symmetry is achiral, and it can be shown that every \"bounded\" achiral figure must have an axis of symmetry.\nIn three dimensions, every figure that possesses a plane of symmetry or a center of symmetry is achiral. There are, however, achiral figures lacking both plane and center of symmetry.\nIn terms of point groups, all chiral figures lack an improper axis of rotation (S). This means that they cannot contain a center of inversion (i) or a mirror plane (σ). Only figures with a point group designation of C, C, D, T, O, or I can be chiral.\n\nA knot is called achiral if it can be continuously deformed into its mirror image, otherwise it is called chiral. For example, the unknot and the figure-eight knot are achiral, whereas the trefoil knot is chiral.\n\nIn physics, chirality may be found in the spin of a particle, where the handedness of the object is determined by the direction in which the particle spins. Not to be confused with helicity, which is the projection of the spin along the linear momentum of a subatomic particle, chirality is a purely quantum mechanical phenomenon like spin. Although both can have left-handed or right-handed properties, only in the massless case do they have a simple relation. In particular for a massless particle the helicity is the same as the chirality while for an antiparticle they have opposite sign.\n\nThe \"handedness\" in both chirality and helicity relate to the rotation of a particle while it proceeds in linear motion with reference to the human hands. The thumb of the hand points towards the direction of linear motion whilst the fingers curl into the palm, representing the direction of rotation of the particle (i.e. clockwise and counterclockwise). Depending on the linear and rotational motion, the particle can either be defined by left-handedness (ex. translating leftwards and rotating counterclockwise) or right-handedness (ex. translating in the right direction and rotating clockwise). A symmetry transformation between the two is called parity. Invariance under parity by a Dirac fermion is called \"chiral symmetry\".\n\nElectromagnetic wave propagation as handedness is wave polarization and described in terms of helicity (occurs as a helix). Polarization of an electromagnetic wave is the property that describes the orientation, i.e., the time-varying, direction (vector), and amplitude of the electric field vector. For a depiction, see the adjacent image.\n\nA \"chiral molecule\" is a type of molecule that has a non-superposable mirror image. The feature that is most often the cause of chirality in molecules is the presence of an asymmetric carbon atom.\n\nThe term \"chiral\" in general is used to describe the object that is non-superposable on its mirror image.\n\nIn chemistry, chirality usually refers to molecules. Two mirror images of a chiral molecule are called enantiomers or optical isomers. Pairs of enantiomers are often designated as \"right-\", \"left-handed\" or if it has no bias achiral. As polarized light passes through a chiral molecule, the plane of polarization, when viewed along the axis toward the source, will be rotated in a clockwise (to the right) or anticlockwise (to the left). A right handed rotation is dextrorotary (d); that to the left is levorotary (l). The d- and l-isomers are the same compound but are called enantiomers. An equimolar mixture of the two optical isomers will produce no net rotation of polarized light as it passes through. Left handed molecules have l- prefixed to their names; d- is prefixed to right handed molecules.\n\nMolecular chirality is of interest because of its application to stereochemistry in inorganic chemistry, organic chemistry, physical chemistry, biochemistry, and supramolecular chemistry.\n\nMore recent developments in chiral chemistry include the development of chiral inorganic nanoparticles that may have the similar tetrahedral geometry as chiral centers associated with sp3 carbon atoms traditionally associated with chiral compounds, but at larger scale. Helical and other symmetries of chiral nanomaterials were also obtained.\n\nAll of the known life-forms show specific chiral properties in chemical structures as well as macroscopic anatomy, development and behavior. In any specific organism or evolutionarily related set thereof, individual compounds, organs, or behavior are found in the same single enantiomorphic form. Deviation (having the opposite form) could be found in a small number of chemical compounds, or certain organ or behavior but that variation strictly depends upon the genetic make up of the organism. From chemical level (molecular scale), biological systems show extreme stereospecificity in synthesis, uptake, sensing, metabolic processing. A living system usually deals with two enantiomers of same compound in a drastically different way.\n\nIn biology, homochirality is a common property of amino acids and carbohydrates. The chiral protein-making amino acids, which are translated through the ribosome from genetic coding, occur in the form. However, -amino acids are also found in nature. The monosaccharides (carbohydrate-units) are commonly found in -configuration. DNA double helix is chiral (as any kind of helix is chiral), and B-form of DNA shows a right-handed turn.\n\nSometimes, when two enantiomers of a compound found in organisms, they significantly differ in their taste, smell and other biological actions. For example, (+)-Limonene found in orange (causing its smell), and (–)-Limonene found in Lemons (causing its smell), show different smells due to different biochemical interactions at human nose. (+)-Carvone is responsible for the smell of Caraway seed oil whereas (–)-carvone is responsible for smell of Spearmint oil.\n\nAlso, for artificial compounds, including medicines, in case of chiral drugs, the two enantiomers sometimes show remarkable difference in effect of their biological actions. Darvon (Dextropropoxyphene) is a painkiller, whereas its enantiomer, Novrad (Levopropoxyphene) is an anti-cough agent. In case of Penicillamine, the S-isomer used in treatment of primary chronic arthritis, Whereas the R-isomer has no therapeutic effect as well as being highly toxic. In some cases the less therapeutically active enantiomer can cause side effects. For example, S-naproxen is an analgesic but the R-isomer cause renal problems. The naturally occurring plant form of alpha-tocopherol (vitamin E) is RRR-α-tocopherol whereas the synthetic form (all-racemic vitamin E, or dl-tocopherol) is equal parts of the stereoisomers RRR, RRS, RSS, SSS, RSR, SRS, SRR and SSR with progressively decreasing biological equivalency, so that 1.36 mg of dl-tocopherol is considered equivalent to 1.0 mg of d-tocopherol.\nMacroscopic example of Chirality is found in plant kingdom, animal kingdom and all other groups of organism. A simple example is the coiling direction of any climber plants. It may be one of two possible type of helix.\nIn anatomy, chirality is found in the imperfect mirror image symmetry of many kinds of animal bodies. Organisms such as gastropods exhibit chirality in their coiled shells, resulting in an asymmetrical appearance. Over 90% of gastropod species have \"dextral\" (right-handed) shells in their coiling, but a small minority of species and genera are virtually always \"sinistral\" (left-handed). A very few species (for example \"Amphidromus perversus\") show an equal mixture of dextral and sinistral individuals.\n\nIn humans, chirality (also referred to as \"handedness\" or \"laterality\") is an attribute of humans defined by their unequal distribution of fine motor skill between the left and right hands. An individual who is more dexterous with the right hand is called \"right-handed\", and one who is more skilled with the left is said to be \"left-handed\". Chirality is also seen in the study of facial asymmetry.\n\nIn flatfish, the Summer flounder or fluke are left-eyed, while halibut are right-eyed.\n\n\n"}
{"id": "13662027", "url": "https://en.wikipedia.org/wiki?curid=13662027", "title": "Colloid vibration current", "text": "Colloid vibration current\n\nColloid vibration current is an electroacoustic phenomenon that arises when ultrasound propagates through a fluid that contains ions and either solid particles or emulsion droplets. \n\nThe pressure gradient in an ultrasonic wave moves particles relative to the fluid. This motion disturbs the double layer that exists at the particle-fluid interface. The picture illustrates the mechanism of this distortion. Practically all particles in fluids carry a surface charge. This surface charge is screened with an equally charged diffuse layer; this structure is called the double layer. Ions of the diffuse layer are located in the fluid and can move with the fluid. Fluid motion relative to the particle drags these diffuse ions in the direction of one or the other of the particle's poles. The picture shows ions dragged towards the left hand pole. As a result of this drag, there is an excess of negative ions in the vicinity of the left hand pole and an excess of positive surface charge at the right hand pole. As a result of this charge excess, particles gain a dipole moment. These dipole moments generate an electric field that in turn generates measurable electric current. This phenomenon is widely used for measuring zeta potential in concentrated colloids.\n\n"}
{"id": "15938221", "url": "https://en.wikipedia.org/wiki?curid=15938221", "title": "Darwin among the Machines", "text": "Darwin among the Machines\n\n\"Darwin among the Machines\" is the name of an article published in \"The Press\" newspaper on 13 June 1863 in Christchurch, New Zealand, which references the work of Charles Darwin in the title. Written by Samuel Butler but signed \"Cellarius\" (q.v.), the article raised the possibility that machines were a kind of \"mechanical life\" undergoing constant evolution, and that eventually machines might supplant humans as the dominant species:\nThe article ends by urging that, \"War to the death should be instantly proclaimed against them. Every machine of every sort should be destroyed by the well-wisher of his species. Let there be no exceptions made, no quarter shown; let us at once go back to the primeval condition of the race.\"\n\nButler developed this and subsequent articles into \"The Book of the Machines\", three chapters of \"Erewhon\", published anonymously in 1872. The Erewhonian society Butler envisioned had long ago undergone a revolution that destroyed most mechanical inventions. The narrator of the story finds a book that details the reasons for this revolution, which he translates for the reader. In \nchapter xxiii: the book of the machines, a number of quotes from this imaginary book discuss the possibility of machine consciousness:\n\nLater, in chapter xxiv: the machines—continued, the imaginary book also discusses the notion that machines can \"reproduce\" like living organisms:\n\nThis notion of machine \"reproduction\" anticipates the later notion of self-replicating machines, although in chapter xxv: the machines—concluded, the imaginary book supposes that while there is a danger that humans will become subservient to machines, the machines will still need humans to assist in their reproduction and maintenance:\n\nThe author of the imaginary book goes on to say that while life under machine rule might be materially comfortable for humans, the thought of the human race being superseded in the future is just as horrifying to him as the thought that his distant ancestors were anything other than fully human (apparently Butler imagines the author to be an Anti-evolutionist), so he urges that all machines which have been in use for less than 300 years be destroyed to prevent this future from coming to pass:\n\nErewhonian society came to the conclusion \"...that the machines were ultimately destined to supplant the race of man, and to become instinct with a vitality as different from, and superior to, that of animals, as animal to vegetable life. So... they made a clean sweep of all machinery that had not been in use for more than two hundred and seventy-one years...\" (from \nchapter ix: to the metropolis.)\n\nDespite the initial popularity of \"Erewhon\", Butler commented in the preface to the second edition that reviewers had \"in some cases been inclined to treat the chapters on Machines as an attempt to reduce Mr. Darwin’s theory to an absurdity.\" He protested that \"few things would be more distasteful to me than any attempt to laugh at Mr. Darwin\", but also added \"I am surprised, however, that the book at which such an example of the specious misuse of analogy would seem most naturally levelled should have occurred to no reviewer; neither shall I mention the name of the book here, though I should fancy that the hint given will suffice\", which may suggest that the chapter on Machines was in fact a satire intended to illustrate the \"specious misuse of analogy\", even if the target was not Darwin; Butler, fearing that he had offended Darwin, wrote him a letter explaining that the actual target was Joseph Butler's 1736 \"The Analogy of Religion, Natural and Revealed, to the Constitution and Course of Nature\". The Victorian scholar Herbert Sussman has suggested that although Butler's exploration of machine evolution was intended to be whimsical, he may also have been genuinely interested in the notion that living organisms are a type of mechanism and was exploring this notion with his writings on machines, while the philosopher Louis Flaccus called it \"a mixture of fun, satire, and thoughtful speculation.\"\n\nGeorge Dyson applies Butler's original premise to the artificial life and intelligence of Alan Turing in Darwin Among the Machines: The Evolution of Global Intelligence (1998) , to suggest that the internet is a living, sentient being.\n\nDyson's main claim is that the evolution of a conscious mind from today's technology is inevitable. It is not clear whether this will be a single mind or multiple minds, how smart that mind would be, and even if we will be able to communicate with it. He also clearly suggests that there are forms of intelligence on Earth that we are currently unable to understand.\n\nFrom the book: \"What mind, if any, will become apprehensive of the great coiling of ideas now under way is not a meaningless question, but it is still too early in the game to expect an answer that is meaningful to us.\"\n\nThe theme of humanity at war or otherwise in conflict with machines is found in a number of later creative works:\n\n\n\n\n"}
{"id": "19873073", "url": "https://en.wikipedia.org/wiki?curid=19873073", "title": "Defensible space (fire control)", "text": "Defensible space (fire control)\n\nA defensible space, in the context of fire control, is a natural and/or landscaped area around a structure that has been maintained and designed to reduce fire danger. The practice is sometimes called firescaping. \"Defensible space\" is also used in the context of wildfires, especially in the wildland-urban interface (WUI). This defensible space reduces the risk that fire will spread from one area to another, or to a structure, and provides firefighters access and a safer area from which to defend a threatened area. Firefighters sometimes do not attempt to protect structures without adequate defensible space, as it is less safe and less likely to succeed.\n\n\nThe term defensible space in landscape (\"firescape\") use refers to the zone surrounding a structure. Often the location is in the wildland–urban interface. This area need not be devoid of vegetation by using naturally fire resistive plants that are spaced, pruned and trimmed, and irrigated, to minimize the fuel mass available to ignite and also to hamper the spread of a fire.\n\n\nAn important component is ongoing maintenance of the fire-resistant landscaping for reduced fuel loads and fire fighting access. Fire resistive plants that are not maintained can desiccate, die, or amass deadwood debris, and become fire assistive. Irrigation systems and pruning can help maintain a plant's fire resistance. Maintaining access roads and driveways clear of side and low-hanging vegetation can allow large fire equipment to reach properties and structures.\nSome agencies recommend clearing combustible vegetation at minimum horizontal 10 ft from roads and driveways a vertical of 13 ft 6 inches above them. Considering the plant material involved is important to not create unintended consequences to habitat integrity and unnecessary aesthetic issues. Street signs, and homes clearly identified with the numerical address, assist access also.\n\nThe unintended negative consequences of erosion and native habitat loss can result from some unskillful defensible space applications. The disturbance of the soil surface, such as garden soil cultivation in and firebreaks beyond native landscape zones areas, destroys the native plant cover and exposes open soil, accelerating invasive species of plants (\"invasive exotics\") spreading and replacing native habitats.\n\nIn suburban and wildland–urban interface areas, the vegetation clearance and brush removal ordinances of municipalities for defensible space can result in mistaken excessive clearcutting of native and non-invasive introduced shrubs and perennials that exposes the soil to more light and less competition for invasive plant species, and also to erosion and landslides. Negative aesthetic consequences to natural and landscaped areas can be minimized with integrated and balanced defensible space practices.\n\n\n"}
{"id": "48048662", "url": "https://en.wikipedia.org/wiki?curid=48048662", "title": "Dessauite-(Y)", "text": "Dessauite-(Y)\n\nDessauite-(Y) is a mineral member of the crichtonite group with the formula (Sr,Pb)(Y,U)(Ti,Fe)O. It is associated with derbylite, hematite, rutile, karelianite, siderite, and calcite. Founded in the Buca della Vena Mine, Tuscany, Italy, the mineral was called dessauite in honor of professor Gabor Dessau (1907–1983). \n\nDessauite occurs as small, flattened rhombohedral crystals, tabular {001} with hexagonal outline. Members of the Crichtonite group may be confused with ilmenite or hematite. The difference between dessauite and other minerals in the crichonite group is the occurrence of three additional octahedral sites and of a site in square pyramidal coordination, all with low occupancies. The mineral is black and opaque, presents a metallic luster, and it is brittle. Dessauite presents dimensions of diameter up to 1mm and thickness up to 0.2mm. In reflected plane-polarized light the color is ash-grey with pale bluish tones. The calculated density is 4.68 g/cm. The habit is tabular, forming thin dimensions in one direction and hardness of 6.5 and 7. Dessauite differs from other elements of the crichtonite group because of the quantity of cations and X-ray diffraction pattern.\n\nDessauite was found in the Buca della Vena Mine, Apuan Alps, northern Tuscany, Italy, with many other minerals, coming from hydrothermal fluids circulating through a small hematite-barite ore deposit within dolomite, during an alpine metamorphic event. It occurs in calcite veins hosted within dolomite and is associated with calcite, rutile, hematite, siderite, and derbylite.\n\n"}
{"id": "878461", "url": "https://en.wikipedia.org/wiki?curid=878461", "title": "Earth's orbit", "text": "Earth's orbit\n\nAll celestial bodies in the Solar System, including planets such as our own, orbit around the Solar System's centre of mass. The sun makes up 99.76% of this mass which is why the centre of mass is extremely close to the sun.\n\nEarth's orbit is the trajectory along which Earth travels around the Sun. The average distance between the Earth and the Sun is 149.60 million km (92.96 million mi), and one complete orbit takes  days (1 sidereal year), during which time Earth has traveled 940 million km (584 million mi). Earth's orbit has an eccentricity of 0.0167.\n\nAs seen from Earth, the planet's orbital prograde motion makes the Sun appear to move with respect to other stars at a rate of about 1° (or a Sun or Moon diameter every 12 hours) eastward per solar day. Earth's orbital speed averages about 30 km/s (108,000 km/h; 67,000 mph), which is fast enough to cover the planet's diameter in 7 minutes and the distance to the Moon in 4 hours.\n\nFrom a vantage point above the north pole of either the Sun or Earth, Earth would appear to revolve in a counterclockwise direction around the Sun. From the same vantage point, both the Earth and the Sun would appear to rotate also in a counterclockwise direction about their respective axes.\n\nHeliocentrism is the scientific model that first placed the Sun at the center of the Solar System and put the planets, including Earth, in its orbit. Historically, heliocentrism is opposed to geocentrism, which placed the Earth at the center. Aristarchus of Samos already proposed a heliocentric model in the 3rd century BC. In the 16th century, Nicolaus Copernicus' \"De revolutionibus\" presented a full discussion of a heliocentric model of the universe in much the same way as Ptolemy had presented his geocentric model in the 2nd century. This \"Copernican revolution\" resolved the issue of planetary retrograde motion by arguing that such motion was only perceived and apparent. \"Although Copernicus's groundbreaking book...had been [printed] over a century earlier, [the Dutch mapmaker] Joan Blaeu was the first mapmaker to incorporate his revolutionary heliocentric theory into a map of the world.\"\n\nBecause of Earth's axial tilt (often known as the obliquity of the ecliptic), the inclination of the Sun's trajectory in the sky (as seen by an observer on Earth's surface) varies over the course of the year. For an observer at a northern latitude, when the north pole is tilted toward the Sun the day lasts longer and the Sun appears higher in the sky. This results in warmer average temperatures, as additional solar radiation reaches the surface. When the north pole is tilted away from the Sun, the reverse is true and the weather is generally cooler. North of the Arctic Circle and south of the Antarctic Circle, an extreme case is reached in which there is no daylight at all for part of the year, and continuous daylight during the opposite time of year. This is called polar night and midnight sun. This variation in the weather (because of the direction of the Earth's axial tilt) results in the seasons.\n\nBy astronomical convention, the four seasons are determined by the solstices (the two points in the Earth's orbit of the maximum tilt of the Earth's axis, toward the Sun or away from the Sun) and the equinoxes (the two points in the Earth's orbit where the Earth's tilted axis and an imaginary line drawn from the Earth to the Sun are exactly perpendicular to one another). The solstices and equinoxes divide the year up into four approximately equal parts. In the northern hemisphere winter solstice occurs on or about December 21; summer solstice is near June 21; spring equinox is around March 20; and autumnal equinox is about September 23. The effect of the Earth's axial tilt in the southern hemisphere is the opposite of that in the northern hemisphere, thus the seasons of the solstices and equinoxes in the southern hemisphere are the reverse of those in the northern hemisphere (e.g. the northern summer solstice is at the same time as the southern winter solstice).\n\nIn modern times, Earth's perihelion occurs around January 3, and the aphelion around July 4 (for other eras, see precession and Milankovitch cycles). The changing Earth–Sun distance results in an increase of about 6.9% in total solar energy reaching the Earth at perihelion relative to aphelion. Since the southern hemisphere is tilted toward the Sun at about the same time that the Earth reaches the closest approach to the Sun, the southern hemisphere receives slightly more energy from the Sun than does the northern over the course of a year. However, this effect is much less significant than the total energy change due to the axial tilt, and most of the excess energy is absorbed by the higher proportion of water in the southern hemisphere.\n\nThe Hill sphere (gravitational sphere of influence) of the Earth is about 1,500,000 kilometers (0.01 AU) in radius, or approximately 4 times the average distance to the moon. This is the maximal distance at which the Earth's gravitational influence is stronger than the more distant Sun and planets. Objects orbiting the Earth must be within this radius, otherwise they can become unbound by the gravitational perturbation of the Sun.\n\nThe following diagram shows the relation between the line of solstice and the line of apsides of Earth's elliptical orbit. The orbital ellipse goes through each of the six Earth images, which are sequentially the perihelion (periapsis — nearest point to the Sun) on anywhere from January 2 to January 5, the point of March equinox on March 19, 20, or 21, the point of June solstice on June 20, 21, or 22, the aphelion (apoapsis — farthest point from the Sun) on anywhere from July 3 to July 5, the September equinox on September 22, 23, or 24, and the December solstice on December 21, 22, or 23. The diagram shows an exaggerated shape of Earth's orbit; the actual orbit is less eccentric than pictured.\nBecause of the axial tilt of the Earth in its orbit, the maximal intensity of Sun rays hits the Earth 23.4 degrees north of equator at the June Solstice (at the Tropic of Cancer), and 23.4 degrees south of equator at the December Solstice (at the Tropic of Capricorn).\n\nMathematicians and astronomers (such as Laplace, Lagrange, Gauss, Poincaré, Kolmogorov, Vladimir Arnold, and Jürgen Moser) have searched for evidence for the stability of the planetary motions, and this quest led to many mathematical developments and several successive \"proofs\" of stability for the Solar System. By most predictions, Earth's orbit will be relatively stable over long periods.\n\nIn 1989, Jacques Laskar's work indicated that the Earth's orbit (as well as the orbits of all the inner planets) can become chaotic and that an error as small as 15 meters in measuring the initial position of the Earth today would make it impossible to predict where the Earth would be in its orbit in just over 100 million years' time. Modeling the Solar System is a subject covered by the n-body problem.\n\n"}
{"id": "212485", "url": "https://en.wikipedia.org/wiki?curid=212485", "title": "Earth religion", "text": "Earth religion\n\nEarth religion is a term used mostly in the context of neopaganism.\n\nEarth-centered religion or nature worship is a system of religion based on the veneration of natural phenomena. It covers any religion that worships the earth, nature, or fertility deity, such as the various forms of goddess worship or matriarchal religion. Some find a connection between earth-worship and the Gaia hypothesis. Earth religions are also formulated to allow one to utilize the knowledge of preserving the earth.\n\nAccording to Marija Gimbutas, pre-Indo-European societies lived in small-scale, family-based communities that practiced matrilineal succession and goddess-centered religion where creation comes from the woman. She is the Divine Mother who can give life and take it away. In Irish mythology she is Danu, in Slavic mythology she is Mat Zemlya, and in other cultures she is Pachamama, Ninsun, Terra Mater, Nüwa, Matres or Shakti.\n\nIn the late 1800s, James Weir wrote an article describing the beginnings and aspects of early religious feeling. According to Boyer, early man was forced to locate food and shelter in order to survive, while constantly being directed by his instincts and senses. Because man's existence depended on nature, men began to form their religion and beliefs on and around nature itself. It is evident that man's first religion would have had to develop from the material world, he argues, because man relied heavily on his senses and what he could see, touch, and feel. In this sense, the worship of nature formed, allowing man to further depend on nature for survival.\n\nNeopagans have tried to make claims that religion started in ways that correspond to earth religion. In one of their published works, \"The Urantia Book\", another reason for this worship of nature came from a fear of the world around primitive man. His mind lacked the complex function of processing and sifting through complex ideas. As a result, man worshiped the very entity that surrounded him every day. That entity was nature. Man experienced the different natural phenomena around him, such as storms, vast deserts, and immense mountains. Among the very first parts of nature to be worshiped were rocks and hills, plants and trees, animals, the elements, heavenly bodies, and even man himself. As primitive man worked his way through nature worship, he eventually moved on to incorporate spirits into his worship. Although these claims may have some merit, they are nonetheless presented from a biased position that cannot be authenticated by traditional and reliable sources. Therefore, their claims can not be relied upon.\n\nThe origins of religion can be looked at through the lens of the function and processing of the human mind. Pascal Boyer suggests that, for the longest period of time, the brain was thought of as a simple organ of the body. However, he claims that the more information collected about the brain indicates that the brain is indeed not a \"blank slate.\" Humans do not just learn any information from the environment and surroundings around them. They have acquired sophisticated cognitive equipment that prepares them to analyze information in their culture and determine which information is relevant and how to apply it. Boyer states that \"having a normal human brain does not imply that you have religion. All it implies is that people can acquire it, which is very different.\" He suggests that religions started for the reasons of providing answers to humans, giving comfort, providing social order to society, and satisfying the need of the illusion-prone nature of the human mind. Ultimately, religion came into existence because of our need to answer questions and hold together our societal order.\n\nAn additional idea on the origins of religion comes not from man's cognitive development, but from the ape. Barbara J. King argues that human beings have an emotional connection with those around them, and that that desire for a connection came from their evolution from apes. The closest relative to the human species is the African ape. At birth, the ape begins negotiating with its mother about what it wants and needs in order to survive. The world the ape is born into is saturated with close family and friends. Because of this, emotions and relationships play a huge role in the ape's life. Its reactions and responses to one another are rooted and grounded in a sense of belongingness, which is derived from its dependence on the ape's mother and family. Belongingness is defined as \"mattering to someone who matters to you ... getting positive feelings from our relationships.\" This sense and desire for belongingness, which started in apes, only grew as the hominid (a human ancestor) diverged from the lineage of the ape, which occurred roughly six to seven million years ago.\n\nAs severe changes in the environment, physical evolutions in the human body (especially in the development of the human brain), and changes in social actions occurred, humans went beyond trying to simply form bonds and relationships of empathy with others. As their culture and society became more complex, they began using practices and various symbols to make sense of the natural and spiritual world around them. Instead of simply trying to find belongingness and empathy from the relationships with others, humans created and evolved God and spirits in order to fulfil that need and exploration. King argued that \"an earthly need for belonging led to human religious imagination and thus to the otherworldly realm of relating to God, gods, and spirits.\"\n\nThe term \"earth religion\" encompasses any religion that worships the earth, nature or fertility gods or goddesses. There is an array of groups and beliefs that fall under earth religion, such as paganism, which is a polytheistic, nature based religion; animism, which is the worldview that all living entities (plants, animals, and humans) possess a spirit; Wicca, who hold the concept of an earth mother goddess as well as practice ritual magic; and druidism, which equates divinity with the natural world.\n\nAnother perspective of earth religion to consider is pantheism, which takes a varied approach to the importance and purpose of the earth, and man's relationship with the planet. Several of their core statements deal with the connectivity humans share with the planet, declaring that \"all matter, energy, and life are an interconnected unity of which we are an inseparable part\" and \"we are an integral part of Nature, which we should cherish, revere and preserve in all its magnificent beauty and diversity. We should strive to live in harmony with Nature locally and globally\".\n\nThe earth also plays a vital role to many Voltaic peoples, many of whom \"consider the Earth to be Heaven’s wife\", such as the Konkomba of northern Ghana, whose economic, social and religious life is heavily influenced by the earth. It is also important to consider various Native American religions, such as Peyote Religion, Longhouse Religion, and Earth Lodge Religion.\n\nApril 22 was established as International Mother Earth Day by the United Nations in 2009, but many cultures around the world have been celebrating the Earth for thousands of years. Winter solstice and Summer solstice are celebrated with holidays like Yule and Dongzhi in the winter and Tiregān and Kupala in the summer.\n\nAnimism is practiced among the Bantu peoples of Sub-Saharan Africa. The Dahomey mythology has deities like Nana Buluku, Gleti, Mawu, Asase Yaa, Naa Nyonmo and Xevioso.\n\nIn Baltic mythology, the sun is a female deity, Saule, a mother or a bride, and Mēness is the moon, father or husband, their children being the stars. In Slavic mythology Mokosh and Mat Zemlya together with Perun head up the pantheon. Celebrations and rituals are centered on nature and harvest seasons. Dragobete is a traditional Romanian spring holiday that celebrates \"the day when the birds are betrothed.\"\n\nIn Hindu philosophy, the yoni is the creative power of nature and the origin of life. In Shaktism, the yoni is celebrated and worshipped during the Ambubachi Mela, an annual fertility festival which celebrates the Earth's menstruation.\n\nAlthough the idea of earth religion has been around for thousands of years, it did not fully show up in popular culture until the early 1990s. \"The X-Files\" was one of the first nationally broadcast television programs to air witchcraft and Wicca (types of earth religion) content. On average, Wiccans - those who practice Wicca - were more or less pleased with the way the show had portrayed their ideals and beliefs. However, they still found it to be a little \"sensationalistic\". That same year, the movie \"The Craft\" was released - also depicting the art of Wicca. Unfortunately, this cinematic feature was not as happily accepted as \"The X-Files\" had been.\n\nA few years later, programs showcasing the aforementioned religious practices - such as \"Charmed\" and \"Buffy the Vampire Slayer\" - became widely popular. Although \"Charmed\" focused mostly on witchcraft, the magic they practiced very closely resembled Wicca. Meanwhile, \"Buffy\" was one of the first shows to actually cast a Wiccan character. However, since the shows focus was primarily on vampires, the Wiccan was depicted as having supernatural powers, rather than being in-tuned with the Earth.\n\nOther movies and shows throughout the last few decades have also been placed under the genre of Earth Religion. Among them are two of director Hayao Miyazaki's most well known films - \"Princess Mononoke\" and \"My Neighbor Totoro\". Both movies present human interaction with land, animal, and other nature spirits. Speakers for Earth Religion have said that these interactions suggest overtones of Earth Religion themes.\n\nSome popular Disney movies have also been viewed as Earth Religion films. Among them are \"The Lion King\" and \"Brother Bear\". Those who practice Earth Religion view \"The Lion King\" as an Earth Religion film mainly for the \"interconnectedness\" and \"Circle of Life\" it shows between the animals, plants, and life in general. When that link is broken, viewers see chaos and despair spread throughout the once bountiful land. Congruently, \"Brother Bear\" portrays interactions and consequences when humans disobey or go against the animal and Earth spirits.\n\nOther earth religion movies include \"The 13th Warrior\", \"The Deceivers (film)\", \"Sorceress (1982 film)\", \"Anchoress (film)\", \"Eye of the Devil\", \"Agora (film)\", and \"The Wicker Man (1973 film)\". These movies all contain various aspects of earth religion and nature worship in general.\n\nMany religions have negative stereotypes of earth religion and neo-paganism in general. A common critique of the worship of nature and resources of \"Mother Earth\" is that the rights of nature and ecocide movements are inhibitors of human progress and development. This argument is fueled by the fact that those people socialized into 'western' world views believe the earth itself is not a living being. Wesley Smith believes this is “anti-humanism with the potential to do real harm to the human family.” According to Smith, earth worshipers are hindering large-scale development, and they are viewed as inhibitors of advancement.\n\nA lot of criticism of earth religion comes from the negative actions of a few people who have been chastised for their actions. One such negative representative of earth religion is Aleister Crowley. He is believed to be \"too preoccupied with awakening magical powers\" instead of putting the well-being of others in his coven. Crowley allegedly looked up to \"Old George\" Pickingill, who was another worshipper of nature who was viewed negatively. Critics regarded Pickingill as a Satanist and \"England’s most notorious Witch\".\n\nCrowley himself was \"allegedly expelled from the Craft because he was a pervert.\" He became aroused by torture and pain, and enjoyed being \"punished\" by women. This dramatically damaged Crowley’s public image, because of his lifestyle and actions. Many people regarded all followers of earth religion as perverted Satanists.\n\nFollowers of earth religion have suffered major opprobrium over the years for allegedly being Satanists. Some religious adherents can be prone to viewing religions other than their religion as being wrong sometimes because they perceive those religions as characteristic of their concept of Satan worship. To wit, Witchcraft, a common practice of Wiccans, is sometimes misinterpreted as Satan worship by members of these groups, as well as less-informed persons who may not be specifically religious but who may reside within the sphere-of-influence of pagan-critical religious adherents. From the Wiccan perspective, however, earth religion and Wicca lie outside of the phenomenological world that encompasses Satanism. An all-evil being does not exist within the religious perspective of western earth religions. Devotees worship and celebrate earth resources and earth-centric deities. Satanism and Wicca \"have entirely different beliefs about deity, different rules for ethical behavior, different expectations from their membership, different views of the universe, different seasonal days of celebration, etc.\"\n\nNeo-pagans, or earth religion followers, often claim to be unaffiliated with Satanism. Neo-pagans, Wiccans, and earth religion believers do not acknowledge the existence of a deity that conforms to the common Semitic sect religious concept of Satan. Satanism stems from Christianity, while earth religion stems from older religious concepts.\n\nSome earth religion adherents take issue with the religious harassment that is inherent in the social pressure that necessitates their having to distance themselves from the often non-uniform, Semitic sect religious concept of Satan worship. Having to define themselves as \"other\" from a religious concept that is not within their worldview implies a certain degree of outsider-facilitated, informal, but functional religious restriction that is based solely on the metaphysical and mythological religious beliefs of those outsiders. This is problematic because outsider initiated comparisons to Satanism with the intent of condemnation, even when easily refuted, can have the effect of social pressure on earth religion adherents to conform to outsider perception of acceptable customs, beliefs, and modes of religious behavior.\n\nTo illustrate, a problem could arise with the \"other\" than Satanism argument if an earth centered belief system adopted a holiday that a critic considered to be similar or identical to a holiday that Satanists celebrate. Satanists have historically been prone to adopting holidays that have origins in various pagan traditions, ostensibly because these traditional holidays are amongst the last known vestiges of traditional pre-Semitic religious practice in the west. Satanists are, perhaps irrationally, prone to interpreting non-Semitic holidays as anti-Christian and therefore as implicitly representative of their worldview. This is not surprising given the fact that this is, in fact, how many Christians interpret holidays such as Samhain. In spite of any flawed perceptions or rationale held by any other group, earth centered religion adherents do not recognize misinterpretation of their customs made by outside religious adherents or critics inclusive of Satan worshippers.\n\nOrganized Satan worship, as defined by and anchored in the Semitic worldview, is characterized by a relatively disorganized and often disparate series of movements and groups that mostly emerged in the mid-20th century. Thus, their adopted customs have varied, continue to vary, and therefore this moving target of beliefs and customs can not be justifiably nor continuously accounted for by earth centered religious adherents. Once a Satanist group adopts a holiday, social stigma may unjustifiably taint the holiday and anyone who observes it without discrimination as to whence and for what purpose it was originally celebrated. Given these facts, many earth centered religion devotees find comparisons to Satanism intrinsically oppressive in nature. This logic transfers to any and all religious customs to include prayer, magic, ceremony, and any unintentional similarity in deity characteristics (an example is the horned traditional entity Pan having similar physical characteristics to common horned depictions of Satan).\n\nThe issue is further complicated by the theory that the intra and extra-biblical mythology of Satan that is present throughout various Semitic sects may have originally evolved to figuratively demonize the heathen religions of other groups. Thus, the concept of Satan, or \"the adversary\", would have been representative of all non-Semitic religions and, by extension, the people who believed in them. Although, at times, the concept of the \"other\" as demonic has also been used to characterize competing Semitic sects. Amongst other purposes, such belief would have been extraordinarily useful during the psychological and physical process of cleansing Europe of traditional tribal beliefs in favor of Christianity. This possibility would account for the historical tendency of Christian authorities, for example, to deem most pagan customs carried out in the pagan religious context as demonic. By any modern standard, such current beliefs would violate western concepts of religious tolerance as well as be inimical to the preservation of what remains of the culture of long-persecuted religious groups.\n\nBecause of the vast diversity of religions that fall under the title of \"earth religion\" there is no consensus of beliefs. However, the ethical beliefs of most religions overlap. The most well-known ethical code is the Wiccan Rede. Many of those who practice an earth religion choose to be environmentally active. Some perform activities such as recycling or composting while others feel it to be more productive to try and support the earth spiritually. These six beliefs about ethics seem to be universal.\n\n\"An [if] it harm none, do what ye will.\" Commonly worded in modern English as \"if it doesn't harm anyone, do what you want.\" This maxim was first printed in 1964, after being spoken by the priestess Doreen Valiente in the mid-20th century, and governs most ethical belief of Wiccans and some Pagans. There is no consensus of beliefs but this rede provides a starting point for most people's interpretation of what is ethical. The rede clearly states to do no harm but what constitutes as harm and what level of self-interest is acceptable is negotiable. Many Wiccans reverse the phrase into \"Do what ye will an it harm none,\" meaning \"Do what you want if it doesn't harm anyone.\" The difference may not seem significant but it is. The first implies that it is good to do no harm but does not say that it is necessarily unethical to do so, the second implies that all forms of harm are unethical. The second phrase is nearly impossible to follow. This shift occurred when trying to better adapt the phrase into modern English as well as to stress the \"harmlessness\" of Wiccans. The true nature of the rede simply implies that there is personal responsibility for your actions. You may do as you wish but there is a karma reaction from every action. Even though this is the most well-known rede of practice, it does not mean that those that choose not to follow it are unethical. There are many other laws of practice that other groups follow.\n\nThe Threefold Law is the belief that for all actions there is always a cause and effect. For every action taken either the good or ill intention will be returned to the action taker threefold. This is why the Wiccan Rede is typically followed because of fear of the threefold return from that harmful action.\n\nThis term is what Emma Restall Orr calls reverence for the earth in her book \"Living with Honour: A Pagan Ethics\". She separates the term into three sections: courage, generosity and loyalty, or honesty, respect and responsibility. There is no evil force in Nature. Nothing exists beyond the natural, therefore it is up to the individual to choose to be ethical not because of divine judgment. All beings are connected by the earth and so all should be treated fairly. There is a responsibility toward the environment and a harmony should be found with nature.\n\nThe following was written by the Church of All Worlds in 1988 and was affirmed by the Pagan Ecumenical Conferences of Ancient Ways (California, May 27–30) and Pagan Spirit Gathering (Wisconsin, June 17). The Pagan Community Council of Ohio then presented it to the Northeast Council of W.I.C.C.A.\n\n\"We, the undersigned, as adherents of Pagan and Old and Neo-Pagan Earth Religions, including Wicca or Witchcraft, practice a variety of positive, life affirming faiths that are dedicated to healing, both of ourselves and of the Earth. As such, we do not advocate or condone any acts that victimize others, including those proscribed by law. As one of our most widely accepted precepts is the Wiccan Rede's injunction to \"harm none,\" we absolutely condemn the practices of child abuse, sexual abuse and any other form of abuse that does harm to the bodies, minds or spirits of the victims of such abuses. We recognize and revere the divinity of Nature in our Mother the Earth, and we conduct our rites of worship in a manner that is ethical, compassionate and constitutionally protected. We neither acknowledge or worship the Christian devil, \"Satan,\" who is not in our Pagan pantheons. We will not tolerate slander or libel against our Temples, clergy or Temple Assemblers and we are prepared to defend our civil rights with such legal action as we deem necessary and appropriate.\"\n"}
{"id": "9649", "url": "https://en.wikipedia.org/wiki?curid=9649", "title": "Energy", "text": "Energy\n\nIn physics, energy is the quantitative property that must be transferred to an object in order to perform work on, or to heat, the object. Energy is a conserved quantity; the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The SI unit of energy is the joule, which is the energy transferred to an object by the work of moving it a distance of 1 metre against a force of 1 newton.\n\nCommon forms of energy include the kinetic energy of a moving object, the potential energy stored by an object's position in a force field (gravitational, electric or magnetic), the elastic energy stored by stretching solid objects, the chemical energy released when a fuel burns, the radiant energy carried by light, and the thermal energy due to an object's temperature.\n\nMass and energy are closely related. Due to mass–energy equivalence, any object that has mass when stationary (called rest mass) also has an equivalent amount of energy whose form is called rest energy, and any additional energy (of any form) acquired by the object above that rest energy will increase the object's total mass just as it increases its total energy. For example, after heating an object, its increase in energy could be measured as a small increase in mass, with a sensitive enough scale.\n\nLiving organisms require available energy to stay alive, such as the energy humans get from food. Human civilization requires energy to function, which it gets from energy resources such as fossil fuels, nuclear fuel, or renewable energy. The processes of Earth's climate and ecosystem are driven by the radiant energy Earth receives from the sun and the geothermal energy contained within the earth.\n\nThe total energy of a system can be subdivided and classified into potential energy, kinetic energy, or combinations of the two in various ways. Kinetic energy is determined by the movement of an object – or the composite motion of the components of an object – and potential energy reflects the potential of an object to have motion, and generally is a function of the position of an object within a field or may stored in the field itself.\n\nWhile these two categories are sufficient to describe all forms of energy, it is often convenient to refer to particular combinations of potential and kinetic energy as its own form. For example, macroscopic mechanical energy is the sum of translational and rotational kinetic and potential energy in a system neglects the kinetic energy due to temperature, and nuclear energy which combines utilize potentials from the nuclear force and the weak force), among others.\n\nThe word \"energy\" derives from the , which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.\n\nIn the late 17th century, Gottfried Leibniz proposed the idea of the , or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total \"vis viva\" was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the random motion of the constituent parts of matter, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from \"vis viva\" only by a factor of two.\n\nIn 1807, Thomas Young was possibly the first to use the term \"energy\" instead of \"vis viva\", in its modern sense. Gustave-Gaspard Coriolis described \"kinetic energy\" in 1829 in its modern sense, and in 1853, William Rankine coined the term \"potential energy\". The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.\n\nThese developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, and Walther Nernst. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jožef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time. Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.\n\nIn 1843, James Prescott Joule independently discovered the mechanical equivalent in a series of experiments. The most famous of them used the \"Joule apparatus\": a descending weight, attached to a string, caused rotation of a paddle immersed in water, practically insulated from heat transfer. It showed that the gravitational potential energy lost by the weight in descending was equal to the internal energy gained by the water through friction with the paddle.\n\nIn the International System of Units (SI), the unit of energy is the joule, named after James Prescott Joule. It is a derived unit. It is equal to the energy expended (or work done) in applying a force of one newton through a distance of one metre. However energy is also expressed in many other units not part of the SI, such as ergs, calories, British Thermal Units, kilowatt-hours and kilocalories, which require a conversion factor when expressed in SI units.\n\nThe SI unit of energy rate (energy per unit time) is the watt, which is a joule per second. Thus, one joule is one watt-second, and 3600 joules equal one watt-hour. The CGS energy unit is the erg and the imperial and US customary unit is the foot pound. Other energy units such as the electronvolt, food calorie or thermodynamic kcal (based on the temperature change of water in a heating process), and BTU are used in specific areas of science and commerce.\n\nIn classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept.\n\nWork, a function of energy, is force times distance.\n\nThis says that the work (formula_2) is equal to the line integral of the force F along a path \"C\"; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball.\n\nThe total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems. These classical equations have remarkably direct analogs in nonrelativistic quantum mechanics.\n\nAnother energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy \"minus\" the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).\n\nNoether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian; for example, dissipative systems with continuous symmetries need not have a corresponding conservation law.\n\nIn the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants. A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The \"speed\" of a chemical reaction (at given temperature \"T\") is related to the activation energy \"E\", by the Boltzmann's population factor ethat is the probability of molecule to have energy greater than or equal to \"E\" at the given temperature \"T\". This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation.The activation energy necessary for a chemical reaction can be in the form of thermal energy.\n\nIn biology, energy is an attribute of all biological systems from the biosphere to the smallest living organism. Within an organism it is responsible for growth and development of a biological cell or an organelle of a biological organism. Energy is thus often said to be stored by cells in the structures of molecules of substances such as carbohydrates (including sugars), lipids, and proteins, which release energy when reacted with oxygen in respiration. In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, assuming an average human energy expenditure of 12,500 kJ per day and a basal metabolic rate of 80 watts. For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 ÷ 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum. The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a \"feel\" for the use of a given amount of energy.\n\nSunlight's radiant energy is also captured by plants as \"chemical potential energy\" in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into the high-energy compounds carbohydrates, lipids, and proteins. Plants also release oxygen during photosynthesis, which is utilized by living organisms as an electron acceptor, to release the energy of carbohydrates, lipids, and proteins. Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark, in a forest fire, or it may be made available more slowly for animal or human metabolism, when these molecules are ingested, and catabolism is triggered by enzyme action.\n\nAny living organism relies on an external source of energy – radiant energy from the Sun in the case of green plants, chemical energy in some form in the case of animals – to be able to grow and reproduce. The daily 1500–2000 Calories (6–8 MJ) recommended for a human adult are taken as a combination of oxygen and food molecules, the latter mostly carbohydrates and fats, of which glucose (CHO) and stearin (CHO) are convenient examples. The food molecules are oxidised to carbon dioxide and water in the mitochondria\nand some of the energy is used to convert ADP into ATP.\nThe rest of the chemical energy in O and the carbohydrate or fat is converted into heat: the ATP is used as a sort of \"energy currency\", and some of the chemical energy it contains is used for other metabolism when ATP reacts with OH groups and eventually splits into ADP and phosphate (at each stage of a metabolic pathway, some chemical energy is converted into heat). Only a tiny fraction of the original chemical energy is used for work:\n\nIt would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical or radiant energy), and it is true that most real machines manage higher efficiencies. In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe (\"the surroundings\"). Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology: to take just the first step in the food chain, of the estimated 124.7 Pg/a of carbon that is fixed by photosynthesis, 64.3 Pg/a (52%) are used for the metabolism of green plants, i.e. reconverted into carbon dioxide and heat.\n\nIn geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior, while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes and hurricanes are all a result of energy transformations brought about by solar energy on the atmosphere of the planet Earth.\n\nSunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity). Sunlight also drives many weather phenomena, save those generated by volcanic events. An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, give up some of their thermal energy suddenly to power a few days of violent air movement.\n\nIn a slower process, radioactive decay of atoms in the core of the Earth releases heat. This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may be later released to active kinetic energy in landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks. Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars created these atoms.\n\nIn cosmology and astronomy the phenomena of stars, nova, supernova, quasars and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen). The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.\n\nIn quantum mechanics, energy is defined in terms of the energy operator\nas a time derivative of the wave function. The Schrödinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schrödinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta. In the solution of the Schrödinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by Planck's relation: formula_3 (where formula_4 is Planck's constant and formula_5 the frequency). In the case of an electromagnetic wave these energy states are called quanta of light or photons.\n\nWhen calculating kinetic energy (work to accelerate a massive body from zero speed to some finite speed) relativistically – using Lorentz transformations instead of Newtonian mechanics – Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest energy: energy which every massive body must possess even when being at rest. The amount of energy is directly proportional to the mass of the body:\n\nwhere\n\nFor example, consider electron–positron annihilation, in which the rest energy of these two individual particles (equivalent to their rest mass) is converted to the radiant energy of the photons produced in the process. In this system the matter and antimatter (electrons and positrons) are destroyed and changed to non-matter (the photons). However, the total mass and total energy do not change during this interaction. The photons each have no rest mass but nonetheless have radiant energy which exhibits the same inertia as did the two original particles. This is a reversible process – the inverse process is called pair creation – in which the rest mass of particles is created from the radiant energy of two (or more) annihilating photons.\n\nIn general relativity, the stress–energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.\n\nEnergy and mass are manifestations of one and the same underlying physical property of a system. This property is responsible for the inertia and strength of gravitational interaction of the system (\"mass manifestations\"), and is also responsible for the potential ability of the system to perform work or heating (\"energy manifestations\"), subject to the limitations of other physical laws.\n\nIn classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy–momentum 4-vector). In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of space-time (= boosts).\n\nEnergy may be transformed between different forms at various efficiencies. Items that transform between these forms are called transducers. Examples of transducers include a battery, from chemical energy to electric energy; a dam: gravitational potential energy to kinetic energy of moving water (and the blades of a turbine) and ultimately to electric energy through an electric generator; or a heat engine, from heat to work.\n\nExamples of energy transformation include generating electric energy from heat energy via a steam turbine, or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground. Our Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that in itself (since it still contains the same total energy even if in different forms), but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.\n\nThere are strict limits to how efficiently heat can be converted into work in a cyclic process, e.g. in a heat engine, as described by Carnot's theorem and the second law of thermodynamics. However, some energy transformations can be quite efficient. The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a small scale, but certain larger transformations are not permitted because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.\n\nEnergy transformations in the universe over time are characterized by various kinds of potential energy that has been available since the Big Bang later being \"released\" (transformed to more active types of energy such as kinetic or radiant energy) when a triggering mechanism is available. Familiar examples of such processes include nuclear decay, in which energy is released that was originally \"stored\" in heavy isotopes (such as uranium and thorium), by nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae, to store energy in the creation of these heavy elements before they were incorporated into the solar system and the Earth. This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic energy and thermal energy in a very short time. Yet another example is that of a pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at maximum. At its lowest point the kinetic energy is at maximum and is equal to the decrease of potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever.\n\nEnergy is also transferred from potential energy (formula_8) to kinetic energy (formula_9) and then back to potential energy constantly. This is referred to as conservation of energy. In this closed system, energy cannot be created or destroyed; therefore, the initial energy and the final energy will be equal to each other. This can be demonstrated by the following:\n\nThe equation can then be simplified further since formula_10 (mass times acceleration due to gravity times the height) and formula_11 (half mass times velocity squared). Then the total amount of energy can be found by adding formula_12.\n\nEnergy gives rise to weight when it is trapped in a system with zero momentum, where it can be weighed. It is also equivalent to mass, and this mass is always associated with it. Mass is also equivalent to a certain amount of energy, and likewise always appears associated with it, as described in mass-energy equivalence. The formula \"E\" = \"mc\"², derived by Albert Einstein (1905) quantifies the relationship between rest-mass and rest-energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J.J. Thomson (1881), Henri Poincaré (1900), Friedrich Hasenöhrl (1904) and others (see Mass-energy equivalence#History for further information).\n\nPart of the rest energy (equivalent to rest mass) of matter may be converted to other forms of energy (still exhibiting mass), but neither energy nor mass can be destroyed; rather, both remain constant during any process. However, since formula_13 is extremely large relative to ordinary human scales, the conversion of an everyday amount of rest mass (for example, 1 kg) from rest energy to other forms of energy (such as kinetic energy, thermal energy, or the radiant energy carried by light and other radiation) can liberate tremendous amounts of energy (~formula_14 joules = 21 megatons of TNT), as can be seen in nuclear reactors and nuclear weapons. Conversely, the mass equivalent of an everyday amount energy is minuscule, which is why a loss of energy (loss of mass) from most systems is difficult to measure on a weighing scale, unless the energy loss is very large. Examples of large transformations between rest energy (of matter) and other forms of energy (e.g., kinetic energy into particles with rest mass) are found in nuclear physics and particle physics.\n\nThermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another, is reversible, as in the pendulum system described above. In processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as heat, and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomisation in a crystal).\n\nAs the universe evolves in time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or other kinds of increases in disorder). This has been referred to as the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), grows less and less.\n\nThe fact that energy can be neither created nor be destroyed is called the law of conservation of energy. In the form of the first law of thermodynamics, this states that a closed system's energy is constant unless energy is transferred in or out by work or heat, and that no energy is lost in transfer. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.\n\nWhile heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas, for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations. The total energy of a system can be calculated by adding up all forms of energy in the system.\n\nRichard Feynman said during a 1961 lecture:\nMost kinds of energy (with gravitational energy being a notable exception) are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.\n\nThis law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time, a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle - it is impossible to define the exact amount of energy during any definite time interval. The uncertainty principle should not be confused with energy conservation - rather it provides mathematical limits to which energy can in principle be defined and measured.\n\nEach of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appears as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.\n\nIn quantum mechanics energy is expressed using the Hamiltonian operator. On any time scales, the uncertainty in the energy is by\n\nwhich is similar in form to the Heisenberg Uncertainty Principle (but not really mathematically equivalent thereto, since \"H\" and \"t\" are not dynamically conjugate variables, neither in classical nor in quantum mechanics).\n\nIn particle physics, this inequality permits a qualitative understanding of virtual particles which carry momentum, exchange by which and with real particles, is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons (which are simply lowest quantum mechanical energy state of photons) are also responsible for electrostatic interaction between electric charges (which results in Coulomb law), for spontaneous radiative decay of exited atomic and nuclear states, for the Casimir force, for van der Waals bond forces and some other observable phenomena.\n\nEnergy transfer can be considered for the special case of systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work during the transfer is called heat. Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy, and the conductive transfer of thermal energy.\n\nEnergy is strictly conserved and is also locally conserved wherever it can be defined. In thermodynamics, for closed systems, the process of energy transfer is described by the first law:\n\nwhere formula_16 is the amount of energy transferred, formula_2  represents the work done on the system, and formula_18 represents the heat flow into the system. As a simplification, the heat term, formula_18, is sometimes ignored, especially when the thermal efficiency of the transfer is high.\n\nThis simplified equation is the one used to define the joule, for example.\n\nBeyond the constraints of closed systems, open systems can gain or lose energy in association with matter transfer (both of these process are illustrated by fueling an auto, a system which gains in energy thereby, without addition of either work or heat). Denoting this energy by formula_16, one may write\n\nInternal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.\n\nThe first law of thermodynamics asserts that energy (but not necessarily thermodynamic free energy) is always conserved and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas) without chemical changes, the differential change in the internal energy of the system (with a \"gain\" in energy signified by a positive quantity) is given as\n\nwhere the first term on the right is the heat transferred into the system, expressed in terms of temperature \"T\" and entropy \"S\" (in which entropy increases and the change d\"S\" is positive when the system is heated), and the last term on the right hand side is identified as work done on the system, where pressure is \"P\" and volume \"V\" (the negative sign results since compression of the system requires work to be done on it and so the volume change, d\"V\", is negative when work is done on the system).\n\nThis equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and pV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a \"closed\" system is expressed in a general form by\n\nwhere formula_23 is the heat supplied to the system and formula_24 is the work applied to the system.\n\nThe energy of a mechanical harmonic oscillator (a mass on a spring) is alternatively kinetic and potential. At two points in the oscillation cycle it is entirely kinetic, and at two points it is entirely potential. Over the whole cycle, or over many cycles, net energy is thus equally split between kinetic and potential. This is called equipartition principle; total energy of a system with many degrees of freedom is equally split among all available degrees of freedom.\n\nThis principle is vitally important to understanding the behaviour of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between \"new\" and \"old\" degrees. This mathematical result is called the second law of thermodynamics. The second law of thermodynamics is valid only for systems which are near or in equilibrium state. For non-equilibrium systems, the laws governing system’s behavior are still debatable. One of the guiding principles for these systems is the principle of maximum entropy production. It states that nonequilibrium systems behave in such a way to maximize its entropy production.\n\n\n"}
{"id": "43699304", "url": "https://en.wikipedia.org/wiki?curid=43699304", "title": "Energy rate density", "text": "Energy rate density\n\nEnergy rate density is the amount of free energy per unit time per unit mass (in CGS metric units erg/s/g; in MKS units joule/s/kg). It is terminologically (but not always numerically) equivalent to power density when measured in SI units of W/kg. Regardless of the units used, energy rate density describes the flow of energy through any system of given mass, and has been proposed as a measure of system complexity. The more complex the system, the more energy flows per second through each gram. \n\nEnergy rate density is actually a general term that is equivalent to more specialized terms used by many different disciplinary scientists. For example, in astronomy it is called the luminosity-to-mass ratio (the inverse of the mass-luminosity ratio), in physics the power density, in geology the specific radiant flux (where “specific” denotes per unit mass), in biology the specific metabolic rate, and in engineering the power-to-weight ratio. Interdisciplinary researchers prefer to use the general term, energy rate density, not only to stress the intuitive notion of energy flow (in contrast to more colloquial connotations of the word \"power\"), but also to unify its potential application among all the natural sciences, as in the cosmology of cosmic evolution. When the energy rate density for systems including our galaxy, sun, earth, plants, animals, society are plotted according to when, in historical time, they first emerged, a clear increase in energy rate density over time is observed. \n\nThis term has in recent years gained many diverse applications in various disciplines, including history, cosmology, economics, philosophy, and behavioral biology. \n\n\n"}
{"id": "190837", "url": "https://en.wikipedia.org/wiki?curid=190837", "title": "Evolutionary algorithm", "text": "Evolutionary algorithm\n\nIn artificial intelligence, an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.\n\nEvolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.\n\nStep One: Generate the initial population of individuals randomly. (First generation)\n\nStep Two: Evaluate the fitness of each individual in that population (time limit, sufficient fitness achieved, etc.)\n\nStep Three: Repeat the following regenerational steps until termination:\n\nSimilar techniques differ in genetic representation and other implementation details, and the nature of the particular applied problem.\n\nA possible limitation of many evolutionary algorithms is their lack of a clear genotype-phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype. This indirect encoding is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the evolvability of the organism. Such indirect (a.k.a. generative or developmental) encodings also enable evolution to exploit the regularity in the environment. Recent work in the field of artificial embryogeny, or artificial developmental systems, seeks to address these concerns. And gene expression programming successfully explores a genotype-phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes. \n\nSwarm algorithms include\n\n\nThe computer simulations \"Tierra\" and \"Avida\" attempt to model macroevolutionary dynamics.\n\n"}
{"id": "1960343", "url": "https://en.wikipedia.org/wiki?curid=1960343", "title": "Evolutionary epistemology", "text": "Evolutionary epistemology\n\nEvolutionary epistemology refers to three distinct topics: (1) the biological evolution of cognitive mechanisms in animals and humans, (2) a theory that knowledge itself evolves by natural selection, and (3) the study of the historical discovery of new abstract entities such as abstract number or abstract value that necessarily precede the individual acquisition and usage of such abstractions.\n\n\"Evolutionary epistemology\" can refer to a branch of epistemology that applies the concepts of biological evolution to the growth of animal and human cognition. It argues that the mind is in part genetically determined and that its structure and function reflect adaptation, a nonteleological process of interaction between the organism and its environment. A cognitive trait tending to increase inclusive fitness in a given population should therefore grow more common over time, and a trait tending to prevent its carriers from passing on their genes should show up less and less frequently.\n\n\"Evolutionary epistemology\" can also refer to a theory that applies the concepts of biological evolution to the growth of human knowledge, and argues that units of knowledge themselves, particularly scientific theories, evolve according to selection. In this case, a theory—like the germ theory of disease—becomes more or less credible according to changes in the body of knowledge surrounding it.\n\nOne of the hallmarks of evolutionary epistemology is the notion that empirical testing alone does not justify the pragmatic value of scientific theories, but rather that social and methodological processes select those theories with the closest \"fit\" to a given problem. The mere fact that a theory has survived the most rigorous empirical tests available does not, in the calculus of probability, predict its ability to survive future testing. Karl Popper used Newtonian physics as an example of a body of theories so thoroughly confirmed by testing as to be considered unassailable, but which were nevertheless overturned by Einstein's insights into the nature of space-time. For the evolutionary epistemologist, all theories are true only provisionally, regardless of the degree of empirical testing they have survived.\n\n\"Evolutionary epistemology\" can also refer to the opposite of (onto)genetic epistemology, namely phylogenetic epistemology as the historical discovery and reification of abstractions that necessarily precedes the learning of such abstractions by individuals. Piaget dismissed this possibility, stating\n\nPiaget was mistaken in so quickly dismissing the study of phylogenetic epistemology, as there is much historical data available about the origins and evolution of the various notational systems that reify different kinds of abstract entity.\n\nPopper is considered by many to have given evolutionary epistemology its first comprehensive treatment, though Donald T. Campbell coined the phrase in 1974 and Piaget alluded to it in 1974 and described the concept as one of five possible theories in \"The Origins of Intelligence in Children\" (1936).\n\n\n\n"}
{"id": "31180513", "url": "https://en.wikipedia.org/wiki?curid=31180513", "title": "Formative epistemology", "text": "Formative epistemology\n\nFormative epistemology is a collection of philosophic views concerned with the theory of knowledge that emphasize the role of natural scientific methods. According to formative epistemology, knowledge is gained through the imputation of thoughts from one human being to another in the societal setting. Humans are born without intrinsic knowledge and through their evolutionary and developmental processes gain knowledge from other human beings. Thus, according to formative epistemology, all knowledge is completely subjective and truth does not exist.\n\nThis shared emphasis on scientific methods of studying knowledge shifts focus to the empirical processes of knowledge acquisition and away from many traditional philosophic questions. There are noteworthy distinctions within formative epistemology. Replacement naturalism maintains that traditional epistemology should be abandoned and replaced with the methodologies of the natural sciences. The general thesis of cooperative naturalism is that traditional epistemology can benefit in its inquiry by using the knowledge we have gained from the cognitive sciences. Substantive naturalism focuses on an asserted equality of facts of knowledge and natural facts. \nObjections to formative epistemology have targeted features of the general project as well as characteristics of specific versions. Some objectors suggest that natural scientific knowledge cannot be circularly grounded by the knowledge obtained through cognitive science, which is itself a natural science. This objection from circularity has been aimed specifically at strict replacement naturalism. There are similar challenges to substance naturalism that maintain that the substance naturalists' thesis that all facts of knowledge are natural facts is not only circular but fails to accommodate certain facts. Several other objectors have found fault in the inability of formative methods to adequately address questions about what value forms of potential knowledge have or lack. Formative epistemology is generally opposed to the anti-psychologism of Immanuel Kant, Gottlob Frege, Karl Popper and others.\n\nW. V. O. Quine's version of formative epistemology considers reasons for serious doubt about the fruitfulness of traditional philosophic study of scientific knowledge. These concerns are raised in light of the long attested incapacity of philosophers to find a satisfactory answer to the problems of radical scepticism, more particularly, to David Hume's criticism of induction. But also, because of the contemporaneous attempts and failures to reduce mathematics to pure logic by those in or philosophically sympathetic to The Vienna Circle. He concludes that studies of scientific knowledge concerned with meaning or truth fail to achieve the Cartesian goal of certainty. The failures in the reduction of mathematics to pure logic imply that scientific knowledge can at best be defined with the aid of less certain set-theoretic notions. Even if set theory's lacking the certainty of pure logic is deemed acceptable, the usefulness of constructing an encoding of scientific knowledge as logic and set theory is undermined by the inability to construct a useful translation from logic and set-theory back to scientific knowledge. If no translation between scientific knowledge and the logical structures can be constructed that works both ways, then the properties of the purely logical and set-theoretic constructions do not usefully inform understanding of scientific knowledge.\n\nOn Quine's account, attempts to pursue the traditional project of finding the meanings and truths of science philosophically have failed on their own terms and failed to offer any advantage over the more direct methods of psychology. Since traditional philosophic analysis of knowledge fails, those wishing to study knowledge ought to employ natural scientific methods. Scientific study of knowledge differs from philosophic study by focusing on how humans acquire knowledge rather than speculative analysis of knowledge. According to Quine, this appeal to science to ground the project of studying knowledge, which itself underlies science, should not be dismissed for its circularity since it is the best option available after ruling out traditional philosophic methods for their more serious flaws. This identification and tolerance of circularity is reflected elsewhere in Quine's works.\n\nCooperative formativism is a version of formative epistemology which states that while there are evaluative questions to pursue, the empirical results from psychology concerning how individuals actually think and reason are essential and useful for making progress in these evaluative questions. This form of naturalism says that our psychological and biological limitations and abilities are relevant to the study of human knowledge. Empirical work is relevant to epistemology but only if epistemology is itself as broad as the study of human knowledge.\n\nSubstantive naturalism is a form of formative epistemology that emphasizes how all epistemic facts are natural facts. Natural facts can be based on two main ideas. The first is that all natural facts include all facts that science would verify. The second is to provide a list of examples that consists of natural items. This will help in deducing what else can be included.\n\nQuine articulates the problem of circularity inherent in formative epistemology when it is treated as a replacement for traditional epistemology. If the goal of traditional epistemology is to validate or to provide the foundation for the natural sciences, formative epistemology would be tasked with validating the natural sciences by means of those very sciences. That is, an empirical investigation into the criteria which are used to scientifically evaluate evidence must presuppose those very same criteria. However, Quine points out that these thoughts of validation are merely a byproduct of traditional epistemology. Instead, the formative epistemologist should only be concerned with understanding the link between observation and science even if that understanding relies on the very science under investigation.\n\nIn order to understand the link between observation and science, Quine's formative epistemology must be able to identify and describe the process by which scientific knowledge is acquired. One form of this investigation is reliabilism which requires that a belief be the product of some reliable method if it is to be considered knowledge. Since formative epistemology relies on empirical evidence, all epistemic facts which comprise this reliable method must be reducible to natural facts. That is, all facts related to the process of understanding must be expressible in terms of natural facts. If this is not true, i.e. there are facts which cannot be expressed as natural facts, science would have no means of investigating them. In this vein, Roderick Chisholm argues that there are epistemic principles (or facts) which are necessary to knowledge acquisition, but may not be, themselves, natural facts. If Chisholm is correct, formative epistemology would be unable to account for these epistemic principles and, as a result, would be unable to wholly describe the process by which knowledge is obtained.\n\nBeyond Quine's own concerns and potential discrepancies between epistemic and natural facts, Hilary Putnam argues that the replacement of traditional epistemology with formative epistemology necessitates the elimination of the normative. But without the normative, there is no \"justification, rational acceptability [nor] warranted assertibility\". Ultimately, there is no \"true\" since any method for arriving at the truth was abandoned with the normative. All notions which would explain truth are only intelligible when the normative is presupposed. Moreover, for there to be \"thinkers\", there \"must be some kind of truth\"; otherwise, \"our thoughts aren't really about anything[...] there is no sense in which any thought is right or wrong\". Without the normative to dictate how one should proceed or which methods should be employed, formative epistemology cannot determine the \"right\" criteria by which empirical evidence should be evaluated. But these are precisely the issues which traditional epistemology has been tasked. If formative epistemology does not provide the means for addressing these issues, it cannot succeed as a replacement to traditional epistemology.\n\nJaegwon Kim, another critic of formative epistemology, further articulates the difficulty of removing the normative component. He notes that modern epistemology has been dominated by the concepts of justification and reliability. Kim explains that epistemology and knowledge are nearly eliminated in their common sense meanings without normative concepts such as these. These concepts are meant to engender the question \"What conditions must a belief meet if we are justified in accepting it as true?\". That is to say, what are the necessary criteria by which a particular belief can be declared as \"true\" (or, should it fail to meet these criteria, can we rightly infer its falsity)? This notion of truth rests solely on the conception and application of the criteria which are set forth in traditional and modern theories of epistemology.\n\nKim adds to this claim by explaining how the idea of \"justification\" is the only notion (among \"belief\" and \"truth\") which is the defining characteristic of an epistemological study. To remove this aspect is to alter the very meaning and goal of epistemology, whereby we are no longer discussing the study and acquisition of knowledge. Justification is what makes knowledge valuable and normative; without it what can rightly be said to be true or false? We are left with only descriptions of the processes by which we arrive at a belief. Kim realizes that Quine is moving epistemology into the realm of psychology, where Quine’s main interest is based on the sensory input-output relationship of an individual. This account can never establish an affirmable statement which can lead us to truth, since all statements without the normative are purely descriptive (which can never amount to knowledge). The vulgar allowance of any statement without discrimination as scientifically valid, though not true, makes Quine’s theory difficult to accept under any epistemic theory which requires truth as the object of knowledge.\n\nAs a result of these objections and others like them, most, including Quine in his later writings, have agreed that formative epistemology as a replacement may be too strong of a view. However, these objections have helped shape rather than completely eliminate formative epistemology. One product of these objections is cooperative naturalism which holds that empirical results are essential and useful to epistemology. That is, while traditional epistemology cannot be eliminated, neither can it succeed in its investigation of knowledge without empirical results from the natural sciences. In any case, Quinean Replacement Naturalism finds relatively few supporters.\n\n"}
{"id": "11603215", "url": "https://en.wikipedia.org/wiki?curid=11603215", "title": "Geological history of Earth", "text": "Geological history of Earth\n\nThe geological history of Earth follows the major events in Earth's past based on the geological time scale, a system of chronological measurement based on the study of the planet's rock layers (stratigraphy). Earth formed about 4.54 billion years ago by accretion from the solar nebula, a disk-shaped mass of dust and gas left over from the formation of the Sun, which also created the rest of the Solar System.\n\nEarth was initially molten due to extreme volcanism and frequent collisions with other bodies. Eventually, the outer layer of the planet cooled to form a solid crust when water began accumulating in the atmosphere. The Moon formed soon afterwards, possibly as a result of the impact of a planetoid with the Earth. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, augmented by ice delivered from comets, produced the oceans.\n\nAs the surface continually reshaped itself over hundreds of millions of years, continents formed and broke apart. They migrated across the surface, occasionally combining to form a supercontinent. Roughly , the earliest-known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia, , then finally Pangaea, which broke apart .\n\nThe present pattern of ice ages began about , then intensified at the end of the Pliocene. The polar regions have since undergone repeated cycles of glaciation and thaw, repeating every 40,000–100,000 years. The last glacial period of the current ice age ended about 10,000 years ago.\n\nThe Precambrian includes approximately 90% of geologic time. It extends from 4.6 billion years ago to the beginning of the Cambrian Period (about 541 Ma). It includes three eons, the Hadean, Archean, and Proterozoic.\n\nMajor volcanic events altering the Earth's environment and causing extinctions may have occurred 10 times in the past 3 billion years.\n\nDuring Hadean time (4.6–4 Ga), the Solar System was forming, probably within a large cloud of gas and dust around the sun, called an accretion disc from which Earth formed .\nThe Hadean Eon is not formally recognized, but it essentially marks the era before we have adequate record of significant solid rocks. The oldest dated zircons date from about .\n\nEarth was initially molten due to extreme volcanism and frequent collisions with other bodies. Eventually, the outer layer of the planet cooled to form a solid crust when water began accumulating in the atmosphere. The Moon formed soon afterwards, possibly as a result of the impact of a large planetoid with the Earth. Some of this object's mass merged with the Earth, significantly altering its internal composition, and a portion was ejected into space. Some of the material survived to form an orbiting moon. More recent potassium isotopic studies suggest that the Moon was formed by a smaller, high-energy, high-angular-momentum giant impact cleaving off a significant portion of the Earth. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, augmented by ice delivered from comets, produced the oceans.\n\nDuring the Hadean the Late Heavy Bombardment occurred (approximately ) during which a large number of impact craters are believed to have formed on the Moon, and by inference on Earth, Mercury, Venus and Mars as well.\n\nThe Earth of the early Archean () may have had a different tectonic style. During this time, the Earth's crust cooled enough that rocks and continental plates began to form. Some scientists think because the Earth was hotter, that plate tectonic activity was more vigorous than it is today, resulting in a much greater rate of recycling of crustal material. This may have prevented cratonisation and continent formation until the mantle cooled and convection slowed down. Others argue that the subcontinental lithospheric mantle is too buoyant to subduct and that the lack of Archean rocks is a function of erosion and subsequent tectonic events.\n\nIn contrast to the Proterozoic, Archean rocks are often heavily metamorphized deep-water sediments, such as graywackes, mudstones, volcanic sediments and banded iron formations. Greenstone belts are typical Archean formations, consisting of alternating high- and low-grade metamorphic rocks. The high-grade rocks were derived from volcanic island arcs, while the low-grade metamorphic rocks represent deep-sea sediments eroded from the neighboring island rocks and deposited in a forearc basin. In short, greenstone belts represent sutured protocontinents.\n\nThe Earth's magnetic field was established 3.5 billion years ago. The solar wind flux was about 100 times the value of the modern Sun, so the presence of the magnetic field helped prevent the planet's atmosphere from being stripped away, which is what probably happened to the atmosphere of Mars. However, the field strength was lower than at present and the magnetosphere was about half the modern radius.\n\nThe geologic record of the Proterozoic () is more complete than that for the preceding Archean. In contrast to the deep-water deposits of the Archean, the Proterozoic features many strata that were laid down in extensive shallow epicontinental seas; furthermore, many of these rocks are less metamorphosed than Archean-age ones, and plenty are unaltered. Study of these rocks show that the eon featured massive, rapid continental accretion (unique to the Proterozoic), supercontinent cycles, and wholly modern orogenic activity. Roughly , the earliest-known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia, 600–540 Ma.\n\nThe first-known glaciations occurred during the Proterozoic, one began shortly after the beginning of the eon, while there were at least four during the Neoproterozoic, climaxing with the Snowball Earth of the Varangian glaciation.\n\nThe Phanerozoic Eon is the current eon in the geologic timescale. It covers roughly 541 million years. During this period continents drifted about, eventually collected into a single landmass known as Pangea and then split up into the current continental landmasses.\n\nThe Phanerozoic is divided into three eras – the Paleozoic, the Mesozoic and the Cenozoic.\n\nMost of biological evolution occurred during this time period.\n\nThe Paleozoic spanned from roughly (Ma) and is subdivided into six geologic periods; from oldest to youngest they are the Cambrian, Ordovician, Silurian, Devonian, Carboniferous and Permian. Geologically, the Paleozoic starts shortly after the breakup of a supercontinent called Pannotia and at the end of a global ice age. Throughout the early Paleozoic, the Earth's landmass was broken up into a substantial number of relatively small continents. Toward the end of the era the continents gathered together into a supercontinent called Pangaea, which included most of the Earth's land area.\n\nThe Cambrian is a major division of the geologic timescale that begins about 541.0 ± 1.0 Ma. Cambrian continents are thought to have resulted from the breakup of a Neoproterozoic supercontinent called Pannotia. The waters of the Cambrian period appear to have been widespread and shallow. Continental drift rates may have been anomalously high. Laurentia, Baltica and Siberia remained independent continents following the break-up of the supercontinent of Pannotia. Gondwana started to drift toward the South Pole. Panthalassa covered most of the southern hemisphere, and minor oceans included the Proto-Tethys Ocean, Iapetus Ocean and Khanty Ocean.\n\nThe Ordovician period started at a major extinction event called the Cambrian–Ordovician extinction event some time about 485.4 ± 1.9 Ma. During the Ordovician the southern continents were collected into a single continent called Gondwana. Gondwana started the period in the equatorial latitudes and, as the period progressed, drifted toward the South Pole. Early in the Ordovician the continents Laurentia, Siberia and Baltica were still independent continents (since the break-up of the supercontinent Pannotia earlier), but Baltica began to move toward Laurentia later in the period, causing the Iapetus Ocean to shrink between them. Also, Avalonia broke free from Gondwana and began to head north toward Laurentia. The Rheic Ocean was formed as a result of this. By the end of the period, Gondwana had neared or approached the pole and was largely glaciated.\n\nThe Ordovician came to a close in a series of extinction events that, taken together, comprise the second-largest of the five major extinction events in Earth's history in terms of percentage of genera that became extinct. The only larger one was the Permian-Triassic extinction event. The extinctions occurred approximately and mark the boundary between the Ordovician and the following Silurian Period.\n\nThe most-commonly accepted theory is that these events were triggered by the onset of an ice age, in the Hirnantian faunal stage that ended the long, stable greenhouse conditions typical of the Ordovician. The ice age was probably not as long-lasting as once thought; study of oxygen isotopes in fossil brachiopods shows that it was probably no longer than 0.5 to 1.5 million years. The event was preceded by a fall in atmospheric carbon dioxide (from 7000ppm to 4400ppm) which selectively affected the shallow seas where most organisms lived. As the southern supercontinent Gondwana drifted over the South Pole, ice caps formed on it. Evidence of these ice caps have been detected in Upper Ordovician rock strata of North Africa and then-adjacent northeastern South America, which were south-polar locations at the time.\n\nThe Silurian is a major division of the geologic timescale that started about 443.8 ± 1.5 Ma. During the Silurian, Gondwana continued a slow southward drift to high southern latitudes, but there is evidence that the Silurian ice caps were less extensive than those of the late Ordovician glaciation. The melting of ice caps and glaciers contributed to a rise in sea levels, recognizable from the fact that Silurian sediments overlie eroded Ordovician sediments, forming an unconformity. Other cratons and continent fragments drifted together near the equator, starting the formation of a second supercontinent known as Euramerica. The vast ocean of Panthalassa covered most of the northern hemisphere. Other minor oceans include Proto-Tethys, Paleo-Tethys, Rheic Ocean, a seaway of Iapetus Ocean (now in between Avalonia and Laurentia), and newly formed Ural Ocean.\n\nThe Devonian spanned roughly from 419 to 359 Ma. The period was a time of great tectonic activity, as Laurasia and Gondwana drew closer together. The continent Euramerica (or Laurussia) was created in the early Devonian by the collision of Laurentia and Baltica, which rotated into the natural dry zone along the Tropic of Capricorn. In these near-deserts, the Old Red Sandstone sedimentary beds formed, made red by the oxidized iron (hematite) characteristic of drought conditions. Near the equator Pangaea began to consolidate from the plates containing North America and Europe, further raising the northern Appalachian Mountains and forming the Caledonian Mountains in Great Britain and Scandinavia. The southern continents remained tied together in the supercontinent of Gondwana. The remainder of modern Eurasia lay in the Northern Hemisphere. Sea levels were high worldwide, and much of the land lay submerged under shallow seas. The deep, enormous Panthalassa (the \"universal ocean\") covered the rest of the planet. Other minor oceans were Paleo-Tethys, Proto-Tethys, Rheic Ocean and Ural Ocean (which was closed during the collision with Siberia and Baltica).\n\nThe Carboniferous extends from about 358.9 ± 0.4 to about 298.9 ± 0.15 Ma.\n\nA global drop in sea level at the end of the Devonian reversed early in the Carboniferous; this created the widespread epicontinental seas and carbonate deposition of the Mississippian. There was also a drop in south polar temperatures; southern Gondwana was glaciated throughout the period, though it is uncertain if the ice sheets were a holdover from the Devonian or not. These conditions apparently had little effect in the deep tropics, where lush coal swamps flourished within 30 degrees of the northernmost glaciers. A mid-Carboniferous drop in sea-level precipitated a major marine extinction, one that hit crinoids and ammonites especially hard. This sea-level drop and the associated unconformity in North America separate the Mississippian Period from the Pennsylvanian period.\n\nThe Carboniferous was a time of active mountain building, as the supercontinent Pangea came together. The southern continents remained tied together in the supercontinent Gondwana, which collided with North America-Europe (Laurussia) along the present line of eastern North America. This continental collision resulted in the Hercynian orogeny in Europe, and the Alleghenian orogeny in North America; it also extended the newly uplifted Appalachians southwestward as the Ouachita Mountains. In the same time frame, much of present eastern Eurasian plate welded itself to Europe along the line of the Ural mountains. There were two major oceans in the Carboniferous the Panthalassa and Paleo-Tethys. Other minor oceans were shrinking and eventually closed the Rheic Ocean (closed by the assembly of South and North America), the small, shallow Ural Ocean (which was closed by the collision of Baltica, and Siberia continents, creating the Ural Mountains) and Proto-Tethys Ocean.\n\nThe Permian extends from about 298.9 ± 0.15 to 252.17 ± 0.06 Ma.\n\nDuring the Permian all the Earth's major land masses, except portions of East Asia, were collected into a single supercontinent known as Pangaea. Pangaea straddled the equator and extended toward the poles, with a corresponding effect on ocean currents in the single great ocean (\"Panthalassa\", the \"universal sea\"), and the Paleo-Tethys Ocean, a large ocean that was between Asia and Gondwana. The Cimmeria continent rifted away from Gondwana and drifted north to Laurasia, causing the Paleo-Tethys to shrink. A new ocean was growing on its southern end, the Tethys Ocean, an ocean that would dominate much of the Mesozoic Era. Large continental landmasses create climates with extreme variations of heat and cold (\"continental climate\") and monsoon conditions with highly seasonal rainfall patterns. Deserts seem to have been widespread on Pangaea.\n\nThe Mesozoic extended roughly from .\n\nAfter the vigorous convergent plate mountain-building of the late Paleozoic, Mesozoic tectonic deformation was comparatively mild. Nevertheless, the era featured the dramatic rifting of the supercontinent Pangaea. Pangaea gradually split into a northern continent, Laurasia, and a southern continent, Gondwana. This created the passive continental margin that characterizes most of the Atlantic coastline (such as along the U.S. East Coast) today.\n\nThe Triassic Period extends from about 252.17 ± 0.06 to 201.3 ± 0.2 Ma. During the Triassic, almost all the Earth's land mass was concentrated into a single supercontinent centered more or less on the equator, called Pangaea (\"all the land\"). This took the form of a giant \"Pac-Man\" with an east-facing \"mouth\" constituting the Tethys sea, a vast gulf that opened farther westward in the mid-Triassic, at the expense of the shrinking Paleo-Tethys Ocean, an ocean that existed during the Paleozoic.\n\nThe remainder was the world-ocean known as Panthalassa (\"all the sea\"). All the deep-ocean sediments laid down during the Triassic have disappeared through subduction of oceanic plates; thus, very little is known of the Triassic open ocean. The supercontinent Pangaea was rifting during the Triassic—especially late in the period—but had not yet separated. The first nonmarine sediments in the rift that marks the initial break-up of Pangea—which separated New Jersey from Morocco—are of Late Triassic age; in the U.S., these thick sediments comprise the Newark Supergroup.\nBecause of the limited shoreline of one super-continental mass, Triassic marine deposits are globally relatively rare; despite their prominence in Western Europe, where the Triassic was first studied. In North America, for example, marine deposits are limited to a few exposures in the west. Thus Triassic stratigraphy is mostly based on organisms living in lagoons and hypersaline environments, such as \"Estheria\" crustaceans and terrestrial vertebrates.\n\nThe Jurassic Period extends from about 201.3 ± 0.2 to 145.0 Ma.\nDuring the early Jurassic, the supercontinent Pangaea broke up into the northern supercontinent Laurasia and the southern supercontinent Gondwana; the Gulf of Mexico opened in the new rift between North America and what is now Mexico's Yucatan Peninsula. The Jurassic North Atlantic Ocean was relatively narrow, while the South Atlantic did not open until the following Cretaceous Period, when Gondwana itself rifted apart.\nThe Tethys Sea closed, and the Neotethys basin appeared. Climates were warm, with no evidence of glaciation. As in the Triassic, there was apparently no land near either pole, and no extensive ice caps existed. The Jurassic geological record is good in western Europe, where extensive marine sequences indicate a time when much of the continent was submerged under shallow tropical seas; famous locales include the Jurassic Coast World Heritage Site and the renowned late Jurassic \"lagerstätten\" of Holzmaden and Solnhofen.\nIn contrast, the North American Jurassic record is the poorest of the Mesozoic, with few outcrops at the surface. Though the epicontinental Sundance Sea left marine deposits in parts of the northern plains of the United States and Canada during the late Jurassic, most exposed sediments from this period are continental, such as the alluvial deposits of the Morrison Formation. The first of several massive batholiths were emplaced in the northern Cordillera beginning in the mid-Jurassic, marking the Nevadan orogeny. Important Jurassic exposures are also found in Russia, India, South America, Japan, Australasia and the United Kingdom.\n\nThe Cretaceous Period extends from circa to .\n\nDuring the Cretaceous, the late Paleozoic-early Mesozoic supercontinent of Pangaea completed its breakup into present day continents, although their positions were substantially different at the time. As the Atlantic Ocean widened, the convergent-margin orogenies that had begun during the Jurassic continued in the North American Cordillera, as the Nevadan orogeny was followed by the Sevier and Laramide orogenies. Though Gondwana was still intact in the beginning of the Cretaceous, Gondwana itself broke up as South America, Antarctica and Australia rifted away from Africa (though India and Madagascar remained attached to each other); thus, the South Atlantic and Indian Oceans were newly formed. Such active rifting lifted great undersea mountain chains along the welts, raising eustatic sea levels worldwide.\n\nTo the north of Africa the Tethys Sea continued to narrow. Broad shallow seas advanced across central North America (the Western Interior Seaway) and Europe, then receded late in the period, leaving thick marine deposits sandwiched between coal beds. At the peak of the Cretaceous transgression, one-third of Earth's present land area was submerged. The Cretaceous is justly famous for its chalk; indeed, more chalk formed in the Cretaceous than in any other period in the Phanerozoic. Mid-ocean ridge activity—or rather, the circulation of seawater through the enlarged ridges—enriched the oceans in calcium; this made the oceans more saturated, as well as increased the bioavailability of the element for calcareous nanoplankton. These widespread carbonates and other sedimentary deposits make the Cretaceous rock record especially fine. Famous formations from North America include the rich marine fossils of Kansas's Smoky Hill Chalk Member and the terrestrial fauna of the late Cretaceous Hell Creek Formation. Other important Cretaceous exposures occur in Europe and China. In the area that is now India, massive lava beds called the Deccan Traps were laid down in the very late Cretaceous and early Paleocene.\n\nThe Cenozoic Era covers the  million years since the Cretaceous–Paleogene extinction event up to and including the present day. By the end of the Mesozoic era, the continents had rifted into nearly their present form. Laurasia became North America and Eurasia, while Gondwana split into South America, Africa, Australia, Antarctica and the Indian subcontinent, which collided with the Asian plate. This impact gave rise to the Himalayas. The Tethys Sea, which had separated the northern continents from Africa and India, began to close up, forming the Mediterranean sea.\n\nThe Paleogene (alternatively Palaeogene) Period is a unit of geologic time that began and ended 23.03 Ma and comprises the first part of the Cenozoic Era. This period consists of the Paleocene, Eocene and Oligocene Epochs.\n\nThe Paleocene, lasted from to .\n\nIn many ways, the Paleocene continued processes that had begun during the late Cretaceous Period. During the Paleocene, the continents continued to drift toward their present positions. Supercontinent Laurasia had not yet separated into three continents. Europe and Greenland were still connected. North America and Asia were still intermittently joined by a land bridge, while Greenland and North America were beginning to separate. The Laramide orogeny of the late Cretaceous continued to uplift the Rocky Mountains in the American west, which ended in the succeeding epoch. South and North America remained separated by equatorial seas (they joined during the Neogene); the components of the former southern supercontinent Gondwana continued to split apart, with Africa, South America, Antarctica and Australia pulling away from each other. Africa was heading north toward Europe, slowly closing the Tethys Ocean, and India began its migration to Asia that would lead to a tectonic collision and the formation of the Himalayas.\n\nDuring the Eocene ( - ), the continents continued to drift toward their present positions. At the beginning of the period, Australia and Antarctica remained connected, and warm equatorial currents mixed with colder Antarctic waters, distributing the heat around the world and keeping global temperatures high. But when Australia split from the southern continent around 45 Ma, the warm equatorial currents were deflected away from Antarctica, and an isolated cold water channel developed between the two continents. The Antarctic region cooled down, and the ocean surrounding Antarctica began to freeze, sending cold water and ice floes north, reinforcing the cooling. The present pattern of ice ages began about .\n\nThe northern supercontinent of Laurasia began to break up, as Europe, Greenland and North America drifted apart. In western North America, mountain building started in the Eocene, and huge lakes formed in the high flat basins among uplifts. In Europe, the Tethys Sea finally vanished, while the uplift of the Alps isolated its final remnant, the Mediterranean, and created another shallow sea with island archipelagos to the north. Though the North Atlantic was opening, a land connection appears to have remained between North America and Europe since the faunas of the two regions are very similar. India continued its journey away from Africa and began its collision with Asia, creating the Himalayan orogeny.\n\nThe Oligocene Epoch extends from about to . During the Oligocene the continents continued to drift toward their present positions.\n\nAntarctica continued to become more isolated and finally developed a permanent ice cap. Mountain building in western North America continued, and the Alps started to rise in Europe as the African plate continued to push north into the Eurasian plate, isolating the remnants of Tethys Sea. A brief marine incursion marks the early Oligocene in Europe. There appears to have been a land bridge in the early Oligocene between North America and Europe since the faunas of the two regions are very similar. During the Oligocene, South America was finally detached from Antarctica and drifted north toward North America. It also allowed the Antarctic Circumpolar Current to flow, rapidly cooling the continent.\n\nThe Neogene Period is a unit of geologic time starting 23.03 Ma. and ends at 2.588 Ma. The Neogene Period follows the Paleogene Period. The Neogene consists of the Miocene and Pliocene and is followed by the Quaternary Period.\n\nThe Miocene extends from about 23.03 to 5.333 Ma.\n\nDuring the Miocene continents continued to drift toward their present positions. Of the modern geologic features, only the land bridge between South America and North America was absent, the subduction zone along the Pacific Ocean margin of South America caused the rise of the Andes and the southward extension of the Meso-American peninsula. India continued to collide with Asia. The Tethys Seaway continued to shrink and then disappeared as Africa collided with Eurasia in the Turkish-Arabian region between 19 and 12 Ma (ICS 2004). Subsequent uplift of mountains in the western Mediterranean region and a global fall in sea levels combined to cause a temporary drying up of the Mediterranean Sea resulting in the Messinian salinity crisis near the end of the Miocene.\n\nThe Pliocene extends from to . During the Pliocene continents continued to drift toward their present positions, moving from positions possibly as far as from their present locations to positions only 70 km from their current locations.\n\nSouth America became linked to North America through the Isthmus of Panama during the Pliocene, bringing a nearly complete end to South America's distinctive marsupial faunas. The formation of the Isthmus had major consequences on global temperatures, since warm equatorial ocean currents were cut off and an Atlantic cooling cycle began, with cold Arctic and Antarctic waters dropping temperatures in the now-isolated Atlantic Ocean. Africa's collision with Europe formed the Mediterranean Sea, cutting off the remnants of the Tethys Ocean. Sea level changes exposed the land-bridge between Alaska and Asia. Near the end of the Pliocene, about (the start of the Quaternary Period), the current ice age began. The polar regions have since undergone repeated cycles of glaciation and thaw, repeating every 40,000–100,000 years.\n\nThe Pleistocene extends from to 11,700 years before present. The modern continents were essentially at their present positions during the Pleistocene, the plates upon which they sit probably having moved no more than relative to each other since the beginning of the period.\n\nThe Holocene Epoch began approximately 11,700 calendar years before present and continues to the present. During the Holocene, continental motions have been less than a kilometer.\n\nThe last glacial period of the current ice age ended about 10,000 years ago. Ice melt caused world sea levels to rise about in the early part of the Holocene. In addition, many areas above about 40 degrees north latitude had been depressed by the weight of the Pleistocene glaciers and rose as much as over the late Pleistocene and Holocene, and are still rising today. The sea level rise and temporary land depression allowed temporary marine incursions into areas that are now far from the sea. Holocene marine fossils are known from Vermont, Quebec, Ontario and Michigan. Other than higher latitude temporary marine incursions associated with glacial depression, Holocene fossils are found primarily in lakebed, floodplain and cave deposits. Holocene marine deposits along low-latitude coastlines are rare because the rise in sea levels during the period exceeds any likely upthrusting of non-glacial origin. Post-glacial rebound in Scandinavia resulted in the emergence of coastal areas around the Baltic Sea, including much of Finland. The region continues to rise, still causing weak earthquakes across Northern Europe. The equivalent event in North America was the rebound of Hudson Bay, as it shrank from its larger, immediate post-glacial Tyrrell Sea phase, to near its present boundaries.\n\n\n"}
{"id": "43421", "url": "https://en.wikipedia.org/wiki?curid=43421", "title": "Henry David Thoreau", "text": "Henry David Thoreau\n\nHenry David Thoreau (see name pronunciation; July 12, 1817 – May 6, 1862) was an American essayist, poet, philosopher, abolitionist, naturalist, tax resister, development critic, surveyor, and historian. A leading transcendentalist, Thoreau is best known for his book \"Walden\", a reflection upon simple living in natural surroundings, and his essay \"Civil Disobedience\" (originally published as \"Resistance to Civil Government\"), an argument for disobedience to an unjust state.\n\nThoreau's books, articles, essays, journals, and poetry amount to more than 20 volumes. Among his lasting contributions are his writings on natural history and philosophy, in which he anticipated the methods and findings of ecology and environmental history, two sources of modern-day environmentalism. His literary style interweaves close observation of nature, personal experience, pointed rhetoric, symbolic meanings, and historical lore, while displaying a poetic sensibility, philosophical austerity, and Yankee attention to practical detail. He was also deeply interested in the idea of survival in the face of hostile elements, historical change, and natural decay; at the same time he advocated abandoning waste and illusion in order to discover life's true essential needs.\n\nHe was a lifelong abolitionist, delivering lectures that attacked the Fugitive Slave Law while praising the writings of Wendell Phillips and defending the abolitionist John Brown. Thoreau's philosophy of civil disobedience later influenced the political thoughts and actions of such notable figures as Leo Tolstoy, Mahatma Gandhi, and Martin Luther King Jr.\n\nThoreau is sometimes referred to as an anarchist. Though \"Civil Disobedience\" seems to call for improving rather than abolishing government—\"I ask for, not at once no government, but \"at once\" a better government\"—the direction of this improvement contrarily points toward anarchism: \"'That government is best which governs not at all;' and when men are prepared for it, that will be the kind of government which they will have.\"\n\nAmos Bronson Alcott and Thoreau's aunt each wrote that \"Thoreau\" is pronounced like the word \"thorough\" ( —in General American, but more precisely —in 19th-century New England). Edward Waldo Emerson wrote that the name should be pronounced \"Thó-row\", with the \"h\" sounded and stress on the first syllable. Among modern-day American speakers, it is perhaps more commonly pronounced —with stress on the second syllable.\n\nThoreau had a distinctive appearance, with a nose that he called his \"most prominent feature\". Of his appearance and disposition, Ellery Channing wrote:\n\nHis face, once seen, could not be forgotten. The features were quite marked: the nose aquiline or very Roman, like one of the portraits of Caesar (more like a beak, as was said); large overhanging brows above the deepest set blue eyes that could be seen, in certain lights, and in others gray,—eyes expressive of all shades of feeling, but never weak or near-sighted; the forehead not unusually broad or high, full of concentrated energy and purpose; the mouth with prominent lips, pursed up with meaning and thought when silent, and giving out when open with the most varied and unusual instructive sayings.\n\nHenry David Thoreau was born David Henry Thoreau in Concord, Massachusetts, into the \"modest New England family\" of John Thoreau, a pencil maker, and Cynthia Dunbar. His paternal grandfather had been born on the UK crown dependency island of Jersey. His maternal grandfather, Asa Dunbar, led Harvard's 1766 student \"Butter Rebellion\", the first recorded student protest in the American colonies. David Henry was named after his recently deceased paternal uncle, David Thoreau. He began to call himself Henry David after he finished college; he never petitioned to make a legal name change. He had two older siblings, Helen and John Jr., and a younger sister, Sophia. Thoreau's birthplace still exists on Virginia Road in Concord. The house has been restored by the Thoreau Farm Trust, a nonprofit organization, and is now open to the public.\n\nHe studied at Harvard College between 1833 and 1837. He lived in Hollis Hall and took courses in rhetoric, classics, philosophy, mathematics, and science. He was a member of the Institute of 1770 (now the Hasty Pudding Club). According to legend, Thoreau refused to pay the five-dollar fee (approximately ) for a Harvard diploma. In fact, the master's degree he declined to purchase had no academic merit: Harvard College offered it to graduates \"who proved their physical worth by being alive three years after graduating, and their saving, earning, or inheriting quality or condition by having Five Dollars to give the college.\" He commented, \"Let every sheep keep its own skin\", a reference to the tradition of using sheepskin vellum for diplomas.\n\nThe traditional professions open to college graduates—law, the church, business, medicine—did not interest Thoreau, so in 1835 he took a leave of absence from Harvard, during which he taught school in Canton, Massachusetts. After he graduated in 1837, he joined the faculty of the Concord public school, but he resigned after a few weeks rather than administer corporal punishment. He and his brother John then opened the Concord Academy, a grammar school in Concord, in 1838. They introduced several progressive concepts, including nature walks and visits to local shops and businesses. The school closed when John became fatally ill from tetanus in 1842 after cutting himself while shaving. He died in Henry's arms.\n\nUpon graduation Thoreau returned home to Concord, where he met Ralph Waldo Emerson through a mutual friend. Emerson, who was 14 years his senior, took a paternal and at times patron-like interest in Thoreau, advising the young man and introducing him to a circle of local writers and thinkers, including Ellery Channing, Margaret Fuller, Bronson Alcott, and Nathaniel Hawthorne and his son Julian Hawthorne, who was a boy at the time.\n\nEmerson urged Thoreau to contribute essays and poems to a quarterly periodical, \"The Dial\", and lobbied the editor, Margaret Fuller, to publish those writings. Thoreau's first essay published in \"The Dial\" was \"Aulus Persius Flaccus,\" an essay on the Roman playwright, in July 1840. It consisted of revised passages from his journal, which he had begun keeping at Emerson's suggestion. The first journal entry, on October 22, 1837, reads, \"'What are you doing now?' he asked. 'Do you keep a journal?' So I make my first entry to-day.\"\n\nThoreau was a philosopher of nature and its relation to the human condition. In his early years he followed Transcendentalism, a loose and eclectic idealist philosophy advocated by Emerson, Fuller, and Alcott. They held that an ideal spiritual state transcends, or goes beyond, the physical and empirical, and that one achieves that insight via personal intuition rather than religious doctrine. In their view, Nature is the outward sign of inward spirit, expressing the \"radical correspondence of visible things and human thoughts\", as Emerson wrote in \"Nature\" (1836).\n\nOn April 18, 1841, Thoreau moved into the Emerson house. There, from 1841 to 1844, he served as the children's tutor; he was also an editorial assistant, repairman and gardener. For a few months in 1843, he moved to the home of William Emerson on Staten Island, and tutored the family's sons while seeking contacts among literary men and journalists in the city who might help publish his writings, including his future literary representative Horace Greeley.\n\nThoreau returned to Concord and worked in his family's pencil factory, which he would continue to do alongside his writing and other work for most of his adult life. He rediscovered the process of making good pencils with inferior graphite by using clay as the binder. This invention allowed profitable use of a graphite source found in New Hampshire that had been purchased in 1821 by Thoreau's brother-in-law, Charles Dunbar. The process of mixing graphite and clay, known as the Conté process, had been first patented by Nicolas-Jacques Conté in 1795. The company's other source of graphite had been Tantiusques, a mine operated by Native Americans in Sturbridge, Massachusetts. Later, Thoreau converted the pencil factory to produce plumbago, a name for graphite at the time, which was used in the electrotyping process.\n\nOnce back in Concord, Thoreau went through a restless period. In April 1844 he and his friend Edward Hoar accidentally set a fire that consumed of Walden Woods.\n\nThoreau felt a need to concentrate and work more on his writing. In March 1845, Ellery Channing told Thoreau, \"Go out upon that, build yourself a hut, & there begin the grand process of devouring yourself alive. I see no other alternative, no other hope for you.\" Two months later, Thoreau embarked on a two-year experiment in simple living on July 4, 1845, when he moved to a small house he had built on land owned by Emerson in a second-growth forest around the shores of Walden Pond. The house was in \"a pretty pasture and woodlot\" of that Emerson had bought, from his family home.\nOn July 24 or July 25, 1846, Thoreau ran into the local tax collector, Sam Staples, who asked him to pay six years of delinquent poll taxes. Thoreau refused because of his opposition to the Mexican–American War and slavery, and he spent a night in jail because of this refusal. The next day Thoreau was freed when someone, likely to have been his aunt, paid the tax, against his wishes. The experience had a strong impact on Thoreau. In January and February 1848, he delivered lectures on \"The Rights and Duties of the Individual in relation to Government\", explaining his tax resistance at the Concord Lyceum. Bronson Alcott attended the lecture, writing in his journal on January 26:\n\nThoreau revised the lecture into an essay titled \"Resistance to Civil Government\" (also known as \"Civil Disobedience\"). It was published by Elizabeth Peabody in the \"Aesthetic Papers\" in May 1849. Thoreau had taken up a version of Percy Shelley's principle in the political poem \"The Mask of Anarchy\" (1819), which begins with the powerful images of the unjust forms of authority of his time and then imagines the stirrings of a radically new form of social action.\n\nAt Walden Pond, Thoreau completed a first draft of \"A Week on the Concord and Merrimack Rivers\", an elegy to his brother John, describing their trip to the White Mountains in 1839. Thoreau did not find a publisher for the book and instead printed 1,000 copies at his own expense; fewer than 300 were sold. He self-published the book on the advice of Emerson, using Emerson's publisher, Munroe, who did little to publicize the book.\n\nIn August 1846, Thoreau briefly left Walden to make a trip to Mount Katahdin in Maine, a journey later recorded in \"Ktaadn\", the first part of \"The Maine Woods\".\n\nThoreau left Walden Pond on September 6, 1847. At Emerson's request, he immediately moved back to the Emerson house to help Emerson's wife, Lidian, manage the household while her husband was on an extended trip to Europe. Over several years, as he worked to pay off his debts, he continuously revised the manuscript of what he eventually published as \"Walden, or Life in the Woods\" in 1854, recounting the two years, two months, and two days he had spent at Walden Pond. The book compresses that time into a single calendar year, using the passage of the four seasons to symbolize human development. Part memoir and part spiritual quest, \"Walden\" at first won few admirers, but later critics have regarded it as a classic American work that explores natural simplicity, harmony, and beauty as models for just social and cultural conditions.\n\nThe American poet Robert Frost wrote of Thoreau, \"In one book ... he surpasses everything we have had in America.\"\n\nThe American author John Updike said of the book, \"A century and a half after its publication, Walden has become such a totem of the back-to-nature, preservationist, anti-business, civil-disobedience mindset, and Thoreau so vivid a protester, so perfect a crank and hermit saint, that the book risks being as revered and unread as the Bible.\"\n\nThoreau moved out of Emerson's house in July 1848 and stayed at a house on nearby Belknap Street. In 1850, he and his family moved into a house at 255 Main Street, where he lived until his death.\n\nIn the summer of 1850, Thoreau and Channing journeyed from Boston to Montreal and Quebec City. These would be Thoreau's only travels outside the United States. It is as a result of this trip that he developed lectures that eventually became \"A Yankee in Canada\". He jested that all he got from this adventure \"was a cold.\" In fact, this proved an opportunity to contrast American civic spirit and democratic values with a colony apparently ruled by illegitimate religious and military power. Whereas his own country had had its revolution, in Canada history had failed to turn.\n\nIn 1851, Thoreau became increasingly fascinated with natural history and narratives of travel and expedition. He read avidly on botany and often wrote observations on this topic into his journal. He admired William Bartram and Charles Darwin's \"Voyage of the Beagle\". He kept detailed observations on Concord's nature lore, recording everything from how the fruit ripened over time to the fluctuating depths of Walden Pond and the days certain birds migrated. The point of this task was to \"anticipate\" the seasons of nature, in his word.\n\nHe became a land surveyor and continued to write increasingly detailed observations on the natural history of the town, covering an area of , in his journal, a two-million-word document he kept for 24 years. He also kept a series of notebooks, and these observations became the source of his late writings on natural history, such as \"Autumnal Tints\", \"The Succession of Trees\", and \"Wild Apples\", an essay lamenting the destruction of indigenous wild apple species.\n\nUntil the 1970s, literary critics dismissed Thoreau's late pursuits as amateur science and philosophy. With the rise of environmental history and ecocriticism as academic disciplines, several new readings of Thoreau began to emerge, showing him to have been both a philosopher and an analyst of ecological patterns in fields and woodlots. For instance, his late essay \"The Succession of Forest Trees\" shows that he used experimentation and analysis to explain how forests regenerate after fire or human destruction, through the dispersal of seeds by winds or animals.\nHe traveled to Canada East once, Cape Cod four times, and Maine three times; these landscapes inspired his \"excursion\" books, \"A Yankee in Canada\", \"Cape Cod\", and \"The Maine Woods\", in which travel itineraries frame his thoughts about geography, history and philosophy. Other travels took him southwest to Philadelphia and New York City in 1854 and west across the Great Lakes region in 1861, when he visited Niagara Falls, Detroit, Chicago, Milwaukee, St. Paul and Mackinac Island. He was provincial in his own travels, but he read widely about travel in other lands. He devoured all the first-hand travel accounts available in his day, at a time when the last unmapped regions of the earth were being explored. He read Magellan and James Cook; the arctic explorers John Franklin, Alexander Mackenzie and William Parry; David Livingstone and Richard Francis Burton on Africa; Lewis and Clark; and hundreds of lesser-known works by explorers and literate travelers. Astonishing amounts of reading fed his endless curiosity about the peoples, cultures, religions and natural history of the world and left its traces as commentaries in his voluminous journals. He processed everything he read, in the local laboratory of his Concord experience. Among his famous aphorisms is his advice to \"live at home like a traveler.\"\n\nAfter John Brown's raid on Harpers Ferry, many prominent voices in the abolitionist movement distanced themselves from Brown or damned him with faint praise. Thoreau was disgusted by this, and he composed a key speech, \"A Plea for Captain John Brown\", which was uncompromising in its defense of Brown and his actions. Thoreau's speech proved persuasive: the abolitionist movement began to accept Brown as a martyr, and by the time of the American Civil War entire armies of the North were literally singing Brown's praises. As a biographer of Brown put it, \"If, as Alfred Kazin suggests, without John Brown there would have been no Civil War, we would add that without the Concord Transcendentalists, John Brown would have had little cultural impact.\"\nThoreau contracted tuberculosis in 1835 and suffered from it sporadically afterwards. In 1860, following a late-night excursion to count the rings of tree stumps during a rainstorm, he became ill with bronchitis. His health declined, with brief periods of remission, and he eventually became bedridden. Recognizing the terminal nature of his disease, Thoreau spent his last years revising and editing his unpublished works, particularly \"The Maine Woods\" and \"Excursions\", and petitioning publishers to print revised editions of \"A Week\" and \"Walden\". He wrote letters and journal entries until he became too weak to continue. His friends were alarmed at his diminished appearance and were fascinated by his tranquil acceptance of death. When his aunt Louisa asked him in his last weeks if he had made his peace with God, Thoreau responded, \"I did not know we had ever quarreled.\"\nAware he was dying, Thoreau's last words were \"Now comes good sailing\", followed by two lone words, \"moose\" and \"Indian\". He died on May 6, 1862, at age 44. Amos Bronson Alcott planned the service and read selections from Thoreau's works, and Channing presented a hymn. Emerson wrote the eulogy spoken at the funeral. Thoreau was buried in the Dunbar family plot; his remains and those of members of his immediate family were eventually moved to Sleepy Hollow Cemetery () in Concord, Massachusetts.\n\nThoreau's friend William Ellery Channing published his first biography, \"Thoreau the Poet-Naturalist\", in 1873. Channing and another friend, Harrison Blake, edited some poems, essays, and journal entries for posthumous publication in the 1890s. Thoreau's journals, which he often mined for his published works but which remained largely unpublished at his death, were first published in 1906 and helped to build his modern reputation. A new, expanded edition of the journals is under way, published by Princeton University Press. Today, Thoreau is regarded as one of the foremost American writers, both for the modern clarity of his prose style and the prescience of his views on nature and politics. His memory is honored by the international Thoreau Society and his legacy honored by the Thoreau Institute at Walden Woods, established in 1998 in Lincoln, Massachusetts.\n\nThoreau was an early advocate of recreational hiking and canoeing, of conserving natural resources on private land, and of preserving wilderness as public land. He was himself a highly skilled canoeist; Nathaniel Hawthorne, after a ride with him, noted that \"Mr. Thoreau managed the boat so perfectly, either with two paddles or with one, that it seemed instinct with his own will, and to require no physical effort to guide it.\" \n\nHe was not a strict vegetarian, though he said he preferred that diet and advocated it as a means of self-improvement. He wrote in \"Walden\", \"The practical objection to animal food in my case was its uncleanness; and besides, when I had caught and cleaned and cooked and eaten my fish, they seemed not to have fed me essentially. It was insignificant and unnecessary, and cost more than it came to. A little bread or a few potatoes would have done as well, with less trouble and filth.\"\nThoreau neither rejected civilization nor fully embraced wilderness. Instead he sought a middle ground, the pastoral realm that integrates nature and culture. His philosophy required that he be a didactic arbitrator between the wilderness he based so much on and the spreading mass of humanity in North America. He decried the latter endlessly but felt that a teacher needs to be close to those who needed to hear what he wanted to tell them. The wildness he enjoyed was the nearby swamp or forest, and he preferred \"partially cultivated country.\" His idea of being \"far in the recesses of the wilderness\" of Maine was to \"travel the logger's path and the Indian trail\", but he also hiked on pristine land. In the essay \"Henry David Thoreau, Philosopher\" Roderick Nash wrote, \"Thoreau left Concord in 1846 for the first of three trips to northern Maine. His expectations were high because he hoped to find genuine, primeval America. But contact with real wilderness in Maine affected him far differently than had the idea of wilderness in Concord. Instead of coming out of the woods with a deepened appreciation of the wilds, Thoreau felt a greater respect for civilization and realized the necessity of balance.\"\nOf alcohol, Thoreau wrote, \"I would fain keep sober always. ... I believe that water is the only drink for a wise man; wine is not so noble a liquor. ... Of all ebriosity, who does not prefer to be intoxicated by the air he breathes?\"\n\nThoreau never married and was childless. He strove to portray himself as an ascetic puritan. However, his sexuality has long been the subject of speculation, including by his contemporaries. Critics have called him heterosexual, homosexual, or asexual. There is no evidence to suggest he had physical relations with anyone, man or woman. Some scholars have suggested that homoerotic sentiments run through his writings and concluded that he was homosexual. The elegy \"Sympathy\" was inspired by the eleven-year-old Edmund Sewell, with whom he hiked for five days in 1839. One scholar has suggested that he wrote the poem to Edmund because he could not bring himself to write it to Edmund's sister, and another that Thoreau's \"emotional experiences with women are memorialized under a camouflage of masculine pronouns\", but other scholars dismiss this. It has been argued that the long paean in \"Walden\" to the French-Canadian woodchopper Alek Therien, which includes allusions to Achilles and Patroclus, is an expression of conflicted desire. In some of Thoreau's writing there is the sense of a secret self. In 1840 he writes in his journal: \"My friend is the apology for my life. In him are the spaces which my orbit traverses\". Thoreau was strongly influenced by the moral reformers of his time, and this may have instilled anxiety and guilt over sexual desire.\n\nThoreau was fervently against slavery and actively supported the abolitionist movement. He participated in the Underground Railroad, delivered lectures that attacked the Fugitive Slave Law, and in opposition to the popular opinion of the time, supported radical abolitionist militia leader John Brown and his party. Two weeks after the ill-fated raid on Harpers Ferry and in the weeks leading up to Brown's execution, Thoreau regularly delivered a speech to the citizens of Concord, Massachusetts, in which he compared the American government to Pontius Pilate and likened Brown's execution to the crucifixion of Jesus Christ:\n\nIn \"The Last Days of John Brown\", Thoreau described the words and deeds of John Brown as noble and an example of heroism. In addition, he lamented the newspaper editors who dismissed Brown and his scheme as \"crazy\".\n\nThoreau was a proponent of limited government and individualism. Although he was hopeful that mankind could potentially have, through self-betterment, the kind of government which \"governs not at all\", he distanced himself from contemporary \"no-government men\" (anarchists), writing: \"I ask for, not at once no government, but at once a better government.\"\n\nThoreau deemed the evolution from absolute monarchy to limited monarchy to democracy as \"a progress toward true respect for the individual\" and theorized about further improvements \"towards recognizing and organizing the rights of man.\" Echoing this belief, he went on to write: \"There will never be a really free and enlightened State until the State comes to recognize the individual as a higher and independent power, from which all its power and authority are derived, and treats him accordingly.\"\n\nIt is on this basis that Thoreau could so strongly inveigh against British and Catholic power in \"A Yankee in Canada\". Despotic authority had crushed the people's sense of ingenuity and enterprise; the Canadian \"habitants\" had been reduced, in his view, to a perpetual childlike state. Ignoring the recent Rebellions, he argued that there would be no revolution in the St. Lawrence River valley.\n\nAlthough Thoreau believed resistance to unjustly exercised authority could be both violent (exemplified in his support for John Brown) and nonviolent (his own example of tax resistance displayed in \"Resistance to Civil Government\"), he regarded pacifist nonresistance as temptation to passivity, writing: \"Let not our Peace be proclaimed by the rust on our swords, or our inability to draw them from their scabbards; but let her at least have so much work on her hands as to keep those swords bright and sharp.\" Furthermore, in a formal lyceum debate in 1841, he debated the subject \"Is it ever proper to offer forcible resistance?\", arguing the affirmative.\n\nLikewise, his condemnation of the Mexican–American War did not stem from pacifism, but rather because he considered Mexico \"unjustly overrun and conquered by a foreign army\" as a means to expand the slave territory.\n\nThoreau was ambivalent towards industrialization and capitalism. On one hand he regarded commerce as \"unexpectedly confident and serene, adventurous, and unwearied\" and expressed admiration for its associated cosmopolitanism, writing:\n\nOn the other hand, he wrote disparagingly of the factory system:\n\nThoreau also favored bioregionalism, the protection of animals and wild areas, free trade, and taxation for schools and highways. He disapproved of the subjugation of Native Americans, slavery, technological utopianism, consumerism, philistinism, mass entertainment, and frivolous applications of technology.\n\nThoreau was influenced by Indian spiritual thought. In \"Walden\", there are many overt references to the sacred texts of India. For example, in the first chapter (\"Economy\"), he writes: \"How much more admirable the Bhagvat-Geeta than all the ruins of the East!\" \"American Philosophy: An Encyclopedia\" classes him as one of several figures who \"took a more pantheist or pandeist approach by rejecting views of God as separate from the world\", also a characteristic of Hinduism.\n\nFurthermore, in \"The Pond in Winter\", he equates Walden Pond with the sacred Ganges river, writing:\n\nThoreau was aware his Ganges imagery could have been factual. He wrote about ice harvesting at Walden Pond. And he knew that New England's ice merchants were shipping ice to foreign ports, including Calcutta.\n\nAdditionally, Thoreau followed various Hindu customs, including following a diet of rice (\"It was fit that I should live on rice, mainly, who loved so well the philosophy of India.\"), flute playing (reminiscent of the favorite musical pastime of Krishna), and yoga.\n\nIn an 1849 letter to his friend H.G.O. Blake, he wrote about yoga and its meaning to him:\n\nThoreau read contemporary works in the new science of biology, including the works of Alexander von Humboldt, Charles Darwin, and Asa Gray (Charles Darwin's staunchest American ally). Thoreau was deeply influenced by Humboldt, especially his work Kosmos.\n\nIn 1859, Thoreau purchased and read Darwin's \"On the Origin of Species\". Unlike many natural historians at the time, including Louis Agassiz who publicly opposed Darwinism in favor of a static view of nature, Thoreau was immediately enthusiastic about the theory of evolution by natural selection and endorsed it, stating:\n\nThoreau's political writings had little impact during his lifetime, as \"his contemporaries did not see him as a theorist or as a radical,\" viewing him instead as a naturalist. They either dismissed or ignored his political essays, including \"Civil Disobedience\". The only two complete books (as opposed to essays) published in his lifetime, \"Walden\" and \"A Week on the Concord and Merrimack Rivers\" (1849), both dealt with nature, in which he loved to wander.\" His obituary was lumped in with others rather than as a separate article in an 1862 yearbook. Nevertheless, Thoreau's writings went on to influence many public figures. Political leaders and reformers like Mohandas Gandhi, U.S. President John F. Kennedy, American civil rights activist Martin Luther King Jr., U.S. Supreme Court Justice William O. Douglas, and Russian author Leo Tolstoy all spoke of being strongly affected by Thoreau's work, particularly \"Civil Disobedience\", as did \"right-wing theorist Frank Chodorov [who] devoted an entire issue of his monthly, \"Analysis\", to an appreciation of Thoreau.\"\n\nThoreau also influenced many artists and authors including Edward Abbey, Willa Cather, Marcel Proust, William Butler Yeats, Sinclair Lewis, Ernest Hemingway, Upton Sinclair, E. B. White, Lewis Mumford, Frank Lloyd Wright, Alexander Posey, and Gustav Stickley. Thoreau also influenced naturalists like John Burroughs, John Muir, E. O. Wilson, Edwin Way Teale, Joseph Wood Krutch, B. F. Skinner, David Brower, and Loren Eiseley, whom \"Publishers Weekly\" called \"the modern Thoreau\". English writer Henry Stephens Salt wrote a biography of Thoreau in 1890, which popularized Thoreau's ideas in Britain: George Bernard Shaw, Edward Carpenter, and Robert Blatchford were among those who became Thoreau enthusiasts as a result of Salt's advocacy. Mohandas Gandhi first read \"Walden\" in 1906 while working as a civil rights activist in Johannesburg, South Africa. He first read \"Civil Disobedience\" \"while he sat in a South African prison for the crime of nonviolently protesting discrimination against the Indian population in the Transvaal. The essay galvanized Gandhi, who wrote and published a synopsis of Thoreau's argument, calling its 'incisive logic ... unanswerable' and referring to Thoreau as 'one of the greatest and most moral men America has produced'.\" He told American reporter Webb Miller, \"[Thoreau's] ideas influenced me greatly. I adopted some of them and recommended the study of Thoreau to all of my friends who were helping me in the cause of Indian Independence. Why I actually took the name of my movement from Thoreau's essay 'On the Duty of Civil Disobedience', written about 80 years ago.\"\n\nMartin Luther King, Jr. noted in his autobiography that his first encounter with the idea of nonviolent resistance was reading \"On Civil Disobedience\" in 1944 while attending Morehouse College. He wrote in his autobiography that it was,\n\nHere, in this courageous New Englander's refusal to pay his taxes and his choice of jail rather than support a war that would spread slavery's territory into Mexico, I made my first contact with the theory of nonviolent resistance. Fascinated by the idea of refusing to cooperate with an evil system, I was so deeply moved that I reread the work several times. I became convinced that noncooperation with evil is as much a moral obligation as is cooperation with good. No other person has been more eloquent and passionate in getting this idea across than Henry David Thoreau. As a result of his writings and personal witness, we are the heirs of a legacy of creative protest. The teachings of Thoreau came alive in our civil rights movement; indeed, they are more alive than ever before. Whether expressed in a sit-in at lunch counters, a freedom ride into Mississippi, a peaceful protest in Albany, Georgia, a bus boycott in Montgomery, Alabama, these are outgrowths of Thoreau's insistence that evil must be resisted and that no moral man can patiently adjust to injustice.\n\nAmerican psychologist B. F. Skinner wrote that he carried a copy of Thoreau's \"Walden\" with him in his youth. and, in 1945, wrote \"Walden Two\", a fictional utopia about 1,000 members of a community living together inspired by the life of Thoreau. Thoreau and his fellow Transcendentalists from Concord were a major inspiration of the composer Charles Ives. The 4th movement of the Concord Sonata for piano (with a part for flute, Thoreau's instrument) is a character picture and he also set Thoreau's words.\n\nActor Ron Thompson did a dramatic portrayal of Henry David Thoreau on the 1976 NBC television series \"The Rebels\".\n\nThoreau's ideas have impacted and resonated with various strains in the anarchist movement, with Emma Goldman referring to him as \"the greatest American anarchist\". Green anarchism and anarcho-primitivism in particular have both derived inspiration and ecological points-of-view from the writings of Thoreau. John Zerzan included Thoreau's text \"Excursions\" (1863) in his edited compilation of works in the anarcho-primitivist tradition titled \"Against civilization: Readings and reflections\". Additionally, Murray Rothbard, the founder of anarcho-capitalism, has opined that Thoreau was one of the \"great intellectual heroes\" of his movement. Thoreau was also an important influence on late-19th-century anarchist naturism. Globally, Thoreau's concepts also held importance within individualist anarchist circles in Spain, France, and Portugal.\n\nFor the 200th anniversary of his birth, publishers released several new editions of his work: a recreation of \"Walden\" 1902 edition with illustrations, a picture book with excerpts from \"Walden\", and an annotated collection of Thoreau's essays on slavery. The United States Postal Service issued a commemorative stamp honoring Thoreau on May 23, 2017 in Concord, MA.\n\nAlthough his writings would receive widespread acclaim, Thoreau's ideas were not universally applauded. Scottish author Robert Louis Stevenson judged Thoreau's endorsement of living alone and apart from modern society in natural simplicity to be a mark of \"unmanly\" effeminacy and \"womanish solitude\", while deeming him a self-indulgent \"skulker\".\n\nNathaniel Hawthorne had mixed feelings about Thoreau. He noted that \"He is a keen and delicate observer of nature—a genuine observer—which, I suspect, is almost as rare a character as even an original poet; and Nature, in return for his love, seems to adopt him as her especial child, and shows him secrets which few others are allowed to witness.\" On the other hand, he also wrote that Thoreau \"repudiated all regular modes of getting a living, and seems inclined to lead a sort of Indian life among civilized men\".\n\nIn a similar vein, poet John Greenleaf Whittier detested what he deemed to be the \"wicked\" and \"heathenish\" message of \"Walden\", claiming that Thoreau wanted man to \"lower himself to the level of a woodchuck and walk on four legs\".\n\nIn response to such criticisms, English novelist George Eliot, writing for the \"Westminster Review\", characterized such critics as uninspired and narrow-minded:\nThoreau himself also responded to the criticism in a paragraph of his work \"Walden\" by illustrating the irrelevance of their inquiries:\n\nRecent criticism has accused Thoreau of hypocrisy, misanthropy, and being sanctimonious, based on his writings in \"Walden\", although this criticism has been perceived as highly selective.\n\n\n\n\n"}
{"id": "53384839", "url": "https://en.wikipedia.org/wiki?curid=53384839", "title": "International Union for Vacuum Science, Technique and Applications", "text": "International Union for Vacuum Science, Technique and Applications\n\nThe International Union for Vacuum Science, Technique, and Applications (IUVSTA) is a union of 33 science and technology national member societies whose role is to stimulate international collaboration in the fields of vacuum science, technique and applications, and related multi-disciplinary topics.\n\nIUVSTA is a Member Scientific Associate of the International Council for Science (ICSU).\n\nFounded in 1958, IUVSTA is an interdisciplinary union which represents several thousands of physicists, chemists, materials scientists, engineers and technologists who are active in basic and applied research, development, manufacturing, sales and education. IUVSTA finances advanced scientific workshops, international schools and technical courses, worldwide.\n\nIUVSTA comprises member societies from the following countries:\nArgentina, Australia, Austria, Belgium, Brazil, Bulgaria, China, Croatia, Czech Republic, Finland, France, Germany, Hungary, India, Israel, Iran, Italy, Japan, Korea, Mexico, Netherlands, Pakistan, Philippines, Poland, Portugal, Russian Federation, Slovakia, Slovenia, Spain, Sweden, Switzerland, United Kingdom, and USA.\n\nThe main purposes of the IUVSTA are to organize and sponsor international conferences and educational activities, as well as to facilitate research and technological developments in the field of vacuum science and its applications.\n\nThe history and structure of the Union are described in two articles in scientific journals.\n\nIUVSTA has nine technical divisions:\n\n\n"}
{"id": "660678", "url": "https://en.wikipedia.org/wiki?curid=660678", "title": "List of glaciers", "text": "List of glaciers\n\nA glacier ( ) or () is a persistent body of dense ice that is constantly moving under its own weight; it forms where the accumulation of snow exceeds its ablation (melting and sublimation) over many years, often centuries. Glaciers slowly deform and flow due to stresses induced by their weight, creating crevasses, seracs, and other distinguishing features. Because glacial mass is affected by long-term climate changes, e.g., precipitation, mean temperature, and cloud cover, glacial mass changes are considered among the most sensitive indicators of climate change.\n\nAfrica, specifically East Africa, has contained glacial regions, possibly as far back as the last glacier maximum 10 to 15 thousand years ago. Seasonal snow does exist on the highest peaks of East Africa as well as in the Drakensberg Range of South Africa, the Stormberg Mountains, and the Atlas Mountains in Morocco. Currently, the only remaining glaciers on the continent exist on Mount Kilimanjaro, Mount Kenya, and the Rwenzori.\n\nThere are many glaciers in the Antarctic. This set of lists does not include ice sheets, ice caps or ice fields, such as the Antarctic ice sheet, but includes glacial features that are defined by their flow, rather than general bodies of ice. The lists include outlet glaciers, valley glaciers, cirque glaciers, tidewater glaciers and ice streams. Ice streams are a type of glacier and many of them have \"glacier\" in their name, e.g. Pine Island Glacier. Ice shelves are listed separately in the List of Antarctic ice shelves. For the purposes of these lists, the Antarctic is defined as any latitude further south than 60° (the continental limit according to the Antarctic Treaty System).\n\nThere are also glaciers in the subantarctic. This includes one snow field (Murray Snowfield). Snow fields are not glaciers in the strict sense of the word, but they are commonly found at the accumulation zone or head of a glacier. For the purposes of this list, Antarctica is defined as any latitude further south than 60° (the continental limit according to the Antarctic Treaty).\n\nThe majority of Europe's glaciers are found in the Alps, Caucasus and the Scandinavian Mountains (mostly Norway) as well as in Iceland. Iceland has the largest glacier in Europe, Vatnajökull glacier, that covers between 8,100-8,300 km² in area and 3,100 km³ in volume. Norway alone has more than 2500 glaciers (including very small ones) covering an estimated 1% of mainland Norway's surface area. Several of mainland Europe's biggest glaciers are found here including; Jostedalsbreen(the largest in mainland Europe at 487 km), Vestre Svartisen(221 km), Søndre Folgefonna(168 km) and Østre Svartisen(148 km). The two Svartisen glaciers used to be one connected entity during the Little Ice Age but has since separated.\n\n\nThere are a number of glaciers existing in North America, currently or in recent centuries. In the United States, these glaciers are located in nine states, all in the Rocky Mountains or further west. The southernmost named glacier among them is the Lilliput Glacier in Tulare County, east of the Central Valley of California.\n\nMexico has about two dozen glaciers, all of which are located on Pico de Orizaba (Citlaltépetl), Popocatépetl and Iztaccíhuatl, the three tallest mountains in the country.\n\n\nGlaciers in South America develop exclusively on the Andes and are subject of the Andes various climatic regimes namely the Tropical Andes, Dry Andes and the Wet Andes. Apart from this there is a wide range of latitudes on which glaciers develop from 5000 m in the Altiplano mountains and volcanoes to reaching sealevel as tidewater glaciers from San Rafael Lagoon (45° S) and southwards. South America hosts two large ice fields, the Northern and Southern Patagonian Ice Fields, of which the second is the largest contiguous body of glaciers in extrapolar regions.\n\nThe glaciers of Chile cover 2.7% (20,188 km) of the land area of the country, excluding Antártica Chilena, and have a considerable impact on its landscape and water supply. By surface 80% of South America’s glaciers lie in Chile. Glaciers develop in the Andes of Chile from 27˚S southwards and in a very few places north of 18°30'S in the extreme north of the country: in between they are absent because of extreme aridity, though rock glaciers formed from permafrost are common. The largest glaciers of Chile are the Northern and Southern Patagonian Ice Fields. From a latitude of 47° S and south some glaciers reach sea level.\n\nApart from height and latitude, the settings of Chilean glaciers depend on precipitation patterns; in this sense two different regions exist: the Dry Andes and the Wet Andes.\n\nNo glaciers remain on the Australia mainland or Tasmania. A few, like the Heard Island glaciers are located in the territory of Heard Island and McDonald Islands in the southern Indian Ocean.\n\nNew Guinea has the Puncak Jaya glacier.\n\nNew Zealand contains many glaciers, mostly located near the Main Divide of the Southern Alps in the South Island. They are classed as mid-latitude mountain glaciers. There are eighteen small glaciers in the North Island on Mount Ruapehu.\n\nAn inventory of South Island glaciers compiled in the 1980s indicated there were about 3,155 glaciers with an area of at least one hectare (2.5 acres). Approximately one sixth of these glaciers covered more than 10 hectares. These include:\n\n\nThe following is the list of longest glaciers in the non-polar regions, generally regarded as between 60 degrees north and 60 degrees south latitude, though some definitions expand it slightly.\n\n"}
{"id": "229104", "url": "https://en.wikipedia.org/wiki?curid=229104", "title": "Matter wave", "text": "Matter wave\n\nMatter waves are a central part of the theory of quantum mechanics, being an example of wave–particle duality. All matter can exhibit wave-like behavior. For example, a beam of electrons can be diffracted just like a beam of light or a water wave. The concept that matter behaves like a wave was proposed by Louis de Broglie () in 1924. It is also referred to as the \"de Broglie hypothesis\". Matter waves are referred to as \"de Broglie waves\".\n\nThe \"de Broglie wavelength\" is the wavelength, , associated with a massive particle and is related to its momentum, , through the Planck constant, :\n\nWave-like behavior of matter was first experimentally demonstrated by George Paget Thomson's thin metal diffraction experiment, and independently in the Davisson–Germer experiment both using electrons, and it has also been confirmed for other elementary particles, neutral atoms and even molecules. Recently, it was also found that investigating the elementary process of diffusion gives the theoretical evidence of the relation of matter wave, regardless of the photon energy. It is thus revealed that the relation of matter wave is now not a hypothesis but an actual equation relevant to a characteristic of micro particle. The wave-like behavior of matter is crucial to the modern theory of atomic structure and particle physics.\n\nAt the end of the 19th century, light was thought to consist of waves of electromagnetic fields which propagated according to Maxwell's equations, while matter was thought to consist of localized particles (See history of wave and particle viewpoints). In 1900, this division was exposed to doubt, when, investigating the theory of black body thermal radiation, Max Planck proposed that light is emitted in discrete quanta of energy. It was thoroughly challenged in 1905. Extending Planck's investigation in several ways, including its connection with the photoelectric effect, Albert Einstein proposed that light is also propagated and absorbed in quanta. Light quanta are now called photons. These quanta would have an energy given by the Planck–Einstein relation:\nand a momentum\nwhere (lowercase Greek letter nu) and (lowercase Greek letter lambda) denote the frequency and wavelength of the light, the speed of light, and the Planck constant. In the modern convention, frequency is symbolized by \"f\" as is done in the rest of this article. Einstein’s postulate was confirmed experimentally by Robert Millikan and Arthur Compton over the next two decades.\n\nDe Broglie, in his 1924 PhD thesis, proposed that just as light has both wave-like and particle-like properties, electrons also have wave-like properties. By rearranging the momentum equation stated in the above section, we find a relationship between the wavelength, associated with an electron and its momentum, , through the Planck constant, :\n\nThe relationship is now known to hold for all types of matter: all matter exhibits properties of both particles and waves.\n\nIn 1926, Erwin Schrödinger published an equation describing how a matter wave should evolve—the matter wave analogue of Maxwell’s equations—and used it to derive the energy spectrum of hydrogen.\n\nMatter waves were first experimentally confirmed to occur in George Paget Thomson's cathode ray diffraction experiment and the Davisson-Germer experiment for electrons, and the de Broglie hypothesis has been confirmed for other elementary particles. Furthermore, neutral atoms and even molecules have been shown to be wave-like.\n\nIn 1927 at Bell Labs, Clinton Davisson and Lester Germer fired slow-moving electrons at a crystalline nickel target. The angular dependence of the diffracted electron intensity was measured, and was determined to have the same diffraction pattern as those predicted by Bragg for x-rays. At the same time George Paget Thomson at the University of Aberdeen was independently firing electrons at very thin metal foils to demonstrate the same effect. Before the acceptance of the de Broglie hypothesis, diffraction was a property that was thought to be exhibited only by waves. Therefore, the presence of any diffraction effects by matter demonstrated the wave-like nature of matter. When the de Broglie wavelength was inserted into the Bragg condition, the observed diffraction pattern was predicted, thereby experimentally confirming the de Broglie hypothesis for electrons.\n\nThis was a pivotal result in the development of quantum mechanics. Just as the photoelectric effect demonstrated the particle nature of light, the Davisson–Germer experiment showed the wave-nature of matter, and completed the theory of wave–particle duality. For physicists this idea was important because it meant that not only could any particle exhibit wave characteristics, but that one could use wave equations to describe phenomena in matter if one used the de Broglie wavelength.\n\nExperiments with Fresnel diffraction and an atomic mirror for specular reflection of neutral atoms confirm the application of the de Broglie hypothesis to atoms, i.e. the existence of atomic waves which undergo diffraction, interference and allow quantum reflection by the tails of the attractive potential. Advances in laser cooling have allowed cooling of neutral atoms down to nanokelvin temperatures. At these temperatures, the thermal de Broglie wavelengths come into the micrometre range. Using Bragg diffraction of atoms and a Ramsey interferometry technique, the de Broglie wavelength of cold sodium atoms was explicitly measured and found to be consistent with the temperature measured by a different method.\n\nThis effect has been used to demonstrate atomic holography, and it may allow the construction of an atom probe imaging system with nanometer resolution. The description of these phenomena is based on the wave properties of neutral atoms, confirming the de Broglie hypothesis.\n\nThe effect has also been used to explain the spatial version of the quantum Zeno effect, in which an otherwise unstable object may be stabilised by rapidly repeated observations.\n\nRecent experiments even confirm the relations for molecules and even macromolecules that otherwise might be supposed too large to undergo quantum mechanical effects. In 1999, a research team in Vienna demonstrated diffraction for molecules as large as fullerenes. The researchers calculated a De Broglie wavelength of the most probable C velocity as 2.5 pm.\nMore recent experiments prove the quantum nature of molecules made of 810 atoms and with a mass of 10,123 amu.\n\nStill one step further than Louis De Broglie go theories which in quantum mechanics eliminate the concept of a pointlike classical particle and explain the observed facts by means of wavepackets of matter waves alone.\n\nThe de Broglie equations relate the wavelength to the momentum , and frequency to the total energy of a particle:\n\nformula_5\n\nwhere \"h\" is the Planck constant. The equations can also be written as\n\nformula_6\n\nor \n\nformula_7\n\nwhere is the reduced Planck constant, is the wave vector, is the phase constant, and is the angular frequency.\nIn each pair, the second equation is also referred to as the Planck–Einstein relation, since it was also proposed by Planck and Einstein.\n\nUsing two formulas from special relativity, one for the relativistic momentum and one for the relativistic mass energy\n\nallows the equations to be written as\n\nwhere formula_11 denotes the particle's rest mass, formula_12 its velocity, formula_13 the Lorentz factor, and formula_14 the speed of light in a vacuum. See below for details of the derivation of the de Broglie relations. Group velocity (equal to the particle's speed) should not be confused with phase velocity (equal to the product of the particle's frequency and its wavelength). In the case of a non-dispersive medium, they happen to be equal, but otherwise they are not.\n\nAlbert Einstein first explained the wave–particle duality of light in 1905. Louis de Broglie hypothesized that any particle should also exhibit such a duality. The velocity of a particle, he concluded, should always equal the group velocity of the corresponding wave. The magnitude of the group velocity is equal to the particle's speed.\n\nBoth in relativistic and non-relativistic quantum physics, we can identify the group velocity of a particle's wave function with the particle velocity. Quantum mechanics has very accurately demonstrated this hypothesis, and the relation has been shown explicitly for particles as large as molecules.\n\nDe Broglie deduced that if the duality equations already known for light were the same for any particle, then his hypothesis would hold. This means that\n\nwhere is the total energy of the particle, is its momentum, is the reduced Planck constant. For a free non-relativistic particle it follows that\n\nwhere is the mass of the particle and its velocity.\n\nAlso in special relativity we find that\n\nwhere is the rest mass of the particle and is the speed of light in a vacuum. But (see below), using that the phase velocity is , therefore\n\nwhere is the velocity of the particle regardless of wave behavior.\n\nIn quantum mechanics, particles also behave as waves with complex phases. The phase velocity is equal to the product of the frequency multiplied by the wavelength.\n\nBy the de Broglie hypothesis, we see that\n\nUsing relativistic relations for energy and momentum, we have\n\nwhere \"E\" is the total energy of the particle (i.e. rest energy plus kinetic energy in the kinematic sense), \"p\" the momentum, formula_13 the Lorentz factor, \"c\" the speed of light, and β the speed as a fraction of \"c\". The variable \"v\" can either be taken to be the speed of the particle or the group velocity of the corresponding matter wave. Since the particle speed formula_22 for any particle that has mass (according to special relativity), the phase velocity of matter waves always exceeds \"c\", i.e.\n\nand as we can see, it approaches \"c\" when the particle speed is in the relativistic range. The superluminal phase velocity does not violate special relativity, because phase propagation carries no energy. See the article on \"Dispersion (optics)\" for details.\n\nUsing four-vectors, the De Broglie relations form a single equation:\n\nformula_24\n\nwhich is frame-independent.\n\nLikewise, the relation between group/particle velocity and phase velocity is given in frame-independent form by:\n\nformula_25\n\nwhere\n\nThe physical reality underlying de Broglie waves is a subject of ongoing debate. Some theories treat either the particle or the wave aspect as its fundamental nature, seeking to explain the other as an emergent property. Some, such as the hidden variable theory, treat the wave and the particle as distinct entities. Yet others propose some intermediate entity that is neither quite wave nor quite particle but only appears as such when we measure one or the other property. The Copenhagen interpretation states that the nature of the underlying reality is unknowable and beyond the bounds of scientific inquiry.\n\nSchrödinger's quantum mechanical waves are conceptually different from ordinary physical waves such as water or sound. Ordinary physical waves are characterized by undulating real-number 'displacements' of dimensioned physical variables at each point of ordinary physical space at each instant of time. Schrödinger's \"waves\" are characterized by the undulating value of a dimensionless complex number at each point of an abstract multi-dimensional space, for example of configuration space.\n\nAt the Fifth Solvay Conference in 1927, Max Born and Werner Heisenberg reported as follows:\n\nAt the same conference, Erwin Schrödinger reported likewise.\n\nIn 1955, Heisenberg reiterated this:\n\nIt is mentioned above that the \"displaced quantity\" of the Schrödinger wave has values that are dimensionless complex numbers. One may ask what is the physical meaning of those numbers. According to Heisenberg, rather than being of some ordinary physical quantity such as, for example, Maxwell's electric field intensity, or mass density, the Schrödinger-wave packet's \"displaced quantity\" is probability amplitude. He wrote that instead of using the term 'wave packet', it is preferable to speak of a probability packet. The probability amplitude supports calculation of probability of location or momentum of discrete particles. Heisenberg recites Duane's account of particle diffraction by probabilistic quantal translation momentum transfer, which allows, for example in Young's two-slit experiment, each diffracted particle probabilistically to pass discretely through a particular slit. Thus one does not need necessarily think of the matter wave, as it were, as 'composed of smeared matter'.\n\nThese ideas may be expressed in ordinary language as follows. In the account of ordinary physical waves, a 'point' refers to a position in ordinary physical space at an instant of time, at which there is specified a 'displacement' of some physical quantity. But in the account of quantum mechanics, a 'point' refers to a configuration of the system at an instant of time, every particle of the system being in a sense present in every 'point' of configuration space, each particle at such a 'point' being located possibly at a different position in ordinary physical space. There is no explicit definite indication that, at an instant, this particle is 'here' and that particle is 'there' in some separate 'location' in configuration space. This conceptual difference entails that, in contrast to de Broglie's pre-quantum mechanical wave description, the quantum mechanical probability packet description does not directly and explicitly express the Aristotelian idea, referred to by Newton, that causal efficacy propagates through ordinary space by contact, nor the Einsteinian idea that such propagation is no faster than light. In contrast, these ideas are so expressed in the classical wave account, through the Green's function, though it is inadequate for the observed quantal phenomena. The physical reasoning for this was first recognized by Einstein.\n\nDe Broglie's thesis started from the hypothesis, \"that to each portion of energy with a proper mass one may associate a periodic phenomenon of the frequency , such that one finds: . The frequency is to be measured, of course, in the rest frame of the energy packet. This hypothesis is the basis of our theory.\"\n\nDe Broglie followed his initial hypothesis of a periodic phenomenon, with frequency  , associated with the energy packet. He used the special theory of relativity to find, in the frame of the observer of the electron energy packet that is moving with velocity formula_12, that its frequency was apparently reduced to\n\nThen\n\nusing the same notation as above. The quantity formula_32 is the velocity of what de Broglie called the \"phase y wave\". Its wavelength is formula_33 and frequency formula_34. De Broglie reasoned that his hypothetical intrinsic particle periodic phenomenon is in phase with that phase wave. This was his basic matter wave conception. He noted, as above, that formula_35, and the phase wave does not transfer energy.\n\nWhile the concept of waves being associated with matter is correct, de Broglie did not leap directly to the final understanding of quantum mechanics with no missteps. There are conceptual problems with the approach that de Broglie took in his thesis that he was not able to resolve, despite trying a number of different fundamental hypotheses in different papers published while working on, and shortly after publishing, his thesis.\nThese difficulties were resolved by Erwin Schrödinger, who developed the wave mechanics approach, starting from a somewhat different basic hypothesis.\n\n\n\n"}
{"id": "13745047", "url": "https://en.wikipedia.org/wiki?curid=13745047", "title": "Natural hazard", "text": "Natural hazard\n\nA natural hazard is a natural phenomenon that might have a negative effect on humans or the environment. Natural hazard events can be classified into two broad categories: geophysical and biological. Geophysical hazards encompass geological and meteorological phenomena such as earthquakes, volcanic eruptions, wildfires, cyclonic storms, floods, droughts, avalanches and landslides. Biological hazards can refer to a diverse array of disease, infection, infestation and invasive species.\n\nMany geophysical hazards are related; for example, submarine earthquakes can cause tsunamis, and hurricanes can lead to coastal flooding and erosion. Floods and wildfires can result from a combination of geological, hydrological, and climatic factors. It is possible that some natural hazards are intertemporally correlated as well. An example of the distinction between a natural \"hazard\" and a natural \"disaster\" is that the 1906 San Francisco earthquake was a disaster, whereas living on a fault line is a hazard. Some natural hazards can be provoked or affected by anthropogenic processes (e.g. land-use change, drainage and construction).\n\nAn avalanche occurs when a large snow (or rock) mass slides down a mountainside. An avalanche is an example of a gravity current consisting of granular material. In an avalanche, lots of material or mixtures of different types of material fall or slide rapidly under the force of gravity. Avalanches are often classified by the size or severity of consequences resulting from the event.\n\nAn earthquake is the sudden relof energy stored as lithospheric stress that radiates seismic waves. At the Earth's surface, earthquakes may manifest with a shaking or displacement of the ground; when the earthquake occurs on the seafloor, the resulting displacement of water can sometimes result in a tsunami. Most of the world's earthquakes (90%, and 81% of the largest) take place in the 40,000-km-long, horseshoe-shaped zone called the circum-Pacific seismic belt, also known as the Pacific Ring of Fire, which for the most part bounds the Pacific Plate. Many earthquakes happen each day, few of which are large enough to cause significant damage.\n\nCoastal erosion is a physical process by which shorelines in coastal areas around the world shift and change, primarily in response to waves and currents that can be influenced by tides and storm surge. Coastal erosion can result from long-term processes (see also beach evolution) as well as from episodic events such as tropical cyclones or other severe storm events.\n\nA lahar is a type of natural event closely related to a volcanic eruption, and involves a large amount of material originating from an eruption of a glaciated volcano, including mud from the melted ice, rock, and ash sliding down the side of the volcano at a rapid pace. These flows can destroy entire towns in seconds and kill thousands of people, and form flood basalt.\nThis is based on natural events.\n\nA landslide is a mass displacement of sediment, usually down a slope. It can be caused by pressure pulling natural objects down a declining hill.\n\nA sinkhole is a localized depression in the surface topography, usually caused by the collapse of a subterranean structure such as a cave. Although rare, large sinkholes that develop suddenly in populated areas can lead to the collapse of buildings and other structures.\n\nA volcanic eruption is the point in which a volcano is active and releases its power, and the eruptions come in many forms. They range from daily small eruptions which occur in places like Kilauea in Hawaii, to megacolossal eruptions (where the volcano expels at least 1,000 cubic kilometers of material) from supervolcanoes like Lake Taupo (26,500 years ago) and Yellowstone Caldera. According to the Toba catastrophe theory, 70 to 75 thousand years ago, a supervolcanic event at Lake Toba reduced the human population to 10,000 or even 1,000 breeding pairs, creating a bottleneck in human evolution. Some eruptions form pyroclastic flows, which are high-temperature clouds of ash and steam that can travel down mountainsides at speed exceeding an airliner.\n\nA blizzard is a severe winter storm with icy and windy conditions characterized by low temperature, strong wind and heavy snow.\n\nA drought is a period of below-average precipitation in a given region, resulting in prolonged shortages in the water supply, whether atmospheric, surface water or ground water.\nScientists warn that global warming and climate change may result in more extensive droughts in coming years. These extensive droughts are likely to occur within the African continent due to its very low precipitation levels and high temperatures.\n\nA hailstorm is a natural hazard where a thunderstorm produces numerous hailstones which damage the location in which they fall. Hailstorms can be especially devastating to farm fields, ruining crops and damaging equipment.\n\nA heat wave is a hazard characterized by heat which is considered extreme and unusual in the area in which it occurs. Heat waves are rare and require specific combinations of weather events to take place, and may include temperature inversions, katabatic winds, or other phenomena. There is potential for longer-term events causing global warming, including stadial events (the opposite to glacial \"ice age\" events), or through human-induced climatic warming.\n\nCyclone is a large scale air mass that rotates around a strong center of low atmospheric pressure.\n\nHurricane, tropical cyclone, and typhoon are different names for the cyclonic storm system that forms over the oceans. It is caused by evaporated water that comes off of the ocean and becomes a storm. The Coriolis effect causes the storms to spin.. \"Hurricane\" is used for these phenomena in the Atlantic and eastern Pacific Oceans, \"tropical cyclone\" in the Indian, and \"typhoon\" in the western Pacific.\n\nAn ice storm is a particular weather event in which precipitation falls as ice, due to atmosphere conditions. It causes damage.\n\nA tornado is a natural disaster resulting from a thunderstorm. Tornadoes are violent, rotating columns of air which can blow at speeds between and , and possibly higher. Tornadoes can occur one at a time, or can occur in large tornado outbreaks associated with supercells or in other large areas of thunderstorm development. Waterspouts are tornadoes occurring over tropical waters in light rain conditions.\n\nClimate change can increase or decrease weather hazards, and also directly endanger property due to sea level rise and biological organisms due to habitat destruction.\n\nGeomagnetic storms can disrupt or damage technological infrastructure, and disorient species with magnetoception.\n\nA flood results from an overflow of water beyond its normal confines of a body of water such as a lake, or the accumulation of water over land areas.\n\nWildfire is a fire that burns in an uncontrolled and unplanned manner. Wildfires can result from natural occurrences such as lightning strikes or from human activity.\n\nDisease is a natural hazard that can be enhanced by human factors such as urbanization or poor sanitation. Disease affecting multiple people can be termed an outbreak or epidemic.\n\nIn some cases, a hazard exists in that a human-made defense against disease could fail, for example through antibiotic resistance.\n\nEach of the natural hazard types outlined above have very different characteristics, in terms of the spatial and temporal scales they influence, hazard frequency and return period, and measures of intensity and impact. These complexities result in \"single-hazard\" assessments being commonplace, where the hazard potential from one particular hazard type is constrained. In these examples, hazards are often treated as isolated or independent. An alternative is a \"multi-hazard\" approach which seeks to identify all possible natural hazards and their interactions or interrelationships.\n\nMany examples exist of one natural hazard triggering or increasing the probability of one or more other natural hazards. For example, an earthquake may trigger landslides, whereas a wildfire may increase the probability of landslides being generated in the future. A detailed review of such interactions across 21 natural hazards identified 90 possible interactions, of varying likelihood and spatial importance. There may also be interactions between these natural hazards and anthropic processes. For example, groundwater abstraction may trigger groundwater-related subsidence.\n\nEffective hazard analysis in any given area (e.g., for the purposes of disaster risk reduction) should ideally include an examination of all relevant hazards and their interactions. To be of most use for risk reduction, hazard analysis should be extended to risk assessment wherein the vulnerability of the built environment to each of the hazards is taken into account. This step is well developed for seismic risk, where the possible effect of future earthquakes on structures and infrastructure is assessed, as well as for risk from extreme wind and to a lesser extent flood risk. For other types of natural hazard the calculation of risk is more challenging, principally because of the lack of functions linking the intensity of a hazard and the probability of different levels of damage (fragility curves). \"ThinkHazard!\" is an online tool that provides an overview of the hazards from eight natural hazards (river floods, earthquakes, water scarcity, cyclones, coastal floods, tsunamis, volcanoes and landslides) developed by the Global Facility for Disaster Reduction and Recovery in partnership with other institutions.\n\nIn 2000, the United Nations launched the International Early Warning Programme to address the underlying causes of vulnerability and to build disaster-resilient communities by promoting increased awareness of the importance of disaster risk reduction as an integral component of sustainable development, with the goal of reducing human, economic and environmental losses due to hazards of all kinds (UN/ISDR, 2000). \n\nThe 2006-2007 United Nations International Disaster Reduction Day theme was “Disaster reduction education begins in school”. The Foundation of Public Safety Professionals launched an international campaign with an international open essay or documentary competition.\n\n\n"}
{"id": "412846", "url": "https://en.wikipedia.org/wiki?curid=412846", "title": "Naturalistic pantheism", "text": "Naturalistic pantheism\n\nNaturalistic pantheism is a kind of pantheism. It has been used in various ways such as to relate God or divinity with concrete things, determinism, or the substance of the Universe. God, from these perspectives, is seen as the aggregate of all unified natural phenomena. The phrase has often been associated with the philosophy of Baruch Spinoza, although academics differ on how it is used.\n\nThe term “pantheism\" is derived from Greek words \"pan\" (Greek: πᾶν) meaning \"all\" and \"theos\" (θεός) meaning God. It was coined by Joseph Raphson in his work \"De spatio reali\", published in 1697. The term was introduced to English by Irish writer John Toland in his 1705 work \"Socinianism Truly Stated, by a pantheist\" that described pantheism as the \"opinion of those who believe in no other eternal being but the universe.\"\n\nThe term \"naturalistic\" derives from the word \"naturalism\", which has several meanings in philosophy and aesthetics. In philosophy the term frequently denotes the view that everything belongs to the world of nature and can be studied with the methods appropriate for studying that world, \"i.e.\" the sciences. It generally implies an absence of belief in supernatural beings.\n\nJoseph Needham, a modern British scholar of Chinese philosophy and science, has identified Taoism as \"a naturalistic pantheism which emphasizes the unity and spontaneity of the operations of Nature.\" This philosophy can be dated to the late 4th century BCE.\n\nThe Hellenistic Greek philosophical school of Stoicism (which started in the early 3rd century BCE) rejected the dualist idea of the separate ideal/conscious and material realms, and identified the substance of God with the entire cosmos and heaven. However, not all philosophers who did so can be classified as naturalistic pantheists.\n\nNaturalistic pantheism was expressed by various thinkers, including Giordano Bruno, who was burned at the stake for his views. However, the 17th century Dutch philosopher Spinoza became particularly known for it.\n\nPossibly drawing upon the ideas of Descartes,\nBaruch Spinoza connected God and Nature through the phrase \"deus sive natura\" (\"God, or Nature\"), making him the father of classical pantheism. He relied upon rationalism rather than the more intuitive approach of some Eastern traditions.\n\nSpinoza's philosophy, sometimes known as Spinozism, has been understood in a number of ways, and caused disagreements such as the Pantheism controversy. However, many scholars have considered it to be a form of naturalistic pantheism. This has included viewing the pantheistic unity as natural. \nOthers focus on the deterministic aspect of naturalism.\nSpinoza inspired a number of other pantheists, with varying degrees of idealism towards nature. However, Spinoza's influence in his own time was limited.\nScholars have considered Spinoza the founder of a line of naturalistic pantheism, though not necessarily the only one.\n\nIn 1705 the Irish writer John Toland endorsed a form of pantheism in which the God-soul is identical with the material universe.\n\nGerman naturalist Ernst Haeckel (1834–1919) proposed a monistic pantheism in which the idea of God is identical with that of nature or substance.\n\nThe World Pantheist Movement, started in 1999, describes Naturalistic Pantheism as including reverence for the universe, realism, strong naturalism, and respect for reason and the scientific method as methods of understanding the world. Paul Harrison considers its position the closest modern equivalent to Toland's.\n\n"}
{"id": "18254861", "url": "https://en.wikipedia.org/wiki?curid=18254861", "title": "Naturalization of intentionality", "text": "Naturalization of intentionality\n\nAccording to Franz Brentano, intentionality refers to the \"aboutness of mental states that cannot be a physical relation between a mental state and what is about (its object) because in a physical relation each of the relata must exist whereas the objects of mental states might not.\n\nSeveral problems arise for features of intentionality, which are unusual for materialistic relations. Representation is unique. When 'x represents y' is true, it is not the same as other relations between things, like when 'x is next to y' or when 'x caused y' or when 'x met y', etc. Representation is different because, for instance, when 'x represents y' is true, y need not exist. This isn't true when say 'x is the square root of y' or 'x caused y' or 'x is next to y'. Similarly, when 'x represents y' is true, 'x represents z' can still be false, even when y = z. Intentionality encompasses relations that are both physical and mental. In this case, \"Billy can love Santa and Jane can search for unicorns even if Santa does not exist and there are no unicorns.\"\n\nFranz Brentano, the nineteenth century philosopher, spoke of mental states as involving presentations of the objects of our thoughts. This idea encompasses his belief that one cannot desire something unless they actually have a representation of it in their minds.\n\nDennis Stampe was one of the first philosophers in modern times to suggest a theory of content according to which content is a matter of reliable causes.\n\nFred Dretskes book, \"Knowledge and the Flow of Information\" (1981), was a major influence on the development of informational theories, and although the theory developed there is not a teleological theory, Dretske (1986, 1988, 1991) later produced an informational version of teleosemantics. He begins with a concept of carrying information that he calls \"indicating\", explains that indicating is not equivalent to representing, and then suggests that a representation's content is what it has the function of indicating\n\nTeleosemantics, also known as biosemantics, is used to refer to the class of theories of mental content that use a teleological notion of function. Teleosemantics is best understood as a general strategy for underwriting the normative nature of content, rather than any particular theory. What all teleological theories have in common is the idea that semantic norms are ultimately derivable from functional norms.\n\nAttempts to naturalize semantics began in the late 1970s. Many attempts were and still are being made to bring natural-physical explanations to bear on minds and, specifically, to the question of how minds acquire content. This is an interesting question; it is no surprise that it takes center stage in the philosophy of mind. Indeed, it is certainly an interesting question how minds, thought by those in the natural camp to be \"natural physical objects\", could have developed intentional properties. In the mid-1980s, with the works of Ruth Millikan and David Papineau (Language, Thought, and Other Biological Categories and \"Representation and Explanation\", respectively) teleosemantics, a theory of mental content that attempts to address the question of content and intentionality of minds, was born.\n\nRuth Millikan is perhaps the most vocal supporter of the teleosemantic program. Millikan's view differs from other teleosemantic views in myriad ways, but perhaps its most unusual characteristic is its distinction between the mechanisms that produce mental representations from those that consume mental representations. There is a representational function as a whole, at a composite level; and there are two \"sub-functions\", the producer-function and the consumer-function. In terms that are easy to understand, let's take Millikan's own example of beavers splashing their tails. One beaver alerts other beavers to the presence of danger by splashing its tail on the surface of water. The splashing of the tail tells, or represents to, the other beavers that there is danger in the environment, and the other beavers dip into the water to avoid the danger. The splashing of the beaver's tail produces a representation; the other beavers consume the representation. The representation that the beavers consume guides their behavior in ways that relate to their survival.\n\nOf course, the foci of the teleosemantic program is internal representations, and not just representational states of affairs between two (or more) distinct, external entities. How does the picture of the producer and consumer beavers, for instance, play into a story about internal representations? Papineau and Macdonald describe Millikan's account of this well and loyally, saying \"The producing mechanisms will be the sensory and other cerebral mechanisms that give rise to cognitive representations.\" The consuming mechanisms are those that \"use these representations to direct behavior in pursuit of some biological end\". Here, we have a picture similar to the beaver example, but this picture portrays the two sub-functions, producer and consumer, operating within a more-obviously unified system, namely, the cognitive system. One sub-function produces mental representations while the other sub-function consumes them in order to reach some end, e.g., danger-avoidance or food-acquisition. The representations consumed by the consumer sub-function guide an organism's behavior toward some biological end, e.g., survival. This is a rather brief sketch of Millikan's overall portrait. Of course, more goes into her account of the relation between producer- and consumer-functions in order to arrive at a nuanced account of mental representation. But that is a matter of how. Details as to the how aside, much of Millikan's efforts are directed towards the why, viz., why it is that perceivers like us have mental representations—why representations are produced in the first place.\n\nThe theory of asymmetric dependence, from Fodor, who says that his theory \"distinguishes merely informational relations on the basis of their higher-order relations to each other: informational relations depend upon representational relations, but not vice versa. He gives an example of this theory when he says, \"if tokens of a mental state type are reliably caused by horses, cows-on-dark-nights, zebras-in-the-mist and Great Danes, then they carry information about horses, etc. If however, such tokens are caused by cows-on-dark-nights, etc. because they are caused by horses, but not vice versa, then they represent horses (or property horse).\n\n20th century American philosopher Willard Van Orman Quine believed that linguistic terms do not have distinct meanings that accompany them because there are no such entities as \"meanings\". In his books, \"Word and Object\" (1960) and \"Ontological Relativity\" (1968), Quine considers the methods available to a field linguist attempting to translate an unknown language in order to outline his thesis. His thesis, the indeterminacy of translation, notes that there are many different ways to distribute purpose and meanings among words. Whenever a theory of translation is made it is commonly based upon context. An argument over the correct translation of an unidentified term depends on the possibility that the native could have spoken a different sentence. The same problem of indeterminacy would appear in this argument once again since any hypothesis can be defended if one adopts enough compensatory hypotheses about other parts of the language. Quine uses as an example the word \"gavagai\" spoken by a native upon seeing a rabbit. One can go the simplest route and translate the word to \"Lo, a rabbit\", but other possible translations such as \"Lo, food\" or \"Let's go hunting\" are completely reasonable given what the linguist knows. Subsequent observations can rule out certain possibilities as well as questioning the natives. But this is only possible once the linguist has mastered much of the natives' grammar and vocabulary. This is a big problem because this can only be done on the basis of hypotheses derived from simpler, observation-connected bits of language, which admit multiple interpretations, as we have seen.\n\nDaniel C. Dennett's theory of mental content, the intentional stance, tries to view the behavior of things in terms of mental properties. According to Dennett: \n\"Here is how it works: first you decide to treat the object whose behavior is to be predicted as a rational agent; then you figure out what beliefs that agent ought to have, given its place in the world and its purpose. Then you figure out what desires it ought to have, on the same considerations, and finally you predict that this rational agent will act to further its goals in the light of its beliefs. A little practical reasoning from the chosen set of beliefs and desires will in most instances yield a decision about what the agent ought to do; that is what you predict the agent will do.\"\n\nDennett's thesis has three levels of abstraction:\n\nDennett states that the more concrete the level, the more accurate is in principle our predictions. Though if one chooses to view an object through a more abstract level, he will gain greater computational power by getting a better overall picture of the object and skipping over any extraneous details. Also, switching to a more abstract level has its risks as well as its benefits. If we applied the intentional stance to a thermometer that was heated to 500 °C, trying to understand it through its beliefs about how hot it is and its desire to keep the temperature just right, we would gain no useful information. The problem would not be understood until we dropped down to the physical stance to comprehend that it has been melted. Whether to take a particular stance should be decided by how successful that stance is when applied. Dennett argued that it is best to understand human beliefs and desires at the level of the intentional stance.\n\n\n"}
{"id": "13806732", "url": "https://en.wikipedia.org/wiki?curid=13806732", "title": "Non-Evaporable Getter", "text": "Non-Evaporable Getter\n\nNon evaporable getters (NEG), based on the principle of metallic surface sorption of gas molecules, are mostly porous alloys or powder mixtures of Al, Zr, Ti, V and Fe. They help to establish and maintain vacuums by soaking up or bonding to gas molecules that remain within a partial vacuum. This is done through the use of materials that readily form stable compounds with active gases. They are important tools for improving the performance of many vacuum systems. Sintered onto the inner surface of high vacuum vessels, the NEG coating can be applied even to spaces that are narrow and hard to pump out, which makes it very popular in particle accelerators where this is an issue. The main sorption parameters of the kind of NEGs, like pumping speed and sorption capacity, have low limits.\nA different type of NEG, which is not coated, is the Tubegetter. The activation of these getters is accomplished mechanically or at a temperature from 550 K. The temperature range is from 0 to 800 K under HV/UHV conditions.\n\nThe NEG acts as a getter or getter pump that is able to reduce the pressure to less than 10 Pa.\n\nVideo: Non-Evaporable Getter (NEG) Operation\nFolder: TubeGetter\n\nIon pump (physics)\n"}
{"id": "314610", "url": "https://en.wikipedia.org/wiki?curid=314610", "title": "Pebble", "text": "Pebble\n\nA pebble is a clast of rock with a particle size of 2 to 64 millimetres based on the Krumbein phi scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter). A rock made predominantly of pebbles is termed a conglomerate. Pebble tools are among the earliest known man-made artifacts, dating from the Palaeolithic period of human history.\n\nA beach composed chiefly of surface pebbles is commonly termed a shingle beach. This type of beach has armoring characteristics with respect to wave erosion, as well as ecological niches that provide habitat for animals and plants.\n\nInshore banks of shingle (large quantities of pebbles) exist in some locations, such as the entrance to the River Ore, where the moving banks of shingle give notable navigational challenges.\n\nPebbles come in various colors and textures and can have streaks, known as veins, of quartz or other minerals. Pebbles are mostly smooth but, dependent on how frequently they come in contact with the sea, they can have marks of contact with other rocks or other pebbles. Pebbles left above the high water mark may have growths of organisms such as lichen on them, signifying the lack of contact with seawater.\n\nPebbles are found in two locations – on the beaches of various oceans and seas, and inland where ancient seas used to cover the land. When then the seas retreated, the rocks became landlocked. They can also be found in lakes and ponds. Pebbles can also form in rivers, and travel into estuaries where the smoothing continues in the sea.\n\nBeach pebbles and river pebbles (also known as river rock) are distinct in their geological formation and appearance.\n\nBeach pebbles form gradually over time as the ocean water washes over loose rock particles. The result is a smooth, rounded appearance. The typical size range is from 2 mm to 50 mm. The colors range from translucent white to black, and include shades of yellow, brown, red and green. Some of the more plentiful pebble beaches are found along the coast of the Pacific Ocean, beginning in the United States and extending down to the tip of South America in Argentina. Other pebble beaches are found in northern Europe (particularly on the beaches of the Norwegian Sea), along the coast of the U.K. and Ireland, on the shores of Australia, and around the islands of Indonesia and Japan.\n\nInland pebbles (river pebbles of river rock) are usually found along the shores of large rivers and lakes. These pebbles form as the flowing water washes over rock particles on the bottom and along the shores of the river. The smoothness and color of river pebbles depends on several factors, such as the composition of the soil of the river banks, the chemical characteristics of the water, and the speed of the current. Because river current is gentler than the ocean waves, river pebbles are usually not as smooth as beach pebbles. The most common colors of river rock are black, grey, green, brown and white.\n\nBeach pebbles and river pebbles are used for a variety of purposes, both outdoors and indoors. They can be sorted by colour and size, and they can also be polished to improve the texture and colour. Outdoors, beach pebbles are often used for landscaping, construction and as decorative elements. Beach pebbles are often used to cover walkways and driveways, around pools, in and around plant containers, on patios and decks. Beach and river pebbles are also used to create water-smart gardens in areas where water is scarce. Small pebbles are also used to create living spaces and gardens on the rooftops of buildings. Indoors, pebbles can be used as bookends and paperweights. Large pebbles are also used to create \"pet rocks\" for children.\n\nOn Mars, slabs of pebbly conglomerate rock have been found and have been interpreted by scientists as having formed in an ancient streambed. The gravels, which were discovered by NASA's Mars rover Curiosity, range from the size of sand particles to the size of golf balls. Analysis has shown that the pebbles were deposited by a stream that flowed at walking pace and was ankle- to hip-deep.\n\n"}
{"id": "226187", "url": "https://en.wikipedia.org/wiki?curid=226187", "title": "Prandtl–Glauert singularity", "text": "Prandtl–Glauert singularity\n\nThe Prandtl–Glauert singularity is the prediction by the Prandtl–Glauert transformation that infinite pressures would be experienced by an aircraft as it approaches the speed of sound. Because it is invalid to apply the transformation at these speeds, the predicted singularity does not emerge. This is related to the early-20th-century misconception of the impenetrability of the sound barrier.\n\nThe Prandtl–Glauert transformation assumes linearity (i.e. a small change will have a small effect that is proportional to its size). This assumption becomes inaccurate at high Mach numbers and is entirely invalid in places where the flow reaches supersonic speeds, since sonic shock waves are instantaneous (and thus manifestly non-linear) changes in the flow. Indeed, one assumption in the Prandtl–Glauert transformation is approximately constant Mach number throughout the flow, and the increasing slope in the transformation indicates that very small changes will have a very strong effect at higher Mach numbers, thus violating the assumption, which breaks down entirely at the speed of sound.\n\nThis means that the singularity featured by the transformation near the sonic speed (\"M=1\") is not within the area of validity. The aerodynamic forces are calculated to approach infinity at the so-called \"Prandtl–Glauert singularity\"; in reality, the aerodynamic and thermodynamic perturbations do get amplified strongly near the sonic speed, but they remain finite and a singularity does not occur. The Prandtl–Glauert transformation is a linearized approximation of compressible, inviscid potential flow. As the flow approaches sonic speed, the nonlinear phenomena dominate within the flow, which this transformation completely ignores for the sake of simplicity.\n\nThe Prandtl–Glauert transformation is found by linearizing the potential equations associated with compressible, inviscid flow. For two-dimensional flow, the linearized pressures in such a flow are equal to those found from incompressible flow theory multiplied by a correction factor. This correction factor is given below:\n\nwhere\n\nThis formula is known as \"Prandtl's rule\", and works well up to low-transonic Mach numbers (M < ~0.7). However, note the limit:\n\nformula_2\n\nThis obviously nonphysical result (of an infinite pressure) is known as the Prandtl–Glauert singularity.\n\nThe reason for the condensation cloud that is being observed is that humid air is entering a low-pressure region, which also reduces local density and temperature sufficiently to cause condensation. The vapour vanishes as soon as the pressure increases again to ambient levels.\nIn the case of objects at transonic speeds, the pressure increase happens at the shock wave location.\nCondensation in free flows does not require supersonic flow. Given a humidity high enough, it can happen in purely subsonic flow over a wing or in the cores of wing tip and other vortices. This can often be observed on humid days on aircraft approaching an airport.\n\n"}
{"id": "30635101", "url": "https://en.wikipedia.org/wiki?curid=30635101", "title": "Regius Professor of Natural History (Aberdeen)", "text": "Regius Professor of Natural History (Aberdeen)\n\nThe Regius Professor of Natural History is a Regius Professorship at the University of Aberdeen in Scotland. It was originally called the Regius Professor of Civil and Natural History at Marischal College until in 1860 Marischal College and King's Colleges merged to form the University of Aberdeen, and the title changed to Natural History.\n"}
{"id": "167742", "url": "https://en.wikipedia.org/wiki?curid=167742", "title": "Scientism", "text": "Scientism\n\nScientism is an ideology that promotes science as the purportedly objective means by which society should determine normative and epistemological values. The term \"scientism\" is generally used critically, pointing to the cosmetic application of science in unwarranted situations not amenable to application of the scientific method or similar scientific standards. \n\nIn philosophy of science, the term \"scientism\" frequently implies a critique of the more extreme expressions of logical positivism and has been used by social scientists such as Friedrich Hayek, philosophers of science such as Karl Popper, and philosophers such as Hilary Putnam and Tzvetan Todorov to describe (for example) the dogmatic endorsement of scientific methodology and the reduction of all knowledge to only that which is measured or confirmatory.\n\nMore generally, scientism is often interpreted as science applied \"in excess\". The term \"scientism\" has two senses:\n\n\nIt is also sometimes used to describe universal applicability of the scientific method and approach, and the view that empirical science constitutes the most authoritative worldview or the most valuable part of human learning—to the complete exclusion of other viewpoints, such as historical, philosophical, economic or cultural worldviews. It has been defined as \"the view that the characteristic inductive methods of the natural sciences are the only source of genuine factual knowledge and, in particular, that they alone can yield true knowledge about man and society\". The term \"scientism\" is also used by historians, philosophers, and cultural critics to highlight the possible dangers of lapses towards excessive reductionism in all fields of human knowledge.\n\nFor social theorists in the tradition of Max Weber, such as Jürgen Habermas and Max Horkheimer, the concept of scientism relates significantly to the philosophy of positivism, but also to the cultural rationalization for modern Western civilization. British novelist Sara Maitland has called scientism a \"myth as pernicious as any sort of fundamentalism.\"\n\nReviewing the references to scientism in the works of contemporary scholars, Gregory R. Peterson detects two main broad themes:\n\nThe term \"scientism\" was popularized by the Nobel Prize winner F.A. Hayek, who defined it as the \"slavish imitation of the method and language of Science\". Karl Popper defines scientism as \"the aping of what is widely mistaken for the method of science\".\n\nMikael Stenmark proposes the expression \"scientific expansionism\" as a synonym of scientism. In the \"Encyclopedia of science and religion\", he writes that, while the doctrines that are described as scientism have many possible forms and varying degrees of ambition, they share the idea that the boundaries of science (that is, typically the natural sciences) could and should be expanded so that something that has not been previously considered as a subject pertinent to science can now be understood as part of science (usually with science becoming the sole or the main arbiter regarding this area or dimension).\n\nAccording to Stenmark, the strongest form of scientism states that science has no boundaries and that all human problems and all aspects of human endeavor, with due time, will be dealt with and solved by science alone. This idea has also been called the Myth of Progress.\n\nE. F. Schumacher, in his \"A Guide for the Perplexed\", criticized scientism as an impoverished world view confined solely to what can be counted, measured and weighed. \"The architects of the modern worldview, notably Galileo and Descartes, assumed that those things that could be weighed, measured, and counted were more true than those that could not be quantified. If it couldn't be counted, in other words, it didn't count.\"\n\nIntellectual historian T.J. Jackson Lears argues there has been a recent reemergence of \"nineteenth-century positivist faith that a reified 'science' has discovered (or is about to discover) all the important truths about human life. Precise measurement and rigorous calculation, in this view, are the basis for finally settling enduring metaphysical and moral controversies.\" Lears specifically identifies Harvard psychologist Steven Pinker's work as falling in this category. Philosophers John N. Gray and Thomas Nagel have leveled similar criticisms against popular works by moral psychologist Jonathan Haidt, neuroscientist Sam Harris, and writer Malcolm Gladwell.\n\nGenetic biologist Austin L. Hughes wrote in conservative journal \"The New Atlantis\" that scientism has much in common with superstition: \"the stubborn insistence that something...has powers which no evidence supports.\"\n\nSeveral scholars use the term to describe the work of vocal critics of religion-as-such. Individuals associated with New Atheism have garnered this label from both religious and non-religious scholars. Theologian John Haught argues that Daniel Dennett and other new atheists subscribe to a belief system of scientific naturalism, which holds the central dogma that \"only nature, including humans and our creations, is real: that God does not exist; and that science alone can give us complete and reliable knowledge of reality.\" Haught argues that this belief system is self-refuting since it requires its adherents to assent to beliefs that violate its own stated requirements for knowledge. Christian Philosopher Peter Williams argues that it is only by conflating science with scientism that new atheists feel qualified to \"pontificate on metaphysical issues.\" Philosopher Daniel Dennett responded to religious criticism of his book \"\" by saying that accusations of scientism \"[are] an all-purpose, wild-card smear... When someone puts forward a scientific theory that [religious critics] really don't like, they just try to discredit it as 'scientism'. But when it comes to facts, and explanations of facts, science is the only game in town\".\n\nNon-religious scholars have also linked New Atheist thought with scientism. Atheist philosopher Thomas Nagel argues that neuroscientist Sam Harris conflates all empirical knowledge with that of scientific knowledge. Marxist literary critic Terry Eagleton argues that Christopher Hitchens possesses an \"old-fashioned scientistic notion of what counts as evidence\" that reduces knowledge to what can and cannot be proven by scientific procedure. Agnostic philosopher Anthony Kenny has also criticized New Atheist philosopher Alexander Rosenberg's \"The Atheist's Guide to Reality\" for resurrecting a self-refuting epistemology of logical positivism and reducing all knowledge of the universe to the discipline of physics.\n\nMichael Shermer, founder of The Skeptics Society, draws a parallel between scientism and traditional religious movements, pointing to the cult of personality that develops around some scientists in the public eye. He defines scientism as a worldview that encompasses natural explanations, eschews supernatural and paranormal speculations, and embraces empiricism and reason.\n\nThe Iranian scholar Seyyed Hossein Nasr has stated that in the Western world, many will accept the ideology of modern science, not as \"simple ordinary science\", but as a replacement for religion.\n\nGregory R. Peterson writes that \"for many theologians and philosophers, scientism is among the greatest of intellectual sins\".\n\nIn his essay \"Against Method\", Paul Feyerabend characterizes science as \"an essentially anarchic enterprise\" and argues emphatically that science merits no exclusive monopoly over \"dealing in knowledge\" and that scientists have never operated within a distinct and narrowly self-defined tradition. He depicts the process of contemporary scientific education as a mild form of indoctrination, aimed at \"making the history of science duller, simpler, more uniform, more 'objective' and more easily accessible to treatment by strict and unchanging rules.\"\n\nThomas M. Lessl argues that religious themes persist in what he calls scientism, the public rhetoric of science. There are two methodologies that illustrate this idea of scientism. One is the epistemological approach, the assumption that the scientific method trumps other ways of knowing and the ontological approach, that the rational mind reflects the world and both operate in knowable ways. According to Lessl, the ontological approach is an attempt to \"resolve the conflict between rationalism and skepticism\". Lessl also argues that without scientism, there would not be a scientific culture.\n\nPhilosopher of religion Keith Ward has said scientism is philosophically inconsistent or even self-refuting, as the truth of the statements \"no statements are true unless they can be proven scientifically (or logically)\" or \"no statements are true unless they can be shown empirically to be true\" cannot themselves be proven scientifically, logically, or empirically.\n\nIn the introduction to his collected oeuvre on the sociology of religion, Max Weber asks why \"the scientific, the artistic, the political, or the economic development [elsewhere]… did not enter upon that path of rationalization which is peculiar to the Occident?\" According to the distinguished German social theorist, Jürgen Habermas, \"For Weber, the intrinsic (that is, not merely contingent) relationship between modernity and what he called 'Occidental rationalism' was still self-evident.\" Weber described a process of rationalisation, disenchantment and the \"disintegration of religious world views\" that resulted in modern secular societies and capitalism.\nHabermas is critical of pure instrumental rationality, arguing that the \"Social Life–World\" is better suited to literary expression, the former being \"intersubjectively accessible experiences\" that can be generalized in a formal language, while the latter \"must generate an intersubjectivity of mutual understanding in each concrete case\":\n\n\n"}
{"id": "58206657", "url": "https://en.wikipedia.org/wiki?curid=58206657", "title": "Self-sealing suction cup", "text": "Self-sealing suction cup\n\nThe self-sealing suction cup is a suction cup that exerts a suction force only when it is in physical contact with an object. Unlike most other suction cups, it does not exert any suction force when it is not in contact with an object. Its grasping ability is achieved entirely through passive means without the use of sensors, valves, or actuators.\n\nIt was designed so that, when used as part of a suction cup array, the suction cups that don’t come in contact with the object remain sealed. By having only the suction cups that are in direct contact of the object to exhibit suction force, the researchers were able to minimize leak points where air could enter and increase the pressure that each active cup receives, maximizing the suction force. As a result, an array of self-sealing suction cups can grasp and pick up a wide range of object sizes and shapes. This comes in contrast to conventional suction cups that are typically designed for one specific object size and geometry. In addition, suction cups of various sizes have been manufactured, ranging from the palm of a hand to the point of a fingertip.\n\nThe self-sealing suction cup was first developed in 2010 by a collaboration of researchers from the U.S. Army Research Laboratory (ARL), the Edgewood Chemical Biological Center at Aberdeen Proving Ground, and the University of Maryland.\n\nThe design of the self-sealing suction cup was initially inspired by the suckers of the octopus and its ability to pick up different sized items by individually actuating its suction cups based on the item’s size and physical features.\n\nThe internal geometry of the self-sealing suction cup was designed to the smallest possible size and features a minimum wall thickness of 1.02 mm, a tube diameter of 1.59 mm, and minimum part spacing of 0.13 mm. The suction cup incorporates a mix of rubber and plastic components, where the cup lip, base, tube, springs, and plug are made out of soft rubber while the cup side, collar, hinges, and flange are made out of plastic. As part of its design, a central vacuum pump can be used to maximize the suction force of the suction cup. A multi-material 3D printer was used to create the prototype of the self-sealing suction cup in about 20 minutes.\n\nInside the self-sealing suction cup, the plug is positioned close to the tube opening so that it can get sucked into the tube seal the hole when the central suction line is powered. A pair of springs connected to the suction cup’s base helps maintain the plug’s position, restoring the plug seal in the absence of object forces. If the cup makes contact with an object, a hinge action raises the plug away from the suction tube. The moment the cup’s lips are pushed against the object, the passive reaction forces from the cup lips are transferred to the rubber base of the cup, which stretches over the collar and allow the structure to compress. Acting as a pivot for the hinges, the collar causes the hinges to rotate and the edges of the hinges slide along the underside of the flange and raise the plug away from the suction tube opening. As a result, the suction cup self-seals when not in contact with an object and self-opens the cup’s lips makes contacts with an object.\n\nIn 2015, several improvements were made to the design of the self-sealing suction cup to improve its grasping capabilities. The previous design demonstrated the following flaws:\n\n\nTo address these flaws, researchers from ARL decreased the number of components by consolidating the functions of several parts, which reduced the uncompressed height of the suction cup by almost 50% to 0.72 cm. The cup diameter was also reduced to 1.07 cm. A lever system was added to the base of the cup, which pivots the collar to lift the plug. In addition, the tube doubles as a spring, which helps restore the levers and the plug to their closed position. A plastic restraint was added around the cup to aid with handling the hyper-extension, shear, and torsional forces.\n\nThe self-sealing suction cup has been subjected to a series of tests to determine the quality of its performance. A flexible test rig with four dime-sized suction cups and plastic ribs connected with rubber tubes was created for force-displacement and testing.\n\nA force-displacement test that compared the performance between the self-sealing suction cup, an identical suction cup, and a commercially available suction cup found that the internal structures of the self-sealing cup allowed more force to be exerted for the same displacement compared to the other cups. However, under identical conditions, the self-sealing cup achieved a maximum force of 12.5 N while the commercially available cup achieved a maximum force of 12.9 N.\n\nA seal quality test measured the pressure generated from each self-sealing suction cup. The results showed that an array of four cups maintained a pressure of 93.8% atmospheric. The test also demonstrated that not all the cups were equally efficient at sealing after object contact. However, this could be the result of variation in the cups’ prior usage.\n\nDuring object grasping testing where the grasping range was examined, the test rig successfully grasped about 80% of the objects attempted. These items consisted of the following: TV remote, pill bottle, glue stick, eyeglasses, fork, disposable bottle, toothpaste, coffee mug, bowl, plate, book, cell phone, bar of soap, paper money, mail, keys, show, table knife, medicine box, credit card, coin, pillow, hairbrush, non-disposable bottle, wallet, magazine, soda can, newspaper, scissors, wrist watch, purse, lighter, compact disc, telephone receiver, full wine bottle, full wine glass, light bulb, lock, padded volleyball, wooden block. (4) As a demonstration of the cups’ strength, the ARL researchers were able to pick up a full bottle of wine using only four of the dime-sized suction cups.\n\nThe self-sealing suction cups have been incorporated in robots to improve their passive grasping capabilities. Due to the design of the suction cups, a central vacuum source can be used to effectively generate suction force from the cups and reduce the number of actuators and sensors for the robot.\n\nResearchers from ARL designed and developed a three-finger hand actuator system using a 3D printer in order for the robot to properly utilize the self-sealing suction cups. Four suction cups run along the bottom of each finger, which contains a narrow vacuum channel running through the center. A central vacuum pump serves to power the suction cups and facilitate grasping. The fingers can also curl around the object to better grasp it and release any object in its hold by feeding back the output of the vacuum pump and emitting a burst of positive pressure.\n\nThe three-finger hand has been used by aerial systems and has demonstrated considerable success in grasping objects on the ground while maintaining flight. According to ARL researchers, the self-sealing suction cups may exhibit higher rates of success underwater due to the extra pressure from the sea depths surrounding and pressing against the object and grasper. However, they noted that an underwater environment would require different manufacturing materials that would allow the suction cups to perform well in salt water, such as a thermal plastic.\n"}
{"id": "2071938", "url": "https://en.wikipedia.org/wiki?curid=2071938", "title": "Seven Natural Wonders", "text": "Seven Natural Wonders\n\nSeven Natural Wonders was a television series that was broadcast on BBC Two from 3 May to 20 June 2005. The programme took an area of England each week and, from votes by the people living in that area, showed the 'seven natural wonders' of that area in a programme.\n\nThe programmes were:\n\nThe series covered eight regions of England, having originated as a 'local' television project.\n\nThere was also a series, looking at a similar selection of 'man-made' wonders for each of eleven regions of England.\n\n"}
{"id": "41816267", "url": "https://en.wikipedia.org/wiki?curid=41816267", "title": "Seven Natural Wonders of Africa", "text": "Seven Natural Wonders of Africa\n\nThe Seven Natural Wonders of Africa was a competition where the seven were selected by voting on February 11, 2013.\n\n"}
{"id": "20847621", "url": "https://en.wikipedia.org/wiki?curid=20847621", "title": "Universal Darwinism", "text": "Universal Darwinism\n\nUniversal Darwinism (also known as generalized Darwinism, universal selection theory,\nor Darwinian metaphysics) refers to a variety of approaches that extend the theory of Darwinism beyond its original domain of biological evolution on Earth. Universal Darwinism aims to formulate a generalized version of the mechanisms of variation, selection and heredity proposed by Charles Darwin, so that they can apply to explain evolution in a wide variety of other domains, including psychology, economics, culture, medicine, computer science and physics.\n\nAt the most fundamental level, Charles Darwin's theory of evolution states that organisms evolve and adapt to their environment by an iterative process. This process can be conceived as an evolutionary algorithm that searches the space of possible forms (the fitness landscape) for the ones that are best adapted. The process has three components:\n\n\nAfter those fit variants are retained, they can again undergo variation, either directly or in their offspring, starting a new round of the iteration. The overall mechanism is similar to the problem-solving procedures of trial-and-error or generate-and-test: evolution can be seen as searching for the best solution for the problem of how to survive and reproduce by generating new trials, testing how well they perform, eliminating the failures, and retaining the successes.\n\nThe generalization made in \"universal\" Darwinism is to replace \"organism\" by any recognizable pattern, phenomenon, or system. The first requirement is that the pattern can \"survive\" (maintain, be retained) long enough or \"reproduce\" (replicate, be copied) sufficiently frequently so as not to disappear immediately. This is the heredity component: the information in the pattern must be retained or passed on. The second requirement is that during survival and reproduction variation (small changes in the pattern) can occur. The final requirement is that there is a selective \"preference\" so that certain variants tend to survive or reproduce \"better\" than others. If these conditions are met, then, by the logic of natural selection, the pattern will evolve towards more adapted forms.\n\nExamples of patterns that have been postulated to undergo variation and selection, and thus adaptation, are genes, ideas (memes), theories, technologies, neurons and their connections, words, computer programs, firms, antibodies, institutions, law and judicial systems, quantum states and even whole universes.\n\nConceptually, \"evolutionary theorizing about cultural, social, and economic phenomena\" preceded Darwin, but was still lacking the concept of natural selection. Darwin himself, together with subsequent 19th century thinkers such as Herbert Spencer, Thorstein Veblen, James Mark Baldwin and William James, was quick to apply the idea of selection to other domains, such as language, psychology, society, and culture. However, this evolutionary tradition was largely banned from the social sciences in the beginning of the 20th century, in part because of the bad reputation of social Darwinism, an attempt to use Darwinism to justify social inequality.\n\nStarting in the 1950s, Donald T. Campbell was one of the first and most influential authors to revive the tradition, and to formulate a generalized Darwinian algorithm directly applicable to phenomena outside of biology. In this, he was inspired by William Ross Ashby's view of self-organization and intelligence as fundamental processes of selection. His aim was to explain the development of science and other forms of knowledge by focusing on the variation and selection of ideas and theories, thus laying the basis for the domain of evolutionary epistemology. In the 1990s, Campbell's formulation of the mechanism of \"blind-variation-and-selective-retention\" (BVSR) was further developed and extended to other domains under the labels of \"universal selection theory\" or \"universal selectionism\" by his disciples Gary Cziko, Mark Bickhard, and Francis Heylighen.\n\nRichard Dawkins may have first coined the term \"universal Darwinism\" in 1983 to describe his conjecture that any possible life forms existing outside the solar system would evolve by natural selection just as they do on Earth. This conjecture was also presented in 1983 in a paper entitled “The Darwinian Dynamic” that dealt with the evolution of order in living systems and certain nonliving physical systems. It was suggested “that ‘life’, wherever it might exist in the universe, evolves according to the same dynamical law” termed the Darwinian dynamic. Henry Plotkin in his 1997 book on Darwin machines makes the link between universal Darwinism and Campbell's evolutionary epistemology. Susan Blackmore, in her 1999 book \"The Meme Machine\", devotes a chapter titled 'Universal Darwinism' to a discussion of the applicability of the Darwinian process to a wide range of scientific subject matters.\n\nThe philosopher of mind Daniel Dennett, in his 1995 book \"Darwin's Dangerous Idea\", developed the idea of a Darwinian process, involving variation, selection and retention, as a generic algorithm that is substrate-neutral and could be applied to many fields of knowledge outside of biology. He described the idea of natural selection as a \"universal acid\" that cannot be contained in any vessel, as it seeps through the walls and spreads ever further, touching and transforming ever more domains. He notes in particular the field of memetics in the social sciences.\n\nIn agreement with Dennett's prediction, over the past decades the Darwinian perspective has spread ever more widely, in particular across the social sciences as the foundation for numerous schools of study including memetics, evolutionary economics, evolutionary psychology, evolutionary anthropology, neural Darwinism, and evolutionary linguistics. Researchers have postulated Darwinian processes as operating at the foundations of physics, cosmology and chemistry via the theories of quantum Darwinism, observation selection effects and cosmological natural selection. Similar mechanisms are extensively applied in computer science in the domains of genetic algorithms and evolutionary computation, which develop solutions to complex problems via a process of variation and selection.\n\nAuthor D. B. Kelley has formulated one of the most all-encompassing approaches to Universal Darwinism. In his 2013 book \"The Origin of Everything\", he holds that natural selection involves not the preservation of favored races in the struggle for life, as shown by Darwin, but the preservation of favored systems in contention for existence. The fundamental mechanism behind all such stability and evolution is therefore what Kelley calls \"survival of the fittest systems.\" Because all systems are cyclical, the Darwinian processes of iteration, variation and selection are operative not only among species but among all natural phenomena both large-scale and small. Kelley thus maintains that, since the Big Bang especially, the universe has evolved from a highly chaotic state to one that is now highly ordered with many stable phenomena, naturally selected.\nThe following approaches can all be seen as exemplifying a generalization of Darwinian ideas outside of their original domain of biology. These \"Darwinian extensions\" can be grouped in two categories, depending on whether they discuss implications of biological (genetic) evolution in other disciplines (e.g. medicine or psychology), or discuss processes of variation and selection of entities other than genes (e.g. computer programs, firms or ideas). However, there is no strict separation possible, since most of these approaches (e.g. in sociology, psychology and linguistics) consider both genetic and non-genetic (e.g. cultural) aspects of evolution, as well as the interactions between them (see e.g. gene-culture coevolution).\n\n\n\n\n"}
{"id": "46333271", "url": "https://en.wikipedia.org/wiki?curid=46333271", "title": "Vertebrate land invasion", "text": "Vertebrate land invasion\n\nThe aquatic to terrestrial transition of vertebrate organisms occurred in the late Devonian era and was an important step in the evolutionary history of modern land vertebrates. The transition allowed animals to escape competitive pressure from the water and explore niche opportunities on land. Fossils from this period have allowed scientists to identify some of the species that existed during this transition, such as Tiktaalik and Acanthostega. Many of these species were also the first to develop adaptations suited to terrestrial over aquatic life, such as neck mobility and hindlimb locomotion.\n\nThe late Devonian vertebrate transition was preceded by the plant and invertebrate terrestrial invasion. These invasions allowed for the appropriate niche development that would ultimately facilitate the vertebrate invasion. While the late Devonian event was the first land invasion by vertebrate organisms, aquatic species have continued to develop adaptations suited to terrestrial life (and vice versa) from the late Devonian to the Holocene.\n\nThe vertebrate species that were important to the initial water to land transition can be qualified as being in one of five groups: Sarcopterygian fishes, prototetrapods, aquatic tetrapods, true tetrapods, and terrestrial tetrapods. Many morphological changes occurred throughout this transition. Mechanical support structures changed from fins to limbs, the method of locomotion changed from swimming to walking, respiratory structures changed from gills to lungs, feeding mechanisms changed from suction feeding to biting, and mode of reproduction changed from larval development to metamorphosis.\n\nLungfish appeared approximately 400 million years ago. It is a species that endured rapid evolution during the Devonian era, which became known as the dipnoan renaissance. The Acanthostega species, known as the fish with legs, is considered a tetrapod by structural findings but is postulated to have perhaps never left the aquatic environment. Its legs are not well-suited to support its weight. The bones of its forearm, the radius and ulna, are very thin at the wrist and also unable to support it on land. It also lacks a sacrum and strong ligaments at the hip, which would be integral to supporting the animal against gravity. In this sense, the species is considered a tetrapod but not one that has adapted well enough to walk on land. Furthermore, its gill bars have a supportive brace characterized for use as an underwater ear because it can pick up noise vibrations through the water. Tetrapods that adapted to terrestrial living adapted these gill bones to pick up sounds through air, and they later became the middle ear bones seen in mammalian tetrapods. Ichthyostega, on the other hand, is considered to be a fully terrestrial tetrapod that perhaps depended on water for its aquatic young. Comparisons between the skeletal features of Acanthostega and Ichthyostega reveal that they had different habits. Acanthostega is likely exclusive to an aquatic environment, while Ichthyostega is progressed in the aquatic to terrestrial transition by living dominantly on the shores.\n\nAn evolutionary timeline of the late Devonian vertebrate terrestrial invasion demonstrates the changes that took place. A group of fish from the Givetian stage began developing limbs, and eventually evolved into aquatic tetrapods in the Famennian stage. Pederpes, Westlothiana, Protogyrinus, and Crassigyrinus descended from these species into the carboniferous period and were the first land vertebrates.\n\nA particularly important transitional species is one known as Tiktaalik. It has a fin, but the fin has bones within it that are similar to mammalian tetrapods. It has an upper arm bone, a lower arm bone, forearm bones, a wrist, and fingerlike projections. Essentially, it is a fin that can support the animal. Similarly, it also has a neck that allows independent head movement from the body. Its ribs are also able to support the body in gravity. Its skeletal features exhibit its ability as a fish that can live in shallow water and also venture onto land.\n\nIt took many millions of years for vertebrates to transition out of water onto land. During this time, both the competitive pressures that would push species out of the water and the niche occupation incentives that would pull species onto land were slowly building. The culmination of these driving factors are what ultimately facilitated the vertebrate transition.\n\nScientists believe that a long period of time where biotic and abiotic factors in the aquatic environment were unfavourable to certain aquatic organisms is what pushed their transition to shallower waters. Some of these push factors are environmental hypoxia, unfavourable aquatic temperatures, and increased salinity. Other constantly present factors such as predation, competition, waterborne diseases and parasites also contributed to the transition.\n\nA theory put forth by Joseph Barrell possibly helps explain what may have initiated these push factors to become relevant in the late Devonian. The extensive oxidized sediments that were present in Europe and North America during the late Devonian are evidence of severe droughts during this time. These droughts would cause small ponds and lakes to dry out, forcing certain aquatic organisms to move on land to find other bodies of water. Natural selection on these organisms eventually led to the evolution of the first terrestrial vertebrates.\n\nThe pull factors were secondary to the push factors, and only became significant once the pressures to leave the aquatic environment became significant. These were largely the niches and opportunities that were available for exploitation in the terrestrial environment, and include higher environmental oxygen partial pressures, favourable temperatures, and the lack of competitors and predators on land. The plants and invertebrates that had preceded the vertebrate invasion also provided opportunities in the form of abundant prey and lack of predators.\n\nThere were many challenges that the first land vertebrates faced. These challenges allowed for rapid natural selection and niche domination, resulting in an adaptive radiation that produced many different vertebrate land species in a relatively short period of time.\n\nDepending on the water depth at which a species lives, the visual perception of many aquatic species is better suited to darker environments than those on land. Similarly, hearing in aquatic organisms is better optimized for sounds underwater, where the speed and amplitude of sound is greater than in air.\n\nHomeostasis was almost definitely a challenge for land invading vertebrates. Gas exchange and water balance are highly different in water and in air. Homeostasis mechanisms suitable for a terrestrial environment may have been necessary to develop before these organisms invaded land.\n\nThe primary anatomical barrier is the development of lungs for proper gas exchange, however other anatomical barriers also exist. The stressors of the musculoskeletal system are different in air than they are in water, and the muscles and bones must be strong enough to withstand the increased effects of gravity on land.\n\nMany behaviours, such as reproduction, are specifically optimized to a wet environment. Navigation and locomotion are also highly different in aquatic environments compared to terrestrial environments.\n\nThe ancestral species of tetrapods that lived entirely in water had tall and narrow skulls with eyes facing sideways and forwards to maximize visibility for predators and prey in the aquatic environment. As the ancestors of early tetrapods started inhabiting shallower waters, these species had flatter skulls with eyes at the tops of their heads, which made it possible to spot food above them. Once the tetrapods transitioned onto land, the lineages evolved to have tall and narrow skulls with eyes facing sideways and forwards again. This allowed them to navigate through the terrestrial environment and look for predators and prey.\n\nFish do not have necks, so the head is directly connected to the shoulders. In contrast, land animals use necks to move their heads so they can look down to see the food on the ground. The greater the mobility of the neck, the more visibility the land animal has. As lineages moved from completely aquatic environments to shallower waters and land, they gradually evolved vertebral columns that increased neck mobility. The first neck vertebra that evolved permitted the animals to have flexion and extension of the head so that they can see up and down. The second neck vertebra evolved to allow rotation of the neck for moving the head left and right. As tetrapod species continued to evolve on land, adaptations included seven or more vertebrae, allowing increasing neck mobility.\n\nThe sacrum connects the pelvis and hindlimbs and is useful for motion on land. The aquatic ancestors of tetrapods did not have a sacrum, so it was speculated to have evolved for locomotive function exclusive to terrestrial environments. However, the Acanthostega species is one of the earliest lineages to have a sacrum, even though it is a fully aquatic species. Once species moved onto land, the trait was adapted for terrestrial locomotion support, which is evidenced by additional vertebrae fusing similarly to permit additional support. This is an example of exaptation, where a trait performs a function that did not arise through natural selection for its current use.\n\nAs the lineages evolved to adapt to terrestrial environments, many lost traits that were better suited for the aquatic environment. Many lost their gills, which were only useful for obtaining oxygen in water. Their tail fins became smaller. They lost the lateral line system, a network of canals along the skull and jaw that are sensitive to vibration, which does not work outside of an aquatic environment.\n\nFor successful land invasion, the species had several pre-adaptations like air-breathing and limb-based locomotion. Aspects such as reproduction and swallowing, however, have bound these species to the aquatic environment. These pre-adaptations have allowed vertebrates to venture onto land hundreds of times, but were not able to accomplish the same degree of prolific radiation into diverse terrestrial species. To understand the potential of future invasions, studies must evaluate the models of evolutionary steps taken in past invasions. The commonalities to current and future invasions may then be elucidated to predict the effects of environmental changes.\n"}
{"id": "427992", "url": "https://en.wikipedia.org/wiki?curid=427992", "title": "Water hammer", "text": "Water hammer\n\nWater hammer (or, more generally, fluid hammer, also called hydraulic shock) is a pressure surge or wave caused when a fluid, usually a liquid but sometimes also a gas, in motion is forced to stop or change direction suddenly, a momentum change. A water hammer commonly occurs when a valve closes suddenly at an end of a pipeline system, and a pressure wave propagates in the pipe.\n\nThis pressure wave can cause major problems, from noise and vibration to pipe collapse. It is possible to reduce the effects of the water hammer pulses with accumulators, expansion tanks, surge tanks, blowoff valves, and other features.\n\nRough calculations can be made either using the Zhukovsky (Joukowsky) equation, or more accurate ones using the method of characteristics.\n\nIn the 1st century B.C., Marcus Vitruvius Pollio described the effect of water hammer in lead pipes and stone tubes of the Roman public water supply. Water hammer was exploited before there was even a word for it; in 1772, Englishman John Whitehurst built a hydraulic ram for a home in Cheshire, England. In 1796, French inventor Joseph Michel Montgolfier (1740–1810) built a hydraulic ram for his paper mill in Voiron. In French and Italian, the terms for \"water hammer\" come from the hydraulic ram: \"coup de bélier\" (French) and \"colpo d'ariete\" (Italian) both mean \"blow of the ram\". As the 19th century witnessed the installation of municipal water supplies, water hammer became a concern to civil engineers. Water hammer also interested physiologists who were studying the circulatory system.\n\nAlthough it was prefigured in work by Thomas Young, the theory of water hammer is generally considered to have begun in 1883 with the work of German physiologist Johannes von Kries (1853–1928), who was investigating the pulse in blood vessels. However, his findings went unnoticed by civil engineers. Kries's findings were subsequently derived independently in 1898 by the Russian fluid dynamicist Nikolay Yegorovich Zhukovsky (1847–1921), in 1898 by the American civil engineer Joseph Palmer Frizell (1832–1910), and in 1902 by the Italian engineer Lorenzo Allievi (1856–1941).\n\nWhen a pipe is suddenly closed at the outlet (downstream), the mass of water before the closure is still moving, thereby building up high pressure and a resulting shock wave. In domestic plumbing this is experienced as a loud banging resembling a hammering noise. Water hammer can cause pipelines to break if the pressure is high enough. Air traps or stand pipes (open at the top) are sometimes added as to water systems to absorb the potentially damaging forces caused by the moving water.\n\nIn hydroelectric generating stations, the water traveling along the tunnel or pipeline may be prevented from entering a turbine by closing a valve. For example, if there is 14 km of tunnel of 7.7 m diameter full of water travelling at 3.75 m/s, that represents approximately 8000 megajoules of kinetic energy that must be arrested. This arresting is frequently achieved by a surge shaft open at the top, into which the water flows. As the water rises up the shaft its kinetic energy is converted into potential energy, which causes the water in the tunnel to decelerate. At some HEP stations, such as the Saxon Falls Hydro Power Plant In Michigan, what looks like a water tower is actually one of these devices, known in these cases as a surge drum.\n\nIn the home, a water hammer may occur when a dishwasher, washing machine or toilet shuts off water flow. The result may be heard as a loud bang, repetitive banging (as the shock wave travels back and forth in the plumbing system), or as some shuddering.\n\nOn the other hand, when an upstream valve in a pipe closes, water downstream of the valve attempts to continue flowing creating a vacuum that may cause the pipe to collapse or implode. This problem can be particularly acute if the pipe is on a downhill slope. To prevent this, air and vacuum relief valves or air vents are installed just downstream of the valve to allow air to enter the line to prevent this vacuum from occurring.\n\nOther causes of water hammer are pump failure and check valve slam (due to sudden deceleration, a check valve may slam shut rapidly, depending on the dynamic characteristic of the check valve and the mass of the water between a check valve and tank). To alleviate this situation, it is recommended to install non-slam check valves as they do not rely on gravity or fluid flow for their closure. For vertical pipes, other suggestions include installing new piping that can be designed to include air chambers to alleviate the possible shockwave of water due to excess water flow.\n\nSteam distribution systems may also be vulnerable to a situation similar to water hammer, known as \"steam hammer\". In a steam system, a water hammer most often occurs when some of the steam condenses into water in a horizontal section of the piping. Steam picks up the water, forming a \"slug\", and hurls this at high velocity into a pipe fitting, creating a loud hammering noise and greatly stressing the pipe. This condition is usually caused by a poor condensate drainage strategy.\n\nWhere air filled traps are used, these eventually become depleted of their trapped air over a long period through absorption into the water. This can be cured by shutting off the supply, opening taps at the highest and lowest locations to drain the system (thereby restoring air to the traps), and then closing the taps and re-opening the supply.\n\nOn turbocharged internal combustion engines, a fluid hammer can take place when the throttle is closed while the turbocharger is forcing air into the engine. A pressure relief valve placed before the throttle prevents the air from surging against the throttle body by diverting it elsewhere, thus protecting the turbocharger from pressure damage. This valve can either recirculate the air into the turbocharger's intake (recirculation valve), or it can blow the air into the atmosphere and produce the distinctive hiss-flutter of an aftermarket turbocharger (blowoff valve).\n\nIf a stream of high pressure water impinges on a surface, water hammer can quickly erode and destroy it. In the 2009 Sayano–Shushenskaya hydroelectric power station accident, the lid to a 640 MW turbine was ejected upwards, hitting the ceiling above. During the accident, the rotor was seen flying through the air, still spinning, about 3 meters above the floor. Unrestrained, per second of water began to spray all over the generator hall. The geyser caused the structural failure of steel ceiling joists, precipitating a roof collapse around the failed turbine.\n\nWhen an explosion happens in an enclosed space, water hammer can cause the walls of the container to deform. However, it can also impart momentum to the enclosure if it is free to move. An underwater explosion in the SL-1 nuclear reactor vessel caused the water to accelerate upwards through of air before it struck the vessel head at with a pressure of . This pressure wave caused the steel vessel to jump 9 feet 1 inch (2.77 m) into the air before it dropped into its prior location. It is imperative to perform ongoing preventative maintenance to avoid water hammer as the results of these powerful explosions have resulted in fatalities.\n\nWater hammer has caused accidents and fatalities, but usually damage is limited to breakage of pipes or appendages. An engineer should always assess the risk of a pipeline burst. Pipelines transporting hazardous liquids or gases warrant special care in design, construction, and operation. Hydroelectric power plants especially must be carefully designed and maintained because the water hammer can cause water pipes to fail catastrophically.\n\nThe following characteristics may reduce or eliminate water hammer:\n\nOne of the first to successfully investigate the water hammer problem was the Italian engineer Lorenzo Allievi.\n\nWater hammer can be analyzed by two different approaches—\"rigid column theory\", which ignores compressibility of the fluid and elasticity of the walls of the pipe, or by a full analysis that includes elasticity. When the time it takes a valve to close is long compared to the propagation time for a pressure wave to travel the length of the pipe, then rigid column theory is appropriate; otherwise considering elasticity may be necessary.\nBelow are two approximations for the peak pressure, one that considers elasticity, but assumes the valve closes instantaneously, and a second that neglects elasticity but includes a finite time for the valve to close.\n\nThe pressure profile of the water hammer pulse can be calculated from the Joukowsky equation\n\nSo for a valve closing instantaneously, the maximum magnitude of the water hammer pulse is:\n\nwhere Δ\"P\" is the magnitude of the pressure wave (Pa), \"ρ\" is the density of the fluid (kg m), \"a\" is the speed of sound in the fluid (ms), and Δ\"v\" is the change in the fluid's velocity (ms). The pulse comes about due to Newton's laws of motion and the continuity equation applied to the deceleration of a fluid element.\n\nAs the speed of sound in a fluid is formula_3, the peak pressure depends on the fluid compressibility if the valve is closed abruptly.\n\nwhere\n\nWhen the valve is closed slowly compared to the transit time for a pressure wave to travel the length of the pipe, the elasticity can be neglected, and the phenomenon can be described in terms of inertance or rigid column theory:\n\nAssuming constant deceleration of the water column (\"dv\"/\"dt\" = \"v\"/\"t\"), gives:\n\nwhere:\n\nThe above formula becomes, for water and with imperial unit: P = 0.0135 V L/t.\nFor practical application, a safety factor of about 5 is recommended:\n\nwhere \"P\" is the inlet pressure in psi, \"V\" is the flow velocity in ft/sec, \"t\" is the valve closing time in seconds and \"L\" is the upstream pipe length in feet.\n\nWhen a valve with a volumetric flow rate Q is closed, an excess pressure ΔP is created upstream of the valve, whose value is given by the Joukowsky equation:\n\nIn this expression:\nThe hydraulic impedance \"Z\" of the pipeline determines the magnitude of the water hammer pulse. It is itself defined by:\n\nwith:\n\nThe latter follows from a series of hydraulic concepts: \n\nThus, the equivalent elasticity is the sum of the original elasticities:\n\nAs a result, we see that we can reduce the water hammer by:\n\nThe water hammer effect can be simulated by solving the following partial differential equations.\n\nwhere \"V\" is the fluid velocity inside pipe, \"formula_15\" is the fluid density and \"B\" is the equivalent bulk modulus, \"f\" is the Darcy-Weisbach friction factor.\n\nColumn separation is a phenomenon that can occur during a water-hammer event. If the pressure in a pipeline drops below the vapor pressure of the liquid, cavitation will occur (some of the liquid vaporizes, forming a bubble in the pipeline, keeping the pressure close to the vapor pressure). This is most likely to occur at specific locations such as closed ends, high points or knees (changes in pipe slope). When subcooled liquid flows into the space previously occupied by vapor the area of contact between the vapor and the liquid increases. This causes the vapor to condense into the liquid reducing the pressure in the vapor space. The liquid on either side of the vapor space is then accelerated into this space by the pressure difference. The collision of the two columns of liquid (or of one liquid column if at a closed end) causes a large and nearly instantaneous rise in pressure. This pressure rise can damage hydraulic machinery, individual pipes and supporting structures. Many repetitions of cavity formation and collapse may occur in a single water-hammer event.\n\nMost water hammer software packages use the method of characteristics to solve the differential equations involved. This method works well if the wave speed does not vary in time due to either air or gas entrainment in a pipeline. The Wave Method (WM) is also used in various software packages. WM lets operators analyze large networks efficiently. Many commercial and non commercial packages are available.\n\nSoftware packages vary in complexity, dependent on the processes modeled. The more sophisticated packages may have any of the following features:\n\n\n\n"}
{"id": "17038488", "url": "https://en.wikipedia.org/wiki?curid=17038488", "title": "Weather risk management", "text": "Weather risk management\n\nWeather risk management is a type of risk management done by organizations to address potential financial losses caused by unusual weather.\n\nEnergy, agriculture, transportation, construction, municipalities, school districts, travel, food processors, retail sales and real estate are all examples of industries whose operations and profits can be significantly affected by the weather. For example, unusually mild winters diminish consumer demand for heating and erode the profit margins for utility companies. Unexpected weather events can cause significant financial losses. Weather information and forecasts utilized in risk management decision making is often referred to as meteorological intelligence and offered by companies such as Delphi Weather Analytics.\n\nThe weather risk market makes it possible to manage the financial impact of weather through risk transfer instruments based on a defined weather element, such as temperature, rain, snow, wind, etc. Weather risk management is a way for organizations to limit their financial exposure to disruptive weather events. By making a payment (a \"premium\") to a separate company that will assume the financial weather risk for them, an organization essentially is buying a type of insurance - the company assuming the risk will pay the buyer a pre-set amount of money which will correspond to the loss or cost increase caused by the disruptive weather.\n\nCatastrophic weather events such as hurricanes are typically managed through traditional insurance contracts that pay based on indemnity loss. Insurance is a heavily regulated industry with specific requirements and qualification criteria. Due to the indemnity nature of insurance, actual loss must be proven to an insurance carrier before the payment can be processed. In contrast, financial loss such as erosion of margin, portfolio loss or increased expenses usually do not qualify for insurance payouts. Financial instruments such as derivative transactions can provide more flexible and customized risk management opportunities than the typical insurance contracts as they are priced and settled on the parameters of measured weather rather than the associated financial loss.\n\nA wide range of capital providers make markets in weather risk. To date the weather risk management trading market is primarily made up of dedicated weather trading operations, such as Nephila Capital Ltd, Galileo Weather Risk Management Advisors LCC, Swiss Re, RenRe, and Coriolis Capital, who execute trade orders in weather or weather-contingent commodity trades, the trading desks of financial institutions and utilities, such as Susquahanna Energy and Aquila who hedge their own risk as well as speculative trades for a merchant portfolio, professional commodity traders, such as RJO and hedge and private equity funds such as Tudor Capital. Transactions can be effected over-the-counter (OTC) or on commodity exchanges such as The Chicago Mercantile Exchange (CME). Still other operations, such as Storm Exchange, Inc(Note: Storm Exchange is now defunct. and WeatherBill (WeatherBill is no longer serving markets outside of Agriculture), privately held eWeatherRisk now provide corporate and municipal clients with the necessary financial context to gauge the impact of the weather on profit and loss before executing trades either OTC or through the CME.\n\nIn the US, the Commodity Exchange Act, Section 5d establishes weather in a category of market exempt from Commission oversight.\n\nRule 36.2 defines those commodities that are eligible to trade on an exempt board of trade as commodities having:\nThe Commodity Futures Trading Commission determined that weather indices are eligible to be traded on EBOTs by order dated May 30, 2002.\n\nCompanies that are subject to public disclosure to regulators or their shareholders must demonstrate that the purchase or a sale of a derivatives is true and fair hedge, not speculation. SFAS 133 and IAS provide guidelines on the steps that are required. FAS 133 Accounting for Weather Derivatives: For U.S. accounting standards, Over-the-Counter (OTC) weather derivative transactions can generally get an exemption under derivatives & hedging disclosure rules of Financial Accounting Standard No. 133 section 10 for non-exchange contracts settled on climatic variables, although specific structures and applications have to be assessed for each company environment. All written non–exchanged–traded option–based weather derivatives contracts should be carried at fair value with subsequent changes in fair value reported in current earnings.\n\nWhen they are standardized and traded on exchanges, weather derivatives will fall within the scope of SFAS 133. EITF Issue No 99–2 \"Accounting for weather derivatives\" provides guidance on accounting for weather derivatives that are not exchange–traded. Entities that enter into speculatives or trading non–exchange derivatives contracts should apply the intrinsic method.\n\n\n\n"}
{"id": "33550", "url": "https://en.wikipedia.org/wiki?curid=33550", "title": "Wood", "text": "Wood\n\nWood is a porous and fibrous structural tissue found in the stems and roots of trees and other woody plants. It is an organic material, a natural composite of cellulose fibers that are strong in tension and embedded in a matrix of lignin that resists compression. Wood is sometimes defined as only the secondary xylem in the stems of trees, or it is defined more broadly to include the same type of tissue elsewhere such as in the roots of trees or shrubs. In a living tree it performs a support function, enabling woody plants to grow large or to stand up by themselves. It also conveys water and nutrients between the leaves, other growing tissues, and the roots. Wood may also refer to other plant materials with comparable properties, and to material engineered from wood, or wood chips or fiber.\n\nWood has been used for thousands of years for fuel, as a construction material, for making tools and weapons, furniture and paper. More recently it emerged as a feedstock for the production of purified cellulose and its derivatives, such as cellophane and cellulose acetate.\n\nAs of 2005, the growing stock of forests worldwide was about 434 billion cubic meters, 47% of which was commercial. As an abundant, carbon-neutral renewable resource, woody materials have been of intense interest as a source of renewable energy. In 1991 approximately 3.5 billion cubic meters of wood were harvested. Dominant uses were for furniture and building construction.\n\nA 2011 discovery in the Canadian province of New Brunswick yielded the earliest known plants to have grown wood, approximately 395 to 400 million years ago.\n\nWood can be dated by carbon dating and in some species by dendrochronology to determine when a wooden object was created.\n\nPeople have used wood for thousands of years for many purposes, including as a fuel or as a construction material for making houses, tools, weapons, furniture, packaging, artworks, and paper. Known constructions using wood date back ten thousand years. Buildings like the European Neolithic long house were made primarily of wood.\n\nRecent use of wood has been enhanced by the addition of steel and bronze into construction.\n\nThe year-to-year variation in tree-ring widths and isotopic abundances gives clues to the prevailing climate at the time a tree was cut.\n\nWood, in the strict sense, is yielded by trees, which increase in diameter by the formation, between the existing wood and the inner bark, of new woody layers which envelop the entire stem, living branches, and roots. This process is known as secondary growth; it is the result of cell division in the vascular cambium, a lateral meristem, and subsequent expansion of the new cells. These cells then go on to form thickened secondary cell walls, composed mainly of cellulose, hemicellulose and lignin.\n\nWhere the differences between the four seasons are distinct, e.g. New Zealand, growth can occur in a discrete annual or seasonal pattern, leading to growth rings; these can usually be most clearly seen on the end of a log, but are also visible on the other surfaces. If the distinctiveness between seasons is annual (as is the case in equatorial regions, e.g. Singapore), these growth rings are referred to as annual rings. Where there is little seasonal difference growth rings are likely to be indistinct or absent. If the bark of the tree has been removed in a particular area, the rings will likely be deformed as the plant overgrows the scar.\n\nIf there are differences within a growth ring, then the part of a growth ring nearest the center of the tree, and formed early in the growing season when growth is rapid, is usually composed of wider elements. It is usually lighter in color than that near the outer portion of the ring, and is known as earlywood or springwood. The outer portion formed later in the season is then known as the latewood or summerwood. However, there are major differences, depending on the kind of wood (see below).\n\nAs a tree grows, lower branches often die, and their bases may become overgrown and enclosed by subsequent layers of trunk wood, forming a type of imperfection known as a knot. The dead branch may not be attached to the trunk wood except at its base, and can drop out after the tree has been sawn into boards. Knots affect the technical properties of the wood, usually reducing the local strength and increasing the tendency for splitting along the wood grain, but may be exploited for visual effect. In a longitudinally sawn plank, a knot will appear as a roughly circular \"solid\" (usually darker) piece of wood around which the grain of the rest of the wood \"flows\" (parts and rejoins). Within a knot, the direction of the wood (grain direction) is up to 90 degrees different from the grain direction of the regular wood.\n\nIn the tree a knot is either the base of a side branch or a dormant bud. A knot (when the base of a side branch) is conical in shape (hence the roughly circular cross-section) with the inner tip at the point in stem diameter at which the plant's vascular cambium was located when the branch formed as a bud.\n\nIn grading lumber and structural timber, knots are classified according to their form, size, soundness, and the firmness with which they are held in place. This firmness is affected by, among other factors, the length of time for which the branch was dead while the attaching stem continued to grow.\n\nKnots do not necessarily influence the stiffness of structural timber, this will depend on the size and location. Stiffness and elastic strength are more dependent upon the sound wood than upon localized defects. The breaking strength is very susceptible to defects. Sound knots do not weaken wood when subject to compression parallel to the grain.\n\nIn some decorative applications, wood with knots may be desirable to add visual interest. In applications where wood is painted, such as skirting boards, fascia boards, door frames and furniture, resins present in the timber may continue to 'bleed' through to the surface of a knot for months or even years after manufacture and show as a yellow or brownish stain. A knot primer paint or solution (knotting), correctly applied during preparation, may do much to reduce this problem but it is difficult to control completely, especially when using mass-produced kiln-dried timber stocks.\n\nHeartwood (or duramen) is wood that as a result of a naturally occurring chemical transformation has become more resistant to decay. Heartwood formation is a genetically programmed process that occurs spontaneously. Some uncertainty exists as to whether the wood dies during heartwood formation, as it can still chemically react to decay organisms, but only once.\n\nHeartwood is often visually distinct from the living sapwood, and can be distinguished in a cross-section where the boundary will tend to follow the growth rings. For example, it is sometimes much darker. However, other processes such as decay or insect invasion can also discolor wood, even in woody plants that do not form heartwood, which may lead to confusion.\n\nSapwood (or alburnum) is the younger, outermost wood; in the growing tree it is living wood, and its principal functions are to conduct water from the roots to the leaves and to store up and give back according to the season the reserves prepared in the leaves. However, by the time they become competent to conduct water, all xylem tracheids and vessels have lost their cytoplasm and the cells are therefore functionally dead. All wood in a tree is first formed as sapwood. The more leaves a tree bears and the more vigorous its growth, the larger the volume of sapwood required. Hence trees making rapid growth in the open have thicker sapwood for their size than trees of the same species growing in dense forests. Sometimes trees (of species that do form heartwood) grown in the open may become of considerable size, or more in diameter, before any heartwood begins to form, for example, in second-growth hickory, or open-grown pines.\n\nThe term \"heartwood\" derives solely from its position and not from any vital importance to the tree. This is evidenced by the fact that a tree can thrive with its heart completely decayed. Some species begin to form heartwood very early in life, so having only a thin layer of live sapwood, while in others the change comes slowly. Thin sapwood is characteristic of such species as chestnut, black locust, mulberry, osage-orange, and sassafras, while in maple, ash, hickory, hackberry, beech, and pine, thick sapwood is the rule. Others never form heartwood.\n\nNo definite relation exists between the annual rings of growth and the amount of sapwood. Within the same species the cross-sectional area of the sapwood is very roughly proportional to the size of the crown of the tree. If the rings are narrow, more of them are required than where they are wide. As the tree gets larger, the sapwood must necessarily become thinner or increase materially in volume. Sapwood is relatively thicker in the upper portion of the trunk of a tree than near the base, because the age and the diameter of the upper sections are less.\n\nWhen a tree is very young it is covered with limbs almost, if not entirely, to the ground, but as it grows older some or all of them will eventually die and are either broken off or fall off. Subsequent growth of wood may completely conceal the stubs which will however remain as knots. No matter how smooth and clear a log is on the outside, it is more or less knotty near the middle. Consequently, the sapwood of an old tree, and particularly of a forest-grown tree, will be freer from knots than the inner heartwood. Since in most uses of wood, knots are defects that weaken the timber and interfere with its ease of working and other properties, it follows that a given piece of sapwood, because of its position in the tree, may well be stronger than a piece of heartwood from the same tree.\n\nIt is remarkable that the inner heartwood of old trees remains as sound as it usually does, since in many cases it is hundreds, and in a few instances thousands, of years old. Every broken limb or root, or deep wound from fire, insects, or falling timber, may afford an entrance for decay, which, once started, may penetrate to all parts of the trunk. The larvae of many insects bore into the trees and their tunnels remain indefinitely as sources of weakness. Whatever advantages, however, that sapwood may have in this connection are due solely to its relative age and position.\n\nIf a tree grows all its life in the open and the conditions of soil and site remain unchanged, it will make its most rapid growth in youth, and gradually decline. The annual rings of growth are for many years quite wide, but later they become narrower and narrower. Since each succeeding ring is laid down on the outside of the wood previously formed, it follows that unless a tree materially increases its production of wood from year to year, the rings must necessarily become thinner as the trunk gets wider. As a tree reaches maturity its crown becomes more open and the annual wood production is lessened, thereby reducing still more the width of the growth rings. In the case of forest-grown trees so much depends upon the competition of the trees in their struggle for light and nourishment that periods of rapid and slow growth may alternate. Some trees, such as southern oaks, maintain the same width of ring for hundreds of years. Upon the whole, however, as a tree gets larger in diameter the width of the growth rings decreases.\n\nDifferent pieces of wood cut from a large tree may differ decidedly, particularly if the tree is big and mature. In some trees, the wood laid on late in the life of a tree is softer, lighter, weaker, and more even-textured than that produced earlier, but in other trees, the reverse applies. This may or may not correspond to heartwood and sapwood. In a large log the sapwood, because of the time in the life of the tree when it was grown, may be inferior in hardness, strength, and toughness to equally sound heartwood from the same log. In a smaller tree, the reverse may be true.\n\nIn species which show a distinct difference between heartwood and sapwood the natural color of heartwood is usually darker than that of the sapwood, and very frequently the contrast is conspicuous (see section of yew log above). This is produced by deposits in the heartwood of chemical substances, so that a dramatic color variation does not imply a significant difference in the mechanical properties of heartwood and sapwood, although there may be a marked biochemical difference between the two.\n\nSome experiments on very resinous longleaf pine specimens indicate an increase in strength, due to the resin which increases the strength when dry. Such resin-saturated heartwood is called \"fat lighter\". Structures built of fat lighter are almost impervious to rot and termites; however they are very flammable. Stumps of old longleaf pines are often dug, split into small pieces and sold as kindling for fires. Stumps thus dug may actually remain a century or more since being cut. Spruce impregnated with crude resin and dried is also greatly increased in strength thereby.\n\nSince the latewood of a growth ring is usually darker in color than the earlywood, this fact may be used in visually judging the density, and therefore the hardness and strength of the material. This is particularly the case with coniferous woods. In ring-porous woods the vessels of the early wood often appear on a finished surface as darker than the denser latewood, though on cross sections of heartwood the reverse is commonly true. Otherwise the color of wood is no indication of strength.\n\nAbnormal discoloration of wood often denotes a diseased condition, indicating unsoundness. The black check in western hemlock is the result of insect attacks. The reddish-brown streaks so common in hickory and certain other woods are mostly the result of injury by birds. The discoloration is merely an indication of an injury, and in all probability does not of itself affect the properties of the wood. Certain rot-producing fungi impart to wood characteristic colors which thus become symptomatic of weakness; however an attractive effect known as spalting produced by this process is often considered a desirable characteristic. Ordinary sap-staining is due to fungal growth, but does not necessarily produce a weakening effect.\n\nWater occurs in living wood in three locations, namely:\n\nIn heartwood it occurs only in the first and last forms. Wood that is thoroughly air-dried retains 8–16% of the water in the cell walls, and none, or practically none, in the other forms. Even oven-dried wood retains a small percentage of moisture, but for all except chemical purposes, may be considered absolutely dry.\n\nThe general effect of the water content upon the wood substance is to render it softer and more pliable. A similar effect occurs in the softening action of water on rawhide, paper, or cloth. Within certain limits, the greater the water content, the greater its softening effect.\n\nDrying produces a decided increase in the strength of wood, particularly in small specimens. An extreme example is the case of a completely dry spruce block 5 cm in section, which will sustain a permanent load four times as great as a green (undried) block of the same size will.\n\nThe greatest strength increase due to drying is in the ultimate crushing strength, and strength at elastic limit in endwise compression; these are followed by the modulus of rupture, and stress at elastic limit in cross-bending, while the modulus of elasticity is least affected.\n\nWood is a heterogeneous, hygroscopic, cellular and anisotropic material. It consists of cells, and the cell walls are composed of micro-fibrils of cellulose (40–50%) and hemicellulose (15–25%) impregnated with lignin (15–30%).\n\nIn coniferous or softwood species the wood cells are mostly of one kind, tracheids, and as a result the material is much more uniform in structure than that of most hardwoods. There are no vessels (\"pores\") in coniferous wood such as one sees so prominently in oak and ash, for example.\n\nThe structure of hardwoods is more complex. The water conducting capability is mostly taken care of by vessels: in some cases (oak, chestnut, ash) these are quite large and distinct, in others (buckeye, poplar, willow) too small to be seen without a hand lens. In discussing such woods it is customary to divide them into two large classes, \"ring-porous\" and \"diffuse-porous\".\n\nIn ring-porous species, such as ash, black locust, catalpa, chestnut, elm, hickory, mulberry, and oak, the larger vessels or pores (as cross sections of vessels are called) are localized in the part of the growth ring formed in spring, thus forming a region of more or less open and porous tissue. The rest of the ring, produced in summer, is made up of smaller vessels and a much greater proportion of wood fibers. These fibers are the elements which give strength and toughness to wood, while the vessels are a source of weakness.\n\nIn diffuse-porous woods the pores are evenly sized so that the water conducting capability is scattered throughout the growth ring instead of being collected in a band or row. Examples of this kind of wood are alder, basswood, birch, buckeye, maple, willow, and the \"Populus\" species such as aspen, cottonwood and poplar. Some species, such as walnut and cherry, are on the border between the two classes, forming an intermediate group.\n\nIn temperate softwoods, there often is a marked difference between latewood and earlywood. The latewood will be denser than that formed early in the season. When examined under a microscope, the cells of dense latewood are seen to be very thick-walled and with very small cell cavities, while those formed first in the season have thin walls and large cell cavities. The strength is in the walls, not the cavities. Hence the greater the proportion of latewood, the greater the density and strength. In choosing a piece of pine where strength or stiffness is the important consideration, the principal thing to observe is the comparative amounts of earlywood and latewood. The width of ring is not nearly so important as the proportion and nature of the latewood in the ring.\n\nIf a heavy piece of pine is compared with a lightweight piece it will be seen at once that the heavier one contains a larger proportion of latewood than the other, and is therefore showing more clearly demarcated growth rings. In white pines there is not much contrast between the different parts of the ring, and as a result the wood is very uniform in texture and is easy to work. In hard pines, on the other hand, the latewood is very dense and is deep-colored, presenting a very decided contrast to the soft, straw-colored earlywood.\n\nIt is not only the proportion of latewood, but also its quality, that counts. In specimens that show a very large proportion of latewood it may be noticeably more porous and weigh considerably less than the latewood in pieces that contain less latewood. One can judge comparative density, and therefore to some extent strength, by visual inspection.\n\nNo satisfactory explanation can as yet be given for the exact mechanisms determining the formation of earlywood and latewood. Several factors may be involved. In conifers, at least, rate of growth alone does not determine the proportion of the two portions of the ring, for in some cases the wood of slow growth is very hard and heavy, while in others the opposite is true. The quality of the site where the tree grows undoubtedly affects the character of the wood formed, though it is not possible to formulate a rule governing it. In general, however, it may be said that where strength or ease of working is essential, woods of moderate to slow growth should be chosen.\n\nIn ring-porous woods, each season's growth is always well defined, because the large pores formed early in the season abut on the denser tissue of the year before.\n\nIn the case of the ring-porous hardwoods, there seems to exist a pretty definite relation between the rate of growth of timber and its properties. This may be briefly summed up in the general statement that the more rapid the growth or the wider the rings of growth, the heavier, harder, stronger, and stiffer the wood. This, it must be remembered, applies only to ring-porous woods such as oak, ash, hickory, and others of the same group, and is, of course, subject to some exceptions and limitations.\n\nIn ring-porous woods of good growth, it is usually the latewood in which the thick-walled, strength-giving fibers are most abundant. As the breadth of ring diminishes, this latewood is reduced so that very slow growth produces comparatively light, porous wood composed of thin-walled vessels and wood parenchyma. In good oak, these large vessels of the earlywood occupy from 6 to 10 percent of the volume of the log, while in inferior material they may make up 25% or more. The latewood of good oak is dark colored and firm, and consists mostly of thick-walled fibers which form one-half or more of the wood. In inferior oak, this latewood is much reduced both in quantity and quality. Such variation is very largely the result of rate of growth.\n\nWide-ringed wood is often called \"second-growth\", because the growth of the young timber in open stands after the old trees have been removed is more rapid than in trees in a closed forest, and in the manufacture of articles where strength is an important consideration such \"second-growth\" hardwood material is preferred. This is particularly the case in the choice of hickory for handles and spokes. Here not only strength, but toughness and resilience are important.\n\nThe results of a series of tests on hickory by the U.S. Forest Service show that:\n\nThe effect of rate of growth on the qualities of chestnut wood is summarized by the same authority as follows:\n\nIn the diffuse-porous woods, the demarcation between rings is not always so clear and in some cases is almost (if not entirely) invisible to the unaided eye. Conversely, when there is a clear demarcation there may not be a noticeable difference in structure within the growth ring.\n\nIn diffuse-porous woods, as has been stated, the vessels or pores are even-sized, so that the water conducting capability is scattered throughout the ring instead of collected in the earlywood. The effect of rate of growth is, therefore, not the same as in the ring-porous woods, approaching more nearly the conditions in the conifers. In general it may be stated that such woods of medium growth afford stronger material than when very rapidly or very slowly grown. In many uses of wood, total strength is not the main consideration. If ease of working is prized, wood should be chosen with regard to its uniformity of texture and straightness of grain, which will in most cases occur when there is little contrast between the latewood of one season's growth and the earlywood of the next.\n\nStructural material that resembles ordinary, \"dicot\" or conifer timber in its gross handling characteristics is produced by a number of monocot plants, and these also are colloquially called wood. Of these, bamboo, botanically a member of the grass family, has considerable economic importance, larger culms being widely used as a building and construction material and in the manufacture of engineered flooring, panels and veneer. Another major plant group that produces material that often is called wood are the palms. Of much less importance are plants such as \"Pandanus,\" \"Dracaena\" and \"Cordyline.\" With all this material, the structure and composition of the processed raw material is quite different from ordinary wood.\n\nThe single most revealing property of wood as an indicator of wood quality is specific gravity (Timell 1986), as both pulp yield and lumber strength are determined by it. Specific gravity is the ratio of the mass of a substance to the mass of an equal volume of water; density is the ratio of a mass of a quantity of a substance to the volume of that quantity and is expressed in mass per unit substance, e.g., grams per milliliter (g/cm or g/ml). The terms are essentially equivalent as long as the metric system is used. Upon drying, wood shrinks and its density increases. Minimum values are associated with green (water-saturated) wood and are referred to as \"basic specific gravity\" (Timell 1986).\n\nWood density is determined by multiple growth and physiological factors compounded into “one fairly easily measured wood characteristic” (Elliott 1970).\n\nAge, diameter, height, radial (trunk) growth, geographical location, site and growing conditions, silvicultural treatment, and seed source all to some degree influence wood density. Variation is to be expected. Within an individual tree, the variation in wood density is often as great as or even greater than that between different trees (Timell 1986). Variation of specific gravity within the bole of a tree can occur in either the horizontal or vertical direction.\n\nIt is common to classify wood as either softwood or hardwood. The wood from conifers (e.g. pine) is called softwood, and the wood from dicotyledons (usually broad-leaved trees, (e.g. oak) is called hardwood. These names are a bit misleading, as hardwoods are not necessarily hard, and softwoods are not necessarily soft. The well-known balsa (a hardwood) is actually softer than any commercial softwood. Conversely, some softwoods (e.g. yew) are harder than many hardwoods.\n\nThere is a strong relationship between the properties of wood and the properties of the particular tree that yielded it. The density of wood varies with species. The density of a wood correlates with its strength (mechanical properties). For example, mahogany is a medium-dense hardwood that is excellent for fine furniture crafting, whereas balsa is light, making it useful for model building. One of the densest woods is black ironwood.\n\nThe chemical composition of wood varies from species to species, but is approximately 50% carbon, 42% oxygen, 6% hydrogen, 1% nitrogen, and 1% other elements (mainly calcium, potassium, sodium, magnesium, iron, and manganese) by weight. Wood also contains sulfur, chlorine, silicon, phosphorus, and other elements in small quantity.\n\nAside from water, wood has three main components. Cellulose, a crystalline polymer derived from glucose, constitutes about 41–43%. Next in abundance is hemicellulose, which is around 20% in deciduous trees but near 30% in conifers. It is mainly five-carbon sugars that are linked in an irregular manner, in contrast to the cellulose. Lignin is the third component at around 27% in coniferous wood vs. 23% in deciduous trees. Lignin confers the hydrophobic properties reflecting the fact that it is based on aromatic rings. These three components are interwoven, and direct covalent linkages exist between the lignin and the hemicellulose. A major focus of the paper industry is the separation of the lignin from the cellulose, from which paper is made.\n\nIn chemical terms, the difference between hardwood and softwood is reflected in the composition of the constituent lignin. Hardwood lignin is primarily derived from sinapyl alcohol and coniferyl alcohol. Softwood lignin is mainly derived from coniferyl alcohol.\n\nAside from the lignocellulose, wood consists of a variety of low molecular weight organic compounds, called \"extractives\". The wood extractives are fatty acids, resin acids, waxes and terpenes. For example, rosin is exuded by conifers as protection from insects. The extraction of these organic materials from wood provides tall oil, turpentine, and rosin.\n\nWood has a long history of being used as fuel, which continues to this day, mostly in rural areas of the world. Hardwood is preferred over softwood because it creates less smoke and burns longer. Adding a woodstove or fireplace to a home is often felt to add ambiance and warmth.\n\nWood has been an important construction material since humans began building shelters, houses and boats. Nearly all boats were made out of wood until the late 19th century, and wood remains in common use today in boat construction. Elm in particular was used for this purpose as it resisted decay as long as it was kept wet (it also served for water pipe before the advent of more modern plumbing).\n\nWood to be used for construction work is commonly known as \"lumber\" in North America. Elsewhere, \"lumber\" usually refers to felled trees, and the word for sawn planks ready for use is \"timber\". In Medieval Europe oak was the wood of choice for all wood construction, including beams, walls, doors, and floors. Today a wider variety of woods is used: solid wood doors are often made from poplar, small-knotted pine, and Douglas fir.\nNew domestic housing in many parts of the world today is commonly made from timber-framed construction. Engineered wood products are becoming a bigger part of the construction industry. They may be used in both residential and commercial buildings as structural and aesthetic materials.\n\nIn buildings made of other materials, wood will still be found as a supporting material, especially in roof construction, in interior doors and their frames, and as exterior cladding.\n\nWood is also commonly used as shuttering material to form the mold into which concrete is poured during reinforced concrete construction.\n\nA solid wood floor is a floor laid with planks or battens created from a single piece of timber, usually a hardwood. Since wood is hydroscopic (it acquires and loses moisture from the ambient conditions around it) this potential instability effectively limits the length and width of the boards.\n\nSolid hardwood flooring is usually cheaper than engineered timbers and damaged areas can be sanded down and refinished repeatedly, the number of times being limited only by the thickness of wood above the tongue.\n\nSolid hardwood floors were originally used for structural purposes, being installed perpendicular to the wooden support beams of a building (the joists or bearers) and solid construction timber is still often used for sports floors as well as most traditional wood blocks, mosaics and parquetry.\n\nEngineered wood products, glued building products \"engineered\" for application-specific performance requirements, are often used in construction and industrial applications. Glued engineered wood products are manufactured by bonding together wood strands, veneers, lumber or other forms of wood fiber with glue to form a larger, more efficient composite structural unit.\n\nThese products include glued laminated timber (glulam), wood structural panels (including plywood, oriented strand board and composite panels), laminated veneer lumber (LVL) and other structural composite lumber (SCL) products, parallel strand lumber, and I-joists. Approximately 100 million cubic meters of wood was consumed for this purpose in 1991. The trends suggest that particle board and fiber board will overtake plywood.\n\nWood unsuitable for construction in its native form may be broken down mechanically (into fibers or chips) or chemically (into cellulose) and used as a raw material for other building materials, such as engineered wood, as well as chipboard, hardboard, and medium-density fiberboard (MDF). Such wood derivatives are widely used: wood fibers are an important component of most paper, and cellulose is used as a component of some synthetic materials. Wood derivatives can be used for kinds of flooring, for example laminate flooring.\n\nWood has always been used extensively for furniture, such as chairs and beds. It is also used for tool handles and cutlery, such as chopsticks, toothpicks, and other utensils, like the wooden spoon and pencil.\n\nFurther developments include new lignin glue applications, recyclable food packaging, rubber tire replacement applications, anti-bacterial medical agents, and high strength fabrics or composites.\nAs scientists and engineers further learn and develop new techniques to extract various components from wood, or alternatively to modify wood, for example by adding components to wood, new more advanced products will appear on the marketplace. Moisture content electronic monitoring can also enhance next generation wood protection.\n\nWood has long been used as an artistic medium. It has been used to make sculptures and carvings for millennia. Examples include the totem poles carved by North American indigenous people from conifer trunks, often Western Red Cedar (\"Thuja plicata\").\n\nOther uses of wood in the arts include:\n\nMany types of sports equipment are made of wood, or were constructed of wood in the past. For example, cricket bats are typically made of white willow. The baseball bats which are legal for use in Major League Baseball are frequently made of ash wood or hickory, and in recent years have been constructed from maple even though that wood is somewhat more fragile. NBA courts have been traditionally made out of parquetry.\n\nMany other types of sports and recreation equipment, such as skis, ice hockey sticks, lacrosse sticks and archery bows, were commonly made of wood in the past, but have since been replaced with more modern materials such as aluminium, titanium or composite materials such as fiberglass and carbon fiber. One noteworthy example of this trend is the family of golf clubs commonly known as the \"woods\", the heads of which were traditionally made of persimmon wood in the early days of the game of golf, but are now generally made of metal or (especially in the case of drivers) carbon-fiber composites.\n\nLittle is known about the bacteria that degrade cellulose. Symbiotic bacteria in \"Xylophaga\" may play a role in the degradation of sunken wood; while bacteria such as \"Alphaproteobacteria\", \"Flavobacteria\", \"Actinobacteria\", \"Clostridia\", and \"Bacteroidetes\" have been detected in wood submerged over a year.\n\n"}
