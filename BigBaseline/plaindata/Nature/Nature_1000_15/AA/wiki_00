{"id": "71318", "url": "https://en.wikipedia.org/wiki?curid=71318", "title": "100-year flood", "text": "100-year flood\n\nA one-hundred-year flood is a flood event that has a 1% probability of occurring in any given year.\n\nThe 100-year flood is also referred to as the 1% flood, since its annual exceedance probability is 1%. For river systems, the 100-year flood is generally expressed as a flowrate. Based on the expected 100-year flood flow rate, the flood water level can be mapped as an area of inundation. The resulting floodplain map is referred to as the 100-year floodplain. Estimates of the 100-year flood flowrate and other streamflow statistics for any stream in the United States are available. In the UK The Environment Agency publishes a comprehensive map of all areas at risk of a 1 in 100 year flood. Areas near the coast of an ocean or large lake also can be flooded by combinations of tide, storm surge, and waves. Maps of the riverine or coastal 100-year floodplain may figure importantly in building permits, environmental regulations, and flood insurance.\n\nA common misunderstanding is that a 100-year flood is likely to occur only once in a 100-year period. In fact, there is approximately a 63.4% chance of one or more 100-year floods occurring in any 100-year period. On the Danube River at Passau, Germany, the actual intervals between 100-year floods during 1501 to 2013 ranged from 37 to 192 years. The probability P that one or more floods occurring during any period will exceed a given flood threshold can be expressed, using the binomial distribution, as\n\nformula_1\n\nwhere T is the threshold return period (e.g. 100-yr, 50-yr, 25-yr, and so forth), and n is the number of years in the period. The probability of exceedance P is also described as the natural, inherent, or hydrologic risk of failure. However, the expected value of the number of 100-year floods occurring in any 100-year period is 1.\n\nTen-year floods have a 10% chance of occurring in any given year (P =0.10); 500-year have a 0.2% chance of occurring in any given year (P =0.002); etc. The percent chance of an X-year flood occurring in a single year is 100/X. A similar analysis is commonly applied to coastal flooding or rainfall data. The recurrence interval of a storm is rarely identical to that of an associated riverine flood, because of rainfall timing and location variations among different drainage basins.\n\nThe field of extreme value theory was created to model rare events such as 100-year floods for the purposes of civil engineering. This theory is most commonly applied to the maximum or minimum observed stream flows of a given river. In desert areas where there are only ephemeral washes, this method is applied to the maximum observed rainfall over a given period of time (24-hours, 6-hours, or 3-hours). The extreme value analysis only considers the most extreme event observed in a given year. So, between the large spring runoff and a heavy summer rain storm, whichever resulted in more runoff would be considered the extreme event, while the smaller event would be ignored in the analysis (even though both may have been capable of causing terrible flooding in their own right).\n\nThere are a number of assumptions which are made to complete the analysis which determines the 100-year flood. First, the extreme events observed in each year must be independent from year-to-year. In other words, the maximum river flow rate from 1984 cannot be found to be significantly correlated with the observed flow rate in 1985. 1985 cannot be correlated with 1986, and so forth. The second assumption is that the observed extreme events must come from the same probability distribution function. The third assumption is that the probability distribution relates to the largest storm (rainfall or river flow rate measurement) that occurs in any one year. The fourth assumption is that the probability distribution function is stationary, meaning that the mean (average), standard deviation and max/min values are not increasing or decreasing over time. This concept is referred to as stationarity.\n\nThe first assumption is often but not always valid and should be tested on a case by case basis. The second assumption is often valid if the extreme events are observed under similar climate conditions. For example, if the extreme events on record all come from late summer thunder storms (as is the case in the southwest U.S.), or from snow pack melting (as is the case in north-central U.S.), then this assumption should be valid. If, however, there are some extreme events taken from thunder storms, others from snow pack melting, and others from hurricanes, then this assumption is most likely not valid. The third assumption is only a problem when trying to forecast a low, but maximum flow event (for example, an event smaller than a 2-year flood). Since this is not typically a goal in extreme analysis, or in civil engineering design, then the situation rarely presents itself. The final assumption about stationarity is difficult to test from data for a single site because of the large uncertainties in even the longest flood records (see next section). More broadly, substantial evidence of climate change strongly suggests that the probability distribution is also changing and that managing flood risks in the future will become even more difficult. The simplest implication of this is that not all of the historical data are, or can be, considered valid as input into the extreme event analysis.\n\nWhen these assumptions are violated there is an \"unknown\" amount of uncertainty introduced into the reported value of what the 100-year flood means in terms of rainfall intensity, or flood depth. When all of the inputs are known the uncertainty can be measured in the form of a confidence interval. For example, one might say there is a 95% chance that the 100-year flood is greater than X, but less than Y.\n\nDirect statistical analysis to estimate the 100-year riverine flood is possible only at the relatively few locations where an annual series of maximum instantaneous flood discharges has been recorded. In the United States as of 2014, taxpayers have supported such records for at least 60 years at fewer than 2,600 locations, for at least 90 years at fewer than 500, and for at least 120 years at only 11. For comparison, the total area of the nation is about , so there are perhaps 3,000 stream reaches that drain watersheds of and 300,000 reaches that drain . In urban areas, 100-year flood estimates are needed for watersheds as small as . For reaches without sufficient data for direct analysis, 100-year flood estimates are derived from indirect statistical analysis of flood records at other locations in a hydrologically similar region or from other hydrologic models. Similarly for coastal floods, tide gauge data exist for only about 1,450 sites worldwide, of which only about 950 added information to the global data center between January 2010 and March 2016.\nMuch longer records of flood elevations exist at a few locations around the world, such as the Danube River at Passau, Germany, but they must be evaluated carefully for accuracy and completeness before any statistical interpretation.\n\nFor an individual stream reach, the uncertainties in any analysis can be large, so 100-year flood estimates have large individual uncertainties for most stream reaches. For the largest recorded flood at any specific location, or any potentially larger event, the recurrence interval always is poorly known. Spatial variability adds more uncertainty, because a flood peak observed at different locations on the same stream during the same event commonly represents a different recurrence interval at each location. If an extreme storm drops enough rain on one branch of a river to cause a 100-year flood, but no rain falls over another branch, the flood wave downstream from their junction might have a recurrence interval of only 10 years. Conversely, a storm that produces a 25-year flood simultaneously in each branch might form a 100-year flood downstream. During a time of flooding, news accounts necessarily simplify the story by reporting the greatest damage and largest recurrence interval estimated at any location. The public can easily and incorrectly conclude that the recurrence interval applies to all stream reaches in the flood area.\n\nPeak elevations of 14 floods as early as 1501 on the Danube River at Passau, Germany, reveal great variability in the actual intervals between floods. Flood events greater than the 50-year flood occurred at intervals of 4 to 192 years since 1501, and the 50-year flood of 2002 was followed only 11 years later by a 500-year flood. Only half of the intervals between 50- and 100-year floods were within 50 percent of the nominal average interval. Similarly, the intervals between 5-year floods during 1955 to 2007 ranged from 5 months to 16 years, and only half were within 2.5 to 7.5 years.\nIn the United States, the 100-year flood provides the risk basis for flood insurance rates. Complete information on the National Flood Insurance Program (NFIP) is available here. A \"regulatory flood\" or \"base flood\" is routinely established for river reaches through a science-based rule-making process targeted to a 100-year flood at the historical average recurrence interval. In addition to historical flood data, the process accounts for previously established regulatory values, the effects of flood-control reservoirs, and changes in land use in the watershed. Coastal flood hazards have been mapped by a similar approach that includes the relevant physical processes. Most areas where serious floods can occur in the United States have been mapped consistently in this manner. On average nationwide, those 100-year flood estimates are well sufficient for the purposes of the NFIP and offer reasonable estimates of future flood risk, if the future is like the past. Approximately 3% of the U.S. population lives in areas subject to the 1% annual chance coastal flood hazard.\n\nIn theory, removing homes and businesses from areas that flood repeatedly can protect people and reduce insurance losses, but in practice it is difficult for people to retreat from established neighborhoods.\n\n\n"}
{"id": "33149795", "url": "https://en.wikipedia.org/wiki?curid=33149795", "title": "Aharonov–Casher effect", "text": "Aharonov–Casher effect\n\nThe Aharonov–Casher effect is a quantum mechanical phenomenon predicted in 1984 in which a traveling magnetic dipole is affected by an electric field. It is dual to the Aharonov–Bohm effect, in which the quantum phase of a charged particle depends upon which side of a magnetic flux tube it comes through. In the Aharonov–Casher effect, the particle has a magnetic moment and the tubes are charged instead. It was observed in a gravitational neutron inferometer in 1989(Cimmino et al.) and later by fluxon interference of magnetic vortices in Josephson junctions (Elion et al.). It has also been seen with electrons and atoms.\n\nIn both effects the particle acquires a phase shift (formula_1) while traveling along some path \"P\". In the Aharonov–Bohm effect it is\n\nWhile for the Aharonov–Casher effect it is\n\nwhere formula_4 is its charge and formula_5 is the magnetic moment. The effects have been observed together(Bogachek and Landman).\n\n\n"}
{"id": "55257518", "url": "https://en.wikipedia.org/wiki?curid=55257518", "title": "Amity-enmity complex", "text": "Amity-enmity complex\n\nThe amity-enmity complex was a term introduced by Sir Arthur Keith. His work, \"A New Theory of Human Evolution\" (1948), posited that humans evolved as differing races, tribes, and cultures, exhibiting patriotism, morality, leadership and nationalism. Those who belong are part of the in-group, and tolerated; all others are classed as out-group, and subject to hostility; 'The code of enmity is a necessary part of the machinery of evolution. He who feels generous towards his enemy... has given up his place in the turmoil of evolutionary competition.' Conscience in humans evolved a duality; to protect and save friends,\nand also to hate and fight enemies. \nKeith's work summarized earlier opinions on human tribalism by Charles Darwin, Alfred Russel Wallace, and Herbert Spencer.\n\n\nThe amity-enmity complex maintains 'tribal spirit' and thus unity, of the community, 'as long as personal contact between its members is possible.' If the community grows beyond this limitation, then disruption, swarming and disintegration occur. Modern mass communication enables communities 'of 100 million' to remain intact.\n\nKeith expressed regret that this phenomenon, which explains so much, had not become common knowledge: \"[W]e eternally experience the misery... of each new manifestation of the complex, then invent some new 'ism' to categorise this behavior as an evil, dealing with a common behavioural trait piecemeal [instead of] finally grasping and understanding the phenomenon.\"\n\nColleges, sports teams, churches, trades unions, female fashions and political parties enable people to exhibit tribal loyalty within large, mass-communicating nations. 'In politics we have to take sides.' But all these 'petty manifestations' are cast aside in time of war.\nBismarck, Abraham Lincoln and Lloyd George are cited as statesmen who knew how to exploit the tribal spirit for political ends.\n\nRobert Ardrey pointed out that similar behavior can be observed in most primates, especially baboons and chimps. \"Nationalism as such is no more than a human expression of the animal drive to maintain and defend a territory... the mentality of the single Germanic tribe under Hitler differed in no way from that of early man or late baboon.\"\n\nThe amity-enmity complex is a serious obstacle to world peace and world government, and may even lead to nuclear holocaust: \"How can we get along without war?... if we fail to get along without war, the future will be as lacking in human problems as it will be remarkably lacking in men.\"\n\nDesmond Morris makes a prescriptive point: \"We must try to step outside our groups and look down on human battlefields with the unbiased eye of a hovering Martian.\" And he warns that \"the truly violent species all appear to have exterminated themselves, a lesson we should not overlook.\" The inherited aggression of the amity-enmity rivalry between communities is rationalized under a \"persistent cloak of ideology... a matter of ideals, moral principles, social philosophies or religious beliefs... [O]nly an immense amount of intellectual restraint will save the situation.\"\n\nAfter World War Two, a debate about the place of instinct and learning (the nature-versus-nurture debate) has occurred. According to Steven Pinker, the \"bitter lessons of lynchings, world wars, and the Holocaust\" have caused \"prevailing theories of mind\" to be \"refashioned to make racism and sexism as untenable as possible. The doctrine of the blank slate became entrenched in intellectual life.\"\n\nPinker makes the point that \"conflicts of interest are inherent to the human condition.\" Man is a product of nature, as much as malarial mosquitoes; both \"are doing exactly what evolution designed them to do, even if the outcome makes people suffer... [We] cannot call their behavior pathological... [T]he belief that violence is an aberration is dangerous.\"\n\n"}
{"id": "19468941", "url": "https://en.wikipedia.org/wiki?curid=19468941", "title": "Balance of nature", "text": "Balance of nature\n\nThe balance of nature is a theory that proposes that ecological systems are usually in a stable equilibrium or homeostasis, which is to say that a small change in some particular parameter (the size of a particular population, for example) will be corrected by some negative feedback that will bring the parameter back to its original \"point of balance\" with the rest of the system. It may apply where populations depend on each other, for example in predator/prey systems, or relationships between herbivores and their food source. It is also sometimes applied to the relationship between the Earth's ecosystem, the composition of the atmosphere, and the world's weather.\n\nThe Gaia hypothesis is a balance of nature-based theory that suggests that the Earth and its ecology may act as co-ordinated systems in order to maintain the balance of nature.\n\nThe theory that nature is permanently in balance has been largely discredited by scientists working in ecology, as it has been found that chaotic changes in population levels are common, but nevertheless the idea continues to be popular in the general public. During the later half of the twentieth century the theory was superseded by catastrophe theory and chaos theory.\n\nThe concept that nature maintains its condition is of ancient provenance; Herodotus commented on the wonderful relationship between predator and prey species, which remained in a steady proportion to one another, with predators never excessively consuming their prey populations. The \"balance of nature\" concept once ruled ecological research, as well as once governing the management of natural resources. This led to a doctrine popular among some conservationists that nature was best left to its own devices, and that human intervention into it was by definition unacceptable. The validity of a \"balance of nature\" was already questioned in the early 1900s, but the general abandonment of the theory by scientists working in ecology only happened in the last quarter of that century when studies showed that it did not match what could be observed among plant and animal populations.\n\nPredator-prey populations tend to show chaotic behavior within limits, where the sizes of populations change in a way that may appear random, but is in fact obeying deterministic laws based only on the relationship between a population and its food source illustrated by the Lotka–Volterra equation. An experimental example of this was shown in an eight-year study on small Baltic Sea creatures such as plankton, which were isolated from the rest of the ocean. Each member of the food web was shown to take turns multiplying and declining, even though the scientists kept the outside conditions constant. An article in the journal \"Nature\" stated; \"Advanced mathematical techniques proved the indisputable presence of chaos in this food web ... short-term prediction is possible, but long-term prediction is not.\"\n\nAlthough some conservationist organizations argue that human activity is incompatible with a balanced ecosystem, there are numerous examples in history showing that several modern day habitats originate from human activity: some of Latin America's rain forests owe their existence to humans planting and transplanting them, while the abundance of grazing animals in the Serengeti plain of Africa is thought by some ecologists to be partly due to human-set fires that created savanna habitats.\n\nPossibly one of the best examples of an ecosystem fundamentally modified by human activity can be observed as a consequence of the Australian Aboriginal practice of \"Fire-stick farming\". The legacy of this practice over long periods has resulted in forests being converted to grasslands capable of sustaining larger populations of faunal prey, particularly in the northern and western regions of the continent. So thorough has been the effect of these deliberate regular burnings that many plant and tree species from affected regions have now completely adapted to the annual fire regime in that they require the passage of a fire before their seeds will even germinate. One school in Los Angeles states, \" “We have let our kids go to the forest area of the playground. However, five years later, we found that none of the flowers were growing, the natural damp soil had been hardened, and all of the beautiful grass had been plucked,”.\n\nDespite being discredited among ecologists, the theory is widely held to be true by the general public, with one authority calling it an \"enduring myth\". At least in Midwestern America, the \"balance of nature\" idea was shown to be widely held by both science majors and the general student population. In a study at the University of Patras, educational sciences students were asked to reason about the future of ecosystems which suffered human-driven disturbances. Subjects agreed that it was very likely for the ecosystems to fully recover their initial state, referring to either a 'recovery process' which restores the initial 'balance', or specific 'recovery mechanisms' as an ecosystem's inherent characteristic. In a 2017 study, Ampatzidis and Ergazaki discuss the learning objectives and design criteria that a learning environment for non-biology major students should meet to support them challenge the \"balance of nature\" idea.\n\n"}
{"id": "4290647", "url": "https://en.wikipedia.org/wiki?curid=4290647", "title": "Biological naturalism", "text": "Biological naturalism\n\nBiological naturalism is a theory about, among other things, the relationship between consciousness and body (i.e. brain), and hence an approach to the mind–body problem. It was first proposed by the philosopher John Searle in 1980 and is defined by two main theses: 1) all mental phenomena from pains, tickles, and itches to the most abstruse thoughts are caused by lower-level neurobiological processes in the brain; and 2) mental phenomena are higher-level features of the brain.\n\nThis entails that the brain has the right causal powers to produce intentionality. However, Searle's biological naturalism does not entail that brains and \"only\" brains can cause consciousness. Searle is careful to point out that while it appears to be the case that certain brain functions are sufficient for producing conscious states, our current state of neurobiological knowledge prevents us from concluding that they are necessary for producing consciousness. In his own words:\n\n\"The fact that brain processes cause consciousness does not imply that only brains can be conscious. The brain is a biological machine, and we might build an artificial machine that was conscious; just as the heart is a machine, and we have built artificial hearts. Because we do not know exactly how the brain does it we are not yet in a position to know how to do it artificially.\" (Biological Naturalism, 2004)\n\nSearle denies Cartesian dualism, the idea that the mind is a separate kind of substance to the body, as this contradicts our entire understanding of physics, and unlike Descartes, he does not bring God into the problem. Indeed, Searle denies any kind of dualism, the traditional alternative to monism, claiming the distinction is a mistake. He rejects the idea that because the mind is not objectively viewable, it does not fall under the rubric of physics.\n\nSearle believes that consciousness \"is a real part of the real world and it cannot be eliminated in favor of, or reduced to, something else\" whether that something else is a neurological state of the brain or a computer program. He contends, for example, that the software known as Deep Blue \"knows\" nothing about chess. He also believes that consciousness is both a cause of events in the body and a response to events in the body.\n\nOn the other hand, Searle doesn't treat consciousness as a ghost in the machine. He treats it, rather, as a state of the brain. The causal interaction of mind and brain can be described thus in naturalistic terms: Events at the micro-level (perhaps at that of individual neurons) cause consciousness. Changes at the macro-level (the whole brain) constitute consciousness. Micro-changes cause and then are impacted by holistic changes, in much the same way that individual football players cause a team (as a whole) to win games, causing the individuals to gain confidence from the knowledge that they are part of a winning team.\n\nHe articulates this distinction by pointing out that the common philosophical term 'reducible' is ambiguous. Searle contends that consciousness is \"causally reducible\" to brain processes without being \"ontologically reducible\". He hopes that making this distinction will allow him to escape the traditional dilemma between reductive materialism and substance dualism; he affirms the essentially physical nature of the universe by asserting that consciousness is completely caused by and realized in the brain, but also doesn't deny what he takes to be the obvious facts that humans really are conscious, and that conscious states have an essentially first-person nature.\n\nIt can be tempting to see the theory as a kind of property dualism, since, in Searle's view, a person's mental properties are categorically different from his or her micro-physical properties. The latter have \"third-person ontology\" whereas the former have \"first-person ontology.\" Micro-structure is accessible objectively by any number of people, as when several brain surgeons inspect a patient's cerebral hemispheres. But pain or desire or belief are accessible subjectively by the person who has the pain or desire or belief, and no one else has that mode of access. However, Searle holds mental properties to be a species of physical property—ones with first-person ontology. So this sets his view apart from a dualism of physical and non-physical properties. His mental properties are putatively physical.\n\nThere have been several criticisms of Searle's idea of biological naturalism.\n\nJerry Fodor suggests that Searle gives us no account at all of exactly \"why\" he believes that a biochemistry like, or similar to, that of the human brain is indispensable for intentionality. Fodor thinks that it seems much more plausible to suppose that it is the way in which an organism (or any other system for that matter) is connected to its environment that is indispensable in the explanation of intentionality. It is easier to see \"how the fact that my thought is causally connected to a tree might bear on its being a thought about a tree. But it's hard to imagine how the fact that (to put it crudely) my thought is made out of hydrocarbons could matter, except on the unlikely hypothesis that only hydrocarbons can be causally connected to trees in the way that brains are.\" \n\nJohn Haugeland takes on the central notion of some set of special \"right causal powers\" that Searle attributes to the biochemistry of the human brain. He asks us to imagine a concrete situation in which the \"right\" causal powers are those that our neurons have to reciprocally stimulate one another. In this case, silicon-based alien life forms can be intelligent just in case they have these \"right\" causal powers; i.e. they possess neurons with synaptics connections that have the power to reciprocally stimulate each other. Then we can take any speaker of the Chinese language and cover his neurons in some sort of wrapper which prevents them from being influenced by neurotransmitters and, hence, from having the right causal powers. At this point, \"Searle's demon\" (an English speaking nanobot, perhaps) sees what is happening and intervenes: he sees through the covering and determines which neurons would have been stimulated and which not and proceeds to stimulate the appropriate neurons and shut down the others himself. The experimental subject's behavior is unaffected. He continues to speak perfect Chinese as before the operation but now the causal powers of his neurotransmitters have been replaced by someone who does not understand the Chinese language. The point is generalizable: for any causal powers, it will always be possible to hypothetically replace them with some sort of Searlian demon which will carry out the operations mechanically. His conclusion is that Searle's is necessarily a dualistic view of the nature of causal powers, \"not intrinsically connected with the actual powers of physical objects.\" \n\nSearle himself actually does not rule out the possibility for alternate arrangements of matter bringing forth consciousness other than biological brains. He also disputes that Biological naturalism is dualistic in nature in a brief essay entitled \"Why I Am Not a Property Dualist\".\n\n\n\n"}
{"id": "8553751", "url": "https://en.wikipedia.org/wiki?curid=8553751", "title": "Biological organisation", "text": "Biological organisation\n\nBiological organization is the hierarchy of complex biological structures and systems that define life using a reductionistic approach. The traditional hierarchy, as detailed below, extends from atoms to biospheres. The higher levels of this scheme are often referred to as an ecological organization concept, or as the field, hierarchical ecology.\n\nEach level in the hierarchy represents an increase in organizational complexity, with each \"object\" being primarily composed of the previous level's basic unit. The basic principle behind the organization is the concept of \"emergence\"—the properties and functions found at a hierarchical level are not present and irrelevant at the lower levels.\n\nThe biological organization of life is a fundamental premise for numerous areas of scientific research, particularly in the medical sciences. Without this necessary degree of organization, it would be much more difficult—and likely impossible—to apply the study of the effects of various physical and chemical phenomena to diseases and physiology (body function). For example, fields such as cognitive and behavioral neuroscience could not exist if the brain was not composed of specific types of cells, and the basic concepts of pharmacology could not exist if it was not known that a change at the cellular level can affect an entire organism. These applications extend into the ecological levels as well. For example, DDT's direct insecticidal effect occurs at the subcellular level, but affects higher levels up to and including multiple ecosystems. Theoretically, a change in one atom could change the entire biosphere.\n\nThe simple standard biological organization scheme, from the lowest level to the highest level, is as follows:\n\nMore complex schemes incorporate many more levels. For example, a molecule can be viewed as a grouping of elements, and an atom can be further divided into subatomic particles (these levels are outside the scope of biological organization). Each level can also be broken down into its own hierarchy, and specific types of these biological objects can have their own hierarchical scheme. For example, genomes can be further subdivided into a hierarchy of genes.\n\nEach level in the hierarchy can be described by its lower levels. For example, the organism may be described at any of its component levels, including the atomic, molecular, cellular, histological (tissue), organ and organ system levels. Furthermore, at every level of the hierarchy, new functions necessary for the control of life appear. These new roles are not functions that the lower level components are capable of and are thus referred to as \"emergent properties\".\n\nEvery organism is organised, though not necessarily to the same degree. An organism can not be organised at the histological (tissue) level if it is not composed of tissues in the first place.\n\nEmpirically, a large proportion of the (complex) biological systems we observe in nature exhibit hierarchic structure. On theoretical grounds we could expect complex systems to be hierarchies in a world in which complexity had to evolve from simplicity. System hierarchies analysis performed in the 1950s, laid the empirical foundations for a field that would be, from the 1980s, hierarchical ecology.\n\nThe theoretical foundations are summarized by thermodynamics.\nWhen biological systems are modeled as physical systems, in its most general abstraction, they are thermodynamic open systems that exhibit self-organised behavior, and the set/subset relations between dissipative structures can be characterized in a hierarchy.\n\nA simpler and more direct way to explain the fundamentals of the \"hierarchical organization of life\", was introduced in Ecology by Odum and others as the \"Simon's hierarchical principle\"; Simon emphasized that hierarchy \"emerges almost inevitably through a wide variety of evolutionary processes, for the simple reason that hierarchical structures are stable\".\n\nTo motivate this deep idea, he offered his \"parable\" about imaginary watchmakers.\n\n\n"}
{"id": "11003420", "url": "https://en.wikipedia.org/wiki?curid=11003420", "title": "Brown energy", "text": "Brown energy\n\nBrown energy may refer to:\n\n"}
{"id": "37970127", "url": "https://en.wikipedia.org/wiki?curid=37970127", "title": "Buttered toast phenomenon", "text": "Buttered toast phenomenon\n\nThe buttered toast phenomenon is an observation that buttered toast tends to land butter-side down after it falls. It is used an idiom representing pessimistic outlooks. Various people have attempted to determine whether there is an actual tendency for bread to fall in this fashion, with varying results.\n\nThe phenomenon is said to be an old proverb from \"the north country.\" Written accounts can be traced to the mid-19th century. The phenomenon is often attributed to a parodic poem of James Payn from 1884:\n\nIn the past, this has often been considered just a pessimistic belief. A study by the BBC's television series \"Q.E.D.\" found that when toast is thrown in the air, it lands butter-side down just one-half of the time (as would be predicted by chance). However, several scientific studies have found that when toast is dropped from a table (as opposed to being thrown in the air), it does fall butter-side down. A study by Robert A J Matthews won the Ig Nobel Prize in 1996.\n\nWhen toast falls out of one's hand, it does so at an angle, by nature of it having slipped from its previous position, then the toast rotates. Given that tables are usually between two and six feet (0.7 to 1.83 meters), there is enough time for the toast to rotate about one-half of a turn, and thus lands upside down relative to its original position. Since the original position is usually butter-side up, the toast lands butter-side down. However, if the table is over 10 feet (3 meters) tall, the toast will rotate a full 360 degrees, and land butter-side up. Also, if the toast travels horizontally at over 3.6 miles per hour (1.6 m/s), the toast will not rotate enough to land butter-side down. In fact, the phenomenon is caused by fundamental physical constants.\n\nThe added weight of the butter has no effect on the falling process, since the butter spreads throughout the slice.\n\nThe following findings are from Mythbusters:\n\nIn the 2010 M. Night Shyamalan film \"Devil\", a group of adults in an elevator become trapped by unknown forces. Slowly over the course of the film, people begin to die in the elevator as the power blinks on and off, pinning the people against each other and creating a false narrative that the others are murderers (However, it is common opinion that the name of the film spoiled this false narrative). In the film, supporting cast member and fictional security guard Ramirez, in his suspicion that there may be unholy forces at play, dropped a slice of toast with jelly on one side, and dropped it on the floor. The toast landed on the jelly-side, after which Ramirez coined the phrase \"Jelly Side Down,\" and subsequently 'proved' that the devil was nearby. However, there is some dispute in the film about whether or not this did actually prove the devil was near.\n\nThis phenomenon was also demonstrated and parodied in the October 29, 2013 \"Nostalgia Critic\" review of \"Devil\", (Reuploaded on July 23, 2016 by Channel Awesome on YouTube) in which actor Doug Walker plays a priest who uses multiple food items to determine whether or not the Devil is near. After the other items show no sign of the Devil, the priest tosses a piece of toast as the last test. The toast lands jelly side down which means the Devil is near and the end of the world is imminent. Everybody in the church screams and panics as the priest repeatedly shouts, \"Jelly side down!\"\n\n"}
{"id": "14194971", "url": "https://en.wikipedia.org/wiki?curid=14194971", "title": "Crushed stone", "text": "Crushed stone\n\nCrushed stone or angular rock is a form of construction aggregate, typically produced by mining a suitable rock deposit and breaking the removed rock down to the desired size using crushers. It is distinct from gravel which is produced by natural processes of weathering and erosion, and typically has a more rounded shape.\n\nAngular crushed stone is the key material for macadam road construction which depends on the interlocking of the individual stones' angular faces for its strength. Crushed natural stone is also used similarly without a binder for riprap, railroad track ballast, and filter stone. It may be used with a binder in a composite material such as concrete, tarmac, or asphalt concrete.\n\nCrushed stone is one of the most accessible natural resources, and is a major basic raw material used by construction, agriculture, and other industries. Despite the low value of its basic products, the crushed stone industry is a major contributor to and an indicator of the economic well-being of a nation.\nThe demand for crushed stone is determined mostly by the level of construction activity, and, therefore, the demand for construction materials.\n\nStone resources of the world are very large. High-purity limestone and dolomite suitable for specialty uses are limited in many geographic areas. Crushed stone substitutes for roadbuilding include sand and gravel, and slag. Substitutes for crushed stone used as construction aggregates include sand and gravel, iron and steel slag, sintered or expanded clay or shale, and perlite or vermiculite.\n\nCrushed stone is a high-volume, low-value commodity. The industry is highly competitive and is characterized by many operations serving local or regional markets. Production costs are determined mainly by the cost of labor, equipment, energy, and water, in addition to the costs of compliance with environmental and safety regulations. These costs vary depending on geographic location, the nature of the deposit, and the number and type of products produced. Crushed stone has one of the lowest average by weight values of all mineral commodities. The average unit price increased from US$1.58 per metric ton, f.o.b. plant, in 1970 to US$4.39 in 1990. However, the unit price in constant 1982 dollars fluctuated between US$3.48 and US$3.91 per metric ton for the same period. Increased productivity achieved through increased use of automation and more efficient equipment was mainly responsible for maintaining the prices at this level.\n\nTransportation is a major factor in the delivered price of crushed stone. The cost of moving crushed stone from the plant to the market often equals or exceeds the sale price of the product at the plant. Because of the high cost of transportation and the large quantities of bulk material that have to be shipped, crushed stone is usually marketed locally. The high cost of transportation is responsible for the wide dispersion of quarries, usually located near highly populated areas. However, increasing land values combined with local environmental concerns are moving crushed stone quarries farther from the end-use locations, increasing the price of delivered material. Economies of scale, which might be realized if fewer, larger operations served larger marketing areas, would probably not offset the increased transportation costs.\n\nAccording to the United States Geological Survey, 1.72 billion tonnes of crushed stone worth $13.8 billion was sold or used in 2006, of which 1.44 billion tonnes was used as construction aggregate, 74.9 million tonnes used for cement manufacture, and 18.1 million tonnes used to make lime. Crushed marble sold or used totaled 11.8 million tonnes, the majority of which was ground very fine and used as calcium carbonate.\n\nIn 2006, 9.40 million tonnes of crushed stone (almost all limestone or dolomite) was used for soil treatment, primarily to reduce soil acidity. Soils tend to become acidic from heavy use of nitrogen-containing fertilizers, unless a soil conditioner is used. Using aglime or agricultural lime, a finely-ground limestone or dolomite, to change the soil from acidic to nearly neutral particularly benefits crops by maximizing availability of plant nutrients, and also by reducing aluminum or manganese toxicity, promoting soil microbe activity, and improving the soil structure.\n\nIn 2006, 5.29 million tonnes of crushed stone (mostly limestone or dolomite) was used as a flux in blast furnaces and in certain steel furnaces to react with gangue minerals (i.e. silica and silicate impurities) to produce liquid slag that floats and can be poured off from the much denser molten metal (i.e., iron). The slag cools to become a stone-like material that is commonly crushed and recycled as construction aggregate.\n\nIn addition, 4.53 million tonnes of crushed stone was used for fillers and extenders (including asphalt fillers or extenders), 2.71 million tonnes for sulfur oxide removal-mine dusting-acid water treatment, and 1.45 million tonnes sold or used for poultry grit or mineral food.\n\nCrushed stone is recycled primarily as construction aggregate or concrete.\nCrushed stone or 'road metal' is used in landscape design and gardening for gardens, parks, and municipal and private projects as a mulch, walkway, path, and driveway pavement, and cell infill for modular permeable paving units. As a mineral mulch its benefits include erosion control, water conservation, weed suppression, and aesthetic qualities. It is often seen used in rock gardens and cactus gardens.\n\n\n"}
{"id": "665333", "url": "https://en.wikipedia.org/wiki?curid=665333", "title": "Cymdeithas Edward Llwyd", "text": "Cymdeithas Edward Llwyd\n\nCymdeithas Edward Llwyd (English: Edward Llwyd Society) is a Welsh natural history organization whose name commemorates the great Welsh natural historian, geographer and linguist Edward Llwyd.\n\nThe Cymdeithas Edward Llwyd organizes regular country walks throughout Wales in sites of interest of the Welsh environment, including SSI's & post-industrial landscapes. These are Welsh-language walking groups, although learners are just as welcome.\n\nThey also organize a variety of Nature & Environmental activities, including lectures, publications on Welsh Nature & Environment & conservation work.\n\n"}
{"id": "1065253", "url": "https://en.wikipedia.org/wiki?curid=1065253", "title": "Darwin machine", "text": "Darwin machine\n\nA Darwin machine (a 1987 coinage by William H. Calvin, by analogy to a Turing machine) is a machine that, like a Turing machine, involves an iteration process that yields a high-quality result, but, whereas a Turing machine uses logic, the Darwin machine uses rounds of variation, selection, and inheritance.\nIn its original connotation, a Darwin machine is any process that bootstraps quality by utilizing all of the six essential features of a Darwinian process: A \"pattern\" is \"copied\" with \"variations\", where populations of one variant pattern \"compete\" with another population, their relative success biased by a \"multifaceted environment\" (natural selection) so that winners predominate in producing the further variants of the next generation (Darwin's \"inheritance principle\").\n\nMore loosely, a Darwin machine is a process that utilizes some subset of the Darwinian essentials, typically natural selection to create a non-reproducing pattern, as in neural Darwinism. Many aspects of neural development utilize overgrowth followed by pruning to a pattern, but the resulting pattern does not itself create further copies.\n\n\"Darwin machine\" has been used multiple times to name computer programs after Charles Darwin.\n\n\n"}
{"id": "27233680", "url": "https://en.wikipedia.org/wiki?curid=27233680", "title": "Decomposed granite", "text": "Decomposed granite\n\nDecomposed granite is classification of rock that is derived from granite via its weathering to the point that the parent material readily fractures into smaller pieces of weaker rock. Further weathering yields material that easily crumbles into a mixtures of gravel-sized particles known as grus, that in turn may break down to produce a mixture of clay and silica sand or silt particles. Different specific granite types have differing propensities to weather, and so differing likelihoods of producing decomposed granite. It has practical uses that include its incorporation into paving and driveway materials, residential gardening materials in arid environments, as well as various types of walkways and heavy-use paths in parks. Different colors of decomposed granite are available, deriving from the natural range of granite colors from different quarry sources, and admixture of other natural and synthetic materials can extend the range of decomposed granite properties.\n\nDecomposed granite is rock of granitic origin that has weathered to the point that it readily fractures into smaller pieces of weak rock. Further weathering produces rock that easily crumbles into mixtures of gravel-sized particles, sand, and silt-sized particles with some clay. Eventually, the gravel may break down to produce a mixture of silica sand, silt particles, and clay. Different specific granite types have differing propensities to weather, and so differing likelihoods of producing decomposed granite.\n\nThe parent granite material is a common type of igneous rock that is granular, with its grains large enough to be distinguished with the unaided eye (i.e., it is phaneritic in texture); it is composed of plagioclase feldspar, orthoclase feldspar, quartz, mica, and possibly other minerals. The chemical transformation of feldspar, one of the primary constituents of granite, into the clay mineral kaolin is one of the important weathering processes. The presence of clay allows water to seep in and further weaken the rock allowing it to fracture or crumble into smaller particles, where, ultimately, the grains of silica produced from the granite are relatively resistant to weathering, and may remain almost unaltered.\n\nDecomposed granite, as a crushed stone form, is used as a pavement building material. It is used on driveways, garden walkways, bocce courts and pétanque terrains, and urban, regional, and national park walkways and heavy-use paths. DG can be installed and compacted to meet handicapped accessibility specifications and criteria, such as the ADA standards in the U.S. Different colors are available based on the various natural ranges available from different quarry sources, and polymeric stabilizers and other additives can be included to change the properties of the natural material. Decomposed granite is also sometimes used as a component of soil mixtures for cultivating bonsai.\n\n"}
{"id": "38045209", "url": "https://en.wikipedia.org/wiki?curid=38045209", "title": "Despeciation", "text": "Despeciation\n\nDespeciation is the loss of a unique species of animal due to its combining with another previously distinct species. It is the opposite of Speciation and is much more rare. It is similar to extinction in that there is a loss of a unique species but without the associated loss of a biological lineage.\n\nFor example, Taylor et al.'s genetic analysis of three-spined sticklebacks across six lakes in southwestern British Columbia found two distinct species in 1977 and 1988 but only one combined species in data from 1997, 2000, and 2002. They concluded that external factors had imperiled the living conditions of the two species, thus eliminating the evolutionary specializations that had kept them unique.\n"}
{"id": "944638", "url": "https://en.wikipedia.org/wiki?curid=944638", "title": "Earth's energy budget", "text": "Earth's energy budget\n\nEarth's energy budget accounts for the balance between the energy Earth receives from the Sun, the energy Earth radiates back into outer space after having been distributed throughout the five components of Earth's climate system and having thus powered the so-called Earth’s heat engine. This system is made up of earth's water, ice, atmosphere, rocky crust, and all living things.\n\nQuantifying changes in these amounts is required to accurately model the Earth's climate. \n\nReceived radiation is unevenly distributed over the planet, because the Sun heats equatorial regions more than polar regions. The atmosphere and ocean work non-stop to even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation. Earth is very close to being in radiative equilibrium, the situation where the incoming solar energy is balanced by an equal flow of heat to space; under that condition, global temperatures will be \"relatively\" stable. Globally, over the course of the year, the Earth system—land surfaces, oceans, and atmosphere—absorbs and then radiates back to space an average of about 240 watts of solar power per square meter. Anything that increases or decreases the amount of incoming or outgoing energy will change global temperatures in response.\n\nHowever, Earth's energy balance and heat fluxes depend on many factors, such as atmospheric composition (mainly aerosols and greenhouse gases), the albedo (reflectivity) of surface properties, cloud cover and vegetation and land use patterns.\n\nChanges in surface temperature due to Earth's energy budget do not occur instantaneously, due to the inertia of the oceans and the cryosphere. The net heat flux is buffered primarily by becoming part of the ocean's heat content, until a new equilibrium state is established between radiative forcings and the climate response.\n\nIn spite of the enormous transfers of energy into and from the Earth, it maintains a relatively constant temperature because, as a whole, there is little net gain or loss: Earth emits via atmospheric and terrestrial radiation (shifted to longer electromagnetic wavelengths) to space about the same amount of energy as it receives via insolation (all forms of electromagnetic radiation).\n\nTo quantify Earth's \"heat budget\" or \"heat balance\", let the insolation received at the top of the atmosphere be 100 units (100 units = about 1,360 watts per square meter facing the sun), as shown in the accompanying illustration. Called the albedo of Earth, around 35 units are reflected back to space: 27 from the top of clouds, 2 from snow and ice-covered areas, and 6 by other parts of the atmosphere. The 65 remaining units are absorbed: 14 within the atmosphere and 51 by the Earth’s surface. These 51 units are radiated to space in the form of terrestrial radiation: 17 directly radiated to space and 34 absorbed by the atmosphere (19 through latent heat of condensation, 9 via convection and turbulence, and 6 directly absorbed). The 48 units absorbed by the atmosphere (34 units from terrestrial radiation and 14 from insolation) are finally radiated back to space. These 65 units (17 from the ground and 48 from the atmosphere) balance the 65 units absorbed from the sun in order to maintain zero net gain of energy by the Earth.\n\nThe total amount of energy received per second at the top of Earth's atmosphere (TOA) is measured in watts and is given by the solar constant times the cross-sectional area of the Earth. Because the surface area of a sphere is four times the cross-sectional surface area of a sphere (i.e. the area of a circle), the average TOA flux is one quarter of the solar constant and so is approximately 340 W/m². Since the absorption varies with location as well as with diurnal, seasonal and annual variations, the numbers quoted are long-term averages, typically averaged from multiple satellite measurements.\n\nOf the ~340 W/m² of solar radiation received by the Earth, an average of ~77 W/m² is reflected back to space by clouds and the atmosphere and ~23 W/m² is reflected by the surface albedo, leaving ~240 W/m² of solar energy input to the Earth's energy budget. This gives the earth a mean net albedo of 0.29.\n\nThe geothermal heat flux from the Earth's interior is estimated to be 47 terawatts. This comes to 0.087 watt/square metre, which represents only 0.027% of Earth's total energy budget at the surface, which is dominated by 173,000 terawatts of incoming solar radiation.\n\nHuman production of energy is even lower, at an estimated 18 TW.\n\nPhotosynthesis has a larger effect: photosynthetic efficiency turns up to 2% of incoming sunlight into biomass, for a total photosynthetic productivity of earth between ~1500–2250 TW (~1%+/-0.26% solar energy hitting the Earth's surface).\n\nOther minor sources of energy are usually ignored in these calculations, including accretion of interplanetary dust and solar wind, light from stars other than the Sun and the thermal radiation from space. Earlier, Joseph Fourier had claimed that deep space radiation was significant in a paper often cited as the first on the greenhouse effect.\n\nLongwave radiation is usually defined as outgoing infrared energy leaving the planet. However, the atmosphere absorbs parts initially, or cloud cover can reflect radiation. Generally, heat energy is transported between the planet's surface layers (land and ocean) to the atmosphere, transported via evapotranspiration and latent heat fluxes or conduction/convection processes. Ultimately, energy is radiated in the form of longwave infrared radiation back into space.\n\nRecent satellite observations indicate additional precipitation, which is sustained by increased energy leaving the surface through evaporation (the latent heat flux), offsetting increases in longwave flux to the surface.\n\nIf the incoming energy flux is not equal to the outgoing energy flux, net heat is added to or lost by the planet (if the incoming flux is larger or smaller than the outgoing respectively).\n\nAn imbalance must show in something on Earth warming or cooling (depending on the direction of the imbalance), and the ocean being the larger thermal reservoir on Earth, is a prime candidate for measurements.\n\nEarth's energy imbalance measurements provided by Argo floats have detected an accumulation of ocean heat content (OHC). The estimated imbalance was measured during a deep solar minimum of 2005–2010 to be 0.58 ± 0.15 W/m². This level of detail cannot be inferred directly from measurements of surface energy fluxes, which have combined uncertainties of the order of ± 17 W/m².\n\nSeveral satellites indirectly measure the energy absorbed and radiated by Earth and by inference the energy imbalance. The NASA Earth Radiation Budget Experiment (ERBE) project involves three such satellites: the Earth Radiation Budget Satellite (ERBS), launched October 1984; NOAA-9, launched December 1984; and NOAA-10, launched September 1986.\n\nToday NASA's satellite instruments, provided by CERES, part of the NASA's Earth Observing System (EOS), are designed to measure both solar-reflected and Earth-emitted radiation.\n\nThe major atmospheric gases (oxygen and nitrogen) are transparent to incoming sunlight but are also transparent to outgoing thermal (infrared) radiation. However, water vapor, carbon dioxide, methane and other trace gases are opaque to many wavelengths of thermal radiation. The Earth's surface radiates the net equivalent of 17 percent of the incoming solar energy in the form of thermal infrared. However, the amount that directly escapes to space is only about 12 percent of incoming solar energy. The remaining fraction, 5 to 6 percent, is absorbed by the atmosphere by greenhouse gas molecules.\nWhen greenhouse gas molecules absorb thermal infrared energy, their temperature rises. Those gases then radiate an increased amount of thermal infrared energy in all directions. Heat radiated upward continues to encounter greenhouse gas molecules; those molecules also absorb the heat, and their temperature rises and the amount of heat they radiate increases. The atmosphere thins with altitude, and at roughly 5–6 kilometres, the concentration of greenhouse gases in the overlying atmosphere is so thin that heat can escape to space.\n\nBecause greenhouse gas molecules radiate infrared energy in all directions, some of it spreads downward and ultimately returns to the Earth's surface, where it is absorbed. The Earth's surface temperature is thus higher than it would be if it were heated only by direct solar heating. This supplemental heating is the natural greenhouse effect. It is as if the Earth is covered by a blanket that allows high frequency radiation (sunlight) to enter, but slows the rate at which the low frequency infrared radiant energy emitted by the Earth leaves.\n\nA change in the incident radiated portion of the energy budget is referred to as a radiative forcing.\n\nClimate sensitivity is the steady state change in the equilibrium temperature as a result of changes in the energy budget.\n\nClimate forcings are changes that cause temperatures to rise or fall, disrupting the energy balance. Natural climate forcings include changes in the Sun's brightness, Milankovitch cycles (small variations in the shape of Earth's orbit and its axis of rotation that occur over thousands of years) and volcanic eruptions that inject light-reflecting particles as high as the stratosphere. Man-made forcings include particle pollution (aerosols) that absorb and reflect incoming sunlight; deforestation, which changes how the surface reflects and absorbs sunlight; and the rising concentration of atmospheric carbon dioxide and other greenhouse gases, which decreases the rate at which heat is radiated to space.\n\nA forcing can trigger feedbacks that intensify (positive feedback) or weaken (negative feedback) the original forcing. For example, loss of ice at the poles, which makes them less reflective, causes greater absorption of energy and so increases the rate at which the ice melts, is an example of a positive feedback.\n\nThe observed planetary energy imbalance during the recent solar minimum shows that solar forcing of climate, although natural and significant, is overwhelmed by anthropogenic climate forcing.\n\nIn 2012, NASA scientists reported that to stop global warming atmospheric CO content would have to be reduced to 350 ppm or less, assuming all other climate forcings were fixed. The impact of anthropogenic aerosols has not been quantified, but individual aerosol types are thought to have substantial heating and cooling effects.\n\n\n"}
{"id": "878461", "url": "https://en.wikipedia.org/wiki?curid=878461", "title": "Earth's orbit", "text": "Earth's orbit\n\nAll celestial bodies in the Solar System, including planets such as our own, orbit around the Solar System's centre of mass. The sun makes up 99.76% of this mass which is why the centre of mass is extremely close to the sun.\n\nEarth's orbit is the trajectory along which Earth travels around the Sun. The average distance between the Earth and the Sun is 149.60 million km (92.96 million mi), and one complete orbit takes  days (1 sidereal year), during which time Earth has traveled 940 million km (584 million mi). Earth's orbit has an eccentricity of 0.0167.\n\nAs seen from Earth, the planet's orbital prograde motion makes the Sun appear to move with respect to other stars at a rate of about 1° (or a Sun or Moon diameter every 12 hours) eastward per solar day. Earth's orbital speed averages about 30 km/s (108,000 km/h; 67,000 mph), which is fast enough to cover the planet's diameter in 7 minutes and the distance to the Moon in 4 hours.\n\nFrom a vantage point above the north pole of either the Sun or Earth, Earth would appear to revolve in a counterclockwise direction around the Sun. From the same vantage point, both the Earth and the Sun would appear to rotate also in a counterclockwise direction about their respective axes.\n\nHeliocentrism is the scientific model that first placed the Sun at the center of the Solar System and put the planets, including Earth, in its orbit. Historically, heliocentrism is opposed to geocentrism, which placed the Earth at the center. Aristarchus of Samos already proposed a heliocentric model in the 3rd century BC. In the 16th century, Nicolaus Copernicus' \"De revolutionibus\" presented a full discussion of a heliocentric model of the universe in much the same way as Ptolemy had presented his geocentric model in the 2nd century. This \"Copernican revolution\" resolved the issue of planetary retrograde motion by arguing that such motion was only perceived and apparent. \"Although Copernicus's groundbreaking book...had been [printed] over a century earlier, [the Dutch mapmaker] Joan Blaeu was the first mapmaker to incorporate his revolutionary heliocentric theory into a map of the world.\"\n\nBecause of Earth's axial tilt (often known as the obliquity of the ecliptic), the inclination of the Sun's trajectory in the sky (as seen by an observer on Earth's surface) varies over the course of the year. For an observer at a northern latitude, when the north pole is tilted toward the Sun the day lasts longer and the Sun appears higher in the sky. This results in warmer average temperatures, as additional solar radiation reaches the surface. When the north pole is tilted away from the Sun, the reverse is true and the weather is generally cooler. North of the Arctic Circle and south of the Antarctic Circle, an extreme case is reached in which there is no daylight at all for part of the year, and continuous daylight during the opposite time of year. This is called polar night and midnight sun. This variation in the weather (because of the direction of the Earth's axial tilt) results in the seasons.\n\nBy astronomical convention, the four seasons are determined by the solstices (the two points in the Earth's orbit of the maximum tilt of the Earth's axis, toward the Sun or away from the Sun) and the equinoxes (the two points in the Earth's orbit where the Earth's tilted axis and an imaginary line drawn from the Earth to the Sun are exactly perpendicular to one another). The solstices and equinoxes divide the year up into four approximately equal parts. In the northern hemisphere winter solstice occurs on or about December 21; summer solstice is near June 21; spring equinox is around March 20; and autumnal equinox is about September 23. The effect of the Earth's axial tilt in the southern hemisphere is the opposite of that in the northern hemisphere, thus the seasons of the solstices and equinoxes in the southern hemisphere are the reverse of those in the northern hemisphere (e.g. the northern summer solstice is at the same time as the southern winter solstice).\n\nIn modern times, Earth's perihelion occurs around January 3, and the aphelion around July 4 (for other eras, see precession and Milankovitch cycles). The changing Earth–Sun distance results in an increase of about 6.9% in total solar energy reaching the Earth at perihelion relative to aphelion. Since the southern hemisphere is tilted toward the Sun at about the same time that the Earth reaches the closest approach to the Sun, the southern hemisphere receives slightly more energy from the Sun than does the northern over the course of a year. However, this effect is much less significant than the total energy change due to the axial tilt, and most of the excess energy is absorbed by the higher proportion of water in the southern hemisphere.\n\nThe Hill sphere (gravitational sphere of influence) of the Earth is about 1,500,000 kilometers (0.01 AU) in radius, or approximately 4 times the average distance to the moon. This is the maximal distance at which the Earth's gravitational influence is stronger than the more distant Sun and planets. Objects orbiting the Earth must be within this radius, otherwise they can become unbound by the gravitational perturbation of the Sun.\n\nThe following diagram shows the relation between the line of solstice and the line of apsides of Earth's elliptical orbit. The orbital ellipse goes through each of the six Earth images, which are sequentially the perihelion (periapsis — nearest point to the Sun) on anywhere from January 2 to January 5, the point of March equinox on March 19, 20, or 21, the point of June solstice on June 20, 21, or 22, the aphelion (apoapsis — farthest point from the Sun) on anywhere from July 3 to July 5, the September equinox on September 22, 23, or 24, and the December solstice on December 21, 22, or 23. The diagram shows an exaggerated shape of Earth's orbit; the actual orbit is less eccentric than pictured.\nBecause of the axial tilt of the Earth in its orbit, the maximal intensity of Sun rays hits the Earth 23.4 degrees north of equator at the June Solstice (at the Tropic of Cancer), and 23.4 degrees south of equator at the December Solstice (at the Tropic of Capricorn).\n\nMathematicians and astronomers (such as Laplace, Lagrange, Gauss, Poincaré, Kolmogorov, Vladimir Arnold, and Jürgen Moser) have searched for evidence for the stability of the planetary motions, and this quest led to many mathematical developments and several successive \"proofs\" of stability for the Solar System. By most predictions, Earth's orbit will be relatively stable over long periods.\n\nIn 1989, Jacques Laskar's work indicated that the Earth's orbit (as well as the orbits of all the inner planets) can become chaotic and that an error as small as 15 meters in measuring the initial position of the Earth today would make it impossible to predict where the Earth would be in its orbit in just over 100 million years' time. Modeling the Solar System is a subject covered by the n-body problem.\n\n"}
{"id": "29247528", "url": "https://en.wikipedia.org/wiki?curid=29247528", "title": "Earth's shadow", "text": "Earth's shadow\n\nEarth's shadow or Earth shadow is the shadow that Earth itself casts onto its atmosphere and into outer space, toward the antisolar point. During twilight (both early dusk and late dawn), the shadow's visible fringe (sometimes called the dark segment or twilight wedge) appears in a clear sky as a dark and diffused band low above the horizon.\n\nEarth's shadow cast onto the atmosphere can be viewed during the \"civil\" stage of twilight, assuming the sky is clear and the horizon is relatively unobstructed. The shadow's fringe appears as a dark bluish to purplish band that stretches over 180° of the horizon opposite the Sun, i.e. in the eastern sky at dusk and in the western sky at dawn. Before sunrise, Earth's shadow appears to recede as the Sun rises; after sunset, the shadow appears to rise as the Sun sets.\n\nEarth's shadow is best seen when the horizon is low, such as over the sea, and when the sky conditions are clear. In addition, the higher the observer's elevation is to view the horizon, the sharper the shadow appears.\n\nA related phenomenon in the same part of the sky is the Belt of Venus, or anti-twilight arch, a pinkish band visible above the bluish shade of Earth's shadow, named after the planet Venus which, when visible, is typically located in this region of the sky.\nNo defined line divides the Earth's shadow and the Belt of Venus; one colored band blends into the other in the sky.\n\nThe Belt of Venus is quite a different phenomenon from the afterglow, which appears in the geometrically opposite part of the sky.\n\nWhen the Sun is near the horizon around sunset or sunrise, the sunlight appears reddish. This is because the light rays are penetrating an especially thick layer of the atmosphere, which works as a filter, scattering all but the longer (redder) wavelengths. \n\nFrom the observer's perspective, the red sunlight directly illuminates small particles in the lower atmosphere in the sky opposite of the Sun. The red light is backscattered to the observer, which is the reason why the Belt of Venus appears pink. \n\nThe lower the setting Sun descends, the less defined the boundary between Earth's shadow and the Belt of Venus appears. This is because the setting Sun now illuminates a thinner part of the upper atmosphere. There the red light is not scattered because fewer particles are present, and the eye only sees the \"normal\" (usual) blue sky, which is due to Rayleigh scattering from air molecules. Eventually, both Earth's shadow and the Belt of Venus dissolve into the darkness of the night sky.\n\nEarth's shadow is as curved as the planet is, and its umbra extends into outer space. (The antumbra, however, extends indefinitely.) When the Sun, Earth, and the Moon are aligned perfectly (or nearly so), with Earth between the Sun and the Moon, Earth's shadow falls onto the lunar surface facing the night side of the planet, such that the shadow gradually darkens the full Moon, causing a lunar eclipse. \n\nEven during a total lunar eclipse, a small amount of sunlight however still reaches the Moon. This indirect sunlight has been refracted as it passed through Earth's atmosphere. The air molecules and particulates in Earth's atmosphere scatter the shorter wavelengths of this sunlight; thus, the longer wavelengths of reddish light reaches the Moon, in the same way that light at sunset or sunrise appears reddish. This weak red illumination gives the eclipsed Moon a dimly reddish or copper color.\n\n\n"}
{"id": "47992", "url": "https://en.wikipedia.org/wiki?curid=47992", "title": "Earthling", "text": "Earthling\n\nEarthling, a term identifying inhabitants of the planet Earth. Earthlings are the only form of life in the universe whose existence is known. Similar terms are Terran and Gaian.\n\nHistorically, the term \"earthling\" referred to a mortal inhabitant of the Earth as opposed to spiritual or divine entities. In Early Modern English, the word was used with the intention of contrasting \"earth\" with \"heaven\", and so presenting man as an inhabitant of the sublunary sphere, as opposed to heavenly creatures or deities. The derivation from the noun \"earth\" by means of the suffix \"-ling\" is already seen in Old English \"yrþling\", in the meaning \"ploughman\". The sense of \"inhabitant of earth\" is first attested in the 1590s. Its use in science fiction dates to 1949, in \"Red Planet\" by Robert A. Heinlein.\n\nThe word \"Human\" has the same imposed meaning, since it originates from a PIE word \"*ǵʰmṓ\" \"earthling\" < \"*dʰéǵʰōm\" \"earth\". Compare Latin \"homo\" \"human\" and \"humus\" \"earth, soil\".\n\nIts modern use in science fiction literature contrasts \"Earth\" (the planet) with outer space or hypothetical other planets with sapient life. The term was often used in 1950s science fiction film and novels by aliens to express a disdainful or patronizing tone towards creatures from Earth. The meaning \"creature from planet Earth\" in the context of space travel may be extended to non-human species, as in \"Russia fetes dog Laika, first earthling in space\".\n\nThe literary effect aimed for is a distancing effect, inviting the readers to contemplate their own species as it might be seen from an external point of view. There is also a belittling effect, in parallel with the use of the -ling suffix in such diminutives as \"duckling\" and \"gosling\". Especially in 1950s science fiction, use of the term is a conscious reversal of common assumptions of anthropocentrism or human exceptionalism, and may be an example of an exonym.\n\nIn some science fiction media (such as the \"Star Trek\" franchise and the 2014 movie \"Guardians of the Galaxy\") the term \"Terran\" is used as a term for humans, stemming from \"terra\", the Latin word for Earth. In the original run of the BBC series \"Doctor Who\", the phrase \"tellurian\" is used.\n\n"}
{"id": "43400705", "url": "https://en.wikipedia.org/wiki?curid=43400705", "title": "Ekeby oak tree", "text": "Ekeby oak tree\n\nThe Ekeby oak tree () is an oak tree in Ekerö outside Stockholm, Sweden, close to Ekebyhov Castle. It is the largest living deciduous tree in Sweden by volume. \n\nThe Ekeby oak is approximately 500 years old. It was declared a natural monument in 1956. There are many old trees around Ekebyhov Castle; the oak, sometimes called \"Ekeröjätten\" (the Ekerö giant) stands alone in a field south of the castle, where it had no competition for space from other trees. It was measured in 2008 as the largest tree by volume in Sweden.\n"}
{"id": "1610231", "url": "https://en.wikipedia.org/wiki?curid=1610231", "title": "Energy density", "text": "Energy density\n\nEnergy density is the amount of energy stored in a given system or region of space per unit volume. Colloquially it may also be used for energy per unit mass, though the accurate term for this is specific energy. Often only the \"useful\" or extractable energy is measured, which is to say that inaccessible energy (such as rest mass energy) is ignored. In cosmological and other general relativistic contexts, however, the energy densities considered are those that correspond to the elements of the stress–energy tensor and therefore do include mass energy as well as energy densities associated with the pressures described in the next paragraph.\n\nEnergy per unit volume has the same physical units as pressure, and in many circumstances is a synonym: for example, the energy density of a magnetic field may be expressed as (and behaves as) a physical pressure, and the energy required to compress a compressed gas a little more may be determined by multiplying the difference between the gas pressure and the external pressure by the change in volume. In short, pressure is a measure of the enthalpy per unit volume of a system. A pressure gradient has the potential to perform work on the surroundings by converting enthalpy to work until equilibrium is reached.\n\nThere are many different types of energy stored in materials, and it takes a particular type of reaction to release each type of energy. In order of the typical magnitude of the energy released, these types of reactions are: nuclear, chemical, electrochemical, and electrical.\n\nNuclear reactions are used by stars and nuclear power plants, both of which derive energy from the binding energy of nuclei. Chemical reactions are used by animals to derive energy from food, and by automobiles to derive energy from gasoline. Liquid hydrocarbons (fuels such as gasoline, diesel and kerozene) are today the most dense way known to economically store and transport chemical energy at a very large scale (1 kg of diesel fuel burns with the oxygen contained in ~15 kg of air). Electrochemical reactions are used by most mobile devices such as laptop computers and mobile phones to release the energy from batteries.\n\nThe following is a list of the thermal energy densities (that is to say: the amount of heat energy that can be extracted) of commonly used or well-known energy storage materials; it doesn't include uncommon or experimental materials. Note that this list does not consider the mass of reactants commonly available such as the oxygen required for combustion or the energy efficiency in use. An extended version of this table is found at Energy density#Extended Reference Table. Major reference = .\n\nThe following unit conversions may be helpful when considering the data in the table: 3.6 MJ = 1 kWh ≈ 1.34 HPh.\n\nIn energy storage applications the energy density relates the mass of an energy store to the volume of the storage facility, e.g. the fuel tank. The higher the energy density of the fuel, the more energy may be stored or transported for the same amount of volume. The energy density of a fuel per unit mass is called the specific energy of that fuel. In general an engine using that fuel will generate less kinetic energy due to inefficiencies and thermodynamic considerations—hence the specific fuel consumption of an engine will always be greater than its rate of production of the kinetic energy of motion.\n\nThe greatest energy source by far is mass itself. This energy, \"E = mc\", where \"m = ρV\", \"ρ\" is the mass per unit volume, \"V\" is the volume of the mass itself and \"c\" is the speed of light. This energy, however, can be released only by the processes of nuclear fission (0.1%), nuclear fusion (1%), or the annihilation of some or all of the matter in the volume \"V\" by matter-antimatter collisions (100%). Nuclear reactions cannot be realized by chemical reactions such as combustion. Although greater matter densities can be achieved, the density of a neutron star would approximate the most dense system capable of matter-antimatter annihilation possible. A black hole, although denser than a neutron star, does not have an equivalent anti-particle form, but would offer the same 100% conversion rate of mass to energy in the form of Hawking radiation. In the case of relatively small black holes (smaller than astronomical objects) the power output would be tremendous.\n\nThe highest density sources of energy aside from antimatter are fusion and fission. Fusion includes energy from the sun which will be available for billions of years (in the form of sunlight) but so far (2018), sustained fusion power production continues to be elusive. \n\nPower from fission of uranium and thorium in nuclear power plants will be available for many decades or even centuries because of the plentiful supply of the elements on earth, though the full potential of this source can only be realised through breeder reactors, which are, apart from the BN-600 reactor, not yet used commercially. Coal, gas, and petroleum are the current primary energy sources in the U.S. but have a much lower energy density. Burning local biomass fuels supplies household energy needs (cooking fires, oil lamps, etc.) worldwide. \n\nThe density of thermal energy contained in the core of a light water reactor (PWR or BWR) of typically 1 GWe (1 000 MW electrical corresponding to ~3 000 MW thermal) is in the range of 10 to 100 MW of thermal energy per cubic meter of cooling water depending on the location considered in the system (the core itself (~30 m), the reactor pressure vessel (~50 m), or the whole primary circuit (~300 m)). This represents a considerable density of energy which requires under all circumstances a continuous water flow at high velocity in order to be able to remove the heat from the core, even after an emergency shutdown of the reactor. The incapacity to cool the cores of three boiling water reactors (BWR) at Fukushima in 2011 after the tsunami and the resulting loss of the external electrical power and of the cold source was the cause of the meltdown of the three cores in only a few hours. Meanwhile, the three reactors were correctly shut down just after the Tōhoku earthquake. This extremely high power density distinguishes nuclear power plants (NPP's) from any thermal power plants (burning coal, fuel or gas) or any chemical plants and explains the large redundancy required to permanently control the neutron reactivity and to remove the residual heat from the core of NPP's.\n\nEnergy density differs from energy conversion efficiency (net output per input) or embodied energy (the energy output costs to provide, as harvesting, refining, distributing, and dealing with pollution all use energy). Large scale, intensive energy use impacts and is impacted by climate, waste storage, and environmental consequences.\n\nNo single energy storage method boasts the best in specific power, specific energy, and energy density. Peukert's Law describes how the amount of useful energy that can be obtained (for a lead-acid cell) depends on how quickly we pull it out. To maximize both specific energy and energy density, one can compute the specific energy density of a substance by multiplying the two values together, where the higher the number, the better the substance is at storing energy efficiently.\n\nAlternative options are discussed for energy storage to increase energy density and decrease charging time.\n\nGravimetric and volumetric energy density of some fuels and storage technologies (modified from the Gasoline article):\n\nThis table lists energy densities of systems that require external components, such as oxidisers or a heat sink or source. These figures do not take into account the mass and volume of the required components as they are assumed to be freely available and present in the atmosphere. Such systems cannot be compared with self-contained systems. These values may not be computed at the same reference conditions.\n\nDivide joule/m by 10 to get MJ/L. Divide MJ/L by 3.6 to get kWh/L.\n\nElectric and magnetic fields store energy. In a vacuum, the (volumetric) energy density is given by\n\nwhere E is the electric field and B is the magnetic field. The solution will be (in SI units) in Joules per cubic metre. In the context of magnetohydrodynamics, the physics of conductive fluids, the magnetic energy density behaves like an additional pressure that adds to the gas pressure of a plasma.\n\nIn normal (linear and nondispersive) substances, the energy density (in SI units) is\n\nwhere D is the electric displacement field and H is the magnetizing field.\n\nIn the case of absence of magnetic fields, by exploting Fröhlich's relationships it is also possible to extend these equations to anisotropy and nonlinearity dielectrics, as well as to calculate the correlated Helmholtz free energy and entropy densities.\n\n\n\n"}
{"id": "52058583", "url": "https://en.wikipedia.org/wiki?curid=52058583", "title": "Energy system", "text": "Energy system\n\nAn energy system is a system primarily designed to supply energy-services to end-users. Taking a structural viewpoint, the IPCC Fifth Assessment Report defines an energy system as \"all components related to the production, conversion, delivery, and use of energy\". The field of energy economics includes energy markets and treats an energy system as the technical and economic systems that satisfy consumer demand for energy in the forms of heat, fuels, and electricity.\n\nThe first two definitions allow for demand-side measures, including daylighting, retrofitted building insulation, and passive solar building design, as well as socio-economic factors, such as aspects of energy demand management and even telecommuting, while the third does not. Neither does the third account for the informal economy in traditional biomass that is significant in many developing countries.\n\nThe analysis of energy systems thus spans the disciplines of engineering and economics. Merging ideas from both areas to form a coherent description, particularly where macroeconomic dynamics are involved, is challenging.\n\nThe concept of an energy system is evolving as new regulations, technologies, and practices enter into service – for example, emissions trading, the development of smart grids, and the greater use of energy demand management, respectively.\n\nFrom a structural perspective, an energy system is like any general system and is made up of a set of interacting component parts, located within an environment. These components derive from ideas found in engineering and economics. Taking a process view, an energy system \"consists of an integrated set of technical and economic activities operating within a complex societal framework\". The identification of the components and behaviors of an energy system depends on the circumstances, the purpose of the analysis, and the questions under investigation. The concept of an energy system is therefore an abstraction which usually precedes some form of computer-based investigation, such as the construction and use of a suitable energy model.\n\nViewed in engineering terms, an energy system lends itself to representation as a flow network: the vertices map to engineering components like power stations and pipelines and the edges map to the interfaces between these components. This approach allows collections of similar or adjacent components to be aggregated and treated as one to simplify the model. Once described thus, flow network algorithms, such as minimum cost flow, may be applied. The components themselves can be treated as simple dynamical systems in their own right.\n\nConversely, relatively pure economic modeling may adopt a sectorial approach with only limited engineering detail present. The sector and sub-sector categories published by the International Energy Agency are often used as a basis for this analysis. A 2009 study of the UK residential energy sector contrasts the use of the technology-rich Markal model with several UK sectoral housing stock models.\n\nInternational energy statistics are typically broken down by carrier, sector and sub-sector, and country. Energy carriers ( energy products) are further classified as primary energy and secondary (or intermediate) energy and sometimes final (or end-use) energy. Published energy datasets are normally adjusted so that they are internally consistent, meaning that all energy stocks and flows must balance. The IEA regularly publishes energy statistics and energy balances with varying levels of detail and cost and also offers mid-term projections based on this data. The notion of an energy carrier, as used in energy economics, is distinct and different from the definition of energy used in physics.\n\nEnergy systems can range in scope, from local, municipal, national, and regional, to global, depending on issues under investigation. Researchers may or may not include demand side measures within their definition of an energy system. The IPCC does so, for instance, but covers these measures in separate chapters on transport, buildings, industry, and agriculture.\n\nHousehold consumption and investment decisions may also be included within the ambit of an energy system. Such considerations are not common because consumer behavior is difficult to characterize, but the trend is to include human factors in models. Household decision-taking may be represented using techniques from bounded rationality and agent-based behavior. The American Association for the Advancement of Science (AAAS) specifically advocates that \"more attention should be paid to incorporating behavioral considerations other than price- and income-driven behavior into economic models [of the energy system]\".\n\nThe concept of an energy-service is central, particularly when defining the purpose of an energy system:\n\nEnergy-services can be defined as amenities that are either furnished through energy consumption or could have been thus supplied. More explicitly:\n\nA consideration of energy-services per capita and how such services contribute to human welfare and individual quality of life is paramount to the debate on sustainable energy. People living in poor regions with low levels of energy-services consumption would clearly benefit from greater consumption, but the same is not generally true for those with high levels of consumption.\n\nThe notion of energy-services has given rise to energy-service companies (ESCo) who contract to provide energy-services to a client for an extended period. The ESCo is then free to choose the best means to do so, including investments in the thermal performance and HVAC equipment of the buildings in question.\n\nISO13600, ISO13601, and ISO13602 form a set of international standards covering technical energy systems (TES). Although withdrawn prior to 2016, these documents provide useful definitions and a framework for formalizing such systems. The standards depict an energy system broken down into supply and demand sectors, linked by the flow of tradable energy commodities (or energywares). Each sector has a set of inputs and outputs, some intentional and some harmful byproducts. Sectors may be further divided into subsectors, each fulfilling a dedicated purpose. The demand sector is ultimately present to supply energyware-based services to consumers (see energy-services).\n\n"}
{"id": "40745870", "url": "https://en.wikipedia.org/wiki?curid=40745870", "title": "Escape and radiate coevolution", "text": "Escape and radiate coevolution\n\nEscape and radiate coevolution is a multistep process that hypothesizes that an organism under constraints from other organisms will develop new defenses, allowing it to \"escape\" and then \"radiate\" into differing species. After a novel defense has been acquired, an organism is able to escape predation and rapidly multiply into new species because of relaxed selective pressure. There are many possible mechanisms available varying between different types of organisms, however they must be novel in order for escape to allow for radiation. This theory applies to predator-prey associations, but is most often applied to plant-herbivore associations.\n\nThis form of coevolution can be complex but is essential to understanding the vast biological diversity among organisms today. Out of the many forms of coevolution, escape and radiate is most likely responsible for providing the most diversity. This is due to the nature of the \"evolutionary arms race\" and the continuous cycle of counter adaptations. It is a relatively new field of study and is rapidly gaining credibility. To date, there has not been a formal study published specifically for escape and radiate coevolution.\n\nThis theory originated in a paper by Ehrlich and Raven, 1964, \"Butterflies and plants: a study in coevolution\". It outlined and laid the foundations of the concept. However, the term \"escape and radiate\" was not coined until Thompson's 1989 \"Concepts of Coevolution\". The theory has not yet been fully analyzed, however, as since its origins it has grown in importance among evolutionary biologists and botanists.\n\nIn order for an organism to \"escape\", and then radiate into varying species it needs a mechanism to escape. These defense mechanisms vary widely and differ for different types of organisms. Plants use chemical defenses in the form of secondary metabolites or allelochemicals. These allelochemicals inhibit the growth, behavior, and health of herbivores, allowing plants to escape. An example of a plant allelochemical are alkaloids that can inhibit protein synthesis in herbivores. Other forms of plant defense include mechanical defenses such as thigmonasty movements which have the plant leaves close in response to tactile stimulation. Indirect mechanisms plant include shedding of plant leaves so less leaves are available which deters herbivores, growth in locations in that are difficult to reach, and even mimicry. For organisms other than plants, examples of defense mechanisms allowing for escape include camouflage, aposematism, heightened senses and physical capabilities, and even defensive behaviors such as feigning death. An example of an organism using one of these defense mechanisms is the granular poison frog which defends itself through aposematism. It is important to understand that in order for escape and radiate coevolution to occur, it is necessary that the developed defense is novel rather than previously established.\n\nInduced defense stemming from adaptive phenotypic plasticity may help a plant defend itself against multiple enemies. Phenotypic plasticity occurs when an organism undergoes an environmental change forcing a change altering its behavior, physiology, etc. These induced defenses allow for an organism to escape.\n\nRadiation is the evolutionary process of diversification of a single species into multiple forms. It includes the physiological and ecological diversity within a rapidly multiplying lineage. There are many types of radiation including adaptive, concordant, and discordant radiation however escape and radiate coevolution does not always follow those specific types.\n\nThis eventually leads to the question, why does escape allow for radiation? Once a novel defense has been acquired, the attacking organism which had evolved adaptations that allowed it to predate is now up against a new defense that it has not yet been evolved to encounter. This gives the defending organism the advantage, and therefore time to rapidly multiply unopposed by the previously attacking organism. This ultimately leads to the physiological and ecological diversity within the rapidly multiplying lineage, hence radiation.\n\nA full study analyzing the effects of escape and radiate coevolution has not yet been completed which hinders knowing how applicable this form of coevolution could be to other areas of study, or global concerns, only hypotheses of its effects can be made. Improved agriculture, conservation, biological diversity, and epidemiology are just some of the areas that could potentially be helped through the study of coevolution and its specific hypotheses such as escape and radiate coevolution.\nA theory as to why we see such vast biological diversity today may be because of escape and radiate coevolution. After the organism escapes, it then radiates into multiple species, and spreads geographically. Evidence of escape and radiate coevolution can be seen through the starburst effect in plant and herbivore clades. When analyzing clades of predator-prey associations, although it varies, the starburst effect is a good indicator that escape and radiate coevolution may be occurring. Eventually this cycle must come to an end because adaptations that entail costs (allocation of resources, vulnerability to other predators) that at some point outweigh their benefits.\nEscape and radiate coevolution may support parallel cladogenesis, wherein plant and herbivore phylogenies might match with ancestral insects exploiting ancestral plants. This is significant because it allows researchers to hypothesize about the relationships between ancestral organisms. Unfortunately, there have not yet been any known examples specifically involving escape and radiate coevolution being used for hypothesizing ancestral relationships.\n\nMany times the organism that has \"escaped\" continuously undergoes selective pressure because the predator it has escaped from evolves to create another adaptation in response, causing the process to continue. These \"offensive\" traits developed by predators range widely. For example, herbivores can develop an adaptation that allows for improved detoxification which allow to overcome plant defenses, thus causing escape and radiate coevolution to continue. Often the term \"evolutionary arms race\" is used to illustrate the idea that continuous evolution is needed to maintain the same relative fitness while the two species are coevolving. This idea also ties in with the Red Queen hypothesis. Counter adaptations among two organisms through escape and radiate coevolution is a major driving force behind diversity.\n\nEscape and radiate coevolution produces much more biological variation than other evolutionary mechanisms. For instance, cospeciation is important for diversity amongst species that share a symbiotic relationship, however this does not create nearly as much diversity in comparison to reciprocal evolutionary change due to natural selection.\nEvidence of rapid diversification following a novel adaptation is shown through the evolution of resin and latex canal tubes in 16 different lineages of plants. Plants with resin or latex canals can easily defend themselves against insect herbivores. When lineages of canal bearing plants are compared to the lineages of canal free plants, it is apparent that canal bearing plants are far more diverse, supporting escape and radiate coevolution.\n\nThe most popular examples of escape and radiate coevolution are of plant-herbivore associations. The most classic example is of butterflies and plants outlined in Ehrlich and Raven's original paper, \"Butterflies and plants: a study in coevolution.\". Erlich and Raven found in 1964 that hostplants for butterflies had a wide range of chemical defenses, allowing them to escape herbivory. Butterflies who developed novel counter detoxification mechanisms against the hostplants chemical defenses were able to utilize the hostplant resources. The process of stepwise adaptation and counteradaptation among the butterflies and hostplants is continuous and creates vast diversity.\n\nTropical trees may also escape and defend themselves. Trees growing in high light were predicted to have few chemical defenses, but rapid synchronous leaf expansion and low leaf nutritional quality during expansion. Species growing in low light have high levels of different chemical defenses, poor nutritional quality and asynchronous leaf expansion. Depending on the level of light the trees were growing in influenced the type of defenses they obtained, either chemical or through leaf expansion. The trees exposed to less light developed various chemicals to defend themselves against herbivores, a defense not utilizing light. This study was significant because it illustrates the separation between defenses and their relationship with an organism escaping and radiating into other species. Development of novel defenses does not necessarily imply that escape is possible for a species of plant if herbivores are adapting at a faster rate.\n\nMilkweed plants contain latex-filled canals which deter insect herbivores. Latex is toxic for small herbivores because it disrupts sodium and potassium levels. This has allowed for milkweeds to \"escape\" and become extremely diverse. There are over 100 different species of milkweeds which shows how diverse the plant is, with escape and radiate coevolution playing a very large role in creating such a high number of species.\n\nKey adaptations are adaptations that allow a group of organisms to diversify. \"Daphnia lumholtzi\" is a water flea that is able to form rigid head spines in response to chemicals released when fish are present. These phenotypically plastic traits serve as an induced defense against these predators. A study showed that \"Daphnia pulicaria\" is competitively superior to \"D. lumholtzi\" in the absence of predators. However, in the presence of fish predation the invasive species formed its defenses and became the dominant water flea in the region. This switch in dominance suggests that the induced defense against fish predation could represent a key adaptation for the invasion success of \"D. lumholtzi\". A defensive trait that qualifies as a key adaptation is most likely an example of escape and radiate coevolution.\n\nThe theory can be applied at the microscopic level such as to bacteria-phage relationships. Bacteria were able to diversify and escape through resistance to phages. The diversity among the hosts and parasites differed among the range of infection and resistance. The implication of this study to humans is its important to understanding the evolution of infectious organisms, and preventing diseases.\n"}
{"id": "57977652", "url": "https://en.wikipedia.org/wiki?curid=57977652", "title": "Evolution in fiction", "text": "Evolution in fiction\n\nEvolution has been an important theme in fiction, including speculative evolution in science fiction, since the late 19th century, though it began before Charles Darwin's time, and reflects progressionist and Lamarckist views as well as Darwin's. Darwinian evolution is pervasive in literature, whether taken optimistically in terms of how humanity may evolve towards perfection, or pessimistically in terms of the dire consequences of the interaction of human nature and the struggle for survival. Other themes include the replacement of humanity, either by other species or by intelligent machines.\n\nCharles Darwin's evolution by natural selection, as set out in his 1859 \"On the Origin of Species\", is the dominant theory in modern biology, but it is accompanied as a philosophy and in fiction by two earlier evolutionary theories, progressionism (orthogenesis) and Lamarckism. Progressionism is the view that evolution is progress towards some goal of perfection, and that it is in some way directed towards that goal. Lamarckism, a philosophy that long predates Jean-Baptiste de Lamarck, is the view that evolution is guided by the inheritance of characteristics acquired by use or disuse during an animal's lifetime.\n\nIdeas of progress and evolution were popular, long before Darwinism, in the 18th century, leading to Nicolas-Edme Rétif's allegorical 1781 story \"\" (The Southern Hemisphere Discovery by a Flying Man). \n\nThe evolutionary biologist Kayla M. Hardwick quotes from the 2013 film \"Man of Steel\", where the villain Faora states: \"The fact that you possess a sense of morality, and we do not, gives us an evolutionary advantage. And if history has taught us anything, it is that evolution always wins.\" She points out that the idea that evolution wins is progressionist, while (she argues) the idea that evolution gives evil an advantage over the moral and good, driving the creation of formidable monsters, is a popular science fiction misconception. Hardwick gives as examples of the evolution of \"bad-guy traits\" the Morlocks in H. G. Wells's 1895 \"The Time Machine\", the bugs' caste system in Robert Heinlein's 1959 \"Starship Troopers\", and the effective colonisation by Don Siegel's 1956 \"Invasion of the Body Snatchers\" aliens.\n\nIn French 19th century literature, evolutionary fantasy was Lamarckian, as seen in Camille Flammarion's 1887 \"Lumen\" and his 1894 \"\", J.-H. Rosny's 1887 \"Les Xipéhuz\" and his 1910 \"La mort de la terre\", and Jules Verne's 1901 \"La grande forêt, le village aérien\". The Lamarckist philosopher Henri Bergson's creative evolution driven by the supposed élan vital likely inspired J. D. Beresford's English evolutionary fantasy, his 1911 \"The Hampdenshire Wonder\".\n\nDarwin's version of evolution has been widely explored in fiction, both in fantasies and in imaginative explorations of its grimmer \"survival of the fittest\" effects, with much attention focused on possible human evolution. H. G. Wells's \"The Time Machine\" already mentioned, his 1896 \"The Island of Dr Moreau\", and his 1898 \"The War of the Worlds\" all pessimistically explore the possible dire consequences of the darker sides of human nature in the struggle for survival. More broadly, Joseph Conrad's 1899 \"Heart of Darkness\" and R. L. Stevenson's 1886 \"Dr Jekyll and Mr Hyde\" portray Darwinian thinking in mainstream English literature.\n\nThe evolutionary biologist J. B. S. Haldane wrote an optimistic tale, \"The Last Judgement\", in the 1927 collection \"Possible Worlds\". This influenced Olaf Stapledon's 1930 \"Last and First Men\", which portrays the many species that evolved from humans in a billion-year timeframe. A different take on Darwinism is the idea, popular from the 1950s onwards, that humans will evolve more or less godlike mental capacity, as in Arthur C. Clarke's 1950 \"Childhood's End\" and Brian Aldiss's 1959 \"Galaxies Like Grains of Sand\". Another science fiction theme is the replacement of humanity on Earth by other species or intelligent machines. For instance, Olof Johannesson's 1966 \"The Great Computer\" gives humans the role of enabling intelligent machines to evolve, while Kurt Vonnegut's 1985 \"Galapagos\" is one of several novels to depict a replacement species.\n\n"}
{"id": "37493759", "url": "https://en.wikipedia.org/wiki?curid=37493759", "title": "Infrasonic passive differential spectroscopy", "text": "Infrasonic passive differential spectroscopy\n\nInfrasonic Passive Seismic Spectroscopy (IPSS) is a Passive Seismic Low Frequency technique used for mapping potential oil and gas hydrocarbon accumulations.\n\nIt is part of the geophysical techniques also known under the generic naming passive seismic which includes also the Passive Seismic Tomography and Micro Seismic Monitoring for petroleum, gas and geothermal applications. In a larger scale, Passive Seismic includes the Global Seismic Network earthquakes monitoring (GSN).\n\nRegarding petroleum and geothermal exploration (within a small scale), effect of fluid distribution on P- wave propagation in partially saturated rocks is the main responsible for the low frequency reservoir-related wavefield absorption.\nThe high level of attenuation, within the infrasonic bandwidth (below 10 Hz) of the seismic field observed in natural oil-saturated porous media during the last years (successfully explained by mesoscopic homogeneous models) is the main responsible of the passive seismic wave field shifting within a low frequency range.\nPressure differences between regions with different fluid/solid properties induce frequency-dependency of the attenuation (Qp and Qs reservoir factors) and velocity dispersion (Vp, Vs) of the low frequency wave field.\n\nInfrasonic Passive Seismic Spectroscopy techniques quantifies then the absorption and the wave field dispersion within the low frequency bandwidth giving the most predominant areas linked with possible oil-saturated and porous media.\n\nThe low frequency seismic field is not usually reachable by the active seismic surveys being either the explosive waves mainly in the high frequency and the vibroseis currently built not to reach such a low frequencies.\n\nSummary of the theoretical background of the passive seismic.\n\nQuintal B.. Journal of Applied Geophysics 82, pp. 119–128, 2012.\nLambert M.-A., Saenger E.H., Quintal B., Schmalholz S.M.. Geophysics 78, pp. T41-T52, 2013.\n\nArtman, B., I. Podladtchikov, and B. Witten, 2010, Source location using time-reverse imaging. Geophysical Prospecting, 58, 861–873.\n\nBiot M. A. 1956a. Journal of the Acoustical Society of America, 28, 168–178.\n\nBiot M.A. 1956b. Journal of the Acoustical Society ofAmerica, 28, 179–191.\n\nBiot M.A. 1962. Journal of Applied Physics 33, 1482–1498.\n\nCarcione, J. M., H. B. Helle, and N. H. Pham (2003),: Comparison with poroelastic numerical experiments. Geophysics, 68, 1389– 1398.\n\nDutta, N. C., and H. Ode, 1979a,: Geophysics, 44, 1777–1788.\n\nPride S.R. and Berryman J.G. 2003. Physical Review E 68, 036604.\n\nRubino, J. G., C. L. Ravazzoli, and J. E. Santos, 2009,: Geophysics, 74, no. 1, N1–N13.\n\nRiahi, N., B. Birkelo, and E. H. Saenger, 2011,: 73rd Annual Conference and Exhibition, EAGE, Extended Abstracts, P198.\n\nAkrawi, K., Campagna, F., Russo, L., Yousif, M. E., Abdelhafeez, M. H.,: Abstract: 10th Middle East Geosciences Conference and Exhibition, EAGE, Article: #90141©2012 GEO-2012,\n\nArtman, B., M. Duclos, B. Birkelo, F. Huguet, J. F. Dutzer, and R. Habiger, 2011, r: 73rd Annual Conference and Exhibition, EAGE, Extended Abstracts, P331.\n\nLambert, M.-A., S. M. Schmalholz, E. H. Saenger, and B. Steiner, 2009,: Geophysical Prospecting, 57, 393–411.\n\nSteiner, B., E. H. Saenger, and S. M. Schmalholz, 2008,: Application to hydrocarbon reservoir localization: Geophysical Research Letters, 35, L03307.\n\nToms, J., 2008. Effect of Fluid Distribution on Compressional Wave Propagation in Partially Saturated Rocks. PhD Thesis.\n\nWhite J.E., Mikhaylova N.G. and Lyakhovitskiy F.M. 1976. Izvestija Academy of Sciences USSR, Physics Solid Earth 11, 654–659.\n"}
{"id": "31744838", "url": "https://en.wikipedia.org/wiki?curid=31744838", "title": "List of dates predicted for apocalyptic events", "text": "List of dates predicted for apocalyptic events\n\nPredictions of apocalyptic events that would result in the extinction of humanity, a collapse of civilization, or the destruction of the planet have been made since at least the beginning of the Common Era. Most predictions are related to Abrahamic religions, often standing for or similar to the eschatological events described in their scriptures. Christian predictions typically refer to events like the Rapture, Great Tribulation, Last Judgment, and the Second Coming of Christ. End-time events are usually predicted to occur within the lifetime of the person making the prediction, and are usually made using the Bible, and in particular the New Testament, as either the primary or exclusive source for the predictions. Often this takes the form of mathematical calculations, such as trying to calculate the point where it will have been 6000 years since the supposed creation of the Earth by the Abrahamic God, which according to the Talmud marks the deadline for the Messiah to appear. Predictions of the end from natural events have also been theorised by various scientists and scientific groups. While these predictions are generally accepted as plausible within the scientific community, the events and phenomena are not expected to occur for hundreds of thousands or even billions of years from now.\n\nLittle research has been done into why people make apocalyptic predictions. Historically, it has been done for reasons such as diverting attention from actual crises like poverty and war, pushing political agendas, and promoting hatred of certain groups; antisemitism was a popular theme of Christian apocalyptic predictions in medieval times, while French and Lutheran depictions of the apocalypse were known to feature English and Catholic antagonists respectively. According to psychologists, possible explanations for why people believe in modern apocalyptic predictions include mentally reducing the actual danger in the world to a single and definable source, an innate human fascination with fear, personality traits of paranoia and powerlessness and a modern romanticism involved with end-times due to its portrayal in contemporary fiction. The prevalence of Abrahamic religions throughout modern history is said to have created a culture which encourages the embracement of a future that will be drastically different from the present. Such a culture is credited with the rise in popularity of predictions that are more secular in nature, such as the 2012 phenomenon, while maintaining the centuries-old theme that a powerful force will bring the end of humanity.\n\nPolls conducted in 2012 across 20 countries found over 14% of people believe the world will end in their lifetime, with percentages ranging from 6% of people in France to 22% in the US and Turkey. Belief in the apocalypse is most prevalent in people with lower rates of education, lower household incomes, and those under the age of 35. In the UK in 2015, 23% of the general public believed the apocalypse was likely to occur in their lifetime, compared to 10% of experts from the Global Challenges Foundation. The general public believed the likeliest cause would be nuclear war, while experts thought it would be artificial intelligence. Only 3% of Britons thought the end would be caused by the Last Judgement, compared to 16% of Americans. Between one and three percent of people from both countries thought the apocalypse would be caused by zombies or alien invasion.\n\nThis section lists eschatological predictions, mostly by religious individuals or groups. Most predictions are related to Abrahamic religions, with numerous predictions standing for the eschatological events described in their scriptures. Christian predictions typically refer to events like the Rapture, Great Tribulation, Last Judgment, and the Second Coming of Christ.\n\n\n"}
{"id": "18012286", "url": "https://en.wikipedia.org/wiki?curid=18012286", "title": "List of herbaria", "text": "List of herbaria\n\nThis is a list of herbaria, organized first by continent where the herbarium is located, then within each continent by size of the collection. A herbarium (\"plural\" \"herbaria\") is a collection of preserved plant specimens. These specimens may be whole plants or plant parts: these will usually be in a dried form, mounted on a sheet, but depending upon the material may also be kept in alcohol or other preservative. The same term is often used in mycology to describe an equivalent collection of preserved fungi and in phycology to describe a collection of algae.\n\nTo preserve their form and color, plants collected in the field are spread flat on sheets of newsprint and dried, usually in a plant press, between blotters or absorbent paper. The specimens, which are then mounted on sheets of stiff white paper, are labeled with all essential data, such as date and place found, description of the plant, altitude, and special habitat conditions. The sheet is then placed in a protective case. As a precaution against insect attack, the pressed plant is frozen or poisoned and the case disinfected.\n\nMost herbaria utilize a standard system of organizing their specimens into herbarium cases. Specimen sheets are stacked in groups by the species to which they belong and placed into a large lightweight folder that is labelled on the bottom edge. Groups of species folders are then placed together into larger, heavier folders by genus. The genus folders are then sorted by taxonomic family according to the standard system selected for use by the herbarium and placed into pigeonholes in herbarium cabinets. Herbaria are essential for the study of plant taxonomy, the study of geographic distributions, and the stabilizing of nomenclature. Herbaria also preserve an historical record of change in vegetation over time. In some cases, plants become extinct in one area, or may become extinct altogether. In such cases, specimens preserved in an herbarium can represent the only record of the plant's original distribution. Environmental scientists make use of such data to track changes in climate and human impact.\n\n"}
{"id": "16657861", "url": "https://en.wikipedia.org/wiki?curid=16657861", "title": "List of mire landscapes in Switzerland", "text": "List of mire landscapes in Switzerland\n\nThe List of mire landscapes in Switzerland is a list of Swiss bogs and wetlands. It is from the \"Federal Inventory of Mire Landscapes of Particular Beauty and National Importance\" in Switzerland. \n\n\n"}
{"id": "60773", "url": "https://en.wikipedia.org/wiki?curid=60773", "title": "List of woods", "text": "List of woods\n\nThis is a list of woods, in particular those most commonly used in the timber and lumber trade.\n\n\n\n\n\n"}
{"id": "822008", "url": "https://en.wikipedia.org/wiki?curid=822008", "title": "Lists of invasive species", "text": "Lists of invasive species\n\nThese are lists of invasive species by country or region. A species is regarded as invasive if it has been introduced by human action to a location, area, or region where it did not previously occur naturally (i.e., is not a native species), becomes capable of establishing a breeding population in the new location without further intervention by humans, and becomes a pest in the new location, threatening agriculture and/or the local biodiversity.\n\nThe term invasive species refers to a subset of those species defined as introduced species, for which see List of introduced species.\n\n\n\n"}
{"id": "58439642", "url": "https://en.wikipedia.org/wiki?curid=58439642", "title": "Margham", "text": "Margham\n\nMargham is an oil and gas field in Dubai, United Arab Emirates (UAE) and the largest onshore gas field in the emirate. The field is managed by Dusup - the Dubai Supply Authority. Condensate production ran at some 25,000 barrels per day in 2010. Margham also has an oil production capability.\n\nProduction at Margham commenced in 1984, with three major gas-bearing formations located up to 10,000 feet below sea level. The field is connected by pipeline to Jebel Ali, where the gas condensate is loaded onto tankers for export. Dry gas is now also sent by pipeline to supply the Dubai grid, with consumption increasing since 2015.\n\nMargham was initially developed as a liquids stripping/gas recycling project (dry gas was pumped back into the reservoir), but now operates as a gas storage facility for Dubai since 2008, allowing Dubai to depend on gas produced from Margham for its elecricity generation and desalination needs. This usage, together with sustainables such as DEWA's Mohammed bin Rashid Al Maktoum Solar Park, means that Dubai has eliminated the use of oil as a domestic energy fuel.\n\nAlthough it is a major producer with ambitions to develop its trading activities to become a major global LNG hub, the UAE is actually a net importer of LNG.\n"}
{"id": "395107", "url": "https://en.wikipedia.org/wiki?curid=395107", "title": "Mechanical philosophy", "text": "Mechanical philosophy\n\nThe mechanical philosophy is a natural philosophy describing the universe as similar to a large-scale mechanism. Mechanical philosophy is associated with the scientific revolution of Early Modern Europe. One of the first expositions of universal mechanism is found in the opening passages of \"Leviathan\" by Hobbes published in 1651.\n\nSome intellectual historians and critical theorists argue that early mechanical philosophy was tied to disenchantment and the rejection of the idea of nature as living or animated by spirits or angels. Other scholars, however, have noted that early mechanical philosophers nevertheless believed in magic, Christianity and spiritualism.\n\nSome ancient philosophies held that the universe is reducible to completely mechanical principles—that is, the motion and collision of matter. This view was closely linked with materialism and reductionism, especially that of the atomists and to a large extent, stoic physics. Later mechanists believed the achievements of the scientific revolution of the 17th century had shown that all phenomena could eventually be explained in terms of \"mechanical laws\": natural laws governing the motion and collision of matter that imply a determinism. If all phenomena can be explained entirely through the motion of matter under physical laws, as the gears of a clock determine that it must strike 2:00 an hour after striking 1:00, all phenomena must be completely determined, past, present or future.\n\nThe natural philosophers directly concerned with developing the mechanical philosophy were largely a French group, together with some of their personal connections. They included Pierre Gassendi, Marin Mersenne and René Descartes. Also involved were the English thinkers Sir Kenelm Digby, Thomas Hobbes and Walter Charleton; and the Dutch natural philosopher Isaac Beeckman.\n\nRobert Boyle used \"mechanical philosophers\" to refer both to those with a theory of \"corpuscles\" or atoms of matter, such as Gassendi and Descartes, and those who did without such a theory. One common factor was the clockwork universe view. His meaning would be problematic in the cases of Hobbes and Galileo Galilei; it would include Nicolas Lemery and Christiaan Huygens, as well as himself. Newton would be a transitional figure. Contemporary usage of \"mechanical philosophy\" dates back to 1952 and Marie Boas Hall.\n\nIn France the mechanical philosophy spread mostly through private academies and salons; in England in the Royal Society. In England it did not have a large initial impact in universities, which were somewhat more receptive in France, the Netherlands and Germany.\n\nOne of the first expositions of universal mechanism is found in the opening passages of \"Leviathan\" (1651) by Hobbes (1651); the book's second chapter invokes the principle of inertia, foundational for the mechanical philosophy. Boyle did not mention him as one of the group; but at the time they were on opposite sides of a controversy. Richard Westfall deems him a mechanical philosopher.\n\nHobbes's major statement of his natural philosophy is in \"De Corpore\" (1655). In part II and III of this work he goes a long way towards identifying fundamental physics with geometry; and he freely mixes concepts from the two areas.\n\nDescartes was also a mechanist. A substance dualist, he argued that reality was composed of two radically different types of substance: extended matter, on the one hand, and immaterial mind, on the other. Descartes argued that one cannot explain the conscious mind in terms of the spatial dynamics of mechanistic bits of matter cannoning off each other. Nevertheless, his understanding of biology was mechanistic in nature:\n\nHis scientific work was based on the traditional mechanistic understanding that animals and humans are completely mechanistic automata. Descartes' dualism was motivated by the seeming impossibility that mechanical dynamics could yield mental experiences.\n\nIsaac Beeckman's theory of mechanical philosophy described in his books \"Centuria\" and \"Journal\" is grounded in two components: matter and motion. To explain matter, Beeckman relied on atomism philosophy which explains that matter is composed of tiny inseparable particles that interact to create the objects seen in life. To explain motion, he supported the idea of inertia, a theory generated by Isaac Newton.\n\nIsaac Newton ushered in a weaker notion of mechanism that tolerated the action at a distance of gravity. Interpretations of Newton's scientific work in light of his occult research have suggested that he did not properly view the universe as mechanistic, but instead populated by mysterious forces and spirits and constantly sustained by God and angels. Later generations of philosophers who were influenced by Newton example were nonetheless often mechanists. Among them were Julien Offray de La Mettrie and Denis Diderot.\n\nThe French mechanist and determinist Pierre Simon de Laplace formulated some implications of the mechanist thesis, writing:\n\n\n"}
{"id": "52555162", "url": "https://en.wikipedia.org/wiki?curid=52555162", "title": "Metal vapor synthesis", "text": "Metal vapor synthesis\n\nIn chemistry, metal vapor synthesis (MVS) is a method for preparing metal complexes by combining freshly produced metal atoms or small particles with ligands. In contrast to the high reactivity of such freshly produced metal atoms, bulk metals typically are unreactive toward neutral ligands. The method has been used to prepare compounds that cannot be prepared by traditional synthetic methods, e.g. Ti(η-toluene). The technique relies on a reactor that evaporates the metal, allowing the vapor to impinge on a cold reactor wall that is coated with the organic ligand. The metal evaporates upon being heated resistively or irradiated with an electron beam. The apparatus operates under high vacuum. In a common implementation, the metal vapor and the organic ligand are co-condensed at liquid nitrogen temperatures.\n\nIn several case where compounds are prepared by MVS, related preparations employ conventional routes. Thus, tris(butadiene)molybdenum was first prepared by co-condensation of butadiene and Mo vapor, but yields are higher for the reduction of molybdenum(V) chloride in the presence of the diene.\n"}
{"id": "50752458", "url": "https://en.wikipedia.org/wiki?curid=50752458", "title": "My Journey into the Wilds of Chicago", "text": "My Journey into the Wilds of Chicago\n\nMy Journey into the Wilds of Chicago is a photo-literary coffee table book authored by Mike MacDonald, with forewords by Bill Kurtis and Stephen Packard. The book is a visual and educational journey through the prairies, savannas, and other natural areas throughout the Chicago metropolitan area.\n\nThe book contains more than 200 photographs and nearly two dozen essays and poems written by MacDonald about Chicago's wild side, ranging in geography from the lakefront to prairie lands just north of the border in Wisconsin, to Kankakee, Lockport, Batavia, and McHenry County.\n\n\"My Journey into the Wilds of Chicago\" served as the basis for the website \"ChicagoNatureNow.com!\", a website run by MacDonald. The website is a digital catalog of Chicago's forest preserves, and provides updates of the area's natural events.\n\nThe book was positively received, including a review from \"Publishers Weekly,\" which said of \"My Journey into the Wilds of Chicago\", \"this impressive, cloth-bound debut is a lucid perspective on the prairie and its native plants and animals; it is celebratory, soulful and poetic, evoking a strong affection for Chicago's unchecked wilderness in a city best known for its iconic lakefront and skyscrapers.\"\n\n"}
{"id": "2831510", "url": "https://en.wikipedia.org/wiki?curid=2831510", "title": "Natural fiber", "text": "Natural fiber\n\nNatural fibers or natural fibres (see spelling differences) are fibres that are produced by plants, animals, and geological processes. They can be used as a component of composite materials, where the orientation of fibers impacts the properties. Natural fibers can also be matted into sheets to make products such as paper, felt or fabric.\n\nThe earliest evidence of humans using fibers is the discovery of wool and dyed flax fibers found in a prehistoric cave in the Republic of Georgia that date back to 36,000 BP. Natural fibers can be used for high-tech applications, such as composite parts for automobiles. Compared to composites reinforced with glass fibers, composites with natural fibers have advantages such as lower density, better thermal insulation, and reduced skin irritation. Further, unlike glass fibers, natural fibers can be broken down by bacteria once they are no longer in use.\n\nNatural fibers are good sweat absorbents and can be found in a variety of textures. Cotton fibers made from the cotton plant, for example, produce fabrics that are light in weight, soft in texture, and which can be made in various sizes and colors. Clothes made of natural fibers such as cotton are often preferred over clothing made of synthetic fibers by people living in hot and humid climates.\n\nNatural fibers are made from plant, animal, and mineral sources. Natural fibers can be classified according to their origin.\n\nAnimal fibers generally comprise proteins such as collagen, keratin and fibroin; examples include silk, sinew, wool, catgut, angora, mohair and alpaca.\n\n\nChitin is the second most abundant natural polymer in the world, with collagen being the first. It is a “linear polysaccharide of β-(1-4)-2-acetamido-2-deoxy-D-glucose”. Chitin is highly crystalline and is usually composed of chains organized in a β sheet. Due to its high crystallinity and chemical structure, it is insoluble in many solvents. It also has a low toxicity in the body and is inert in the intestines. Chitin also has antibacterial properties.\n\nChitin forms crystals that make fibrils that become surrounded by proteins. These fibrils can bundle to make larger fibers that contribute to the hierarchical structure of many biological materials. These fibrils can form randomly oriented networks that provide the mechanical strength of the organic layer in different biological materials.\n\nChitin provides protection and structural support to many living organisms. It makes up the cell walls of fungi and yeast, the shells of mollusks, the exoskeletons of insects and arthropods. In shells and exoskeletons, the chitin fibers contribute to their hierarchical structure.\n\nIn nature, pure chitin (100% acetylation) does not exist. It instead exists as a copolymer with chitin's deacetylated derivative, chitosan. When the acetylized composition of the copolymer is over 50% acetylated it is chitin. This copolymer of chitin and chitosan is a random or block copolymer.\n\nChitosan is a deacetylated derivative of chitin. When the acetylized composition of the copolymer is below 50% it is chitosan. Chitosan is a semicrystalline “polymer of β-(1-4)-2-amino-2-deoxy-D-glucose”. One difference between chitin and chitosan is that chitosan is soluble in acidic aqueous solutions. Chitosan is easier to process that chitin, but it is less stable because it is more hydrophilic and has pH sensitivity. Due to its ease of processing, chitosan is used in biomedical applications.\n\nCollagen is a structural protein, often referred to as “the steel of biological materials”. There are multiple types of collagen: Type I (comprising skin, tendons and ligaments, vasculature and organs, as well as teeth and bone); Type II (a component in cartilage); Type III (often found in reticular fibers); and others. Collagen has a hierarchical structure, forming triple helices, fibrils, and fibers.\n\nKeratin is a structural protein located at the hard surfaces in many vertebrates. Keratin has two forms, α-keratin and β-keratin and are used by different classes of animals. The naming convention for proteins follows that for keratin, alpha keratin is helical and beta keratin is sheet like. Alpha keratin is found in mammalian hair, skin, nails, horn and quills, while beta keratin can be found in avian and reptilian species in scales, feathers, and beaks. The two different structures of keratin have dissimilar mechanical properties, as seen in their dissimilar applications. The relative alignment of the keratin fibrils has a significant impact on the mechanical properties. In human hair the filaments of alpha keratin are highly aligned, giving a tensile strength of approximately 200MPa. This tensile strength is an order of magnitude higher than human nails (20MPa), because human hair’s keratin filaments are more aligned.\n\nCompared to synthetic fibers, natural fibers tend have decreased stiffness and strength.\n\nProperties also decrease with the age of the fiber. Younger fibers tend to be stronger and more elastic than older ones. Many natural fibers exhibit strain rate sensitivity due to their viscoelastic nature. Bone contains collagen and exhibits strain rate sensitivity in that the stiffness increases with strain rate, also known as strain hardening. Spider silk has hard and elastic regions that together contribute to its strain rate sensitivity, these cause the silk to exhibit strain hardening as well. Properties of natural fibers are also dependent on the moisture content in the fiber.\n\nThe presence of water plays a crucial role in the mechanical behavior of natural fibers. Hydrated, biopolymers generally have enhanced ductility and toughness. Water plays the role of a plasticizer, a small molecule easing passage of polymer chains and in doing so increasing ductility and toughness. When using natural fibers in applications outside of their native use, the original level of hydration must be taken into account. For example when hydrated, the Young’s Modulus of collagen decreases from 3.26 to 0.6 GPa and becomes both more ductile and tougher. Additionally the density of collagen decreases from 1.34 to 1.18 g/cm^3.\n\nOf industrial value are four animal fibers, wool, silk, camel hair, and angora as well as four plant fibers, cotton, flax, hemp, and jute. Dominant in terms of scale of production and use is cotton for textiles.\n\nNatural fibers are also used in composite materials, much like synthetic or glass fibers. These composites, called biocomposites, are a natural fiber in a matrix of synthetic polymers. One of the first biofiber-reinforced plastics in use was a cellulose fiber in phenolics in 1908. Usage includes applications where energy absorption is important, such as insulation, noise absorbing panels, or collapsable areas in automobiles.\n\nNatural fibers can have different advantages over synthetic reinforcing fibers. Most notably they are biodegradable and renewable. Additionally, they often have low densities and lower processing costs than synthetic materials. Design issues with natural fiber-reinforced composites include poor strength (natural fibers are not as strong as glass fibers) and difficulty with actually bonding the fibers and the matrix. Hydrophobic polymer matrices offer insufficient adhesion for hydrophilic fibers.\n\nNanocomposites are desirable for their mechanical properties. When fillers in a composite are at the nanometer length scale, the surface to volume ratio of the filler material is high, which influences the bulk properties of the composite more compared to traditional composites. The properties of these nanosized elements is markedly different than that of its bulk constituent.\n\nIn regards to natural fibers, some of the best example of nanocomposites appear in biology. Bone, abalone shell, nacre, and tooth enamel are all nanocomposites. As of 2010, most synthetic polymer nanocomposites exhibit inferior toughness and mechanical properties compared to biological nanocomposites. Completely synthetic nanocomposites do exist, however nanosized biopolymers are also being tested in synthetic matrices. Several types of protein based, nanosized fibers are being used in nanocomposites. These include collagen, cellulose, chitin and tunican. These structural proteins must be processed before use in composites.\n\nTo use cellulose as an example, semicrystalline microfibrils are sheared in the amorphous region, resulting in microcrystalline cellulose (MCC). These small, crystalline cellulose fibrils are at this points reclassified as a whisker and can be 2 to 20 nm in diameter with shapes ranging from spherical to cylindrical. Whiskers of collagen, chitin, and cellulose have all be used to make biological nanocomposites. The matrix of these composites are commonly hydrophobic synthetic polymers such as polyethylene, and polyvinyl chloride and copolymers of polystyrene and polyacrylate.\n\nTraditionally in composite science a strong interface between the matrix and filler is required to achieve favorable mechanical properties. If this is not the case, the phases tend to separate along the weak interface and makes for very poor mechanical properties. In a MCC composite however this is not the case, if the interaction between the filler and matrix is stronger than the filler-filler interaction the mechanical strength of the composite is noticeably decreased.\n\nDifficulties in natural fiber nanocomposites arise from dispersity and the tendency small fibers to aggregate in the matrix. Because of the high surface area to volume ratio the fibers have a tendency to aggregate, more so than in micro-scale composites. Additionally secondary processing of collagen sources to obtain sufficient purity collagen micro fibrils adds a degree of cost and challenge to creating a load bearing cellulose or other filler based nanocomposite.\n\nNatural fibers often show promise as biomaterials in medical applications. Chitin is notable in particular and has been incorporated into a variety of uses. Chitin based materials have also been used to remove industrial pollutants from water, processed into fibers and films, and used as biosensors in the food industry. Chitin has also been used several of medical applications. It has been incorporated as a bone filling material for tissue regeneration, a drug carrier and excipient, and as an antitumor agent. Insertion of foreign materials into the body often triggers an immune response, which can have a variety of positive or negative outcomes depending on the bodies response to the material. Implanting something made from naturally synthesized proteins, such as a keratin based implant, has the potential to be recognized as natural tissue by the body. This can lead either to integration in rare cases where the structure of the implant promotes regrowth of tissue with the implant forming a superstructure or degradation of the implant in which the backbones of the proteins are recognized for cleavage by the body.\n\n\n"}
{"id": "38393", "url": "https://en.wikipedia.org/wiki?curid=38393", "title": "Natural rubber", "text": "Natural rubber\n\nNatural rubber, also called India rubber or caoutchouc, as initially produced, consists of polymers of the organic compound isoprene, with minor impurities of other organic compounds, plus water. Malaysia and Indonesia are two of the leading rubber producers. Forms of polyisoprene that are used as natural rubbers are classified as elastomers.\n\nCurrently, rubber is harvested mainly in the form of the latex from the rubber tree or others. The latex is a sticky, milky colloid drawn off by making incisions in the bark and collecting the fluid in vessels in a process called \"tapping\". The latex then is refined into rubber ready for commercial processing. In major areas, latex is allowed to coagulate in the collection cup. The coagulated lumps are collected and processed into dry forms for marketing.\n\nNatural rubber is used extensively in many applications and products, either alone or in combination with other materials. In most of its useful forms, it has a large stretch ratio and high resilience, and is extremely waterproof.\n\nThe major commercial source of natural rubber latex is the Pará rubber tree (\"Hevea brasiliensis\"), a member of the spurge family, \"Euphorbiaceae\". This species is preferred because it grows well under cultivation. A properly managed tree responds to wounding by producing more latex for several years.\n\nCongo rubber, formerly a major source of rubber, came from vines in the genus \"Landolphia\" (\"L. kirkii\", \"L. heudelotis\", and \"L. owariensis\").\n\nDandelion milk contains latex. The latex exhibits the same quality as the natural rubber from rubber trees. In the wild types of dandelion, latex content is low and varies greatly. In Nazi Germany, research projects tried to use dandelions as a base for rubber production, but failed. In 2013, by inhibiting one key enzyme and using modern cultivation methods and optimization techniques, scientists in the Fraunhofer Institute for Molecular Biology and Applied Ecology (IME) in Germany developed a cultivar that is suitable for commercial production of natural rubber. In collaboration with Continental Tires, IME began a pilot facility.\n\nMany other plants produce forms of latex rich in isoprene polymers, though not all produce usable forms of polymer as easily as the Pará. Some of them require more elaborate processing to produce anything like usable rubber, and most are more difficult to tap. Some produce other desirable materials, for example gutta-percha (\"Palaquium gutta\") and chicle from \"Manilkara\" species. Others that have been commercially exploited, or at least showed promise as rubber sources, include the rubber fig (\"Ficus elastica\"), Panama rubber tree (\"Castilla elastica\"), various spurges (\"Euphorbia\" spp.), lettuce (\"Lactuca\" species), the related \"Scorzonera tau-saghyz\", various \"Taraxacum\" species, including common dandelion (\"Taraxacum officinale\") and Russian dandelion (\"Taraxacum kok-saghyz\"), and perhaps most importantly for its hypoallergenic properties, guayule (\"Parthenium argentatum\"). The term gum rubber is sometimes applied to the tree-obtained version of natural rubber in order to distinguish it from the synthetic version.\n\nThe first use of rubber was by the indigenous cultures of Mesoamerica. The earliest archeological evidence of the use of natural latex from the \"Hevea\" tree comes from the Olmec culture, in which rubber was first used for making balls for the Mesoamerican ballgame. Rubber was later used by the Maya and Aztec cultures – in addition to making balls Aztecs used rubber for other purposes such as making containers and to make textiles waterproof by impregnating them with the latex sap.\n\nThe Pará rubber tree is indigenous to South America. Charles Marie de La Condamine is credited with introducing samples of rubber to the \"Académie Royale des Sciences\" of France in 1736. In 1751, he presented a paper by François Fresneau to the Académie (published in 1755) that described many of rubber's properties. This has been referred to as the first scientific paper on rubber. In England, Joseph Priestley, in 1770, observed that a piece of the material was extremely good for rubbing off pencil marks on paper, hence the name \"rubber\". It slowly made its way around England. In 1764 François Fresnau discovered that turpentine was a rubber solvent. Giovanni Fabbroni is credited with the discovery of naphtha as a rubber solvent in 1779.\n\nSouth America remained the main source of the limited amounts of latex rubber used during much of the 19th century. The trade was heavily protected and exporting seeds from Brazil was a capital offense, although no law prohibited it. Nevertheless, in 1876, Henry Wickham smuggled 70,000 Pará rubber tree seeds from Brazil and delivered them to Kew Gardens, England. Only 2,400 of these germinated. Seedlings were then sent to India, British Ceylon (Sri Lanka), Dutch East Indies (Indonesia), Singapore, and British Malaya. Malaya (now Peninsular Malaysia) was later to become the biggest producer of rubber.\n\nIn the early 1900s, the Congo Free State in Africa was also a significant source of natural rubber latex, mostly gathered by forced labor. King Leopold II's colonial state brutally enforced production quotas. Tactics to enforce the rubber quotas included removing the hands of victims to prove they had been killed. Soldiers often came back from raids with baskets full of chopped-off hands. Villages that resisted were razed to encourage better compliance locally. See Atrocities in the Congo Free State for more information on the rubber trade in the Congo Free State in the late 1800s and early 1900s. Liberia and Nigeria started production.\n\nIn India, commercial cultivation was introduced by British planters, although the experimental efforts to grow rubber on a commercial scale were initiated as early as 1873 at the Calcutta Botanical Gardens. The first commercial \"Hevea\" plantations were established at Thattekadu in Kerala in 1902. In later years the plantation expanded to Karnataka, Tamil Nadu and the Andaman and Nicobar Islands of India. India today is the world's 3rd largest producer and 4th largest consumer.\n\nIn Singapore and Malaya, commercial production was heavily promoted by Sir Henry Nicholas Ridley, who served as the first Scientific Director of the Singapore Botanic Gardens from 1888 to 1911. He distributed rubber seeds to many planters and developed the first technique for tapping trees for latex without causing serious harm to the tree. Because of his fervent promotion of this crop, he is popularly remembered by the nickname \"Mad Ridley\".\n\nCharles Goodyear developed vulcanization in 1839, although Mesoamericans used stabilized rubber for balls and other objects as early as 1600 BC.\n\nBefore World War II significant uses included door and window profiles, hoses, belts, gaskets, matting, flooring and dampeners (antivibration mounts) for the automotive industry. The use of rubber in car tires (initially solid rather than pneumatic) in particular consumed a significant amount of rubber. Gloves (medical, household and industrial) and toy balloons were large consumers of rubber, although the type of rubber used is concentrated latex. Significant tonnage of rubber was used as adhesives in many manufacturing industries and products, although the two most noticeable were the paper and the carpet industries. Rubber was commonly used to make rubber bands and pencil erasers.\n\nRubber produced as a fiber, sometimes called 'elastic', had significant value to the textile industry because of its excellent elongation and recovery properties. For these purposes, manufactured rubber fiber was made as either an extruded round fiber or rectangular fibers cut into strips from extruded film. Because of its low dye acceptance, feel and appearance, the rubber fiber was either covered by yarn of another fiber or directly woven with other yarns into the fabric. Rubber yarns were used in foundation garments. While rubber is still used in textile manufacturing, its low tenacity limits its use in lightweight garments because latex lacks resistance to oxidizing agents and is damaged by aging, sunlight, oil and perspiration. The textile industry turned to neoprene (polymer of chloroprene), a type of synthetic rubber, as well as another more commonly used elastomer fiber, spandex (also known as elastane), because of their superiority to rubber in both strength and durability.\n\nRubber exhibits unique physical and chemical properties. Rubber's stress–strain behavior exhibits the Mullins effect and the Payne effect and is often modeled as hyperelastic. Rubber strain crystallizes.\n\nDue to the presence of weakened allylic C-H bonds in each repeat unit, natural rubber is susceptible to vulcanisation as well as being sensitive to ozone cracking.\n\nThe two main solvents for rubber are turpentine and naphtha (petroleum). Because rubber does not dissolve easily, the material is finely divided by shredding prior to its immersion.\n\nAn ammonia solution can be used to prevent the coagulation of raw latex.\n\nRubber begins to melt at approximately .\n\nOn a microscopic scale, relaxed rubber is a disorganized cluster of erratically changing wrinkled chains. In stretched rubber, the chains are almost linear. The restoring force is due to the preponderance of wrinkled conformations over more linear ones. For the quantitative treatment see ideal chain, for more examples see entropic force.\n\nCooling below the glass transition temperature permits local conformational changes but a reordering is practically impossible because of the larger energy barrier for the concerted movement of longer chains. \"Frozen\" rubber's elasticity is low and strain results from small changes of bond lengths and angles: this caused the \"Challenger\" disaster, when the American Space Shuttle's flattened o-rings failed to relax to fill a widening gap. The glass transition is fast and reversible: the force resumes on heating.\n\nThe parallel chains of stretched rubber are susceptible to crystallization. This takes some time because turns of twisted chains have to move out of the way of the growing crystallites. Crystallization has occurred, for example, when, after days, an inflated toy balloon is found withered at a relatively large remaining volume. Where it is touched, it shrinks because the temperature of the hand is enough to melt the crystals.\n\nVulcanization of rubber creates di- and polysulfide bonds between chains, which limits the degrees of freedom and results in chains that tighten more quickly for a given strain, thereby increasing the elastic force constant and making the rubber harder and less extensible.\n\nRaw rubber storage depots and rubber processing can produce malodour that is serious enough to become a source of complaints and protest to those living in the vicinity.\n\nMicrobial impurities originate during the processing of block rubber. These impurities break down during storage or thermal degradation and produce volatile organic compounds. Examination of these compounds using gas chromatography/mass spectrometry (GC/MS) and gas chromatography (GC) indicates that they contain sulphur, ammonia, alkenes, ketones, esters, hydrogen sulphite, nitrogen, and low molecular weight fatty acids (C2-C5).\n\nWhen latex concentrate is produced from rubber, sulphuric acid is used for coagulation. This produces malodourous hydrogen sulphide.\n\nThe industry can mitigate these bad odours with scrubber systems.\n\nLatex is the polymer cis-1,4-polyisoprene – with a molecular weight of 100,000 to 1,000,000 daltons. Typically, a small percentage (up to 5% of dry mass) of other materials, such as proteins, fatty acids, resins, and inorganic materials (salts) are found in natural rubber. Polyisoprene can also be created synthetically, producing what is sometimes referred to as \"synthetic natural rubber\", but the synthetic and natural routes are different. Some natural rubber sources, such as gutta-percha, are composed of trans-1,4-polyisoprene, a structural isomer that has similar properties.\n\nNatural rubber is an elastomer and a thermoplastic. Once the rubber is vulcanized, it is a thermoset. Most rubber in everyday use is vulcanized to a point where it shares properties of both; i.e., if it is heated and cooled, it is degraded but not destroyed.\n\nThe final properties of a rubber item depend not just on the polymer, but also on modifiers and fillers, such as carbon black, factice, whiting and others.\n\nRubber particles are formed in the cytoplasm of specialized latex-producing cells called laticifers within rubber plants. Rubber particles are surrounded by a single phospholipid membrane with hydrophobic tails pointed inward. The membrane allows biosynthetic proteins to be sequestered at the surface of the growing rubber particle, which allows new monomeric units to be added from outside the biomembrane, but within the lacticifer. The rubber particle is an enzymatically active entity that contains three layers of material, the rubber particle, a biomembrane and free monomeric units. The biomembrane is held tightly to the rubber core due to the high negative charge along the double bonds of the rubber polymer backbone. Free monomeric units and conjugated proteins make up the outer layer. The rubber precursor is isopentenyl pyrophosphate (an allylic compound), which elongates by Mg-dependent condensation by the action of rubber transferase. The monomer adds to the pyrophosphate end of the growing polymer. The process displaces the terminal high-energy pyrophosphate. The reaction produces a cis polymer. The initiation step is catalyzed by prenyltransferase, which converts three monomers of isopentenyl pyrophosphate into farnesyl pyrophosphate. The farnesyl pyrophosphate can bind to rubber transferase to elongate a new rubber polymer.\n\nThe required isopentenyl pyrophosphate is obtained from the mevalonate pathway, which derives from acetyl-CoA in the cytosol. In plants, isoprene pyrophosphate can also be obtained from the 1-deox-D-xyulose-5-phosphate/2-C-methyl-D-erythritol-4-phosphate pathway within plasmids. The relative ratio of the farnesyl pyrophosphate initiator unit and isoprenyl pyrophosphate elongation monomer determines the rate of new particle synthesis versus elongation of existing particles. Though rubber is known to be produced by only one enzyme, extracts of latex host numerous small molecular weight proteins with unknown function. The proteins possibly serve as cofactors, as the synthetic rate decreases with complete removal.\n\nClose to 28 million tons of rubber were produced in 2013, of which approximately 44% was natural. Since the bulk is synthetic, which is derived from petroleum, the price of natural rubber is determined, to a large extent, by the prevailing global price of crude oil. Asia was the main source of natural rubber, accounting for about 94% of output in 2005. The three largest producers, Thailand, Indonesia (2.4 million tons) and Malaysia, together account for around 72% of all natural rubber production. Natural rubber is not cultivated widely in its native continent of South America due to the existence of South American leaf blight, and other natural predators.\n\nRubber latex is extracted from rubber trees. The economic life period of rubber trees in plantations is around 32 years — up to 7 years of immature phase and about 25 years of productive phase.\n\nThe soil requirement is well-drained, weathered soil consisting of laterite, lateritic types, sedimentary types, nonlateritic red or alluvial soils.\n\nThe climatic conditions for optimum growth of rubber trees are:\n\nMany high-yielding clones have been developed for commercial planting. These clones yield more than 2,000 kg of dry rubber per hectare per year, under ideal conditions.\n\nIn places such as Kerala and Sri Lanka where coconuts are in abundance, the half shell of coconut was used as the latex collection container. Glazed pottery or aluminium or plastic cups became more common in Kerala and other countries. The cups are supported by a wire that encircles the tree. This wire incorporates a spring so it can stretch as the tree grows. The latex is led into the cup by a galvanised \"spout\" knocked into the bark. Tapping normally takes place early in the morning, when the internal pressure of the tree is highest. A good tapper can tap a tree every 20 seconds on a standard half-spiral system, and a common daily \"task\" size is between 450 and 650 trees. Trees are usually tapped on alternate or third days, although many variations in timing, length and number of cuts are used. \"Tappers would make a slash in the bark with a small hatchet. These slanting cuts allowed latex to flow from ducts located on the exterior or the inner layer of bark (cambium) of the tree. Since the cambium controls the growth of the tree, growth stops if it is cut. Thus, rubber tapping demanded accuracy, so that the incisions would not be too many given the size of the tree, or too deep, which could stunt its growth or kill it.\"\n\nIt is usual to tap a pannel at least twice, sometimes three times, during the tree's life. The economic life of the tree depends on how well the tapping is carried out, as the critical factor is bark consumption. A standard in Malaysia for alternate daily tapping is 25 cm (vertical) bark consumption per year. The latex-containing tubes in the bark ascend in a spiral to the right. For this reason, tapping cuts usually ascend to the left to cut more tubes.\n\nThe trees drip latex for about four hours, stopping as latex coagulates naturally on the tapping cut, thus blocking the latex tubes in the bark. Tappers usually rest and have a meal after finishing their tapping work, then start collecting the liquid \"field latex\" at about midday.\n\nThe four types of field coagula are \"cuplump\", \"treelace\", \"smallholders' lump\" and \"earth scrap\". Each has significantly different properties. Some trees continue to drip after the collection leading to a small amount of \"cup lump\" that is collected at the next tapping. The latex that coagulates on the cut is also collected as \"tree lace\". Tree lace and cup lump together account for 10–20% of the dry rubber produced. Latex that drips onto the ground, \"earth scrap\", is also collected periodically for processing of low-grade product.\n\nCup lump is the coagulated material found in the collection cup when the tapper next visits the tree to tap it again. It arises from latex clinging to the walls of the cup after the latex was last poured into the bucket, and from late-dripping latex exuded before the latex-carrying vessels of the tree become blocked. It is of higher purity and of greater value than the other three types.\n\nTree lace is the coagulum strip that the tapper peels off the previous cut before making a new cut. It usually has higher copper and manganese contents than cup lump. Both copper and manganese are pro-oxidants and can damage the physical properties of the dry rubber.\n\nSmallholders' lump is produced by smallholders who collect rubber from trees far from the nearest factory. Many Indonesian smallholders, who farm paddies in remote areas, tap dispersed trees on their way to work in the paddy fields and collect the latex (or the coagulated latex) on their way home. As it is often impossible to preserve the latex sufficiently to get it to a factory that processes latex in time for it to be used to make high quality products, and as the latex would anyway have coagulated by the time it reached the factory, the smallholder will coagulate it by any means available, in any container available. Some smallholders use small containers, buckets etc., but often the latex is coagulated in holes in the ground, which are usually lined with plastic sheeting. Acidic materials and fermented fruit juices are used to coagulate the latex — a form of assisted biological coagulation. Little care is taken to exclude twigs, leaves, and even bark from the lumps that are formed, which may also include tree lace.\n\nEarth scrap is material that gathers around the base of the tree. It arises from latex overflowing from the cut and running down the bark, from rain flooding a collection cup containing latex, and from spillage from tappers' buckets during collection. It contains soil and other contaminants, and has variable rubber content, depending on the amount of contaminants. Earth scrap is collected by field workers two or three times a year and may be cleaned in a scrap-washer to recover the rubber, or sold to a contractor who cleans it and recovers the rubber. It is of low quality.\n\nLatex coagulates in the cups if kept for long and must be collected before this happens. The collected latex, \"field latex\", is transferred into coagulation tanks for the preparation of dry rubber or transferred into air-tight containers with sieving for ammoniation. Ammoniation preserves the latex in a colloidal state for longer periods of time.\n\nLatex is generally processed into either latex concentrate for manufacture of dipped goods or coagulated under controlled, clean conditions using formic acid. The coagulated latex can then be processed into the higher-grade, technically specified block rubbers such as SVR 3L or SVR CV or used to produce Ribbed Smoke Sheet grades.\n\nNaturally coagulated rubber (cup lump) is used in the manufacture of TSR10 and TSR20 grade rubbers. Processing for these grades is a size reduction and cleaning process to remove contamination and prepare the material for the final stage of drying.\n\nThe dried material is then baled and palletized for storage and shipment.\n\nNatural rubber is often vulcanized, a process by which the rubber is heated and sulfur, peroxide or bisphenol are added to improve resistance and elasticity and to prevent it from perishing. Before World War II, carbon black was often used as an additive to rubber to improve its strength, especially in vehicle tires.\n\nNatural rubber latex is shipped from factories in south-west Asia, South America, and West and Center Africa to destinations around the world. As the cost of natural rubber has risen significantly and rubber products are dense, the shipping methods offering the lowest cost per unit weight are preferred. Depending on destination, warehouse availability, and transportation conditions, some methods are preferred by certain buyers. In international trade, latex rubber is mostly shipped in 20-foot ocean containers. Inside the container, smaller containers are used to store the latex.\n\nUncured rubber is used for cements; for adhesive, insulating, and friction tapes; and for crepe rubber used in insulating blankets and footwear. Vulcanized rubber has many more applications. Resistance to abrasion makes softer kinds of rubber valuable for the treads of vehicle tires and conveyor belts, and makes hard rubber valuable for pump housings and piping used in the handling of abrasive sludge.\n\nThe flexibility of rubber is appealing in hoses, tires and rollers for devices ranging from domestic clothes wringers to printing presses; its elasticity makes it suitable for various kinds of shock absorbers and for specialized machinery mountings designed to reduce vibration. Its relative gas impermeability makes it useful in the manufacture of articles such as air hoses, balloons, balls and cushions. The resistance of rubber to water and to the action of most fluid chemicals has led to its use in rainwear, diving gear, and chemical and medicinal tubing, and as a lining for storage tanks, processing equipment and railroad tank cars. Because of their electrical resistance, soft rubber goods are used as insulation and for protective gloves, shoes and blankets; hard rubber is used for articles such as telephone housings, parts for radio sets, meters and other electrical instruments. The coefficient of friction of rubber, which is high on dry surfaces and low on wet surfaces, leads to its use for power-transmission belting and for water-lubricated bearings in deep-well pumps. Indian rubber balls or lacrosse balls are made of rubber.\n\nAround 25 million tonnes of rubber are produced each year, of which 30 percent is natural. The remainder is synthetic rubber derived from petrochemical sources. The top end of latex production results in latex products such as surgeons' gloves, condoms, balloons and other relatively high-value products. The mid-range which comes from the technically specified natural rubber materials ends up largely in tires but also in conveyor belts, marine products, windshield wipers and miscellaneous goods. Natural rubber offers good elasticity, while synthetic materials tend to offer better resistance to environmental factors such as oils, temperature, chemicals and ultraviolet light. \"Cured rubber\" is rubber that has been compounded and subjected to the vulcanisation process to create cross-links within the rubber matrix.\n\nSome people have a serious latex allergy, and exposure to natural latex rubber products such as latex gloves can cause anaphylactic shock. The antigenic proteins found in \"Hevea\" latex may be deliberately reduced (though not eliminated) through processing.\n\nLatex from non-\"Hevea\" sources, such as Guayule, can be used without allergic reaction by persons with an allergy to \"Hevea\" latex.\n\nSome allergic reactions are not to the latex itself, but from residues of chemicals used to accelerate the cross-linking process. Although this may be confused with an allergy to latex, it is distinct from it, typically taking the form of Type IV hypersensitivity in the presence of traces of specific processing chemicals.\n\nNatural rubber is susceptible to degradation by a wide range of bacteria.\nThe bacteria \"Streptomyces coelicolor\", \"Pseudomonas citronellolis\", and \"Nocardia\" spp. are capable of degrading vulcanized natural rubber.\n\n\n"}
{"id": "1870708", "url": "https://en.wikipedia.org/wiki?curid=1870708", "title": "Near-Earth supernova", "text": "Near-Earth supernova\n\nA near-Earth supernova is an explosion resulting from the death of a star that occurs close enough to the Earth (roughly less than 10 to 300 parsecs (30 to 1000 light-years) away) to have noticeable effects on Earth's biosphere.\n\nOn average, a supernova explosion occurs within of the Earth every 240 million years. Gamma rays are responsible for most of the adverse effects a supernova can have on a living terrestrial planet. In Earth's case, gamma rays induce a chemical reaction in the upper atmosphere, converting molecular nitrogen into nitrogen oxides, depleting the ozone layer enough to expose the surface to harmful solar and cosmic radiation (mainly ultra-violet). Phytoplankton and reef communities would be particularly affected, which could severely deplete the base of the marine food chain.\n\nSpeculation as to the effects of a nearby supernova on Earth often focuses on large stars as Type II supernova candidates. Several prominent stars within a few hundred light years of the Sun are candidates for becoming supernovae in as little as a millennium. Although they would be spectacular to look at, were these \"predictable\" supernovae to occur, they are thought to have little potential to affect Earth.\n\nIt is estimated that a Type II supernova closer than eight parsecs (26 light-years) would destroy more than half of the Earth's ozone layer. Such estimates are based on atmospheric modeling and the measured radiation flux from SN 1987A, a Type II supernova in the Large Magellanic Cloud. Estimates of the rate of supernova occurrence within 10 parsecs of the Earth vary from 0.05–0.5 per Ga to 10 per Ga. Several studies assume that supernovae are concentrated in the spiral arms of the galaxy, and that supernova explosions near the Sun usually occur during the ~10 million years that the Sun takes to pass through one of these regions. Examples of relatively near supernovae are the Vela Supernova Remnant (~800 ly, ~12,000 years ago) and Geminga (~550 ly, ~300,000 years ago).\n\nType Ia supernovae are thought to be potentially the most dangerous if they occur close enough to the Earth. Because Type Ia supernovae arise from dim, common white dwarf stars, it is likely that a supernova that could affect the Earth will occur unpredictably and take place in a star system that is not well studied. The closest known candidate is IK Pegasi. It is currently estimated, however, that by the time it could become a threat, its velocity in relation to the Solar System would have carried IK Pegasi to a safe distance.\n\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system. Supernova production of heavy elements over astronomic periods of time ultimately made the chemistry of life on Earth possible.\n\nPast supernovae might be detectable on Earth in the form of metal isotope signatures in rock strata. Subsequently, iron-60 enrichment has been reported in deep-sea rock of the Pacific Ocean by researchers from the Technical University of Munich. Twenty-three atoms of this iron isotope were found in the top 2 cm of crust (this layer corresponds to times from 13.4 to 0 million years ago). It is estimated that the supernova must have occurred in the last 5 million years or else it would have had to happen very close to the solar system to account for so much iron-60 still being here. A supernova occurring so close would have probably caused a mass extinction, which did not happen in that time frame. The quantity of iron seems to indicate that the supernova was less than 30 parsecs away. On the other hand, the authors estimate the frequency of supernovae at a distance less than \"D\" (for reasonably small \"D\") as around (\"D\"/10 pc) per Ga, which gives a probability of only around 5% for a supernova within 30 pc in the last 5 million years. They point out that the probability may be higher because the Solar System is entering the Orion Arm of the Milky Way.\n\nGamma ray bursts from \"dangerously close\" supernova explosions occur two or more times per billion years, and this has been proposed as the cause of the end Ordovician extinction, which resulted in the death of nearly 60% of the oceanic life on Earth.\n\nIn 1998 a supernova remnant, RX J0852.0-4622, was found in front (apparently) of the larger Vela Supernova Remnant. Gamma rays from the decay of titanium-44 (half-life about 60 years) were independently discovered emanating from it, showing that it must have exploded fairly recently (perhaps around 1200 CE), but there is no historical record of it. The flux of gamma rays and x-rays indicates that the supernova was relatively close to us (perhaps 200 parsecs or 660 ly). If so, this is an unexpected event because supernovae less than 200 parsecs away are estimated to occur less than once per 100,000 years.\n\n"}
{"id": "22756813", "url": "https://en.wikipedia.org/wiki?curid=22756813", "title": "Nicolas Antoine Boulanger", "text": "Nicolas Antoine Boulanger\n\nNicolas Antoine Boulanger (11 November 1722, Paris – 16 September 1759, Paris) was a French philosopher and man of letters during the Age of Enlightenment.\n\nBorn the son of a paper merchant in Paris, Boulanger studied first mathematics, and later ancient languages. He composed several philosophical works in which he sought to come up with naturalistic explanations for superstitions and religious practices, all of which were published posthumously. His major works were \"Research into the Origins of Oriental Despotism\" («Recherches sur l’origine du despotisme oriental», 1761) and \"Antiquity Unveiled\" («L’Antiquité dévoilée par ses usages», 1766). Boulanger's collected works were published in 1792.\n\nThe German-born Baron d'Holbach (Paul-Henri Thiry, 1723–1789) published his controversial anti-religious work \"Christianity Unveiled\" («Christianisme dévoilé», 1761), using Boulanger's name as his pseudonym, just two years after the philosopher's death. Boulanger also was one of the first modern critics of Paul.\n\nThe Koronian asteroid 7346 Boulanger, discovered in 1993, was named in his honor.\n"}
{"id": "997476", "url": "https://en.wikipedia.org/wiki?curid=997476", "title": "Night sky", "text": "Night sky\n\nThe term night sky, usually associated with astronomy from Earth, refers to the nighttime appearance of celestial objects like stars, planets, and the Moon, which are visible in a clear sky between sunset and sunrise, when the Sun is below the horizon.\n\nNatural light sources in a night sky include moonlight, starlight, and airglow, depending on location and timing. Aurorae light up the skies above the polar circles. Occasionally, a large coronal mass ejection from the Sun or simply high levels of solar wind may extend the phenomenon toward the Equator.\n\nThe night sky and studies of it have a historical place in both ancient and modern cultures. In the past, for instance, farmers have used the status of the night sky as a calendar to determine when to plant crops. Many cultures have drawn constellations between stars in the sky, using them in association with legends and mythology about their deities.\n\nThe anciently developed belief of astrology is generally based on the belief that relationships between heavenly bodies influence or convey information about events on Earth. The \"scientific\" study of celestial objects visible at night takes place in the science of observational astronomy.\n\nThe visibility of celestial objects in the night sky is affected by light pollution. The presence of the Moon in the night sky has historically hindered astronomical observation by increasing the amount of ambient brightness. With the advent of artificial light sources, however, light pollution has been a growing problem for viewing the night sky. Optical filters and modifications to light fixtures can help to alleviate this problem, but for optimal views, both professional and amateur astronomers seek locations far from urban skyglow.\n\nThe fact that the sky is not completely dark at night, even in the absence of moonlight and city lights, can be easily observed, since if the sky were absolutely dark, one would not be able to see the silhouette of an object against the sky.\n\nThe intensity of the sky varies greatly over the day and the primary cause differs as well. During daytime when the sun is above the horizon direct scattering of sunlight (Rayleigh scattering) is the overwhelmingly dominant source of light. In twilight, the period of time between sunset and sunrise, the situation is more complicated and a further differentiation is required. Twilight is divided in three segments according to how far the sun is below the horizon in segments of 6°.\n\nAfter sunset the civil twilight sets in, and ends when the sun drops more than 6° below the horizon. This is followed by the nautical twilight, when the sun reaches heights of -6° and -12°, after which comes the astronomical twilight defined as the period from -12° to -18°. When the sun drops more than 18° below the horizon the sky generally attains its minimum brightness.\n\nSeveral sources can be identified as the source of the intrinsic brightness of the sky, namely airglow, indirect scattering of sunlight, scattering of starlight, and artificial light pollution.\n\nDepending on local sky cloud cover, pollution, humidity, and light pollution levels, the stars visible to the unaided naked eye appear as hundreds, thousands or tens of thousands of white pinpoints of light in an otherwise near black sky together with some faint nebulae or clouds of light . In ancient times the stars were often assumed to be equidistant on a dome above the earth because they are much too far away for stereopsis to offer any depth cues. Visible stars range in color from blue (hot) to red (cold), but with such small points of faint light, most look white because they stimulate the rod cells without triggering the cone cells. If it is particularly dark and a particularly faint celestial object is of interest, averted vision may be helpful.\n\nThe stars of the night sky cannot be counted unaided because they are so numerous and there is no way to track which have been counted and which have not. Further complicating the count, fainter stars may appear and disappear depending on exactly where the observer is looking. The result is an impression of an extraordinarily vast star field.\n\nBecause stargazing is best done from a dark place away from city lights, dark adaptation is important to achieve and maintain. It takes several minutes for eyes to adjust to the darkness necessary for seeing the most stars, and surroundings on the ground are hard to discern. A red flashlight (torch) can be used to illuminate star charts, telescope parts, and the like without undoing the dark adaptation. (See Purkinje effect).\n\nThere are no markings on the night sky, though there exist many sky maps to aid stargazers in identifying constellations and other celestial objects. Constellations are prominent because their stars tend to be brighter than other nearby stars in the sky. Different cultures have created different groupings of constellations based on differing interpretations of the more-or-less random patterns of dots in the sky. Constellations were identified without regard to distance to each star, but instead as if they were all dots on a dome.\n\nOrion is among the most prominent and recognizable constellations. The Big Dipper (which has a wide variety of other names) is helpful for navigation in the northern hemisphere because it points to Polaris, the north star.\n\nThe pole stars are special because they are approximately in line with the Earth's axis of rotation so they appear to stay in one place while the other stars rotate around them through the course of a night (or a year).\n\nPlanets, named for the Greek word for \"wanderer,\" process through the star field a little each day, executing loops with time scales dependent on the length of the planet's year or orbital period around solar system. Planets, to the naked eye, appear as points of light in the sky with variable brightness. Planets shine due to sunlight reflecting or scattering from the planets' surface or atmosphere. Thus the relative sun-planet-earth positions determine the planet's brightness. With a telescope or good binoculars, the planets appear as discs demonstrating finite size, and it is possible to observe orbiting moons which cast shadows onto the host planet's surface. Venus is the most prominent planet, often called the \"morning star\" or \"evening star\" because it is brighter than the stars and often the only \"star\" visible near sunrise or sunset, depending on its location in its orbit. Mercury, Mars, Jupiter and Saturn are also visible to the naked eye.\n\nEarth's Moon is a grey disc in the sky with cratering visible to the naked eye. It spans, depending on its exact location, 29-33 arcminutes - which is about the size of a thumbnail at arm's length, and is readily identified. Over 28 days, the moon goes through a full cycle of lunar phases. People can generally identify phases within a few days by looking at the moon. Unlike stars and most planets, the light reflected from the moon is bright enough to be seen during the day. (Venus can sometimes be seen even after sunrise.)\n\nSome of the most spectacular moons come during the full moon phase near sunset or sunrise. The moon on the horizon benefits from the moon illusion which makes it appear larger. The light reflected from the moon traveling through the atmosphere also colors the moon orange and/or red.\n\nComets come to the night sky only rarely. Comets are illuminated by the sun, and their tails extend away from the sun. A comet with visible tail is quite unusual - a great comet appears about once a decade. They tend to be visible only shortly before sunrise or after sunset because those are the times they are close enough to the sun to show a tail.\n\nClouds obscure the view of other objects in the sky, though varying thicknesses of cloudcover have differing effects. A very thin cirrus cloud in front of the moon might produce a rainbow-colored ring around the moon. Stars and planets are too small or dim to take on this effect, and are instead only dimmed (often to the point of invisibility). Thicker cloudcover obscures celestial objects entirely, making the sky black or reflecting city lights back down. Clouds are often close enough to afford some depth perception, though they are hard to see without moonlight or light pollution.\n\nOn clear dark nights in unpolluted areas, when the moon is thin or below the horizon, the Milky Way, a band of what looks like white dust, can be seen.\n\nThe Magellanic Clouds of the southern sky are easily mistaken to be Earth-based clouds (hence the name) but are in fact collections of stars found outside the Milky Way known as dwarf galaxies.\n\nZodiacal light is a glow that appears near the points where the sun rises and sets, and is caused by sunlight interacting with interplanetary dust.\n\nShortly after sunset and before sunrise, artificial satellites often look like stars—similar in brightness and size—but move relatively quickly. Those that fly in low Earth orbit cross the sky in a couple of minutes. Some satellites, including space debris, appear to blink or have a periodic fluctuation in brightness because they are rotating. Satellite flares can appear brighter than Venus, with notable examples including the International Space Station (ISS) and Iridium Satellites.\n\nMeteors (commonly known as shooting stars) streak across the sky very infrequently. During a meteor shower, they may average one a minute at irregular intervals, but otherwise their appearance is a random surprise. The occasional meteor will make a bright, fleeting streak across the sky, and they can be very bright in comparison to the night sky.\n\nAircraft are also visible at night, distinguishable at a distance from other objects because their lights blink.\n\n\n"}
{"id": "939466", "url": "https://en.wikipedia.org/wiki?curid=939466", "title": "Orders of magnitude (energy)", "text": "Orders of magnitude (energy)\n\nThis list compares various energies in joules (J), organized by order of magnitude.\n\n"}
{"id": "36104129", "url": "https://en.wikipedia.org/wiki?curid=36104129", "title": "Playa de Gulpiyuri", "text": "Playa de Gulpiyuri\n\nPlaya de Gulpiyuri is a flooded sinkhole with an inland beach located near Llanes, in Asturias Northern Spain, around 100 m from the Cantabrian Sea. Roughly 40 meters in length, it is fully tidal due to a series of underground tunnels carved by the salt water of the Cantabrian Sea which allows water from the Bay of Biscay to create small waves. \n\nIt is a popular tourist destination, natural monument, and part of Spain's Regional Network of Protected Natural Areas.\n"}
{"id": "169115", "url": "https://en.wikipedia.org/wiki?curid=169115", "title": "Preternatural", "text": "Preternatural\n\nThe preternatural or praeternatural is that which appears outside or beside (Latin \"\") the natural. It is \"suspended between the mundane and the miraculous\".\n\nIn theology, the term is often used to distinguish marvels or deceptive trickery, often attributed to witchcraft or demons, from the purely divine power of the genuinely supernatural to violate the laws of nature. In the early modern period the term was used by scientists to refer to abnormalities and strange phenomena of various kinds that seemed to depart from the norms of nature.\n\nMedieval theologians made a clear distinction between the natural, the preternatural and the supernatural. Thomas Aquinas argued that the supernatural consists in \"God’s unmediated actions\"; the natural is \"what happens always or most of the time\"; and the preternatural is \"what happens rarely, but nonetheless by the agency of created beings...Marvels belong, properly speaking, to the realm of the preternatural.\" Theologians, following Aquinas, argued that only God had the power to disregard the laws of nature that he has created, but that demons could manipulate the laws of nature by a form of trickery, to deceive the unwary into believing they had experienced real miracles. According to historian Lorraine Daston,\n\nAlthough demons, astral intelligences, and other spirits might manipulate natural causes with superhuman dexterity and thereby work marvels, as mere creatures they could never transcend from the preternatural to the supernatural and work genuine miracles.\n\nBy the 16th century, the term \"preternatural\" was increasingly used to refer to demonic activity comparable to the use of magic by human adepts: The Devil, \"being a natural Magician … may perform many acts in ways above our knowledge, though not transcending our natural power.\" According to the philosophy of the time, preternatural phenomena were not contrary to divine law, but used hidden, or occult powers that violated the \"normal\" pattern of natural phenomena.\n\nWith the emergence of early modern science, the concept of the preternatural increasingly came to be used to refer to strange or abnormal phenomena that seemed to violate the normal working of nature, but which were not associated with magic and witchcraft. This was a development of the idea that preternatural phenomena were fake miracles. As Daston puts it, \"To simplify the historical sequence somewhat: first, preternatural phenomena were demonized and thereby incidentally naturalized; then the demons were deleted, leaving only the natural causes.\" The use of the term was especially common in medicine, for example in John Brown's \"A Compleat Treatise of Preternatural Tumours\" (1678), or William Smellie's \"A Collection of Preternatural Cases and Observations in Midwifery\" (1754).\n\nIn the 19th century the term was appropriated in anthropology to refer to folk beliefs about fairies, trolls and other such creatures which were not thought of as demonic, but which were perceived to affect the natural world in unpredictable ways. According to Thorstein Veblen, such preternatural agents were often thought of as forces somewhere between supernatural beings and material processes. \"The preternatural agency is not necessarily conceived to be a personal agent in the full sense, but it is an agency which partakes of the attributes of personality to the extent of somewhat arbitrarily influencing the outcome of any enterprise, and especially of any contest.\"\n\nThe linguistic association between individual agents and unexplained or unfortunate circumstances remains. Many people attribute occurrences that are known to be material processes, such as \"gremlins in the engine\", a \"ghost in the machine\", or attributing motives to objects: \"the clouds are threatening\". The anthropomorphism in our daily life is a combination of the above cultural stems, as well as the manifestation of our pattern-projecting minds.\n\nIn 2011, Penn State Press began publishing a learned journal entitled \"Preternature: Critical and Historical Studies on the Preternatural\". Edited by Kirsten Uszkalo and Richard Raiswell, the journal is dedicated to publishing articles, reviews and short editions of original texts that deal with conceptions and perceptions of the preternatural in any culture and in any historical period. The journal covers \"magics, witchcraft, spiritualism, occultism, prophecy, monstrophy, demonology, and folklore.\"\n\n\n"}
{"id": "827792", "url": "https://en.wikipedia.org/wiki?curid=827792", "title": "Rare Earth hypothesis", "text": "Rare Earth hypothesis\n\nIn planetary astronomy and astrobiology, the Rare Earth hypothesis argues that the origin of life and the evolution of biological complexity such as sexually reproducing, multicellular organisms on Earth (and, subsequently, human intelligence) required an improbable combination of astrophysical and geological events and circumstances.\n\nAccording to the hypothesis, complex extraterrestrial life is an improbable phenomenon and likely to be rare. The term \"Rare Earth\" originates from \"Rare Earth: Why Complex Life Is Uncommon in the Universe\" (2000), a book by Peter Ward, a geologist and paleontologist, and Donald E. Brownlee, an astronomer and astrobiologist, both faculty members at the University of Washington.\n\nA contrary view was argued in the 1970s and 1980s by Carl Sagan and Frank Drake, among others. It holds that Earth is a typical rocky planet in a typical planetary system, located in a non-exceptional region of a common barred-spiral galaxy. Given the principle of mediocrity (in the same vein as the Copernican principle), it is probable that we are typical, and the universe teems with complex life. However, Ward and Brownlee argue that planets, planetary systems, and galactic regions that are as friendly to complex life as the Earth, the Solar System, and our galactic region are rare.\n\nThe Rare Earth hypothesis argues that the evolution of biological complexity requires a host of fortuitous circumstances, such as a galactic habitable zone, a central star and planetary system having the requisite character, the circumstellar habitable zone, a right-sized terrestrial planet, the advantage of a gas giant guardian like Jupiter and a large natural satellite, conditions needed to ensure the planet has a magnetosphere and plate tectonics, the chemistry of the lithosphere, atmosphere, and oceans, the role of \"evolutionary pumps\" such as massive glaciation and rare bolide impacts, and whatever led to the appearance of the eukaryote cell, sexual reproduction and the Cambrian explosion of animal, plant, and fungi phyla. The evolution of human intelligence may have required yet further events, which are extremely unlikely to have happened were it not for the Cretaceous–Paleogene extinction event 66 million years ago removing dinosaurs as the dominant terrestrial vertebrates.\n\nIn order for a small rocky planet to support complex life, Ward and Brownlee argue, the values of several variables must fall within narrow ranges. The universe is so vast that it could contain many Earth-like planets. But if such planets exist, they are likely to be separated from each other by many thousands of light years. Such distances may preclude communication among any intelligent species evolving on such planets, which would solve the Fermi paradox: \"If extraterrestrial aliens are common, why aren't they obvious?\"\n\n\"Rare Earth\" suggests that much of the known universe, including large parts of our galaxy, are \"dead zones\" unable to support complex life. Those parts of a galaxy where complex life is possible make up the galactic habitable zone, primarily characterized by distance from the Galactic Center. As that distance increases:\nItem #1 rules out the outer reaches of a galaxy; #2 and #3 rule out galactic inner regions. Hence a galaxy's habitable zone may be a ring sandwiched between its uninhabitable center and outer reaches.\n\nAlso, a habitable planetary system must maintain its favorable location long enough for complex life to evolve. A star with an eccentric (elliptic or hyperbolic) galactic orbit will pass through some spiral arms, unfavorable regions of high star density; thus a life-bearing star must have a galactic orbit that is nearly circular, with a close synchronization between the orbital velocity of the star and of the spiral arms. This further restricts the galactic habitable zone within a fairly narrow range of distances from the Galactic Center. Lineweaver et al. calculate this zone to be a ring 7 to 9 kiloparsecs in radius, including no more than 10% of the stars in the Milky Way, about 20 to 40 billion stars. Gonzalez, et al. would halve these numbers; they estimate that at most 5% of stars in the Milky Way fall in the galactic habitable zone.\n\nApproximately 77% of observed galaxies are spiral, two-thirds of all spiral galaxies are barred, and more than half, like the Milky Way, exhibit multiple arms. According to Rare Earth, our own galaxy is unusually quiet and dim (see below), representing just 7% of its kind. Even so, this would still represent more than 200 billion galaxies in the known universe.\n\nOur galaxy also appears unusually favorable in suffering fewer collisions with other galaxies over the last 10 billion years, which can cause more supernovae and other disturbances. Also, the Milky Way's central black hole seems to have neither too much nor too little activity.\n\nThe orbit of the Sun around the center of the Milky Way is indeed almost perfectly circular, with a period of 226 Ma (million years), closely matching the rotational period of the galaxy. However, the majority of stars in barred spiral galaxies populate the spiral arms rather than the halo and tend to move in gravitationally aligned orbits, so there is little that is unusual about the Sun's orbit. While the Rare Earth hypothesis predicts that the Sun should rarely, if ever, have passed through a spiral arm since its formation, astronomer Karen Masters has calculated that the orbit of the Sun takes it through a major spiral arm approximately every 100 million years. Some researchers have suggested that several mass extinctions do correspond with previous crossings of the spiral arms.\n\nThe terrestrial example suggests that complex life requires liquid water, requiring an orbital distance neither too close nor too far from the central star, another scale of habitable zone or Goldilocks Principle: \nThe habitable zone varies with the star's type and age.\n\nFor advanced life, the star must also be highly stable, which is typical of middle star life, about 4.6 billion years old. Proper metallicity and size are also important to stability. The Sun has a low 0.1% luminosity variation. To date no solar twin star twin, with an exact match of the sun's luminosity variation, has been found, though some come close. The star must have no stellar companions, as in binary systems, which would disrupt the orbits of planets. Estimates suggest 50% or more of all star systems are binary. The habitable zone for a main sequence star very gradually moves out over its lifespan until it becomes a white dwarf and the habitable zone vanishes.\n\nThe liquid water and other gases available in the habitable zone bring the benefit of greenhouse warming. Even though the Earth's atmosphere contains a water vapor concentration from 0% (in arid regions) to 4% (in rain forest and ocean regions) and – as of February 2018 – only 408.05 parts per million of , these small amounts suffice to raise the average surface temperature by about 40 °C, with the dominant contribution being due to water vapor, which together with clouds makes up between 66% and 85% of Earth's greenhouse effect, with contributing between 9% and 26% of the effect.\n\nRocky planets must orbit within the habitable zone for life to form. Although the habitable zone of such hot stars as Sirius or Vega is wide, hot stars also emit much more ultraviolet radiation that ionizes any planetary atmosphere. They may become red giants before advanced life evolves on their planets.\nThese considerations rule out the massive and powerful stars of type F6 to O (see stellar classification) as homes to evolved metazoan life.\n\nSmall red dwarf stars conversely have small habitable zones wherein planets are in tidal lock, with one very hot side always facing the star and another very cold side; and they are also at increased risk of solar flares (see Aurelia). Life therefore cannot arise in such systems. Rare Earth proponents claim that only stars from F7 to K1 types are hospitable. Such stars are rare: G type stars such as the Sun (between the hotter F and cooler K) comprise only 9% of the hydrogen-burning stars in the Milky Way.\n\nSuch aged stars as red giants and white dwarfs are also unlikely to support life. Red giants are common in globular clusters and elliptical galaxies. White dwarfs are mostly dying stars that have already completed their red giant phase. Stars that become red giants expand into or overheat the habitable zones of their youth and middle age (though theoretically planets at a much greater distance may become habitable).\n\nAn energy output that varies with the lifetime of the star will likely prevent life (e.g., as Cepheid variables). A sudden decrease, even if brief, may freeze the water of orbiting planets, and a significant increase may evaporate it and cause a greenhouse effect that prevents the oceans from reforming.\n\nAll known life requires the complex chemistry of metallic elements. The absorption spectrum of a star reveals the presence of metals within, and studies of stellar spectra reveal that many, perhaps most, stars are poor in metals. Because heavy metals originate in supernova explosions, metallicity increases in the universe over time. Low metallicity characterizes the early universe: globular clusters and other stars that formed when the universe was young, stars in most galaxies other than large spirals, and stars in the outer regions of all galaxies. Metal-rich central stars capable of supporting complex life are therefore believed to be most common in the quiet suburbs of the larger spiral galaxies—where radiation also happens to be weak.\n\nRare Earth proponents argue that a planetary system capable of sustaining complex life must be structured more or less like the Solar System, with small and rocky inner planets and outer gas giants. Without the protection of 'celestial vacuum cleaner' planets with strong gravitational pull, a planet would be subject to more catastrophic asteroid collisions.\n\nObservations of exo-planets have shown that arrangements of planets similar to our Solar System are rare. Most planetary systems have super Earths, several times larger than Earth, close to their star, whereas our Solar System's inner region has only a few small rocky planets and none inside Mercury's orbit. Only 10% of stars have giant planets similar to Jupiter and Saturn, and those few rarely have stable nearly circular orbits distant from their star. Konstantin Batygin and colleagues argue that these features can be explained if, early in the history of the Solar System, Jupiter and Saturn drifted towards the Sun, sending showers of planetesimals towards the super-Earths which sent them spiralling into the Sun, and ferrying icy building blocks into the terrestrial region of the Solar System which provided the building blocks for the rocky planets. The two giant planets then drifted out again to their present position. However, in the view of Batygin and his colleagues: \"The concatenation of chance events required for this delicate choreography suggest that small, Earth-like rocky planets – and perhaps life itself – could be rare throughout the cosmos.\"\n\nRare Earth argues that a gas giant must not be too close to a body where life is developing. Close placement of gas giant(s) could disrupt the orbit of a potential life-bearing planet, either directly or by drifting into the habitable zone.\n\nNewtonian dynamics can produce chaotic planetary orbits, especially in a system having large planets at high orbital eccentricity.\n\nThe need for stable orbits rules out stars with systems of planets that contain large planets with orbits close to the host star (called \"hot Jupiters\"). It is believed that hot Jupiters have migrated inwards to their current orbits. In the process, they would have catastrophically disrupted the orbits of any planets in the habitable zone. To exacerbate matters, hot Jupiters are much more common orbiting F and G class stars.\n\nIt is argued that life requires terrestrial planets like Earth and as gas giants lack such a surface, that complex life cannot arise there.\n\nA planet that is too small cannot hold much atmosphere, making surface temperature low and variable and oceans impossible. A small planet will also tend to have a rough surface, with large mountains and deep canyons. The core will cool faster, and plate tectonics may be brief or entirely absent. A planet that is too large will retain too dense an atmosphere like Venus. Although Venus is similar in size and mass to Earth, its surface atmospheric pressure is 92 times that of Earth, and surface temperature of 735 K (462 °C; 863 °F). Earth had a similar early atmosphere to Venus, but may have lost it in the giant impact event.\n\nRare Earth proponents argue that plate tectonics and a strong magnetic field are essential for biodiversity, global temperature regulation, and the carbon cycle.\nThe lack of mountain chains elsewhere in the Solar System is direct evidence that Earth is the only body with plate tectonics, and thus the only nearby body capable of supporting life.\n\nPlate tectonics depend on the right chemical composition and a long-lasting source of heat from radioactive decay. Continents must be made of less dense felsic rocks that \"float\" on underlying denser mafic rock. Taylor emphasizes that tectonic subduction zones require the lubrication of oceans of water. Plate tectonics also provides a means of biochemical cycling.\n\nPlate tectonics and as a result continental drift and the creation of separate land masses would create diversified ecosystems and biodiversity, one of the strongest defences against extinction. An example of species diversification and later competition on Earth's continents is the Great American Interchange. North and Middle America drifted into South America at around 3.5 to 3 Ma. The fauna of South America evolved separately for about 30 million years, since Antarctica separated. Many species were subsequently wiped out in mainly South America by competing Northern American animals.\n\nThe Moon is unusual because the other rocky planets in the Solar System either have no satellites (Mercury and Venus), or only tiny satellites which are probably captured asteroids (Mars).\n\nThe Giant-impact theory hypothesizes that the Moon resulted from the impact of a Mars-sized body, dubbed Theia, with the young Earth. This giant impact also gave the Earth its axial tilt (inclination) and velocity of rotation. Rapid rotation reduces the daily variation in temperature and makes photosynthesis viable. The \"Rare Earth\" hypothesis further argues that the axial tilt cannot be too large or too small (relative to the orbital plane). A planet with a large tilt will experience extreme seasonal variations in climate. A planet with little or no tilt will lack the stimulus to evolution that climate variation provides. In this view, the Earth's tilt is \"just right\". The gravity of a large satellite also stabilizes the planet's tilt; without this effect the variation in tilt would be chaotic, probably making complex life forms on land impossible.\n\nIf the Earth had no Moon, the ocean tides resulting solely from the Sun's gravity would be only half that of the lunar tides. A large satellite gives rise to tidal pools, which may be essential for the formation of complex life, though this is far from certain.\n\nA large satellite also increases the likelihood of plate tectonics through the effect of tidal forces on the planet's crust. The impact that formed the Moon may also have initiated plate tectonics, without which the continental crust would cover the entire planet, leaving no room for oceanic crust. It is possible that the large scale mantle convection needed to drive plate tectonics could not have emerged in the absence of crustal inhomogeneity. A further theory indicates that such a large moon may also contribute to maintaining a planet's magnetic shield by continually acting upon a metallic planetary core as dynamo, thus protecting the surface of the planet from charged particles and cosmic rays, and helping to ensure the atmosphere is not stripped over time by solar winds.\n\nA terrestrial planet of the right size is needed to retain an atmosphere, like Earth and Venus. On Earth, once the giant impact of Theia thinned Earth's atmosphere, other events were needed to make the atmosphere capable of sustaining life. The Late Heavy Bombardment reseeded Earth with water lost after the impact of Theia. The development of an ozone layer formed protection from ultraviolet (UV) sunlight. Nitrogen and carbon dioxide are needed in a correct ratio for life to form. Lightning is needed for nitrogen fixation. The carbon dioxide gas needed for life comes from sources such as volcanoes and geysers. Carbon dioxide is only needed at low levels (currently at 400 ppm); at high levels it is poisonous. Precipitation is needed to have a stable water cycle. A proper atmosphere must reduce diurnal temperature variation.\n\nRegardless of whether planets with similar physical attributes to the Earth are rare or not, some argue that life usually remains simple bacteria. Biochemist Nick Lane argues that simple cells (prokaryotes) emerged soon after Earth's formation, but since almost half the planet's life had passed before they evolved into complex ones (eukaryotes) all of whom share a common ancestor, this event can only have happened once. In some views, prokaryotes lack the cellular architecture to evolve into eukaryotes because a bacterium expanded up to eukaryotic proportions would have tens of thousands of times less energy available; two billion years ago, one simple cell incorporated itself into another, multiplied, and evolved into mitochondria that supplied the vast increase in available energy that enabled the evolution of complex life. If this incorporation occurred only once in four billion years or is otherwise unlikely, then life on most planets remains simple. An alternative view is that mitochondria evolution was environmentally triggered, and that mitochondria-containing organisms appeared soon after the first traces of atmospheric oxygen.\n\nThe evolution and persistence of sexual reproduction is another mystery in biology. The purpose of sexual reproduction is unclear, as in many organisms it has a 50% cost (fitness disadvantage) in relation to asexual reproduction. Mating types (types of gametes, according to their compatibility) may have arisen as a result of anisogamy (gamete dimorphism), or the male and female genders may have evolved before anisogamy. It is also unknown why most sexual organisms use a binary mating system, and why some organisms have gamete dimorphism. Charles Darwin was the first to suggest that sexual selection drives speciation; without it, complex life would probably not have evolved.\n\nWhile life on Earth is regarded to have spawned relatively early in the planet's history, the evolution from multicellular to intelligent organisms took around 800 million years. Civilizations on Earth have existed for about 12,000 years and radio communication reaching space has existed for less than 100 years. Relative to the age of the Solar System (~4.57 Ga) this is a short time, in which extreme climatic variations, super volcanoes, and large meteorite impacts were absent. These events would severely harm intelligent life, as well as life in general. For example, the Permian-Triassic mass extinction, caused by widespread and continuous volcanic eruptions in an area the size of Western Europe, led to the extinction of 95% of known species around 251.2 Ma ago. About 65 million years ago, the Chicxulub impact at the Cretaceous–Paleogene boundary (~65.5 Ma) on the Yucatán peninsula in Mexico led to a mass extinction of the most advanced species at that time.\n\nIf there were intelligent extraterrestrial civilizations able to make contact with distant Earth, they would have to live in the same 12Ka period of the 800Ma evolution of life.\n\nThe following discussion is adapted from Cramer. The Rare Earth equation is Ward and Brownlee's riposte to the Drake equation. It calculates formula_1, the number of Earth-like planets in the Milky Way having complex life forms, as:\n\nwhere:\nWe assume formula_5. The Rare Earth hypothesis can then be viewed as asserting that the product of the other nine Rare Earth equation factors listed below, which are all fractions, is no greater than 10 and could plausibly be as small as 10. In the latter case, formula_1 could be as small as 0 or 1. Ward and Brownlee do not actually calculate the value of formula_1, because the numerical values of quite a few of the factors below can only be conjectured. They cannot be estimated simply because we have but one data point: the Earth, a rocky planet orbiting a G2 star in a quiet suburb of a large barred spiral galaxy, and the home of the only intelligent species we know, namely ourselves.\n\nThe Rare Earth equation, unlike the Drake equation, does not factor the probability that complex life evolves into intelligent life that discovers technology (Ward and Brownlee are not evolutionary biologists). Barrow and Tipler review the consensus among such biologists that the evolutionary path from primitive Cambrian chordates, e.g., \"Pikaia\" to \"Homo sapiens\", was a highly improbable event. For example, the large brains of humans have marked adaptive disadvantages, requiring as they do an expensive metabolism, a long gestation period, and a childhood lasting more than 25% of the average total life span. Other improbable features of humans include:\n\nWriters who support the Rare Earth hypothesis:\n\nCases against the Rare Earth Hypothesis take various forms.\n\nThe hypothesis concludes, more or less, that complex life is rare because it can evolve only on the surface of an Earth-like planet or on a suitable satellite of a planet. Some biologists, such as Jack Cohen, believe this assumption too restrictive and unimaginative; they see it as a form of circular reasoning.\n\nAccording to David Darling, the Rare Earth hypothesis is neither hypothesis nor prediction, but merely a description of how life arose on Earth. In his view Ward and Brownlee have done nothing more than select the factors that best suit their case.\n\nWhat matters is not whether there's anything unusual about the Earth; there's going to be something idiosyncratic about every planet in space. What matters is whether any of Earth's circumstances are not only unusual but also essential for complex life. So far we've seen nothing to suggest there is.\n\nCritics also argue that there is a link between the Rare Earth Hypothesis and the creationist ideas of intelligent design.\n\nAn increasing number of extrasolar planet discoveries are being made with planets in planetary systems known as of . Rare Earth proponents argue life cannot arise outside Sun-like systems. However, some exobiologists have suggested that stars outside this range may give rise to life under the right circumstances; this possibility is a central point of contention to the theory because these late-K and M category stars make up about 82% of all hydrogen-burning stars.\n\nCurrent technology limits the testing of important Rare Earth criteria: surface water, tectonic plates, a large moon and biosignatures are currently undetectable. Though planets the size of Earth are difficult to detect and classify, scientists now think that rocky planets are common around Sun-like stars. The Earth Similarity Index (ESI) of mass, radius and temperature provides a means of measurement, but falls short of the full Rare Earth criteria.\n\nSome argue that Rare Earth's estimates of rocky planets in habitable zones (formula_3 in the Rare Earth equation) are too restrictive. James Kasting cites the Titius-Bode law to contend that it is a misnomer to describe habitable zones as narrow when there is a 50% chance of at least one planet orbiting within one. In 2013 a study that was published in the journal Proceedings of the National Academy of Sciences calculated that about \"one in five\" of all sun-like stars are expected to have earthlike planets \"within the habitable zones of their stars\"; 8.8 billion of them therefore exist in the Milky Way galaxy alone. On 4 November 2013, astronomers reported, based on \"Kepler\" space mission data, that there could be as many as 40 billion Earth-sized planets orbiting in the habitable zones of sun-like stars and red dwarf stars within the Milky Way Galaxy. 11 billion of these estimated planets may be orbiting sun-like stars.\n\nThe requirement for a system to have a Jovian planet as protector (Rare Earth equation factor formula_15) has been challenged, affecting the number of proposed extinction events (Rare Earth equation factor formula_16). Kasting's 2001 review of Rare Earth questions whether a Jupiter protector has any bearing on the incidence of complex life. Computer modelling including the 2005 Nice model and 2007 Nice 2 model yield inconclusive results in relation to Jupiter's gravitational influence and impacts on the inner planets. A study by Horner and Jones (2008) using computer simulation found that while the total effect on all orbital bodies within the Solar System is unclear, Jupiter has caused more impacts on Earth than it has prevented. Lexell's Comet, a 1770 near miss that passed closer to Earth than any other comet in recorded history, was known to be caused by the gravitational influence of Jupiter. Grazier (2017) claims that the idea of Jupiter as a shield is a misinterpretation of a 1996 study by George Wetherill, and using computer models Grazier was able to demonstrate that Saturn protects Earth from more asteroids and comets than does Jupiter.\n\nWard and Brownlee argue that for complex life to evolve (Rare Earth equation factor formula_12), tectonics must be present to generate biogeochemical cycles, and predicted that such geological features would not be found outside of Earth, pointing to a lack of observable mountain ranges and subduction. There is, however, no scientific consensus on the evolution of plate tectonics on Earth. Though it is believed that tectonic motion first began around three billion years ago, by this time photosynthesis and oxygenation had already begun. Furthermore, recent studies point to plate tectonics as an episodic planetary phenomenon, and that life may evolve during periods of \"stagnant-lid\" rather than plate tectonic states.\n\nRecent evidence also points to similar activity either having occurred or continuing to occur elsewhere. The geology of Pluto, for example, described by Ward and Brownlee as \"without mountains or volcanoes ... devoid of volcanic activity\", has since been found to be quite the contrary, with a geologically active surface possessing organic molecules and mountain ranges like Tenzing Montes and Hillary Montes comparable in relative size to those of Earth, and observations suggest the involvement of endogenic processes. Plate tectonics has been suggested as a hypothesis for the Martian dichotomy, and in 2012 geologist An Yin put forward evidence for active plate tectonics on Mars. Europa has long been suspected to have plate tectonics and in 2014 NASA announced evidence of active subduction. In 2017, scientists studying the geology of Charon confirmed that icy plate tectonics also operated on Pluto's largest moon.\n\nKasting suggests that there is nothing unusual about the occurrence of plate tectonics in large rocky planets and liquid water on the surface as most should generate internal heat even without the assistance of radioactive elements. Studies by Valencia and Cowan suggest that plate tectonics may be inevitable for terrestrial planets Earth sized or larger, that is, Super-Earths, which are now known to be more common in planetary systems.\n\nThe hypothesis that molecular oxygen, necessary for animal life, is rare and that a Great Oxygenation Event (Rare Earth equation factor formula_12) could only have been triggered and sustained by tectonics, appears to have been invalidated by more recent discoveries.\n\nWard and Brownlee ask \"whether oxygenation, and hence the rise of animals, would ever have occurred on a world where there were no continents to erode\". Extraterrestrial free oxygen has recently been detected around other solid objects, including Mercury, Venus, Mars Jupiter's four Galilean moons, Saturn's moons Enceladus, Dione and Rhea and even the atmosphere of a comet. This has led scientists to speculate whether processes other than photosynthesis could be capable of generating an environment rich in free oxygen. Wordsworth (2014) concludes that oxygen generated other than through photodissociation may be likely on Earth-like exoplanets, and could actually lead to false positive detections of life. Narita (2015) suggests photocatalysis by titanium dioxide as a geochemical mechanism for producing oxygen atmospheres.\n\nSince Ward & Brownlee's assertion that \"there is irrefutable evidence that oxygen is a necessary ingredient for animal life\", anaerobic metazoa have been found that indeed do metabolise without oxygen. Spinoloricus nov. sp., for example, a species discovered in the hypersaline anoxic L'Atalante basin at the bottom of the Mediterranean Sea in 2010, appears to metabolise with hydrogen, lacking mitochondria and instead using hydrogenosomes. Stevenson (2015) has proposed other membrane alternatives for complex life in worlds without oxygen. In 2017, scientists from the NASA Astrobiology Institute discovered the necessary chemical preconditions for the formation of azotosomes on Saturn's moon Titan, a world that lacks atmospheric oxygen. Independent studies by Schirrmeister and by Mills concluded that Earth's multicellular life existed prior to the Great Oxygenation Event, not as a consequence of it.\n\nNASA scientists Hartman and McKay argue that plate tectonics may in fact slow the rise of oxygenation (and thus stymie complex life rather than promote it). Computer modelling by Tilman Spohn in 2014 found that plate tectonics on Earth may have arisen from the effects of complex life's emergence, rather than the other way around as the Rare Earth might suggest. The action of lichens on rock may have contributed to the formation of subduction zones in the presence of water. Kasting argues that if oxygenation caused the Cambrian explosion then any planet with oxygen producing photosynthesis should have complex life.\n\nThe importance of Earth's magnetic field to the development of complex life has been disputed. Kasting argues that the atmosphere provides sufficient protection against cosmic rays even during times of magnetic pole reversal and atmosphere loss by sputtering. Kasting also dismisses the role of the magnetic field in the evolution of eukaryotes, citing the age of the oldest known magnetofossils.\n\nThe requirement of a large moon (Rare Earth equation factor formula_14) has also been challenged. Even if it were required, such an occurrence may not be as unique as predicted by the Rare Earth Hypothesis. Recent work by Edward Belbruno and J. Richard Gott of Princeton University suggests that giant impactors such as those that may have formed the Moon can indeed form in planetary trojan points ( or Lagrangian point) which means that similar circumstances may occur in other planetary systems.\n\nRare Earth's assertion that the Moon's stabilization of Earth's obliquity and spin is a requirement for complex life has been questioned. Kasting argues that a moonless Earth would still possess habitats with climates suitable for complex life and questions whether the spin rate of a moonless Earth can be predicted. Although the giant impact theory posits that the impact forming the Moon increased Earth's rotational speed to make a day about 5 hours long, the Moon has slowly \"stolen\" much of this speed to reduce Earth's solar day since then to about 24 hours and continues to do so: in 100 million years Earth's solar day will be roughly 24 hours 38 minutes (the same as Mars's solar day); in 1 billion years, 30 hours 23 minutes. Larger secondary bodies would exert proportionally larger tidal forces that would in turn decelerate their primaries faster and potentially increase the solar day of a planet in all other respects like Earth to over 120 hours within a few billion years. This long solar day would make effective heat dissipation for organisms in the tropics and subtropics extremely difficult in a similar manner to tidal locking to a red dwarf star. Short days (high rotation speed) causes high wind speeds at ground level. Long days (slow rotation speed) cause the day and night temperatures to be too extreme.\n\nMany Rare Earth proponents argue that the Earth's plate tectonics would probably not exist if not for the tidal forces of the Moon. The hypothesis that the Moon's tidal influence initiated or sustained Earth's plate tectonics remains unproven, though at least one study implies a temporal correlation to the formation of the Moon. Evidence for the past existence of plate tectonics on planets like Mars which may never have had a large moon would counter this argument. Kasting argues that a large moon is not required to initiate plate tectonics.\n\nRare Earth proponents argue that simple life may be common, though complex life requires specific environmental conditions to arise. Critics consider life could arise on a moon of a gas giant, though this is less likely if life requires volcanicity. The moon must have stresses to induce tidal heating, but not so dramatic as seen on Jupiter's Io. However, the moon is within the gas giant's intense radiation belts, sterilizing any biodiversity before it can get established. Dirk Schulze-Makuch disputes this, hypothesizing alternative biochemistries for alien life. While Rare Earth proponents argue that only microbial extremophiles could exist in subsurface habitats beyond Earth, some argue that complex life can also arise in these environments. Examples of extremophile animals such as the \"Hesiocaeca methanicola\", an animal that inhabits ocean floor methane clathrates substances more commonly found in the outer Solar System, the tardigrades which can survive in the vacuum of space or \"Halicephalobus mephisto\" which exists in crushing pressure, scorching temperatures and extremely low oxygen levels 3.6 kilometres deep in the Earth's crust, are sometimes cited by critics as complex life capable of thriving in \"alien\" environments. Jill Tarter counters the classic counterargument that these species adapted to these environments rather than arose in them, by suggesting that we cannot assume conditions for life to emerge which are not actually known. There are suggestions that complex life could arise in sub-surface conditions which may be similar to those where life may have arisen on Earth, such as the tidally heated subsurfaces of Europa or Enceladus. Ancient circumvental ecosystems such as these support complex life on Earth such as Riftia pachyptila that exist completely independent of the surface biosphere.\n\n\n"}
{"id": "55613190", "url": "https://en.wikipedia.org/wiki?curid=55613190", "title": "Scotlandite", "text": "Scotlandite\n\nScotlandite is a sulfite mineral first discovered in a mine at Leadhills in South Lanarkshire, Scotland, an area known to mineralogists and geologists for its wide range of different mineral species found in the veins that lie deep in the mine shafts. This specific mineral is found in the Susanna vein of Leadhills, where the crystals are formed as chisel-shaped or bladed. Scotlandite was actually the first naturally occurring sulfite, which has the ideal chemical formula of PbSO. The mineral has been approved by the Commission on New Minerals and Mineral Names, IMA, to be named scotlandite for Scotland.\n\nScotlandite is found in association with pyromorphite, anglesite, lanarkite, leadhillite, susannite, and barite. It occurs in cavities in massive barite and anglesite, and is closely associated with lanarkite and susannite. Scotlandite represents the latest phase in the crystallization sequence of the associated lead secondary minerals. It can often be found in the vuggy anglesite as yellowish single crystals up to 1 millimeter in length that sometimes arrange in a fan-shaped aggregates. Anglesite can usually be recognized in a very thin coating on scotlandite which is used to protect the sulfite from further oxidation. A second variety of scotlandite can also occur in discontinuously distributed cavities between the anglesite mass containing the first variety and the barite matrix. This variety is characterized by tiny, whitish to water-clear crystals, and crystal clusters less than one millimeter in size, which encrust large portions of the interior of the cavities. Scotlandite is a uniquely rare mineral, as it occurs in small amounts in few locations around the world.\n\nScotlandite is a pale yellow, greyish-white, colorless, transparent mineral with an adamantine or pearly luster. It exhibits a hardness of 2 on the Mohs hardness scale. Scotlandite occurs as chisel-shaped or bladed crystals elongated along the c-axis, with a tendency to form radiating clusters. Its crystals are characterized by the {100}, {010}, {011}, {021}, {031}, and {032}. faces. Scotlandite shows perfect cleavage along the {100} plane and a less good one along the {010} plane. The measured density is 6.37 g/cm.\n\nScotlandite is biaxial positive, which means it will refract light along two axes. The mineral is optically biaxial positive, 2V 35° 24'(Na). The refractive indices are: α ~ 2.035, β ~ 2.040, and γ ~ 2.085 (Na). Dispersion is strong, v » r. The extinction is β//b, and α [001] = 20° (γ [100] = 4° in the obtuse angle β. H(Mohs) < 2. D = 6.37 and calculated D = 6.40 g cm. The infrared spectrum of scotlandite shows conclusively that it is an anhydrous sulfite, with no OH groups or other polyatomic anions being present. It is also proven by electron microprobe analysis and infrared spectroscopy that scotlandite must be a polymorph of lead sulfite.\n\nScotlandite is a sulfite compared with chemically related compounds, it is very close to the value of anglesite (6.38 g cm), but distinctly different from that of lanarkite (6.92 g cm). Orthorhombic lead sulfite is of higher density (D = 6.54, calculated D = 6.56 g cm), and has the same chemical properties as well. The empirical chemical formula for scotlandite calculated on the basis of Pb+S = 2, is PbSO or more ideally PbSO.\n\nA small crystal of scotlandite, showing some cleavage faces, was examined using Weissenberg and precession techniques. Scotlandite is in the monoclinic crystal system. The only systematic extinctions observed from the single crystal patterns were 0k0 where k was odd. Thus the possible space group is either P2 or P2/m. The unit cell parameters obtained from the single crystal study were used to index the X-ray powder pattern and were then refined with the indexed powder data. The results are: a = 4.505 Å, b = 5.333 Å, c = 6.405 Å; β= 106.24°; Z = 2. If the present a and c axes are interchanged, the unit cell of scotlandite is very similar, isotypic, to that of molybdomenite, PbSeO. Lead is coordinated to nine oxygen atoms with Pb-O=2.75 Å, and possibly further to one sulfur atom with Pb−S=3.46 Å. The average S−O distance in the pyramidal SO group is 1.52 Å.\n\nList of Minerals\n"}
{"id": "44214", "url": "https://en.wikipedia.org/wiki?curid=44214", "title": "Slate", "text": "Slate\n\nSlate is a fine-grained, foliated, homogeneous metamorphic rock derived from an original shale-type sedimentary rock composed of clay or volcanic ash through low-grade regional metamorphism. It is the finest grained foliated metamorphic rock. Foliation may not correspond to the original sedimentary layering, but instead is in planes perpendicular to the direction of metamorphic compression.\n\nThe foliation in slate is called \"slaty cleavage\". It is caused by strong compression causing fine grained clay flakes to regrow in planes perpendicular to the compression. When expertly \"cut\" by striking parallel to the foliation, with a specialized tool in the quarry, many slates will display a property called fissility, forming smooth flat sheets of stone which have long been used for roofing, floor tiles, and other purposes. Slate is frequently grey in color, especially when seen, en masse, covering roofs. However, slate occurs in a variety of colors even from a single locality; for example, slate from North Wales can be found in many shades of grey, from pale to dark, and may also be purple, green or cyan. Slate is not to be confused with shale, from which it may be formed, or schist.\n\nThe word \"slate\" is also used for certain types of object made from slate rock. It may mean a single roofing tile made of slate, or a writing slate. These were traditionally a small, smooth piece of the rock, often framed in wood, used with chalk as a notepad or noticeboard, and especially for recording charges in pubs and inns. The phrases \"clean slate\" and \"blank slate\" come from this usage.\n\nBefore the mid-19th century, the terms \"slate\", \"shale\" and \"schist\" were not sharply distinguished. In the context of underground coal mining in the United States, the term slate was commonly used to refer to shale well into the 20th century. For example, \"roof slate\" referred to shale above a coal seam, and \"draw slate\" referred to shale that fell from the mine roof as the coal was removed.\n\nSlate is mainly composed of the minerals quartz and muscovite or illite, often along with biotite, chlorite, hematite, and pyrite and, less frequently apatite, graphite, kaolinite, magnetite, tourmaline, or zircon as well as feldspar. Occasionally, as in the purple slates of North Wales, ferrous reduction spheres form around iron nuclei, leaving a light green spotted texture. These spheres are sometimes deformed by a subsequent applied stress field to ovoids, which appear as ellipses when viewed on a cleavage plane of the specimen.\n\nSlate can be made into roofing slates, a type of roof shingle, or more specifically a type of roof tile, which are installed by a slater. Slate has two lines of breakability – cleavage and grain – which make it possible to split the stone into thin sheets. When broken, slate retains a natural appearance while remaining relatively flat and easy to stack. A \"slate boom\" occurred in Europe from the 1870s until the first world war, allowed by the use of the steam engine in manufacturing slate tiles and improvements in road and waterway transportation systems.\n\nSlate is particularly suitable as a roofing material as it has an extremely low water absorption index of less than 0.4%, making the material waterproof. In fact, this natural slate, which requires only minimal processing, has the lowest embodied energy of all roofing materials.\nNatural slate is used by building professionals as a result of its beauty and durability. Slate is incredibly durable and can last several hundred years, often with little or no maintenance. Its low water absorption makes it very resistant to frost damage and breakage due to freezing. Natural slate is also fire resistant and energy efficient.\nSlate roof tiles are usually fixed (fastened) either with nails, or with hooks as is common with Spanish slate. In the UK, fixing is typically with double nails onto timber battens (England and Wales) or nailed directly onto timber sarking boards (Scotland and Northern Ireland). Nails were traditionally of copper, although there are modern alloy and stainless steel alternatives. Both these methods, if used properly, provide a long-lasting weathertight roof with a lifespan of around 80–100 years. Slate roofs are still used today.\n\nSome mainland European slate suppliers suggest that using hook fixing means that:\n\nThe metal hooks are, however, visible and may be unsuitable for historic properties.\n\nSlate tiles are often used for interior and exterior flooring, stairs, walkways and wall cladding. Tiles are installed and set on mortar and grouted along the edges. Chemical sealants are often used on tiles to improve durability and appearance, increase stain resistance, reduce efflorescence, and increase or reduce surface smoothness. Tiles are often sold gauged, meaning that the back surface is ground for ease of installation. Slate flooring can be slippery when used in external locations subject to rain. Slate tiles were used in 19th century UK building construction (apart from roofs) and in slate quarrying areas such as Blaenau Ffestiniog and Bethesda, Wales there are still many buildings wholly constructed of slate. Slates can also be set into walls to provide a rudimentary damp-proof membrane. Small offcuts are used as shims to level floor joists. In areas where slate is plentiful it is also used in pieces of various sizes for building walls and hedges, sometimes combined with other kinds of stone. In modern homes slate is often used as table coasters.\n\nBecause it is a good electrical insulator and fireproof, it was used to construct early-20th-century electric switchboards and relay controls for large electric motors. Fine slate can also be used as a whetstone to hone knives.\n\nDue to its thermal stability and chemical inertness, slate has been used for laboratory bench tops and for billiard table tops. In 18th- and 19th-century schools, slate was extensively used for blackboards and individual writing slates, for which slate or chalk pencils were used.\n\nIn areas where it is available, high-quality slate is used for tombstones and commemorative tablets. In some cases slate was used by the ancient Maya civilization to fashion stelae.\n\nSlate was traditional material of choice for black Go stones in Japan. It is now considered to be a luxury.\n\nMost slate in Europe today comes from Spain, the world's largest producer and exporter of natural slate, and 90 percent of Europe's natural slate used for roofing originates from the slate industry there.\n\nLesser slate-producing regions in Europe include Wales (with a museum at Llanberis), Cornwall (famously the village of Delabole), Cumbria (see Burlington Slate Quarries, Honister Slate Mine and Skiddaw Slate) and, formerly in the West Highlands of Scotland, around Ballachulish and the Slate Islands in the United Kingdom. Parts of France (Anjou, Loire Valley, Ardennes, Brittany, Savoie) and Belgium (Ardennes), Liguria in northern Italy, especially between the town of Lavagna (which means \"chalkboard\" in Italian) and Fontanabuona valley; Portugal especially around Valongo in the north of the country.\n\nGermany's Moselle River region, Hunsrück, Eifel, Westerwald, Thuringia and north Bavaria (with a former mine open as a museum at Fell); and Alta, Norway (actually schist, not a true slate). Some of the slate from Wales and Cumbria is colored slate (non-blue): purple and formerly green in Wales and green in Cumbria.\n\nSlate is abundant in Brazil, the world's second-biggest producer of slate, around Papagaios in Minas Gerais, which extracts 95 percent of Brazil's slate. However, not all \"slate\" products from Brazil are entitled to bear the CE mark.\n\nSlate is produced on the east coast of Newfoundland, in Eastern Pennsylvania, Buckingham County, Virginia, and the Slate Valley of Vermont and New York, where colored slate is mined in the Granville, New York area. Pennsylvania slate is widely used in the manufacture of turkey calls used for hunting turkeys in the U.S. The tones produced from the slate (when scratched with various species of wood strikers) imitates almost exactly the calls of all four species of wild turkey in North America: eastern, Rio Grande, Osceola and Merriam's.\n\nA major slating operation existed in Monson, Maine during the late 19th and early 20th centuries, where the slate is usually dark purple to blackish, and many local structures are still roofed with slate tiles. The roof of St. Patrick's Cathedral, New York was made of Monson slate, as is the headstone of John F. Kennedy.\n\nSlate is found in the Arctic, and was used by Inuit to make the blades for ulus.\n\nChina has vast slate deposits; in recent years its export of finished and unfinished slate has increased. It has slate in various colors.\n\nDeposits of slate exist throughout the Australian continent, with large reserves quarried in the Adelaide Hills (Willunga and Kanmantoo) and the Mid North (Mintaro and Spalding).\n\nBecause slate was formed in low heat and pressure, compared to a number of other metamorphic rocks, some fossils can be found in slate; sometimes even microscopic remains of delicate organisms can be found in slate.\n\n\n"}
{"id": "2430727", "url": "https://en.wikipedia.org/wiki?curid=2430727", "title": "Subsolar point", "text": "Subsolar point\n\nThe subsolar point on a planet is the point at which its sun is perceived to be directly overhead (at the zenith); that is, where the sun's rays strike the planet exactly perpendicular to its surface. It can also mean the point closest to the sun on an astronomical object, even though the sun might not be visible.\nTo an observer on a planet with an orientation and rotation similar to those of Earth, the subsolar point will appear to move westward, completing one circuit around the globe each day, approximately moving along the equator. However, it will also move north and south between the tropics over the course of a year, so it is spiraling like a helix.\n\nThe subsolar point contacts the Tropic of Cancer on the June solstice and the Tropic of Capricorn on the December solstice. The subsolar point crosses the Equator on the March and September equinoxes. \nWhen the point passes through Hawaii, the only U.S. state in which this happens, it is known as Lahaina Noon.\n\n"}
{"id": "32840848", "url": "https://en.wikipedia.org/wiki?curid=32840848", "title": "The Cloud (poem)", "text": "The Cloud (poem)\n\n\"The Cloud\" is a major 1820 poem written by Percy Bysshe Shelley. \"The Cloud\" was written during late 1819 or early 1820, and submitted for publication on 12 July 1820. The work was published in the 1820 collection \"Prometheus Unbound, A Lyrical Drama, in Four Acts, With Other Poems\" by Charles and James Ollier in London in August 1820. The work was proof-read by John Gisborne. There were multiple drafts of the poem. The poem consists of six stanzas in anapestic or antidactylus meter, a foot with two unaccented syllables followed by an accented syllable.\n\nThe cloud is a metaphor for the unending cycle of nature: \"I silently laugh at my own cenotaph/ ... I arise and unbuild it again.\" As with the wind and the leaves in \"Ode to the West Wind\", the skylark in \"To a Skylark\", and the plant in \"The Sensitive Plant\", Shelley endows the cloud with sentient traits that personify the forces of nature.\n\nIn \"The Cloud\", Shelley relies on the imagery of transformation or metamorphosis, a cycle of birth, death, and rebirth: \"I change, but I cannot die.\" Mutability or change is a fact of physical nature.\n\nLightning or electricity is the \"pilot\" or guide for the cloud. Lightning is attracted to the \"genii\" in the earth which results in lightning flashes. The genii symbolize the positive charge of the surface of the earth while the cloud possesses a negative charge.\n\nBritish scientist and poet Erasmus Darwin, the grandfather of Charles Darwin, had written about plant life and science in the poem collection \"The Botanic Garden\" (1791) and on \"spontaneous vitality\", that \"microscopic animals are said to remain dead for many days or weeks ... and quickly to recover life and motion\" when water and heat are added, in \"The Temple of Nature\" (1803). Percy Bysshe Shelley had cited Darwin in his Preface to the anonymously published novel \"Frankenstein; or, The Modern Prometheus\" (1818), explaining how the novel was written and its meaning. He argued that imparting life to a corpse \"as not of impossible occurrence\".\n\nThe cloud is a personification and a metaphor for the perpetual cycle of transformation and change in nature. All life and matter are interconnected and undergo unending change and metamorphosis.\n\nA review of the 1820 \"Prometheus Unbound\" collection in the September and October 1821 issues of \"The London Magazine\" noted the originality of \"The Cloud\": \"It is impossible to peruse them without admiring the peculiar property of the author's mind, which can doff in an instant the cumbersome garments of metaphysical speculations, and throw itself naked as it were into the arms of nature and humanity. The beautiful and singularly original poem of 'The Cloud' will evince proofs of our opinion, and show the extreme force and freshness with which the writer can impregnate his poetry.\"\n\nIn the October 1821 issue of \"Quarterly Review\", W.S. Walker argued that \"The Cloud\" is related to \"Prometheus Unbound\" in that they are both absurd and \"galimatias\".\n\nJohn Todhunter wrote in 1880 that \"The Cloud\" and \"To a Skylark\" were \"the two most popular of Shelley's lyrics\".\n\nIn 1889, Francis Thompson asserted that \"The Cloud\" was the \"most typically Shelleyan of all the poems\" because it contained \"the child's faculty of make-believe raised to the nth power\" and that \"He is still at play, save only that his play is such as manhood stops to watch, and his playthings are those which the gods give their children. The universe is his box of toys. He dabbles his fingers in the dayfall. He is gold-dusty with tumbling amidst the stars.\"\n\nOn 20 April 1919, a silent black and white movie was released in the US entitled \"The Cloud\" which was \"a visual poem featuring clouds and landscapes in accompaniment to the words of Shelley's poem 'The Cloud'.\" The film was directed by W.A. Van Scoy and produced by the Post Nature Pictures company.\n\n\n"}
{"id": "51130935", "url": "https://en.wikipedia.org/wiki?curid=51130935", "title": "The End of Night (book)", "text": "The End of Night (book)\n\nThe End of Night: Searching for Natural Darkness in an Age of Artificial Light is a 2013 non-fiction book by Paul Bogard on the gradual disappearance, due to light pollution, of true darkness from the night skies of most people on the planet. Bogard examines the effects of this loss on human physical and mental health, society, and ecosystems, and how it might be mitigated.\n\nThe book has been translated into Chinese, German, Japanese, Korean, and Spanish.\n\nBogard's book is structured into nine chapters, roughly corresponding to the nine levels of the Bortle scale, which attempts to quantify the subjective brightness and suitability for astronomy of the sky in different environments. On his use of the scale, which was invented in 2001, Bogard has said, \"one of the reasons why identifying different depths of darkness is so important is that we don’t recognize that we’re losing it, unless we have a name to recognize it by.\"\n\nBogard begins at a Bortle level 9 environment, by the Luxor Sky Beam, the brightest spotlight on Earth, located on the Las Vegas Strip. He explores the nighttime landscapes of London and Paris, and examines the planning, or lack thereof, in each city's lighting. He visits locations throughout the continental US, as well as Florence, the Canary Islands, and the isle of Sark, in his quest to understand the nature of light pollution. He experiences firsthand the deleterious effects of night shift work, talks with a former prison inmate about the psychological effects of uninterrupted light, and shares his own fear of the dark. Bogard ultimately finds a Bortle level 1 environment: an environment so perfectly free of stray light that the Milky Way casts noticeable shadows.\n\nBogard argues against the long-held assumption of a correlation between bright light and reduced crime, citing research that finds no such link. Rather than suggesting a return to the completely unlit nights of centuries past, however, he argues for a careful consideration of where and how light is deployed, in order to provide sufficient nighttime illumination for safety, without creating glare and other unwanted effects.\n\n\"Telegraph\" reviewer Stephanie Cross wrote that \"the appeal of Bogard’s book derives not just from his often wide-eyed enthusiasm for his subject, but also from the constellation of characters he encounters on his journeys into the night.\" In \"The Guardian\", novelist Salley Vickers wrote that \"Bogard sets about his investigations with an energetic purposiveness and enterprise,\" but complained that \"the book comes to seem a little thin, moving too rapidly from one chatty anecdotal meeting to another.\" \"The Wall Street Journal\" questioned Bogard's statements on the relationship between light and safety, and concluded ambivalently: \"\"The End of Night\" delivers a forceful, if incomplete, critique of our overexposed world.\"\n\nThe book was awarded the 2014 Nautilus Silver Award. It was named an Amazon Best Book of the Month and Nonfiction Editor's Pick for July 2013, and \"Gizmodo\" selected it as one of its Best Books of 2013. The book was shortlisted for the PEN/E. O. Wilson Literary Science Writing Award, and was a finalist for the Sigurd F. Olson Nature Writing Award.\n\nBorn in northern Minnesota, Bogard is an assistant professor of English at James Madison University.\n\n"}
{"id": "10340923", "url": "https://en.wikipedia.org/wiki?curid=10340923", "title": "Vacuum consolidation", "text": "Vacuum consolidation\n\nVacuum consolidation (or vacuum preloading) is a soft soil improvement method that has been successfully used by geotechnical engineers and specialists of ground improvement companies in countries such as Australia, China, Korea, Thailand and France for soil improvement or land reclamation (Chu et al., 2005). It does not necessarily require surcharge fill and vacuum loads of 80kPa or greater can, typically, be maintained for as long as required. However, if loads of 80kPa or greater are needed in order to achieve the target soil improvement, additional surcharge may be placed on top of the vacuum system. The vacuum preloading method is cheaper and faster than the fill surcharge method for an equivalent load in suitable areas. Where the underlying ground consists of permeable materials, such as sand or sandy clay, the cost of the technique will be significantly increased due to the requirement of cut-off walls into non-permeable layers to seal off the vacuum. It has been suggested by Carter et al. (2005) that the settlement resulting from vacuum preloading is less than that from a surcharge load of the same magnitude as vacuum consolidation is influenced by drainage boundary conditions.\n\n"}
{"id": "1945275", "url": "https://en.wikipedia.org/wiki?curid=1945275", "title": "Wigner effect", "text": "Wigner effect\n\nThe Wigner effect (named for its discoverer, Eugene Wigner), also known as the discomposition effect or Wigner's Disease, is the dislocation of atoms in a solid caused by neutron radiation. \n\nAny solid can display the Wigner effect. The effect is of most concern in neutron moderators, such as graphite, intended to reduce the speed of fast neutrons, thereby turning them into thermal neutrons capable of sustaining a nuclear chain reaction involving uranium-235.\n\nTo create the Wigner effect, neutrons that collide with the atoms in a crystal structure must have enough energy to displace them from the lattice. This amount (threshold displacement energy) is approximately 25 eV. A neutron's energy can vary widely, but it is not uncommon to have energies up to and exceeding 10 MeV (10,000,000 eV) in the centre of a nuclear reactor. A neutron with a significant amount of energy will create a displacement cascade in a matrix via elastic collisions. For example, a 1 MeV neutron striking graphite will create 900 displacements; not all displacements will create defects, because some of the struck atoms will find and fill the vacancies that were either small pre-existing voids or vacancies newly formed by the other struck atoms.\n\nThe atoms that do not find a vacancy come to rest in non-ideal locations; that is, not along the symmetrical lines of the lattice. These atoms are referred to as interstitial atoms, or simply interstitials. An interstitial atom and its associated vacancy are known as a Frenkel defect. Because these atoms are not in the ideal location, they have an energy associated with them, much as a ball at the top of a hill has gravitational potential energy. This energy is referred to as Wigner energy. When a large number of interstitials have accumulated, they pose a risk of releasing all of their energy suddenly, creating a rapid, very great increase in temperature. Sudden, unplanned increases in temperature can present a large risk for certain types of nuclear reactors with low operating temperatures; one such was the indirect cause of the Windscale fire. Accumulation of energy in irradiated graphite has been recorded as high as 2.7 kJ/g, but is typically much lower than this. Graphite, having a heat capacity of 0.720 J/g°C, could see a sudden increase in temperature of about 3750 °C (6780 °F).\n\nDespite some reports, Wigner energy buildup had nothing to do with the cause of the Chernobyl disaster: this reactor, like all contemporary power reactors, operated at a high enough temperature to allow the displaced graphite structure to realign itself before any potential energy could be stored. Wigner energy may have played some part following the prompt critical neutron spike, when the accident entered the graphite fire phase of events.\n\nA buildup of Wigner energy can be relieved by heating the material. This process is known as annealing. In graphite this occurs at 250 °C.\n\nIn 2003, it was postulated that Wigner energy can be stored by the formation of metastable defect structures in graphite. Notably, the large energy release observed at 200–250 °C has been described in terms of a metastable interstitial-vacancy pair. The interstitial atom becomes trapped on the lip of the vacancy, and there is a barrier for it to recombine to give perfect graphite.\n\n"}
{"id": "56106", "url": "https://en.wikipedia.org/wiki?curid=56106", "title": "Wildfire", "text": "Wildfire\n\nA wildfire or wildland fire is a fire in an area of combustible vegetation occurring in rural areas. Depending on the type of vegetation present, a wildfire can also be classified more specifically as a brush fire, bushfire, desert fire, forest fire, grass fire, hill fire, peat fire, vegetation fire, and veld fire.\n\nFossil charcoal indicates that wildfires began soon after the appearance of terrestrial plants 420 million years ago. Wildfire's occurrence throughout the history of terrestrial life invites conjecture that fire must have had pronounced evolutionary effects on most ecosystems' flora and fauna. Earth is an intrinsically flammable planet owing to its cover of carbon-rich vegetation, seasonally dry climates, atmospheric oxygen, and widespread lightning and volcanic ignitions.\n\nWildfires can be characterized in terms of the cause of ignition, their physical properties, the combustible material present, and the effect of weather on the fire. Wildfires can cause damage to property and human life, though naturally occurring wildfires may have beneficial effects on native vegetation, animals, and ecosystems that have evolved with fire. High-severity wildfire creates complex early seral forest habitat (also called \"snag forest habitat\"), which often has higher species richness and diversity than unburned old forest. Many plant species depend on the effects of fire for growth and reproduction. Wildfires in ecosystems where wildfire is uncommon or where non-native vegetation has encroached may have strongly negative ecological effects. Wildfire behavior and severity result from the combination of factors such as available fuels, physical setting, and weather. Analyses of historical meteorological data and national fire records in western North America show the primacy of climate in driving large regional fires via wet periods that create substantial fuels or drought and warming that extend conducive fire weather.\n\nStrategies for wildfire prevention, detection, and suppression have varied over the years. One common and inexpensive technique is controlled burning, intentionally igniting smaller fires to minimize the amount of flammable material available for a potential wildfire. Vegetation may be burned periodically to maintain high species diversity and limit the accumulation of plants and other debris that may serve as fuel. Wildland fire use is the cheapest and most ecologically appropriate policy for many forests. Fuels may also be removed by logging, but fuels treatments and thinning have no effect on severe fire behavior when under extreme weather conditions. Wildfire itself is reportedly \"the most effective treatment for reducing a fire's rate of spread, fireline intensity, flame length, and heat per unit of area\" according to Jan Van Wagtendonk, a biologist at the Yellowstone Field Station. Building codes in fire-prone areas typically require that structures be built of flame-resistant materials and a defensible space be maintained by clearing flammable materials within a prescribed distance from the structure.\n\nThree major natural causes of wildfire ignitions exist: \n\nThe most common direct human causes of wildfire ignition include arson, discarded cigarettes, power-line arcs (as detected by arc mapping), and sparks from equipment. Ignition of wildland fires via contact with hot rifle-bullet fragments is also possible under the right conditions. Wildfires can also be started in communities experiencing shifting cultivation, where land is cleared quickly and farmed until the soil loses fertility, and slash and burn clearing. Forested areas cleared by logging encourage the dominance of flammable grasses, and abandoned logging roads overgrown by vegetation may act as fire corridors. Annual grassland fires in southern Vietnam stem in part from the destruction of forested areas by US military herbicides, explosives, and mechanical land-clearing and -burning operations during the Vietnam War.\n\nThe most common cause of wildfires varies throughout the world. In Canada and northwest China, lightning operates as the major source of ignition. In other parts of the world, human involvement is a major contributor. In Africa, Central America, Fiji, Mexico, New Zealand, South America, and Southeast Asia, wildfires can be attributed to human activities such as agriculture, animal husbandry, and land-conversion burning. In China and in the Mediterranean Basin, human carelessness is a major cause of wildfires. In the United States and Australia, the source of wildfires can be traced both to lightning strikes and to human activities (such as machinery sparks, cast-away cigarette butts, or arson). Coal seam fires burn in the thousands around the world, such as those in Burning Mountain, New South Wales; Centralia, Pennsylvania; and several coal-sustained fires in China. They can also flare up unexpectedly and ignite nearby flammable material.\n\nThe spread of wildfires varies based on the flammable material present, its vertical arrangement and moisture content, and weather conditions. Fuel arrangement and density is governed in part by topography, as land shape determines factors such as available sunlight and water for plant growth. Overall, fire types can be generally characterized by their fuels as follows: \n\nWildfires occur when all of the necessary elements of a fire triangle come together in a susceptible area: an ignition source is brought into contact with a combustible material such as vegetation, that is subjected to sufficient heat and has an adequate supply of oxygen from the ambient air. A high moisture content usually prevents ignition and slows propagation, because higher temperatures are required to evaporate any water within the material and heat the material to its fire point. Dense forests usually provide more shade, resulting in lower ambient temperatures and greater humidity, and are therefore less susceptible to wildfires. Less dense material such as grasses and leaves are easier to ignite because they contain less water than denser material such as branches and trunks. Plants continuously lose water by evapotranspiration, but water loss is usually balanced by water absorbed from the soil, humidity, or rain. When this balance is not maintained, plants dry out and are therefore more flammable, often a consequence of droughts.\n\nA wildfire \"front\" is the portion sustaining continuous flaming combustion, where unburned material meets active flames, or the smoldering transition between unburned and burned material. As the front approaches, the fire heats both the surrounding air and woody material through convection and thermal radiation. First, wood is dried as water is vaporized at a temperature of . Next, the pyrolysis of wood at releases flammable gases. Finally, wood can smoulder at or, when heated sufficiently, ignite at . Even before the flames of a wildfire arrive at a particular location, heat transfer from the wildfire front warms the air to , which pre-heats and dries flammable materials, causing materials to ignite faster and allowing the fire to spread faster. High-temperature and long-duration surface wildfires may encourage flashover or \"torching\": the drying of tree canopies and their subsequent ignition from below.\n\nWildfires have a rapid \"forward rate of spread\" (FROS) when burning through dense, uninterrupted fuels. They can move as fast as in forests and in grasslands. Wildfires can advance tangential to the main front to form a \"flanking\" front, or burn in the opposite direction of the main front by \"backing\". They may also spread by \"jumping\" or \"spotting\" as winds and vertical convection columns carry \"firebrands\" (hot wood embers) and other burning materials through the air over roads, rivers, and other barriers that may otherwise act as firebreaks. Torching and fires in tree canopies encourage spotting, and dry ground fuels that surround a wildfire are especially vulnerable to ignition from firebrands. Spotting can create \"spot fires\" as hot embers and firebrands ignite fuels downwind from the fire. In Australian bushfires, spot fires are known to occur as far as from the fire front.\n\nEspecially large wildfires may affect air currents in their immediate vicinities by the stack effect: air rises as it is heated, and large wildfires create powerful updrafts that will draw in new, cooler air from surrounding areas in thermal columns. Great vertical differences in temperature and humidity encourage pyrocumulus clouds, strong winds, and fire whirls with the force of tornadoes at speeds of more than . Rapid rates of spread, prolific crowning or spotting, the presence of fire whirls, and strong convection columns signify extreme conditions.\n\nThe thermal heat from wildfire can cause significant weathering of rocks and boulders, heat can rapidly expand a boulder and thermal shock can occur, which may cause an object's structure to fail.\n\nHeat waves, droughts, cyclical climate changes such as El Niño, and regional weather patterns such as high-pressure ridges can increase the risk and alter the behavior of wildfires dramatically. Years of precipitation followed by warm periods can encourage more widespread fires and longer fire seasons. Since the mid-1980s, earlier snowmelt and associated warming has also been associated with an increase in length and severity of the wildfire season in the Western United States. Global warming may increase the intensity and frequency of droughts in many areas, creating more intense and frequent wildfires. A 2015 study indicates that the increase in fire risk in California may be attributable to human-induced climate change. A study of alluvial sediment deposits going back over 8,000 years found warmer climate periods experienced severe droughts and stand-replacing fires and concluded climate was such a powerful influence on wildfire that trying to recreate presettlement forest structure is likely impossible in a warmer future.\n\nIntensity also increases during daytime hours. Burn rates of smoldering logs are up to five times greater during the day due to lower humidity, increased temperatures, and increased wind speeds. Sunlight warms the ground during the day which creates air currents that travel uphill. At night the land cools, creating air currents that travel downhill. Wildfires are fanned by these winds and often follow the air currents over hills and through valleys. Fires in Europe occur frequently during the hours of 12:00 p.m. and 2:00 p.m. Wildfire suppression operations in the United States revolve around a 24-hour \"fire day\" that begins at 10:00 a.m. due to the predictable increase in intensity resulting from the daytime warmth.\n\nWildfire's occurrence throughout the history of terrestrial life invites conjecture that fire must have had pronounced evolutionary effects on most ecosystems' flora and fauna. Wildfires are common in climates that are sufficiently moist to allow the growth of vegetation but feature extended dry, hot periods. Such places include the vegetated areas of Australia and Southeast Asia, the veld in southern Africa, the fynbos in the Western Cape of South Africa, the forested areas of the United States and Canada, and the Mediterranean Basin.\n\nHigh-severity wildfire creates complex early seral forest habitat (also called “snag forest habitat”), which often has higher species richness and diversity than unburned old forest. Plant and animal species in most types of North American forests evolved with fire, and many of these species depend on wildfires, and particularly high-severity fires, to reproduce and grow. Fire helps to return nutrients from plant matter back to soil, the heat from fire is necessary to the germination of certain types of seeds, and the snags (dead trees) and early successional forests created by high-severity fire create habitat conditions that are beneficial to wildlife. Early successional forests created by high-severity fire support some of the highest levels of native biodiversity found in temperate conifer forests. Post-fire logging has no ecological benefits and many negative impacts; the same is often true for post-fire seeding.\n\nAlthough some ecosystems rely on naturally occurring fires to regulate growth, some ecosystems suffer from too much fire, such as the chaparral in southern California and lower-elevation deserts in the American Southwest. The increased fire frequency in these ordinarily fire-dependent areas has upset natural cycles, damaged native plant communities, and encouraged the growth of non-native weeds. Invasive species, such as \"Lygodium microphyllum\" and \"Bromus tectorum\", can grow rapidly in areas that were damaged by fires. Because they are highly flammable, they can increase the future risk of fire, creating a positive feedback loop that increases fire frequency and further alters native vegetation communities.\n\nIn the Amazon Rainforest, drought, logging, cattle ranching practices, and slash-and-burn agriculture damage fire-resistant forests and promote the growth of flammable brush, creating a cycle that encourages more burning. Fires in the rainforest threaten its collection of diverse species and produce large amounts of CO. Also, fires in the rainforest, along with drought and human involvement, could damage or destroy more than half of the Amazon rainforest by the year 2030. Wildfires generate ash, reduce the availability of organic nutrients, and cause an increase in water runoff, eroding away other nutrients and creating flash flood conditions. A 2003 wildfire in the North Yorkshire Moors burned off of heather and the underlying peat layers. Afterwards, wind erosion stripped the ash and the exposed soil, revealing archaeological remains dating back to 10,000 BC. Wildfires can also have an effect on climate change, increasing the amount of carbon released into the atmosphere and inhibiting vegetation growth, which affects overall carbon uptake by plants.\n\nIn tundra there is a natural pattern of accumulation of fuel and wildfire which varies depending on the nature of vegetation and terrain. Research in Alaska has shown fire-event return intervals, (FRIs) that typically vary from 150 to 200 years with dryer lowland areas burning more frequently than wetter upland areas.\n\nPlants in wildfire-prone ecosystems often survive through adaptations to their local fire regime. Such adaptations include physical protection against heat, increased growth after a fire event, and flammable materials that encourage fire and may eliminate competition. For example, plants of the genus \"Eucalyptus\" contain flammable oils that encourage fire and hard sclerophyll leaves to resist heat and drought, ensuring their dominance over less fire-tolerant species. Dense bark, shedding lower branches, and high water content in external structures may also protect trees from rising temperatures. Fire-resistant seeds and reserve shoots that sprout after a fire encourage species preservation, as embodied by pioneer species. Smoke, charred wood, and heat can stimulate the germination of seeds in a process called \"serotiny\". Exposure to smoke from burning plants promotes germination in other types of plants by inducing the production of the orange butenolide.\n\nGrasslands in Western Sabah, Malaysian pine forests, and Indonesian \"Casuarina\" forests are believed to have resulted from previous periods of fire. Chamise deadwood litter is low in water content and flammable, and the shrub quickly sprouts after a fire. Cape lilies lie dormant until flames brush away the covering and then blossom almost overnight. Sequoia rely on periodic fires to reduce competition, release seeds from their cones, and clear the soil and canopy for new growth. Caribbean Pine in Bahamian pineyards have adapted to and rely on low-intensity, surface fires for survival and growth. An optimum fire frequency for growth is every 3 to 10 years. Too frequent fires favor herbaceous plants, and infrequent fires favor species typical of Bahamian dry forests.\n\nMost of the Earth's weather and air pollution resides in the troposphere, the part of the atmosphere that extends from the surface of the planet to a height of about . The vertical lift of a severe thunderstorm or pyrocumulonimbus can be enhanced in the area of a large wildfire, which can propel smoke, soot, and other particulate matter as high as the lower stratosphere. Previously, prevailing scientific theory held that most particles in the stratosphere came from volcanoes, but smoke and other wildfire emissions have been detected from the lower stratosphere. Pyrocumulus clouds can reach over wildfires. Satellite observation of smoke plumes from wildfires revealed that the plumes could be traced intact for distances exceeding . Computer-aided models such as CALPUFF may help predict the size and direction of wildfire-generated smoke plumes by using atmospheric dispersion modeling.\n\nWildfires can affect local atmospheric pollution, and release carbon in the form of carbon dioxide. Wildfire emissions contain fine particulate matter which can cause cardiovascular and respiratory problems. Increased fire byproducts in the troposphere can increase ozone concentration beyond safe levels. Forest fires in Indonesia in 1997 were estimated to have released between 0.81 and 2.57 gigatonnes (0.89 and 2.83 billion short tons) of CO into the atmosphere, which is between 13%–40% of the annual global carbon dioxide emissions from burning fossil fuels. Atmospheric models suggest that these concentrations of sooty particles could increase absorption of incoming solar radiation during winter months by as much as 15%.\n\nIn the Welsh Borders, the first evidence of wildfire is rhyniophytoid plant fossils preserved as charcoal, dating to the Silurian period (about ). Smoldering surface fires started to occur sometime before the Early Devonian period . Low atmospheric oxygen during the Middle and Late Devonian was accompanied by a decrease in charcoal abundance. Additional charcoal evidence suggests that fires continued through the Carboniferous period. Later, the overall increase of atmospheric oxygen from 13% in the Late Devonian to 30-31% by the Late Permian was accompanied by a more widespread distribution of wildfires. Later, a decrease in wildfire-related charcoal deposits from the late Permian to the Triassic periods is explained by a decrease in oxygen levels.\n\nWildfires during the Paleozoic and Mesozoic periods followed patterns similar to fires that occur in modern times. Surface fires driven by dry seasons are evident in Devonian and Carboniferous progymnosperm forests. Lepidodendron forests dating to the Carboniferous period have charred peaks, evidence of crown fires. In Jurassic gymnosperm forests, there is evidence of high frequency, light surface fires. The increase of fire activity in the late Tertiary is possibly due to the increase of C-type grasses. As these grasses shifted to more mesic habitats, their high flammability increased fire frequency, promoting grasslands over woodlands. However, fire-prone habitats may have contributed to the prominence of trees such as those of the genera \"Eucalyptus\", \"Pinus\" and \"Sequoia\", which have thick bark to withstand fires and employ serotiny.\n\nThe human use of fire for agricultural and hunting purposes during the Paleolithic and Mesolithic ages altered the preexisting landscapes and fire regimes. Woodlands were gradually replaced by smaller vegetation that facilitated travel, hunting, seed-gathering and planting. In recorded human history, minor allusions to wildfires were mentioned in the Bible and by classical writers such as Homer. However, while ancient Hebrew, Greek, and Roman writers were aware of fires, they were not very interested in the uncultivated lands where wildfires occurred. Wildfires were used in battles throughout human history as early thermal weapons. From the Middle ages, accounts were written of occupational burning as well as customs and laws that governed the use of fire. In Germany, regular burning was documented in 1290 in the Odenwald and in 1344 in the Black Forest. In the 14th century Sardinia, firebreaks were used for wildfire protection. In Spain during the 1550s, sheep husbandry was discouraged in certain provinces by Philip II due to the harmful effects of fires used in transhumance. As early as the 17th century, Native Americans were observed using fire for many purposes including cultivation, signaling, and warfare. Scottish botanist David Douglas noted the native use of fire for tobacco cultivation, to encourage deer into smaller areas for hunting purposes, and to improve foraging for honey and grasshoppers. Charcoal found in sedimentary deposits off the Pacific coast of Central America suggests that more burning occurred in the 50 years before the Spanish colonization of the Americas than after the colonization. In the post-World War II Baltic region, socio-economic changes led more stringent air quality standards and bans on fires that eliminated traditional burning practices. In the mid-19th century, explorers from observed Australian Aborigines using fire for ground clearing, hunting, and regeneration of plant food in a method later named fire-stick farming. Such careful use of fire has been employed for centuries in the lands protected by Kakadu National Park to encourage biodiversity.\n\nWildfires typically occurred during periods of increased temperature and drought. An increase in fire-related debris flow in alluvial fans of northeastern Yellowstone National Park was linked to the period between AD 1050 and 1200, coinciding with the Medieval Warm Period. However, human influence caused an increase in fire frequency. Dendrochronological fire scar data and charcoal layer data in Finland suggests that, while many fires occurred during severe drought conditions, an increase in the number of fires during 850 BC and 1660 AD can be attributed to human influence. Charcoal evidence from the Americas suggested a general decrease in wildfires between 1 AD and 1750 compared to previous years. However, a period of increased fire frequency between 1750 and 1870 was suggested by charcoal data from North America and Asia, attributed to human population growth and influences such as land clearing practices. This period was followed by an overall decrease in burning in the 20th century, linked to the expansion of agriculture, increased livestock grazing, and fire prevention efforts. A meta-analysis found that 17 times more land burned annually in California before 1800 compared to recent decades (1,800,000 hectares/year compared to 102,000 hectares/year).\n\nAccording to a paper published in Science, the number of natural and human-caused fires decreased by 24.3% between 1998 and 2015. Researchers explain this a transition from nomadism to settled lifestyle and intensification of agriculture that lead to a drop in the use of fire for land clearing.\n\nIncreases of certain native tree species (i.e. conifers) in favor of others (i.e. leaf trees) also increases wildfire risk, especially if these trees are also planted in monocultures\n\nSome invasive species, moved in by humans (i.e., for the pulp and paper industry) have in some cases also increased the intensity of wildfires. Examples include species such as Eucalyptus in California and gamba grass in Australia.\n\nWildfire prevention refers to the preemptive methods aimed at reducing the risk of fires as well as lessening its severity and spread. Prevention techniques aim to manage air quality, maintain ecological balances, protect resources, and to affect future fires. North American firefighting policies permit naturally caused fires to burn to maintain their ecological role, so long as the risks of escape into high-value areas are mitigated. However, prevention policies must consider the role that humans play in wildfires, since, for example, 95% of forest fires in Europe are related to human involvement. Sources of human-caused fire may include arson, accidental ignition, or the uncontrolled use of fire in land-clearing and agriculture such as the slash-and-burn farming in Southeast Asia.\n\nIn 1937, U.S. President Franklin D. Roosevelt initiated a nationwide fire prevention campaign, highlighting the role of human carelessness in forest fires. Later posters of the program featured Uncle Sam, characters from the Disney movie \"Bambi\", and the official mascot of the U.S. Forest Service, Smokey Bear. Reducing human-caused ignitions may be the most effective means of reducing unwanted wildfire. Alteration of fuels is commonly undertaken when attempting to affect future fire risk and behavior. Wildfire prevention programs around the world may employ techniques such as \"wildland fire use\" and \"prescribed or controlled burns\". \"Wildland fire use\" refers to any fire of natural causes that is monitored but allowed to burn. \"Controlled burns\" are fires ignited by government agencies under less dangerous weather conditions.\n\nVegetation may be burned periodically to maintain high species diversity and frequent burning of surface fuels limits fuel accumulation. Wildland fire use is the cheapest and most ecologically appropriate policy for many forests. Fuels may also be removed by logging, but fuels treatments and thinning have no effect on severe fire behavior Wildfire models are often used to predict and compare the benefits of different fuel treatments on future wildfire spread, but their accuracy is low.\n\nWildfire itself is reportedly \"the most effective treatment for reducing a fire's rate of spread, fireline intensity, flame length, and heat per unit of area\" according to Jan van Wagtendonk, a biologist at the Yellowstone Field Station.\n\nBuilding codes in fire-prone areas typically require that structures be built of flame-resistant materials and a defensible space be maintained by clearing flammable materials within a prescribed distance from the structure. Communities in the Philippines also maintain fire lines wide between the forest and their village, and patrol these lines during summer months or seasons of dry weather. Continued residential development in fire-prone areas and rebuilding structures destroyed by fires has been met with criticism. The ecological benefits of fire are often overridden by the economic and safety benefits of protecting structures and human life.\n\nFast and effective detection is a key factor in wildfire fighting. Early detection efforts were focused on early response, accurate results in both daytime and nighttime, and the ability to prioritize fire danger. Fire lookout towers were used in the United States in the early 20th century and fires were reported using telephones, carrier pigeons, and heliographs. Aerial and land photography using instant cameras were used in the 1950s until infrared scanning was developed for fire detection in the 1960s. However, information analysis and delivery was often delayed by limitations in communication technology. Early satellite-derived fire analyses were hand-drawn on maps at a remote site and sent via overnight mail to the fire manager. During the Yellowstone fires of 1988, a data station was established in West Yellowstone, permitting the delivery of satellite-based fire information in approximately four hours.\n\nCurrently, public hotlines, fire lookouts in towers, and ground and aerial patrols can be used as a means of early detection of forest fires. However, accurate human observation may be limited by operator fatigue, time of day, time of year, and geographic location. Electronic systems have gained popularity in recent years as a possible resolution to human operator error. A government report on a recent trial of three automated camera fire detection systems in Australia did, however, conclude \"...detection by the camera systems was slower and less reliable than by a trained human observer\". These systems may be semi- or fully automated and employ systems based on the risk area and degree of human presence, as suggested by GIS data analyses. An integrated approach of multiple systems can be used to merge satellite data, aerial imagery, and personnel position via Global Positioning System (GPS) into a collective whole for near-realtime use by wireless Incident Command Centers.\n\nA small, high risk area that features thick vegetation, a strong human presence, or is close to a critical urban area can be monitored using a local sensor network. Detection systems may include wireless sensor networks that act as automated weather systems: detecting temperature, humidity, and smoke. These may be battery-powered, solar-powered, or \"tree-rechargeable\": able to recharge their battery systems using the small electrical currents in plant material. Larger, medium-risk areas can be monitored by scanning towers that incorporate fixed cameras and sensors to detect smoke or additional factors such as the infrared signature of carbon dioxide produced by fires. Additional capabilities such as night vision, brightness detection, and color change detection may also be incorporated into sensor arrays.\n\nSatellite and aerial monitoring through the use of planes, helicopter, or UAVs can provide a wider view and may be sufficient to monitor very large, low risk areas. These more sophisticated systems employ GPS and aircraft-mounted infrared or high-resolution visible cameras to identify and target wildfires. Satellite-mounted sensors such as Envisat's Advanced Along Track Scanning Radiometer and European Remote-Sensing Satellite's Along-Track Scanning Radiometer can measure infrared radiation emitted by fires, identifying hot spots greater than . The National Oceanic and Atmospheric Administration's Hazard Mapping System combines remote-sensing data from satellite sources such as Geostationary Operational Environmental Satellite (GOES), Moderate-Resolution Imaging Spectroradiometer (MODIS), and Advanced Very High Resolution Radiometer (AVHRR) for detection of fire and smoke plume locations. However, satellite detection is prone to offset errors, anywhere from for MODIS and AVHRR data and up to for GOES data. Satellites in geostationary orbits may become disabled, and satellites in polar orbits are often limited by their short window of observation time. Cloud cover and image resolution and may also limit the effectiveness of satellite imagery.\n\nin 2015 a new fire detection tool is in operation at the U.S. Department of Agriculture (USDA) Forest Service (USFS) which uses data from the Suomi National Polar-orbiting Partnership (NPP) satellite to detect smaller fires in more detail than previous space-based products. The high-resolution data is used with a computer model to predict how a fire will change direction based on weather and land conditions. The active fire detection product using data from Suomi NPP's Visible Infrared Imaging Radiometer Suite (VIIRS) increases the resolution of fire observations to 1,230 feet (375 meters). Previous NASA satellite data products available since the early 2000s observed fires at 3,280 foot (1 kilometer) resolution. The data is one of the intelligence tools used by the USFS and Department of Interior agencies across the United States to guide resource allocation and strategic fire management decisions. The enhanced VIIRS fire product enables detection every 12 hours or less of much smaller fires and provides more detail and consistent tracking of fire lines during long duration wildfires – capabilities critical for early warning systems and support of routine mapping of fire progression. Active fire locations are available to users within minutes from the satellite overpass through data processing facilities at the USFS Remote Sensing Applications Center, which uses technologies developed by the NASA Goddard Space Flight Center Direct Readout Laboratory in Greenbelt, Maryland. The model uses data on weather conditions and the land surrounding an active fire to predict 12–18 hours in advance whether a blaze will shift direction. The state of Colorado decided to incorporate the weather-fire model in its firefighting efforts beginning with the 2016 fire season.\n\nIn 2014, an international campaign was organized in South Africa's Kruger National Park to validate fire detection products including the new VIIRS active fire data. In advance of that campaign, the Meraka Institute of the Council for Scientific and Industrial Research in Pretoria, South Africa, an early adopter of the VIIRS 375m fire product, put it to use during several large wildfires in Kruger.\n\nThe demand for timely, high-quality fire information has increased in recent years. Wildfires in the United States burn an average of 7 million acres of land each year. For the last 10 years, the USFS and Department of Interior have spent a combined average of about $2–4 billion annually on wildfire suppression.\n\nWildfire suppression depends on the technologies available in the area in which the wildfire occurs. In less developed nations the techniques used can be as simple as throwing sand or beating the fire with sticks or palm fronds. In more advanced nations, the suppression methods vary due to increased technological capacity. Silver iodide can be used to encourage snow fall, while fire retardants and water can be dropped onto fires by unmanned aerial vehicles, planes, and helicopters. Complete fire suppression is no longer an expectation, but the majority of wildfires are often extinguished before they grow out of control. While more than 99% of the 10,000 new wildfires each year are contained, escaped wildfires under extreme weather conditions are difficult to suppress without a change in the weather. Wildfires in Canada and the US burn an average of per year.\n\nAbove all, fighting wildfires can become deadly. A wildfire's burning front may also change direction unexpectedly and jump across fire breaks. Intense heat and smoke can lead to disorientation and loss of appreciation of the direction of the fire, which can make fires particularly dangerous. For example, during the 1949 Mann Gulch fire in Montana, USA, thirteen smokejumpers died when they lost their communication links, became disoriented, and were overtaken by the fire. In the Australian February 2009 Victorian bushfires, at least 173 people died and over 2,029 homes and 3,500 structures were lost when they became engulfed by wildfire.\n\nIn California, the U.S. Forest Service spends about $200 million per year to suppress 98% of wildfires and up to $1 billion to suppress the other 2% of fires that escape initial attack and become large.\n\nWildland fire fighters face several life-threatening hazards including heat stress, fatigue, smoke and dust, as well as the risk of other injuries such as burns, cuts and scrapes, animal bites, and even rhabdomyolysis. Between 2000–2016, more than 350 wildland firefighters died on-duty.\n\nEspecially in hot weather condition, fires present the risk of heat stress, which can entail feeling heat, fatigue, weakness, vertigo, headache, or nausea. Heat stress can progress into heat strain, which entails physiological changes such as increased heart rate and core body temperature. This can lead to heat-related illnesses, such as heat rash, cramps, exhaustion or heat stroke. Various factors can contribute to the risks posed by heat stress, including strenuous work, personal risk factors such as age and fitness, dehydration, sleep deprivation, and burdensome personal protective equipment. Rest, cool water, and occasional breaks are crucial to mitigating the effects of heat stress.\n\nSmoke, ash, and debris can also pose serious respiratory hazards to wildland fire fighters. The smoke and dust from wildfires can contain gases such as carbon monoxide, sulfur dioxide and formaldehyde, as well as particulates such as ash and silica. To reduce smoke exposure, wildfire fighting crews should, whenever possible, rotate firefighters through areas of heavy smoke, avoid downwind firefighting, use equipment rather than people in holding areas, and minimize mop-up. Camps and command posts should also be located upwind of wildfires. Protective clothing and equipment can also help minimize exposure to smoke and ash.\n\nFirefighters are also at risk of cardiac events including strokes and heart attacks. Fire fighters should maintain good physical fitness. Fitness programs, medical screening and examination programs which include stress tests can minimize the risks of firefighting cardiac problems. Other injury hazards wildland fire fighters face include slips, trips and falls, burns, scrapes and cuts from tools and equipment, being struck by trees, vehicles, or other objects, plant hazards such as thorns and poison ivy, snake and animal bites, vehicle crashes, electrocution from power lines or lightning storms, and unstable building structures.\n\nFire retardants are used to slow wildfires by inhibiting combustion. They are aqueous solutions of ammonium phosphates and ammonium sulfates, as well as thickening agents. The decision to apply retardant depends on the magnitude, location and intensity of the wildfire. In certain instances, fire retardant may also be applied as a precautionary fire defense measure.\n\nTypical fire retardants contain the same agents as fertilizers. Fire retardant may also affect water quality through leaching, eutrophication, or misapplication. Fire retardant's effects on drinking water remain inconclusive. Dilution factors, including water body size, rainfall, and water flow rates lessen the concentration and potency of fire retardant. Wildfire debris (ash and sediment) clog rivers and reservoirs increasing the risk for floods and erosion that ultimately slow and/or damage water treatment systems. There is continued concern of fire retardant effects on land, water, wildlife habitats, and watershed quality, additional research is needed. However, on the positive side, fire retardant (specifically its nitrogen and phosphorus components) has been shown to have a fertilizing effect on nutrient-deprived soils and thus creates a temporary increase in vegetation.\n\nCurrent USDA procedure maintains that the aerial application of fire retardant in the United States must clear waterways by a minimum of 300 feet in order to safeguard effects of retardant runoff. Aerial uses of fire retardant are required to avoid application near waterways and endangered species (plant and animal habitats). After any incident of fire retardant misapplication, the U.S. Forest Service requires reporting and assessment impacts be made in order to determine mitigation, remediation, and/or restrictions on future retardant uses in that area.\n\nWildfire modeling is concerned with numerical simulation of wildfires in order to comprehend and predict fire behavior. Wildfire modeling aims to aid wildfire suppression, increase the safety of firefighters and the public, and minimize damage. Using computational science, wildfire modeling involves the statistical analysis of past fire events to predict spotting risks and front behavior. Various wildfire propagation models have been proposed in the past, including simple ellipses and egg- and fan-shaped models. Early attempts to determine wildfire behavior assumed terrain and vegetation uniformity. However, the exact behavior of a wildfire's front is dependent on a variety of factors, including windspeed and slope steepness. Modern growth models utilize a combination of past ellipsoidal descriptions and Huygens' Principle to simulate fire growth as a continuously expanding polygon. Extreme value theory may also be used to predict the size of large wildfires. However, large fires that exceed suppression capabilities are often regarded as statistical outliers in standard analyses, even though fire policies are more influenced by large wildfires than by small fires.\n\nWildfire risk is the chance that a wildfire will start in or reach a particular area and the potential loss of human values if it does. Risk is dependent on variable factors such as human activities, weather patterns, availability of wildfire fuels, and the availability or lack of resources to suppress a fire. Wildfires have continually been a threat to human populations. However, human induced geographical and climatic changes are exposing populations more frequently to wildfires and increasing wildfire risk. It is speculated that the increase in wildfires arises from a century of wildfire suppression coupled with the rapid expansion of human developments into fire-prone wildlands. Wildfires are naturally occurring events that aid in promoting forest health. Global warming and climate changes are causing an increase in temperatures and more droughts nationwide which contributes to an increase in wildfire risk.\n\nThe most noticeable adverse effect of wildfires is the destruction of property. However, the release of hazardous chemicals from the burning of wildland fuels also significantly impacts health in humans.\n\nWildfire smoke is composed primarily of carbon dioxide and water vapor. Other common smoke components present in lower concentrations are carbon monoxide, formaldehyde, acrolein, polyaromatic hydrocarbons, and benzene. Small particulates suspended in air which come in solid form or in liquid droplets are also present in smoke. 80 -90% of wildfire smoke, by mass, is within the fine particle size class of 2.5 micrometers in diameter or smaller.\n\nDespite carbon dioxide's high concentration in smoke, it poses a low health risk due to its low toxicity. Rather, carbon monoxide and fine particulate matter, particularly 2.5 µm in diameter and smaller, have been identified as the major health threats. Other chemicals are considered to be significant hazards but are found in concentrations that are too low to cause detectable health effects.\n\nThe degree of wildfire smoke exposure to an individual is dependent on the length, severity, duration, and proximity of the fire. People are exposed directly to smoke via the respiratory tract though inhalation of air pollutants. Indirectly, communities are exposed to wildfire debris that can contaminate soil and water supplies.\n\nThe U.S. Environmental Protection Agency (EPA) developed the Air Quality Index (AQI), a public resource that provides national air quality standard concentrations for common air pollutants. The public can use this index as a tool to determine their exposure to hazardous air pollutants based on visibility range.\n\nAfter a wildfire, hazards remain. Residents returning to their homes may be at risk from falling fire-weakened trees. Humans and pets may also be harmed by falling into ash pits.\n\nFirefighters are at the greatest risk for acute and chronic health effects resulting from wildfire smoke exposure. Due to firefighters' occupational duties, they are frequently exposed to hazardous chemicals at a close proximity for longer periods of time. A case study on the exposure of wildfire smoke among wildland firefighters shows that firefighters are exposed to significant levels of carbon monoxide and respiratory irritants above OSHA-permissible exposure limits (PEL) and ACGIH threshold limit values (TLV). 5–10% are overexposed. The study obtained exposure concentrations for one wildland firefighter over a 10-hour shift spent holding down a fireline. The firefighter was exposed to a wide range of carbon monoxide and respiratory irritant (combination of particulate matter 3.5 µm and smaller, acrolein, and formaldehype) levels. Carbon monoxide levels reached up to 160ppm and the TLV irritant index value reached a high of 10. In contrast, the OSHA PEL for carbon monoxide is 30ppm and for the TLV respiratory irritant index, the calculated threshold limit value is 1; any value above 1 exceeds exposure limits.\n\nBetween 2001 and 2012, over 200 fatalities occurred among wildland firefighters. In addition to heat and chemical hazards, firefighters are also at risk for electrocution from power lines; injuries from equipment; slips, trips, and falls; injuries from vehicle rollovers; heat-related illness; insect bites and stings; stress; and rhabdomyolysis.\n\nResidents in communities surrounding wildfires are exposed to lower concentrations of chemicals, but they are at a greater risk for indirect exposure through water or soil contamination. Exposure to residents is greatly dependent on individual susceptibility. Vulnerable persons such as children (ages 0–4), the elderly (ages 65 and older), smokers, and pregnant women are at an increased risk due to their already compromised body systems, even when the exposures are present at low chemical concentrations and for relatively short exposure periods.\n\nAdditionally, there is evidence of an increase in maternal stress, as documented by researchers M.H. O'Donnell and A.M. Behie, thus affecting birth outcomes. In Australia, studies show that male infants born with drastically higher average birth weights were born in mostly severely fire-affected areas. This is attributed to the fact that maternal signals directly affect fetal growth patterns.\n\nAsthma is one of the most common chronic disease among children in the United States affecting estimated 6.2 million children. A recent area of research on asthma risk focuses specifically on the risk of air pollution during the gestational period. Several pathophysiology processes are involved are in this. In human's considerable airway development occurs during the 2 and 3 trimester and continue until 3 years of age. It is hypothesized that exposure to these toxins during this period could have consequential effects as the epithelium of the lungs during this time could have increased permeability to toxins. Exposure to air pollution during parental and pre-natal stage could induce epigenetic changes which are responsible for the development of asthma. Recent Meta-Analyses have found significant association between PM, NO and development of asthma during childhood despite heterogeneity among studies. Furthermore, maternal exposure to chronic stressor, which are most like to be present in distressed communities, which is also a relevant co relate of childhood asthma which may further help explain the early childhood exposure to air pollution, neighborhood poverty and childhood risk. Living in distressed neighborhood is not only linked to pollutant source location and exposure but can also be associated with degree of magnitude of chronic individual stress which can in turn alter the allostatic load of the maternal immune system leading to adverse outcomes in children, including increased susceptibility to air pollution and other hazards.\n\nWildfire smoke contains particulate matter that may have adverse effects upon the human respiratory system. Evidence of the health effects of wildfire smoke should be relayed to the public so that exposure may be limited. Evidence of health effects can also be used to influence policy to promote positive health outcomes.\n\nInhalation of smoke from a wildfire can be a health hazard. Wildfire smoke is composed of combustion products i.e. carbon dioxide, carbon monoxide, water vapor, particulate matter, organic chemicals, nitrogen oxides and other compounds. The principal health concern is the inhalation of particulate matter and carbon monoxide.\n\nParticulate matter (PM) is a type of air pollution made up of particles of dust and liquid droplets. They are characterized into three categories based on the diameter of the particle: coarse PM, fine PM, and ultrafine PM. Coarse particles are between 2.5 micrometers and 10 micrometers, fine particles measure 0.1 to 2.5 micrometers, and ultrafine particle are less than 0.1 micrometer.  Each size can enter the body through inhalation, but the PM impact on the body varies by size. Coarse particles are filtered by the upper airways and these particles can accumulate and cause pulmonary inflammation. This can result in eye and sinus irritation as well as sore throat and coughing. Coarse PM is often composed of materials that are heavier and more toxic that lead to short-term effects with stronger impact.\n\nSmaller particulate moves further into the respiratory system creating issues deep into the lungs and the bloodstream. In asthma patients, PM causes inflammation but also increases oxidative stress in the epithelial cells. These particulates also cause apoptosis and autophagy in lung epithelial cells. Both processes cause the cells to be damaged and impacts the cell function. This damage impacts those with respiratory conditions such as asthma where the lung tissues and function are already compromised. The third PM type is ultra-fine PM (UFP). UFP can enter the bloodstream like PM however studies show that it works into the blood much quicker. The inflammation and epithelial damage done by UFP has also shown to be much more severe. PM is of the largest concern in regards to wildfire. This is particularly hazardous to the very young, elderly and those with chronic conditions such as asthma, chronic obstructive pulmonary disease (COPD), cystic fibrosis and cardiovascular conditions. The illnesses most commonly with exposure to fine particle from wildfire smoke are bronchitis, exacerbation of asthma or COPD, and pneumonia. Symptoms of these complications include wheezing and shortness of breath and cardiovascular symptoms include chest pain, rapid heart rate and fatigue.\n\nSmoke from wildfires can cause health problems, especially for children and those who already have respiratory problems. Several epidemiological studies have demonstrated a close association between air pollution and respiratory allergic diseases such as bronchial asthma. \n\nAn observational study of smoke exposure related to the 2007 San Diego wildfires revealed an increase both in healthcare utilization and respiratory diagnoses, especially asthma among the group sampled. Projected climate scenarios of wildfire occurrences predict significant increases in respiratory conditions among young children. Particulate Matter (PM) triggers a series of biological processes including inflammatory immune response, oxidative stress, which are associated with harmful changes in allergic respiratory diseases.\n\nAlthough some studies demonstrated no significant acute changes in lung function among people with asthma related to PM from wildfires, a possible explanation for these counterintuitive findings is the increased use of quick-relief medications, such as inhalers, in response to elevated levels of smoke among those already diagnosed with asthma. In investigating the association of medication use for obstructive lung disease and wildfire exposure, researchers found increases both in the usage of inhalers and initiation of long-term control as in oral steroids. More specifically, some people with asthma reported higher use of quick-relief medications (inhalers). After two major wildfires in California, researchers found an increase in physician prescriptions for quick-relief medications in the years following the wildfires than compared to the year before each occurrence. \n\nThere is consistent evidence between wildfire smoke and the exacerbation of asthma.\n\nCarbon monoxide (CO) is a colorless, odorless gas that can be found at the highest concentration at close proximity to a smoldering fire. For this reason, carbon monoxide inhalation is a serious threat to the health of wildfire firefighters. CO in smoke can be inhaled into the lungs where it is absorbed into the bloodstream and reduces oxygen delivery to the body's vital organs. At high concentrations, it can cause headache, weakness, dizziness, confusion, nausea, disorientation, visual impairment, coma and even death. However, even at lower concentrations, such as those found at wildfires, individuals with cardiovascular disease may experience chest pain and cardiac arrhythmia. A recent study tracking the number and cause of wildfire firefighter deaths from 1990–2006 found that 21.9% of the deaths occurred from heart attacks.\n\nAnother important and somewhat less obvious health effect of wildfires is psychiatric diseases and disorders. Both adults and children from countries ranging from the United States and Canada to Greece and Australia who were directly and indirectly affected by wildfires were found by researchers to demonstrate several different mental conditions linked to their experience with the wildfires. These include post-traumatic stress disorder (PTSD), depression, anxiety, and phobias.\n\nIn a new twist to wildfire health effects, former uranium mining sites were burned over in the summer of 2012 near North Fork, Idaho. This prompted concern from area residents and Idaho State Department of Environmental Quality officials over the potential spread of radiation in the resultant smoke, since those sites had never been completely cleaned up from radioactive remains.\n\nThe western US has seen an increase in both frequency and intensity of wildfires over the last several decades. This increase has been attributed to the arid climate of the western US and the effects of global warming. An estimated 46 million people were exposed to wildfire smoke from 2004 to 2009 in the Western United States. Evidence has demonstrated that wildfire smoke can increase levels of particulate matter in the atmosphere.\n\nThe EPA has defined acceptable concentrations of particulate matter in the air, through the National Ambient Air Quality Standards and monitoring of ambient air quality has been mandated. Due to these monitoring programs and the incidence of several large wildfires near populated areas, epidemiological studies have been conducted and demonstrate an association between human health effects and an increase in fine particulate matter due to wildfire smoke.\n\nThe EPA has defined acceptable concentrations of particulate matter in the air. The National Ambient Air Quality Standards are part of the Clean Air Act and provide mandated guidelines for pollutant levels and the monitoring of ambient air quality. In addition to these monitoring programs, the increased incidence of wildfires near populated areas have precipitated several epidemiological studies. Such studies have demonstrated an association between negative human health effects and an increase in fine particulate matter due to wildfire smoke. The size of the particulate matter is significant as smaller particulate matter (fine) is easily inhaled into the human respiratory tract. Often, small particulate matter can be inhaled into deep lung tissue causing respiratory distress, illness, or disease. \n\nAn increase in PM smoke emitted from the Hayman fire in Colorado in June 2002, was associated with an increase in respiratory symptoms in patients with COPD. Looking at the wildfires in Southern California in October 2003 in a similar manner, investigators have shown an increase in hospital admissions due to asthma symptoms while being exposed to peak concentrations of PM in smoke. Another epidemiological study found a 7.2% (95% confidence interval: 0.25%, 15%) increase in risk of respiratory related hospital admissions during smoke wave days with high wildfire-specific particulate matter 2.5 compared to matched non-smoke-wave days.\n\nChildren participating in the Children's Health Study were also found to have an increase in eye and respiratory symptoms, medication use and physician visits. Recently, it was demonstrated that mothers who were pregnant during the fires gave birth to babies with a slightly reduced average birth weight compared to those who were not exposed to wildfire during birth. Suggesting that pregnant women may also be at greater risk to adverse effects from wildfire. Worldwide it is estimated that 339,000 people die due to the effects of wildfire smoke each year.\n\nWhile the size of particulate matter is an important consideration for health effects, the chemical composition of particulate matter (PM) from wildfire smoke should also be considered. Antecedent studies have demonstrated that the chemical composition of PM from wildfire smoke can yield different estimates of human health outcomes as compared to other sources of smoke. Health outcomes for people exposed to wildfire smoke may differ from those exposed to smoke from alternative sources such as solid fuels. \n\n"}
