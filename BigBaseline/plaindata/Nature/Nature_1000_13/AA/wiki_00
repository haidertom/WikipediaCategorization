{"id": "41401861", "url": "https://en.wikipedia.org/wiki?curid=41401861", "title": "AMSilk", "text": "AMSilk\n\nAMSilk is an industrial supplier of synthetic silk biopolymers. The polymers are biocompatible and breathable. The company was founded in 2008 and has its headquarters at the IZB in Planegg near Munich. AMSilk is an industrial biotechnology company with a proprietary production process for their silk materials.\n\nAMSilk is known for creating a biodegradable running shoe for Adidas made of recombinant spider silk. Jens Klein, CEO of AMSilk, said during an interview that the biodegradable material can help reduce the amount of waste that has to be burned or pollutes the environment.\n\nAMSilk is also developing breast implants made of biodegradable spider silk in collaboration with the German company Polytech.\n\n"}
{"id": "2249310", "url": "https://en.wikipedia.org/wiki?curid=2249310", "title": "Aether theories", "text": "Aether theories\n\nAether theories (also known as ether theories) in physics propose the existence of a medium, the aether (also spelled \"ether\", from the Greek word (), meaning \"upper air\" or \"pure, fresh air\"), a space-filling substance or field, thought to be necessary as a transmission medium for the propagation of electromagnetic or gravitational forces. The assorted \"aether theories\" embody the various conceptions of this \"medium\" and \"substance\"\". This early modern aether has little in common with the aether of classical elements from which the name was borrowed. Since the development of special relativity, theories using a substantial aether fell out of use in modern physics, and were replaced by more abstract models.\n\nIsaac Newton suggests the existence of an aether in the Third Book of \"Opticks\" (1718): \"Doth not this aethereal medium in passing out of water, glass, crystal, and other compact and dense bodies in empty spaces, grow denser and denser by degrees, and by that means refract the rays of light not in a point, but by bending them gradually in curve lines? ...Is not this medium much rarer within the dense bodies of the Sun, stars, planets and comets, than in the empty celestial space between them? And in passing from them to great distances, doth it not grow denser and denser perpetually, and thereby cause the gravity of those great bodies towards one another, and of their parts towards the bodies; every body endeavouring to go from the denser parts of the medium towards the rarer?\"\n\nIn the 19th century, luminiferous aether (or ether), meaning light-bearing aether, was a theorized medium for the propagation of light (electromagnetic radiation). However, a series of increasingly complex experiments had been carried out in the late 1800s like the Michelson-Morley experiment in an attempt to detect the motion of Earth through the aether, and had failed to do so. A range of proposed aether-dragging theories could explain the null result but these were more complex, and tended to use arbitrary-looking coefficients and physical assumptions. Joseph Larmor discussed the aether in terms of a moving magnetic field caused by the acceleration of electrons.\n\nJames Clerk Maxwell said of the aether, \"In several parts of this treatise an attempt has been made to explain electromagnetic phenomena by means of mechanical action transmitted from one body to another by means of a medium occupying the space between them. The undulatory theory of light also assumes the existence of a medium. We have now to show that the properties of the electromagnetic medium are identical with those of the luminiferous medium.\"\n\nHendrik Lorentz and George Francis FitzGerald offered within the framework of Lorentz ether theory a more elegant solution to how the motion of an absolute aether could be undetectable (length contraction), but if their equations were correct, Albert Einstein's 1905 special theory of relativity could generate the same mathematics without referring to an aether at all. This led most physicists to conclude that this early modern notion of a luminiferous aether was not a useful concept. Einstein however stated that this consideration was too radical and too anticipatory and that his theory of relativity still needed the presence of a medium with certain properties.\n\nFrom the 16th until the late 19th century, gravitational phenomena had also been modelled utilizing an aether. The most well-known formulation is Le Sage's theory of gravitation, although other models were proposed by Isaac Newton, Bernhard Riemann, and Lord Kelvin. None of those concepts are considered to be viable by the scientific community today.\n\nEinstein sometimes used the word \"aether\" for the gravitational field within general relativity, but this terminology never gained widespread support.\n\nQuantum mechanics can be used to describe spacetime as being non-empty at extremely small scales, fluctuating and generating particle pairs that appear and disappear incredibly quickly. It has been suggested by some such as Paul Dirac that this quantum vacuum may be the equivalent in modern physics of a particulate aether. However, Dirac's aether hypothesis was motivated by his dissatisfaction with quantum electrodynamics, and it never gained support from the mainstream scientific community.\n\nRobert B. Laughlin, Nobel Laureate in Physics, endowed chair in physics, Stanford University, had this to say about ether in contemporary theoretical physics:\n\nLouis de Broglie stated, \"Any particle, ever isolated, has to be imagined as in continuous \"energetic contact\" with a hidden medium.\"\n\nAccording to the philosophical point of view of Einstein, Dirac, Bell, Polyakov, ’t Hooft, Laughlin, de Broglie, Maxwell, Newton and other theorists, there might be a medium with physical properties filling 'empty' space, an aether, enabling the observed physical processes.\n\nAlbert Einstein in 1894 or 1895: \"The velocity of a wave is proportional to the square root of the elastic forces which cause [its] propagation, and inversely proportional to the mass of the aether moved by these forces.\"\n\nAlbert Einstein in 1920: \"We may say that according to the general theory of relativity space is endowed with physical qualities; in this sense, therefore, there exists an Aether. According to the general theory of relativity space without Aether is unthinkable; for in such space there not only would be no propagation of light, but also no possibility of existence for standards of space and time (measuring-rods and clocks), nor therefore any space-time intervals in the physical sense. But this Aether may not be thought of as endowed with the quality characteristic of ponderable media, as consisting of parts which may be tracked through time. The idea of motion may not be applied to it.\"\n\nPaul Dirac wrote in 1951: \"Physical knowledge has advanced much since 1905, notably by the arrival of quantum mechanics, and the situation [about the scientific plausibility of Aether] has again changed. If one examines the question in the light of present-day knowledge, one finds that the Aether is no longer ruled out by relativity, and good reasons can now be advanced for postulating an Aether ... We have now the velocity at all points of space-time, playing a fundamental part in electrodynamics. It is natural to regard it as the velocity of some real physical thing. Thus with the new theory of electrodynamics [vacuum filled with virtual particles] we are rather forced to have an Aether\".\n\nJohn Bell in 1986, interviewed by Paul Davies in \"The Ghost in the Atom\" has suggested that an Aether theory might help resolve the EPR paradox by allowing a reference frame in which signals go faster than light. He suggests Lorentz contraction is perfectly coherent, not inconsistent with relativity, and could produce an aether theory perfectly consistent with the Michelson-Morley experiment. Bell suggests the aether was wrongly rejected on purely philosophical grounds: \"what is unobservable does not exist\" [p. 49]. Einstein found the non-aether theory simpler and more elegant, but Bell suggests that doesn't rule it out. Besides the arguments based on his interpretation of quantum mechanics, Bell also suggests resurrecting the aether because it is a useful pedagogical device. That is, many problems are solved more easily by imagining the existence of an aether.\n\nEinstein remarked \"God does not play dice with the Universe\". And those agreeing with him are looking for a classical, deterministic aether theory that would imply quantum-mechanical predictions as a statistical approximation, a hidden variable theory. In particular, Gerard 't Hooft conjectured that: \"We should not forget that quantum mechanics does not really describe what kind of dynamical phenomena are actually going on, but rather gives us probabilistic results. To me, it seems extremely plausible that any reasonable theory for the dynamics at the Planck scale would lead to processes that are so complicated to describe, that one should expect apparently stochastic fluctuations in any approximation theory describing the effects of all of this at much larger scales. It seems quite reasonable first to try a classical, deterministic theory for the Planck domain. One might speculate then that what we call quantum mechanics today, may be nothing else than an ingenious technique to handle this dynamics statistically.\" In their paper Blasone, Jizba and Kleinert \"have attempted to substantiate the recent proposal of G. ’t Hooft in which quantum theory is viewed as not a complete field theory, but is in fact an emergent phenomenon arising from a deeper level of dynamics. The underlying dynamics are taken to be classical mechanics with singular Lagrangians supplied with an appropriate information loss condition. With plausible assumptions about the actual nature of the constraint dynamics, quantum theory is shown to emerge when the classical Dirac-Bergmann algorithm for constrained dynamics is applied to the classical path integral [...].\"\n\nLouis de Broglie, \"If a hidden sub-quantum medium is assumed, knowledge of its nature would seem desirable. It certainly is of quite complex character. It could not serve as a universal reference medium, as this would be contrary to relativity theory.\"\n\nIn 1982, Ioan-Iovitz Popescu, a Romanian physicist, wrote that the aether is \"a form of existence of the matter, but it differs qualitatively from the common (atomic and molecular) substance or radiation (photons)\". The \"fluid aether\" is \"governed by the principle of inertia and its presence produces a modification of the space-time geometry\". Built upon Le Sage's \"ultra-mundane corpuscles\", Popescu's theory posits a finite Universe \"filled with some particles of exceedingly small mass, traveling chaotically at speed of light\" and material bodies \"made up of such particles called \"etherons\"\".\n\nSid Deutsch, a professor of electrical engineering and bioengineerig, conjectures that a \"spherical, spinning\" \"aether particle\" must exist in order \"to carry electromagnetic waves\" and derives its diameter and mass using the density of dark matter.\n\nA degenerate Fermi fluid model, \"composed primarily of \"electrons and positrons\"\" that has the consequence of a speed of light decreasing \"with time on the scale of the age of the universe\" was proposed by Allen Rothwarf. In a cosmological extension the model was \"extended to predict a \"decelerating expansion of the universe\"\".\n\n\n"}
{"id": "30938173", "url": "https://en.wikipedia.org/wiki?curid=30938173", "title": "Ate-u-tiv", "text": "Ate-u-tiv\n\nAte-u-Tiv (sometimes written as \"Ate u Tiv\" and less popularly known as \"Tsun\") is a traditional architectural masterpiece of the Tiv People of the Middle-belt Region of Nigeria in West Africa.\n\nThe word \"Atē\" stands for the round, open hut; while \"Átē-ŭ-Tiv\" attributes it to the Tiv people. The Ate-u-Tiv serves as a relaxation and reception point for \"vanya\" (guests) and allows \"mbamaren, ônov man angbianev\" (family members) to \"tema imiôngo\" (chat), sharing ideas and telling stories. The \"Orya\" (family head) receives guests and attend to family issues (discussions) from the \"Ate\".\n\nTo construct a traditional Ate-u-Tiv, you will require a minimum of six poles called \"mtôm\" which are y-shaped at the top; these serve as the pillars. The total number of poles required will however depend on the diameter of the Ate to be constructed. These are erected upright into the ground in a circle with equal spacing between poles. Next is \"ukyaver\" which comprise stems of slim climbing plants. These are used in constructing a sort of lintel to hold the roof.\n\nThe roof of an \"Ate-u-Tiv\" comprise \"ihyange\" (paulins) and \"ihila\" (grass). The ihyange are woven together in a cone-shape with ukyaver holding together the rafters. The completed structure is then hoisted unto the pillars/lintel with the coned-top upright. Ihila, having been woven together, is then used to provide a thick-layered roofing. This roof filters incoming air making it cool and clean and at the same time stopping the rains.\n\nThe Tiv people are well known for their hospitality and the \"Ate-u-Tiv\" is an important component of this hospitality. In order to readily receive visitors, each compound builds an \"Ate\" which is furnished with chairs made from wood, canes, etc. In modern days, the components of the \"Ate\" may vary. Some roofs are now a combination of iron roofing sheets covered by grass that may not necessarily be \"ihila\"; paulins are regularly made of plywood, etc. The \"Ate\" design now adorns public places such as hotel gardens, public amusement parks, zoos, museums, etc. New usage of the term \"Ate-u-Tiv\" may refer to a meeting place, social network or forum.\n\nLike Tiv music, modernization has not changed Tiv architecture, particularly the design and usage of the \"Ate-u-Tiv\". Ate-u-Tiv has remained a symbol of Tiv hospitality.\n\n\n"}
{"id": "19468941", "url": "https://en.wikipedia.org/wiki?curid=19468941", "title": "Balance of nature", "text": "Balance of nature\n\nThe balance of nature is a theory that proposes that ecological systems are usually in a stable equilibrium or homeostasis, which is to say that a small change in some particular parameter (the size of a particular population, for example) will be corrected by some negative feedback that will bring the parameter back to its original \"point of balance\" with the rest of the system. It may apply where populations depend on each other, for example in predator/prey systems, or relationships between herbivores and their food source. It is also sometimes applied to the relationship between the Earth's ecosystem, the composition of the atmosphere, and the world's weather.\n\nThe Gaia hypothesis is a balance of nature-based theory that suggests that the Earth and its ecology may act as co-ordinated systems in order to maintain the balance of nature.\n\nThe theory that nature is permanently in balance has been largely discredited by scientists working in ecology, as it has been found that chaotic changes in population levels are common, but nevertheless the idea continues to be popular in the general public. During the later half of the twentieth century the theory was superseded by catastrophe theory and chaos theory.\n\nThe concept that nature maintains its condition is of ancient provenance; Herodotus commented on the wonderful relationship between predator and prey species, which remained in a steady proportion to one another, with predators never excessively consuming their prey populations. The \"balance of nature\" concept once ruled ecological research, as well as once governing the management of natural resources. This led to a doctrine popular among some conservationists that nature was best left to its own devices, and that human intervention into it was by definition unacceptable. The validity of a \"balance of nature\" was already questioned in the early 1900s, but the general abandonment of the theory by scientists working in ecology only happened in the last quarter of that century when studies showed that it did not match what could be observed among plant and animal populations.\n\nPredator-prey populations tend to show chaotic behavior within limits, where the sizes of populations change in a way that may appear random, but is in fact obeying deterministic laws based only on the relationship between a population and its food source illustrated by the Lotka–Volterra equation. An experimental example of this was shown in an eight-year study on small Baltic Sea creatures such as plankton, which were isolated from the rest of the ocean. Each member of the food web was shown to take turns multiplying and declining, even though the scientists kept the outside conditions constant. An article in the journal \"Nature\" stated; \"Advanced mathematical techniques proved the indisputable presence of chaos in this food web ... short-term prediction is possible, but long-term prediction is not.\"\n\nAlthough some conservationist organizations argue that human activity is incompatible with a balanced ecosystem, there are numerous examples in history showing that several modern day habitats originate from human activity: some of Latin America's rain forests owe their existence to humans planting and transplanting them, while the abundance of grazing animals in the Serengeti plain of Africa is thought by some ecologists to be partly due to human-set fires that created savanna habitats.\n\nPossibly one of the best examples of an ecosystem fundamentally modified by human activity can be observed as a consequence of the Australian Aboriginal practice of \"Fire-stick farming\". The legacy of this practice over long periods has resulted in forests being converted to grasslands capable of sustaining larger populations of faunal prey, particularly in the northern and western regions of the continent. So thorough has been the effect of these deliberate regular burnings that many plant and tree species from affected regions have now completely adapted to the annual fire regime in that they require the passage of a fire before their seeds will even germinate. One school in Los Angeles states, \" “We have let our kids go to the forest area of the playground. However, five years later, we found that none of the flowers were growing, the natural damp soil had been hardened, and all of the beautiful grass had been plucked,”.\n\nDespite being discredited among ecologists, the theory is widely held to be true by the general public, with one authority calling it an \"enduring myth\". At least in Midwestern America, the \"balance of nature\" idea was shown to be widely held by both science majors and the general student population. In a study at the University of Patras, educational sciences students were asked to reason about the future of ecosystems which suffered human-driven disturbances. Subjects agreed that it was very likely for the ecosystems to fully recover their initial state, referring to either a 'recovery process' which restores the initial 'balance', or specific 'recovery mechanisms' as an ecosystem's inherent characteristic. In a 2017 study, Ampatzidis and Ergazaki discuss the learning objectives and design criteria that a learning environment for non-biology major students should meet to support them challenge the \"balance of nature\" idea.\n\n"}
{"id": "8553751", "url": "https://en.wikipedia.org/wiki?curid=8553751", "title": "Biological organisation", "text": "Biological organisation\n\nBiological organization is the hierarchy of complex biological structures and systems that define life using a reductionistic approach. The traditional hierarchy, as detailed below, extends from atoms to biospheres. The higher levels of this scheme are often referred to as an ecological organization concept, or as the field, hierarchical ecology.\n\nEach level in the hierarchy represents an increase in organizational complexity, with each \"object\" being primarily composed of the previous level's basic unit. The basic principle behind the organization is the concept of \"emergence\"—the properties and functions found at a hierarchical level are not present and irrelevant at the lower levels.\n\nThe biological organization of life is a fundamental premise for numerous areas of scientific research, particularly in the medical sciences. Without this necessary degree of organization, it would be much more difficult—and likely impossible—to apply the study of the effects of various physical and chemical phenomena to diseases and physiology (body function). For example, fields such as cognitive and behavioral neuroscience could not exist if the brain was not composed of specific types of cells, and the basic concepts of pharmacology could not exist if it was not known that a change at the cellular level can affect an entire organism. These applications extend into the ecological levels as well. For example, DDT's direct insecticidal effect occurs at the subcellular level, but affects higher levels up to and including multiple ecosystems. Theoretically, a change in one atom could change the entire biosphere.\n\nThe simple standard biological organization scheme, from the lowest level to the highest level, is as follows:\n\nMore complex schemes incorporate many more levels. For example, a molecule can be viewed as a grouping of elements, and an atom can be further divided into subatomic particles (these levels are outside the scope of biological organization). Each level can also be broken down into its own hierarchy, and specific types of these biological objects can have their own hierarchical scheme. For example, genomes can be further subdivided into a hierarchy of genes.\n\nEach level in the hierarchy can be described by its lower levels. For example, the organism may be described at any of its component levels, including the atomic, molecular, cellular, histological (tissue), organ and organ system levels. Furthermore, at every level of the hierarchy, new functions necessary for the control of life appear. These new roles are not functions that the lower level components are capable of and are thus referred to as \"emergent properties\".\n\nEvery organism is organised, though not necessarily to the same degree. An organism can not be organised at the histological (tissue) level if it is not composed of tissues in the first place.\n\nEmpirically, a large proportion of the (complex) biological systems we observe in nature exhibit hierarchic structure. On theoretical grounds we could expect complex systems to be hierarchies in a world in which complexity had to evolve from simplicity. System hierarchies analysis performed in the 1950s, laid the empirical foundations for a field that would be, from the 1980s, hierarchical ecology.\n\nThe theoretical foundations are summarized by thermodynamics.\nWhen biological systems are modeled as physical systems, in its most general abstraction, they are thermodynamic open systems that exhibit self-organised behavior, and the set/subset relations between dissipative structures can be characterized in a hierarchy.\n\nA simpler and more direct way to explain the fundamentals of the \"hierarchical organization of life\", was introduced in Ecology by Odum and others as the \"Simon's hierarchical principle\"; Simon emphasized that hierarchy \"emerges almost inevitably through a wide variety of evolutionary processes, for the simple reason that hierarchical structures are stable\".\n\nTo motivate this deep idea, he offered his \"parable\" about imaginary watchmakers.\n\n\n"}
{"id": "25646039", "url": "https://en.wikipedia.org/wiki?curid=25646039", "title": "Boston Journal of Natural History", "text": "Boston Journal of Natural History\n\nThe Boston Journal of Natural History (1834-1863) was a scholarly journal published by the Boston Society of Natural History in mid-19th century Massachusetts. Contributors included Charles T. Jackson, Augustus A. Gould, and others. Each volume featured lithographic illustrations, some in color, drawn/engraved by E.W. Bouvé, B.F. Nutting, A. Sonrel, \"et al.\" and printed by Pendleton's Lithography and other firms.\n\nThe journal was continued by \"Memoirs Read Before the Boston Society of Natural History\" in 1863.\n\n"}
{"id": "10019499", "url": "https://en.wikipedia.org/wiki?curid=10019499", "title": "Burned area emergency response", "text": "Burned area emergency response\n\nBurned area emergency response (BAER) is an emergency risk management reaction to post wildfire conditions that pose risks to human life and property or could further destabilize or degrade the burned lands. Even though wildfires are natural events, the presence of people and man-made structures in and adjacent to the burned area frequently requires continued emergency risk management actions. High severity wildfires pose a continuing flood, debris flow and mudflow risk to people living within and downstream from a burned watershed as well as a potential loss of desirable watershed values.\n\nThe burned area emergency response risk management process begins during or shortly after wildfire containment with risk assessments evaluating the effects of the wildfire against values needing protection. These risk assessments can range from simple to complex. An organized interdisciplinary team of subject matter experts (e.g., hydrologists, soil scientists, botanists, cultural resource specialists, engineers, etc.) used among other assessment tools hydrological modeling and soil burn severity mapping to assess potential flooding and vegetation recovery after the Cerro Grande Fire in 2000.\n\nA BAER plan is developed based on the risk assessments and burned area land management objectives. The BAER Plan identifies the most effective treatments to address the identified risks. Plan implementation timeframes are dictated primarily by anticipated future events (e.g., next significant rainstorm) which also influence treatment options.\n\nBurned area emergency response has mostly concentrated on risk reduction treatments with varying degrees of success. Risk avoidance, transfer and retention treatments are integral in the burned area emergency response risk management process.\n\nRisk reduction treatments are designed to protect human life and safety and reduce flood severity, soil erosion and prevent the establishment of non-native plants. On 10 wildfires studied in Colorado, rainfall amount and intensity followed by bare mineral soil explained 63% of soil erosion variation. Research has shown that the risk of flooding, debris flows and mudflows are significantly increased with increasing rainfall intensities and burn severity and that some risk reduction treatments help for low but not high intensity rainfall events.\n\nMulches, erosion cloth and seeding retard overland flow and protect soil from rain drop impact and increase soil moisture holding capacity. Landscape structures (e.g., log erosion barriers, contour trenches, straw wattles) trap sediment and prevent slope rilling. Strip tillage and chemicals break up or reduce hydrophobic soils and improve infiltration. Wood and straw mulch reduced erosion rates by 60 to 80%, contour-felled log erosion barriers 50 to 70%, hydromulch 19% and post fire seeding had little effect the first year when rainfall events were small and intensities low.\n\nIn stream flood control treatments slow, delay, redistribute, or redirect water, mud and debris. Straw bale check dams, silt screens and debris retention basins slow water flow and trap sediment. Riparian vegetation stabilizes streambanks. Roads and culverts are armored and debris removed as needed. Water diversion implements protect facilities and property.\n\nThe chance of introducing new invasive plants to the burned area is reduced by restricting access or thoroughly cleaning all equipment, people and animals of seeds before entering a burned area. Research has shown that non-native plant cover is positively associated with post-wildfire seeded grass cover. Even though post-wildfire seeding operations require seed mix purity standards and the number of contaminated seeds may be small on a percentage based, that the application of very large amounts of seed (thousands of pounds) ensures that a significant number of non-native plant seeds will be distributed.\n\nAvoidance treatments remove values at risk from risk prone areas. Frequently homes and other values are located on alluvial fans at the base of watersheds. The presence of the alluvial fans indicates a history of significant flooding, debris flows and mudflows with potential personal and property damage potential. Mobile property is temporally or permanently relocated. Evacuation planning and early warning systems are frequently used to protect people at risk. Flood peaks increase more rapidly with increases in rainfall intensity above a threshold value for the maximum 30 min intensity of approximately 10 mm per hour. That this rainfall intensity could be used to set threshold limits in rain gauges that are part of an early warning flood system after wildfire.\n\nOften it is not feasible to avoid or reduce risks. Flood insurance is a means of transferring risk to another party for values with insurable value.\n\nAccepting the risk is an option when values at risk are small and inevitable or when the risks cannot be reduced, avoided or transferred (i.e., infrequent catastrophic events).\n\n\n"}
{"id": "19349161", "url": "https://en.wikipedia.org/wiki?curid=19349161", "title": "Cambrian explosion", "text": "Cambrian explosion\n\nThe Cambrian explosion or Cambrian radiation was an event approximately in the Cambrian period when most major animal phyla appeared in the fossil record. It lasted for about 20–25 million years. It resulted in the divergence of most modern metazoan phyla. The event was accompanied by major diversification of other organisms.\n\nBefore the Cambrian explosion, most organisms were simple, composed of individual cells occasionally organized into colonies. Over the following 70 to 80 million years, the rate of diversification accelerated, and the variety of life began to resemble that of today. Almost all present animal phyla appeared during this period.\n\nThe Cambrian explosion has generated extensive scientific debate.\n\nThe seemingly rapid appearance of fossils in the \"Primordial Strata\" was noted by William Buckland in the 1840s, and in his 1859 book \"On the Origin of Species\", Charles Darwin discussed the then inexplicable lack of earlier fossils as one of the main difficulties for his theory of descent with slow modification through natural selection. The long-running puzzlement about the appearance of the Cambrian fauna, seemingly abruptly, without precursor, centers on three key points: whether there really was a mass diversification of complex organisms over a relatively short period of time during the early Cambrian; what might have caused such rapid change; and what it would imply about the origin of animal life. Interpretation is difficult due to a limited supply of evidence, based mainly on an incomplete fossil record and chemical signatures remaining in Cambrian rocks.\n\nThe first discovered Cambrian fossils were trilobites, described by Edward Lhuyd, the curator of Oxford Museum, in 1698. Although their evolutionary importance was not known, on the basis of their old age, William Buckland (1784–1856) realised that a dramatic step-change in the fossil record had occurred around the base of what we now call the Cambrian. Nineteenth-century geologists such as Adam Sedgwick and Roderick Murchison used the fossils for dating rock strata, specifically for establishing the Cambrian and Silurian periods. By 1859, leading geologists including Roderick Murchison, were convinced that what was then called the lowest Silurian stratum showed the origin of life on Earth, though others, including Charles Lyell, differed. In \"On the Origin of Species\", Charles Darwin considered this sudden appearance of a solitary group of trilobites, with no apparent antecedents, and absence of other fossils, to be \"undoubtedly of the gravest nature\" among the difficulties in his theory of natural selection. He reasoned that earlier seas had swarmed with living creatures, but that their fossils had not been found due to the imperfections of the fossil record. In the sixth edition of his book, he stressed his problem further as:\n\nAmerican paleontologist Charles Walcott, who studied the Burgess Shale fauna, proposed that an interval of time, the \"Lipalian\", was not represented in the fossil record or did not preserve fossils, and that the ancestors of the Cambrian animals evolved during this time.\n\nEarlier fossil evidence has since been found. The earliest claim is that the history of life on earth goes back : Rocks of that age at Warrawoona, Australia, were claimed to contain fossil stromatolites, stubby pillars formed by colonies of microorganisms. Fossils (\"Grypania\") of more complex eukaryotic cells, from which all animals, plants, and fungi are built, have been found in rocks from , in China and Montana. Rocks dating from contain fossils of the Ediacara biota, organisms so large that they are likely multicelled, but very unlike any modern organism. In 1948, Preston Cloud argued that a period of \"eruptive\" evolution occurred in the Early Cambrian, but as recently as the 1970s, no sign was seen of how the 'relatively' modern-looking organisms of the Middle and Late Cambrian arose.\n\nThe intense modern interest in this \"Cambrian explosion\" was sparked by the work of Harry B. Whittington and colleagues, who, in the 1970s, reanalysed many fossils from the Burgess Shale and concluded that several were as complex as, but different from, any living animals. The most common organism, \"Marrella\", was clearly an arthropod, but not a member of any known arthropod class. Organisms such as the five-eyed \"Opabinia\" and spiny slug-like \"Wiwaxia\" were so different from anything else known that Whittington's team assumed they must represent different phyla, seemingly unrelated to anything known today. Stephen Jay Gould's popular 1989 account of this work, \"Wonderful Life\", brought the matter into the public eye and raised questions about what the explosion represented. While differing significantly in details, both Whittington and Gould proposed that all modern animal phyla had appeared almost simultaneously in a rather short span of geological period. This view led to the modernization of Darwin's tree of life and the theory of punctuated equilibrium, which Eldredge and Gould developed in the early 1970s and which views evolution as long intervals of near-stasis \"punctuated\" by short periods of rapid change.\n\nOther analyses, some more recent and some dating back to the 1970s, argue that complex animals similar to modern types evolved well before the start of the Cambrian.\n\nRadiometric dates for much of the Cambrian, obtained by analysis of radioactive elements contained within rocks, have only recently become available, and for only a few regions.\n\nRelative dating (\"A\" was before \"B\") is often assumed sufficient for studying processes of evolution, but this, too, has been difficult, because of the problems involved in matching up rocks of the same age across different continents.\n\nTherefore, dates or descriptions of sequences of events should be regarded with some caution until better data become available.\n\nFossils of organisms' bodies are usually the most informative type of evidence. Fossilization is a rare event, and most fossils are destroyed by erosion or metamorphism before they can be observed. Hence, the fossil record is very incomplete, increasingly so as earlier times are considered. Despite this, they are often adequate to illustrate the broader patterns of life's history. Also, biases exist in the fossil record: different environments are more favourable to the preservation of different types of organism or parts of organisms. Further, only the parts of organisms that were already mineralised are usually preserved, such as the shells of molluscs. Since most animal species are soft-bodied, they decay before they can become fossilised. As a result, although 30-plus phyla of living animals are known, two-thirds have never been found as fossils.\nThe Cambrian fossil record includes an unusually high number of lagerstätten, which preserve soft tissues. These allow paleontologists to examine the internal anatomy of animals, which in other sediments are only represented by shells, spines, claws, etc. – if they are preserved at all. The most significant Cambrian lagerstätten are the early Cambrian Maotianshan shale beds of Chengjiang (Yunnan, China) and Sirius Passet (Greenland); the middle Cambrian Burgess Shale (British Columbia, Canada); and the late Cambrian Orsten (Sweden) fossil beds.\n\nWhile lagerstätten preserve far more than the conventional fossil record, they are far from complete. Because lagerstätten are restricted to a narrow range of environments (where soft-bodied organisms can be preserved very quickly, e.g. by mudslides), most animals are probably not represented; further, the exceptional conditions that create lagerstätten probably do not represent normal living conditions. In addition, the known Cambrian lagerstätten are rare and difficult to date, while Precambrian lagerstätten have yet to be studied in detail.\n\nThe sparseness of the fossil record means that organisms usually exist long before they are found in the fossil record – this is known as the Signor–Lipps effect.\n\nTrace fossils consist mainly of tracks and burrows, but also include coprolites (fossil feces) and marks left by feeding. Trace fossils are particularly significant because they represent a data source that is not limited to animals with easily fossilized hard parts, and reflects organisms' behaviour. Also, many traces date from significantly earlier than the body fossils of animals that are thought to have been capable of making them. While exact assignment of trace fossils to their makers is generally impossible, traces may, for example, provide the earliest physical evidence of the appearance of moderately complex animals (comparable to earthworms).\n\nSeveral chemical markers indicate a drastic change in the environment around the start of the Cambrian. The markers are consistent with a mass extinction, or with a massive warming resulting from the release of methane ice.\nSuch changes may reflect a cause of the Cambrian explosion, although they may also have resulted from an increased level of biological activity – a possible result of the explosion. Despite these uncertainties, the geochemical evidence helps by making scientists focus on theories that are consistent with at least one of the likely environmental changes.\n\nCladistics is a technique for working out the \"family tree\" of a set of organisms. It works by the logic that, if groups B and C have more similarities to each other than either has to group A, then B and C are more closely related to each other than either is to A. Characteristics that are compared may be anatomical, such as the presence of a notochord, or molecular, by comparing sequences of DNA or protein. The result of a successful analysis is a hierarchy of clades – groups whose members are believed to share a common ancestor. The cladistic technique is sometimes problematic, as some features, such as wings or camera eyes, evolved more than once, convergently – this must be taken into account in analyses.\n\nFrom the relationships, it may be possible to constrain the date that lineages first appeared. For instance, if fossils of B or C date to X million years ago and the calculated \"family tree\" says A was an ancestor of B and C, then A must have evolved more than X million years ago.\n\nIt is also possible to estimate how long ago two living clades diverged – i.e. about how long ago their last common ancestor must have lived  – by assuming that DNA mutations accumulate at a constant rate. These \"molecular clocks\", however, are fallible, and provide only a very approximate timing: they are not sufficiently precise and reliable for estimating when the groups that feature in the Cambrian explosion first evolved, and estimates produced by different techniques vary by a factor of two. However, the clocks can give an indication of branching rate, and when combined with the constraints of the fossil record, recent clocks suggest a sustained period of diversification through the Ediacaran and Cambrian.\n\nA phylum is the highest level in the Linnaean system for classifying organisms. Phyla can be thought of as groupings of animals based on general body plan. Despite the seemingly different external appearances of organisms, they are classified into phyla based on their internal and developmental organizations. For example, despite their obvious differences, spiders and barnacles both belong to the phylum Arthropoda, but earthworms and tapeworms, although similar in shape, belong to different phyla. As chemical and genetic testing becomes more accurate, previously hypothesised phyla are often entirely reworked.\n\nA phylum is not a fundamental division of nature, such as the difference between electrons and protons. It is simply a very high-level grouping in a classification system created to describe all currently living organisms. This system is imperfect, even for modern animals: different books quote different numbers of phyla, mainly because they disagree about the classification of a huge number of worm-like species. As it is based on living organisms, it accommodates extinct organisms poorly, if at all.\n\nThe concept of stem groups was introduced to cover evolutionary \"aunts\" and \"cousins\" of living groups, and have been hypothesized based on this scientific theory. A crown group is a group of closely related living animals plus their last common ancestor plus all its descendants. A stem group is a set of offshoots from the lineage at a point earlier than the last common ancestor of the crown group; it is a relative concept, for example tardigrades are living animals that form a crown group in their own right, but Budd (1996) regarded them as also being a stem group relative to the arthropods.\n\nThe term \"Triploblastic\" means consisting of three layers, which are formed in the embryo, quite early in the animal's development from a single-celled egg to a larva or juvenile form. The innermost layer forms the digestive tract (gut); the outermost forms skin; and the middle one forms muscles and all the internal organs except the digestive system. Most types of living animal are triploblastic – the best-known exceptions are Porifera (sponges) and Cnidaria (jellyfish, sea anemones, etc.).\n\nThe bilaterians are animals that have right and left sides at some point in their life histories. This implies that they have top and bottom surfaces and, importantly, distinct front and back ends. All known bilaterian animals are triploblastic, and all known triploblastic animals are bilaterian. Living echinoderms (sea stars, sea urchins, sea cucumbers, etc.) 'look' radially symmetrical (like wheels) rather than bilaterian, but their larvae exhibit bilateral symmetry and some of the earliest echinoderms may have been bilaterally symmetrical. Porifera and Cnidaria are radially symmetrical, not bilaterian, and not triploblastic.\n\nThe term \"Coelomate\" means having a body cavity (coelom) containing the internal organs. Most of the phyla featured in the debate about the Cambrian explosion are coelomates: arthropods, annelid worms, molluscs, echinoderms, and chordates – the noncoelomate priapulids are an important exception. All known coelomate animals are triploblastic bilaterians, but some triploblastic bilaterian animals do not have a coelom – for example flatworms, whose organs are surrounded by unspecialized tissues.\n\nUnderstanding of the Cambrian explosion relies upon knowing what was there beforehand – did the event herald the sudden appearance of a wide range of animals and behaviours, or did such things exist beforehand?\n\nPhylogenetic analysis has been used to support the view that during the Cambrian explosion, metazoans (multi-celled animals) evolved monophyletically from a single common ancestor: flagellated colonial protists similar to modern choanoflagellates.\n\nChanges in the abundance and diversity of some types of fossil have been interpreted as evidence for \"attacks\" by animals or other organisms. Stromatolites, stubby pillars built by colonies of microorganisms, are a major constituent of the fossil record from about , but their abundance and diversity declined steeply after about . This decline has been attributed to disruption by grazing and burrowing animals.\n\nPrecambrian marine diversity was dominated by small fossils known as acritarchs. This term describes almost any small organic walled fossil – from the egg cases of small metazoans to resting cysts of many different kinds of green algae. After appearing around , acritarchs underwent a boom around , increasing in abundance, diversity, size, complexity of shape, and especially size and number of spines. Their increasingly spiny forms in the last 1 billion years may indicate an increased need for defence against predation. Other groups of small organisms from the Neoproterozoic era also show signs of antipredator defenses. A consideration of taxon longevity appears to support an increase in predation pressure around this time.\nIn general, the fossil record shows a very slow appearance of these lifeforms in the Precambrian, with many cyanobacterial species making up much of the underlying sediment.\n\nThe layers of the Doushantuo formation from around \nharbour microscopic fossils that may represent early bilaterians. Some have been described as animal embryos and eggs, although some may represent the remains of giant bacteria.\nAnother fossil, \"Vernanimalcula\", has been interpreted as a coelomate bilaterian,\nbut may simply be an infilled bubble.\n\nThese fossils form the earliest hard-and-fast evidence of animals, as opposed to other predators.\n\nThe traces of organisms moving on and directly underneath the microbial mats that covered the Ediacaran sea floor are preserved from the Ediacaran period, about . They were probably made by organisms resembling earthworms in shape, size, and how they moved. The burrow-makers have never been found preserved, but, because they would need a head and a tail, the burrowers probably had bilateral symmetry – which would in all probability make them bilaterian animals. They fed above the sediment surface, but were forced to burrow to avoid predators.\n\nAround the start of the Cambrian (about ), many new types of traces first appear, including well-known vertical burrows such as \"Diplocraterion\" and \"Skolithos\", and traces normally attributed to arthropods, such as \"Cruziana\" and \"Rusophycus\". The vertical burrows indicate that worm-like animals acquired new behaviours, and possibly new physical capabilities. Some Cambrian trace fossils indicate that their makers possessed hard exoskeletons, although they were not necessarily mineralised.\n\nBurrows provide firm evidence of complex organisms; they are also much more readily preserved than body fossils, to the extent that the absence of trace fossils has been used to imply the genuine absence of large, motile, bottom-dwelling organisms. They provide a further line of evidence to show that the Cambrian explosion represents a real diversification, and is not a preservational artefact.\n\nThis new habit changed the seafloor's geochemistry, and led to decreased oxygen in the ocean and increased CO2-levels in the seas and the atmosphere, resulting in global warming for tens of millions years, and could be responsible for mass extinctions. But as burrowing became established, it allowed an explosion of its own, for as burrowers disturbed the sea floor, they aerated it, mixing oxygen into the toxic muds. This made the bottom sediments more hospitable, and allowed a wider range of organisms to inhabit them – creating new niches and the scope for higher diversity.\n\nAt the start of the Ediacaran period, much of the acritarch fauna, which had remained relatively unchanged for hundreds of millions of years, became extinct, to be replaced with a range of new, larger species, which would prove far more ephemeral. This radiation, the first in the fossil record, is followed soon after by an array of unfamiliar, large, fossils dubbed the Ediacara biota, which flourished for 40 million years until the start of the Cambrian. Most of this \"Ediacara biota\" were at least a few centimeters long, significantly larger than any earlier fossils. The organisms form three distinct assemblages, increasing in size and complexity as time progressed.\n\nMany of these organisms were quite unlike anything that appeared before or since, resembling discs, mud-filled bags, or quilted mattresses – one palæontologist proposed that the strangest organisms should be classified as a separate kingdom, Vendozoa.\n\nAt least some may have been early forms of the phyla at the heart of the \"Cambrian explosion\" debate, having been interpreted as early molluscs (\"Kimberella\"), echinoderms (\"Arkarua\"); and arthropods (\"Spriggina\", \"Parvancorina\"). Still, debate exists about the classification of these specimens, mainly because the diagnostic features that allow taxonomists to classify more recent organisms, such as similarities to living organisms, are generally absent in the ediacarans. However, there seems little doubt that \"Kimberella\" was at least a triploblastic bilaterian animal. These organisms are central to the debate about how abrupt the Cambrian explosion was. If some were early members of the animal phyla seen today, the \"explosion\" looks a lot less sudden than if all these organisms represent an unrelated \"experiment\", and were replaced by the animal kingdom fairly soon thereafter (40M years is \"soon\" by evolutionary and geological standards).\n\nPaul Knauth, a geologist at Arizona State University, maintains that photosynthesizing organisms such as algae may have grown over a 750- to 800-million-year-old formation in Death Valley known as the Beck Spring Dolomite. In the early 1990s, samples from this 1,000-foot thick layer of dolomite revealed that the region housed flourishing mats of photosynthesizing, unicellular life forms which antedated the Cambrian explosion.\n\nMicrofossils have been unearthed from holes riddling the otherwise barren surface of the dolomite. These geochemical and microfossil findings support the idea that during the Precambrian period, complex life evolved both in the oceans and on land. Knauth contends that animals may well have had their origins in freshwater lakes and streams, and not in the oceans.\n\nSome 30 years later, a number of studies have documented an abundance of geochemical and microfossil evidence showing that life covered the continents as far back as 2.2 billion years ago. Many paleobiologists now accept the idea that simple life forms existed on land during the Precambrian, but are opposed to the more radical idea that multicellular life thrived on land more than 600 million years ago.\n\nThe first Ediacaran and lowest Cambrian (Nemakit-Daldynian) skeletal fossils represent tubes and problematic sponge spicules. The oldest sponge spicules are monaxon siliceous, aged around , known from the Doushantou Formation in China and from deposits of the same age in Mongolia, although the interpretation of these fossils as spicules has been challenged. In the late Ediacaran-lowest Cambrian, numerous tube dwellings of enigmatic organisms appeared. It was organic-walled tubes (e.g. \"Saarina\") and chitinous tubes of the sabelliditids (e.g. \"Sokoloviina\", \"Sabellidites\", \"Paleolina\") that prospered up to the beginning of the Tommotian. The mineralized tubes of \"Cloudina\", \"Namacalathus\", \"Sinotubulites\", and a dozen more of the other organisms from carbonate rocks formed near the end of the Ediacaran period from , as well as the triradially symmetrical mineralized tubes of anabaritids (e.g. \"Anabarites\", \"Cambrotubulus\") from uppermost Ediacaran and lower Cambrian. Ediacaran mineralized tubes are often found in carbonates of the stromatolite reefs and thrombolites, i.e. they could live in an environment adverse to the majority of animals.\n\nAlthough they are as hard to classify as most other Ediacaran organisms, they are important in two other ways. First, they are the earliest known calcifying organisms (organisms that built shells from calcium carbonate). Secondly, these tubes are a device to rise over a substrate and competitors for effective feeding and, to a lesser degree, they serve as armor for protection against predators and adverse conditions of environment. Some \"Cloudina\" fossils show small holes in shells. The holes possibly are evidence of boring by predators sufficiently advanced to penetrate shells. A possible \"evolutionary arms race\" between predators and prey is one of the hypotheses that attempt to explain the Cambrian explosion.\n\nIn the lowest Cambrian, the stromatolites were decimated. This allowed animals to begin colonization of warm-water pools with carbonate sedimentation. At first, it was anabaritids and \"Protohertzina\" (the fossilized grasping spines of chaetognaths) fossils. Such mineral skeletons as shells, sclerites, thorns, and plates appeared in uppermost Nemakit-Daldynian; they were the earliest species of halkierids, gastropods, hyoliths and other rare organisms. The beginning of the Tommotian has historically been understood to mark an explosive increase of the number and variety of fossils of molluscs, hyoliths, and sponges, along with a rich complex of skeletal elements of unknown animals, the first archaeocyathids, brachiopods, tommotiids, and others. Also soft-bodied extant phyla such as comb jellies, scalidophorans, entoproctans, horseshoe worms and lobopodians had armored forms. This sudden increase is partially an artefact of missing strata at the Tommotian type section, and most of this fauna in fact began to diversify in a series of pulses through the Nemakit-Daldynian and into the Tommotian.\n\nSome animals may already have had sclerites, thorns, and plates in the Ediacaran (e.g. \"Kimberella\" had hard sclerites, probably of carbonate), but thin carbonate skeletons cannot be fossilized in siliciclastic deposits. Older (~750 Ma) fossils indicate that mineralization long preceded the Cambrian, probably defending small photosynthetic algae from single-celled eukaryotic predators.\n\nTrace fossils (burrows, etc.) are a reliable indicator of what life was around, and indicate a diversification of life around the start of the Cambrian, with the freshwater realm colonized by animals almost as quickly as the oceans.\n\nFossils known as \"small shelly fauna\" have been found in many parts on the world, and date from just before the Cambrian to about 10 million years after the start of the Cambrian (the Nemakit-Daldynian and Tommotian ages; see timeline). These are a very mixed collection of fossils: spines, sclerites (armor plates), tubes, archeocyathids (sponge-like animals), and small shells very like those of brachiopods and snail-like molluscs – but all tiny, mostly 1 to 2 mm long.\n\nWhile small, these fossils are far more common than complete fossils of the organisms that produced them; crucially, they cover the window from the start of the Cambrian to the first lagerstätten: a period of time otherwise lacking in fossils. Hence, they supplement the conventional fossil record and allow the fossil ranges of many groups to be extended.\n\nThe earliest trilobite fossils are about 530 million years old, but the class was already quite diverse and worldwide, suggesting they had been around for quite some time.\nThe fossil record of trilobites began with the appearance of trilobites with mineral exoskeletons – not from the time of their origin.\n\nThe earliest generally accepted echinoderm fossils appeared a little bit later, in the Late Atdabanian; unlike modern echinoderms, these early Cambrian echinoderms were not all radially symmetrical.\n\nThese provide firm data points for the \"end\" of the explosion, or at least indications that the crown groups of modern phyla were represented.\n\nThe Burgess Shale and similar lagerstätten preserve the soft parts of organisms, which provide a wealth of data to aid in the classification of enigmatic fossils. It often preserved complete specimens of organisms only otherwise known from dispersed parts, such as loose scales or isolated mouthparts. Further, the majority of organisms and taxa in these horizons are entirely soft-bodied, hence absent from the rest of the fossil record. Since a large part of the ecosystem is preserved, the ecology of the community can also be tentatively reconstructed.\nHowever, the assemblages may represent a \"museum\": a deep-water ecosystem that is evolutionarily \"behind\" the rapidly diversifying fauna of shallower waters.\n\nBecause the lagerstätten provide a mode and quality of preservation that is virtually absent outside of the Cambrian, many organisms appear completely different from anything known from the conventional fossil record. This led early workers in the field to attempt to shoehorn the organisms into extant phyla; the shortcomings of this approach led later workers to erect a multitude of new phyla to accommodate all the oddballs. It has since been realised that most oddballs diverged from lineages before they established the phyla known today – slightly different designs, which were fated to perish rather than flourish into phyla, as their cousin lineages did.\n\nThe preservational mode is rare in the preceding Ediacaran period, but those assemblages known show no trace of animal life – perhaps implying a genuine absence of macroscopic metazoans.\n\nCrustaceans, one of the four great modern groups of arthropods, are very rare throughout the Cambrian. Convincing crustaceans were once thought to be common in Burgess Shale-type biotas, but none of these individuals can be shown to fall into the crown group of \"true crustaceans\". The Cambrian record of crown-group crustaceans comes from microfossils. The Swedish Orsten horizons contain later Cambrian crustaceans, but only organisms smaller than 2 mm are preserved. This restricts the data set to juveniles and miniaturised adults.\n\nA more informative data source is the organic microfossils of the Mount Cap formation, Mackenzie Mountains, Canada. This late Early Cambrian assemblage () consists of microscopic fragments of arthropods' cuticle, which is left behind when the rock is dissolved with hydrofluoric acid. The diversity of this assemblage is similar to that of modern crustacean faunas. Analysis of fragments of feeding machinery found in the formation shows that it was adapted to feed in a very precise and refined fashion. This contrasts with most other early Cambrian arthropods, which fed messily by shovelling anything they could get their feeding appendages on into their mouths. This sophisticated and specialised feeding machinery belonged to a large (about 30 cm) organism, and would have provided great potential for diversification; specialised feeding apparatus allows a number of different approaches to feeding and development, and creates a number of different approaches to avoid being eaten.\n\nAfter an extinction at the Cambrian–Ordovician boundary, another radiation occurred, which established the taxa that would dominate the Palaeozoic.\n\nDuring this radiation, the total number of orders doubled, and families tripled, increasing marine diversity to levels typical of the Palaeozoic, and disparity to levels approximately equivalent to today's.\n\nThe event lasted for about the next 20–25 million years. Different authors break the explosion down into stages in different ways.\n\nEd Landing recognizes three stages: Stage 1, spanning the Ediacaran-Cambrian boundary, corresponds to a diversification of biomineralizing animals and of deep and complex burrows; Stage 2, corresponding to the radiation of molluscs and stem-group Brachiopods (hyoliths and tommotiids), which apparently arose in intertidal waters; and Stage 3, seeing the Atdabanian diversification of trilobites in deeper waters, but little change in the intertidal realm.\n\nGraham Budd synthesises various schemes to produce a compatible view of the SSF record of the Cambrian explosion, divided slightly differently into four intervals: a \"Tube world\", lasting from , spanning the Ediacaran-Cambrian boundary, dominated by Cloudina, Namacalathus and pseudoconodont-type elements; a \"Sclerite world\", seeing the rise of halkieriids, tommotiids, and hyoliths, lasting to the end of the Fortunian (c. 525 Ma); a brachiopod world, perhaps corresponding to the as yet unratified Cambrian Stage 2; and Trilobite World, kicking off in Stage 3.\n\nComplementary to the shelly fossil record, trace fossils can be divided into five subdivisions: \"Flat world\" (late Ediacaran), with traces restricted to the sediment surface; Protreozoic III (after Jensen), with increasing complexity; \"pedum\" world, initiated at the base of the Cambrian with the base of the \"T.pedum\" zone (see discussion at Cambrian#Dating the Cambrian); \"Rusophycus\" world, spanning and thus corresponding exactly to the periods of Sclerite World and Brachiopod World under the SSF paradigm; and \"Cruziana\" world, with an obvious correspondence to Trilobite World.\n\nThere is strong evidence for species of Cnidaria and Porifera existing in the Ediacaran and possible members of Porifera even before that during the Cryogenian. Bryozoans don't appear in the fossil record until after the Cambrian, in the Lower Ordovician.\n\nThe fossil record as Darwin knew it seemed to suggest that the major metazoan groups appeared in a few million years of the early to mid-Cambrian, and even in the 1980s, this still appeared to be the case.\n\nHowever, evidence of Precambrian Metazoa is gradually accumulating. If the Ediacaran \"Kimberella\" was a mollusc-like protostome (one of the two main groups of coelomates), the protostome and deuterostome lineages must have split significantly before (deuterostomes are the other main group of coelomates). Even if it is not a protostome, it is widely accepted as a bilaterian. Since fossils of rather modern-looking cnidarians (jellyfish-like organisms) have been found in the Doushantuo lagerstätte, the cnidarian and bilaterian lineages must have diverged well over .\n\nTrace fossils and predatory borings in \"Cloudina\" shells provide further evidence of Ediacaran animals. Some fossils from the Doushantuo formation have been interpreted as embryos and one (\"Vernanimalcula\") as a bilaterian coelomate, although these interpretations are not universally accepted. Earlier still, predatory pressure has acted on stromatolites and acritarchs since around .\n\nSome say that the evolutionary change was accelerated by an order of magnitude, but the presence of Precambrian animals somewhat dampens the \"bang\" of the explosion; not only was the appearance of animals gradual, but their evolutionary radiation (\"diversification\") may also not have been as rapid as once thought. Indeed, statistical analysis shows that the Cambrian explosion was no faster than any of the other radiations in animals' history. However, it does seem that some innovations linked to the explosion – such as resistant armour – only evolved once in the animal lineage; this makes a lengthy Precambrian animal lineage harder to defend. Further, the conventional view that all the phyla arose in the Cambrian is flawed; while the phyla may have diversified in this time period, representatives of the crown groups of many phyla do not appear until much later in the Phanerozoic. Further, the mineralised phyla that form the basis of the fossil record may not be representative of other phyla, since most mineralised phyla originated in a benthic setting. The fossil record is consistent with a Cambrian explosion that was limited to the benthos, with pelagic phyla evolving much later.\n\nEcological complexity among marine animals increased in the Cambrian, as well later in the Ordovician. However, recent research has overthrown the once-popular idea that disparity was exceptionally high throughout the Cambrian, before subsequently decreasing. In fact, disparity remains relatively low throughout the Cambrian, with modern levels of disparity only attained after the early Ordovician radiation.\n\nThe diversity of many Cambrian assemblages is similar to today's, and at a high (class/phylum) level, diversity is thought by some to have risen relatively smoothly through the Cambrian, stabilizing somewhat in the Ordovician. This interpretation, however, glosses over the astonishing and fundamental pattern of basal polytomy and phylogenetic telescoping at or near the Cambrian boundary, as seen in most major animal lineages. Thus Harry Blackmore Whittington's questions regarding the abrupt nature of the Cambrian explosion remain, and have yet to be satisfactorily answered.\n\nBudd and Mann suggested that the Cambrian explosion was the result of a type of survivorship bias called the \"Push of the past\". As groups at their origin tend to go extinct, it follows that any long-lived group would have experienced an unusually rapid rate of diversification early on, creating the illusion of a general speed-up in diversification rates. However, rates of diversification could remain at background levels and still generate this sort of effect in the surviving lineages.\n\nDespite the evidence that moderately complex animals (triploblastic bilaterians) existed before and possibly long before the start of the Cambrian, it seems that the pace of evolution was exceptionally fast in the early Cambrian. Possible explanations for this fall into three broad categories: environmental, developmental, and ecological changes. Any explanation must explain both the timing and magnitude of the explosion.\n\nEarth's earliest atmosphere contained no free oxygen (O); the oxygen that animals breathe today, both in the air and dissolved in water, is the product of billions of years of photosynthesis. Cyanobacteria were the first organisms to evolve the ability to photosynthesize, introducing a steady supply of oxygen into the environment. Initially, oxygen levels did not increase substantially in the atmosphere. The oxygen quickly reacted with iron and other minerals in the surrounding rock and ocean water. Once a saturation point was reached for the reactions in rock and water, oxygen was able to exist as a gas in its diatomic form. Oxygen levels in the atmosphere increased substantially afterward. As a general trend, the concentration of oxygen in the atmosphere has risen gradually over about the last 2.5 billion years.\n\nOxygen levels seem to have a positive correlation with diversity in eukaryotes well before the Cambrian period. The last common ancestor of all extant eukaryotes is thought to have lived around 1.8 billion years ago. Around 800 million years ago, there was a notable increase in the complexity and number of eukaryotes species in the fossil record. Before the spike in diversity, eukaryotes are thought to have lived in highly sulfuric environments. Sulfide interferes with mitochondrial function in aerobic organisms, limiting the amount of oxygen that could be used to drive metabolism. Oceanic sulfide levels decreased around 800 million years ago, which supports the importance of oxygen in eukaryotic diversity.\n\nThe shortage of oxygen might well have prevented the rise of large, complex animals. The amount of oxygen an animal can absorb is largely determined by the area of its oxygen-absorbing surfaces (lungs and gills in the most complex animals; the skin in less complex ones); but, the amount needed is determined by its volume, which grows faster than the oxygen-absorbing area if an animal's size increases equally in all directions. An increase in the concentration of oxygen in air or water would increase the size to which an organism could grow without its tissues becoming starved of oxygen. However, members of the Ediacara biota reached metres in length tens of millions of years before the Cambrian explosion. Other metabolic functions may have been inhibited by lack of oxygen, for example the construction of tissue such as collagen, required for the construction of complex structures, or to form molecules for the construction of a hard exoskeleton. However, animals are not affected when similar oceanographic conditions occur in the Phanerozoic; there is no convincing correlation between oxygen levels and evolution, so oxygen may have been no more a prerequisite to complex life than liquid water or primary productivity.\n\nThe amount of ozone (O) required to shield Earth from biologically lethal UV radiation, wavelengths from 200 to 300 nanometers (nm), is believed to have been in existence around the Cambrian explosion. The presence of the ozone layer may have enabled the development of complex life and life on land, as opposed to life being restricted to the water.\n\nIn the late Neoproterozoic (extending into the early Ediacaran period), the Earth suffered massive glaciations in which most of its surface was covered by ice. This may have caused a mass extinction, creating a genetic bottleneck; the resulting diversification may have given rise to the Ediacara biota, which appears soon after the last \"Snowball Earth\" episode.\nHowever, the snowball episodes occurred a long time before the start of the Cambrian, and it is hard to see how so much diversity could have been caused by even a series of bottlenecks; the cold periods may even have \"delayed\" the evolution of large size organisms.\n\nNewer research suggests that volcanically active midocean ridges caused a massive and sudden surge of the calcium concentration in the oceans, making it possible for marine organisms to build skeletons and hard body parts.\nAlternatively a high influx of ions could have been provided by the widespread erosion that produced Powell's Great Unconformity.\n\nAn increase of calcium may also have been caused by erosion of the Transgondwanan Supermountain that existed at the time the explosion. The roots of the mountain are preserved in present-day East Africa as an orogen.\n\nA range of theories are based on the concept that minor modifications to animals' development as they grow from embryo to adult may have been able to cause very large changes in the final adult form. The Hox genes, for example, control which organs individual regions of an embryo will develop into. For instance, if a certain \"Hox\" gene is expressed, a region will develop into a limb; if a different Hox gene is expressed in that region (a minor change), it could develop into an eye instead (a phenotypically major change).\n\nSuch a system allows a large range of disparity to appear from a limited set of genes, but such theories linking this with the explosion struggle to explain why the origin of such a development system should by itself lead to increased diversity or disparity. Evidence of Precambrian metazoans combines with molecular data to show that much of the genetic architecture that could feasibly have played a role in the explosion was already well established by the Cambrian.\n\nThis apparent paradox is addressed in a theory that focuses on the physics of development. It is proposed that the emergence of simple multicellular forms provided a changed context and spatial scale in which novel physical processes and effects were mobilized by the products of genes that had previously evolved to serve unicellular functions. Morphological complexity (layers, segments, lumens, appendages) arose, in this view, by self-organization.\n\nHorizontal gene transfer has also been identified as a possible factor in the rapid acquisition of the biochemical capability of biomineralization among organisms during this period, based on evidence that the gene for a critical protein in the process was originally transferred from a bacterium into sponges.\n\nThese focus on the interactions between different types of organism. Some of these hypotheses deal with changes in the food chain; some suggest arms races between predators and prey, and others focus on the more general mechanisms of coevolution. Such theories are well suited to explaining why there was a rapid increase in both disparity and diversity, but they must explain why the \"explosion\" happened when it did.\n\nEvidence for such an extinction includes the disappearance from the fossil record of the Ediacara biota and shelly fossils such as \"Cloudina\", and the accompanying perturbation in the record.\n\nMass extinctions are often followed by adaptive radiations as existing clades expand to occupy the ecospace emptied by the extinction. However, once the dust had settled, overall disparity and diversity returned to the pre-extinction level in each of the Phanerozoic extinctions.\n\nAndrew Parker has proposed that predator-prey relationships changed dramatically after eyesight evolved. Prior to that time, hunting and evading were both close-range affairs – smell, vibration, and touch were the only senses used. When predators could see their prey from a distance, new defensive strategies were needed. Armor, spines, and similar defenses may also have evolved in response to vision. He further observed that, where animals lose vision in unlighted environments such as caves, diversity of animal forms tends to decrease. Nevertheless, many scientists doubt that vision could have caused the explosion. Eyes may well have evolved long before the start of the Cambrian. It is also difficult to understand why the evolution of eyesight would have caused an explosion, since other senses, such as smell and pressure detection, can detect things at a greater distance in the sea than sight can; but the appearance of these other senses apparently did not cause an evolutionary explosion.\n\nThe ability to avoid or recover from predation often makes the difference between life and death, and is therefore one of the strongest components of natural selection. The pressure to adapt is stronger on the prey than on the predator: if the predator fails to win a contest, it loses a meal; if the prey is the loser, it loses its life.\n\nBut, there is evidence that predation was rife long before the start of the Cambrian, for example in the increasingly spiny forms of acritarchs, the holes drilled in \"Cloudina\" shells, and traces of burrowing to avoid predators. Hence, it is unlikely that the \"appearance\" of predation was the trigger for the Cambrian \"explosion\", although it may well have exhibited a strong influence on the body forms that the \"explosion\" produced. However, the intensity of predation does appear to have increased dramatically during the Cambrian as new predatory \"tactics\" (such as shell-crushing) emerged. This rise of predation during the Cambrian was confirmed by the temporal pattern of the median predator ratio at the scale of genus, in fossil communities covering the Cambrian and Ordovician periods, but this pattern is not correlated to diversification rate. This lack of correlation between predator ratio and diversification over the Cambrian and Ordovician suggests that predators did not trigger the large evolutionary radiation of animals during this interval. Thus the role of predators as triggerers of diversification may have been limited to the very beginning of the \"Cambrian explosion\".\n\nGeochemical evidence strongly indicates that the total mass of plankton has been similar to modern levels since early in the Proterozoic. Before the start of the Cambrian, their corpses and droppings were too small to fall quickly towards the seabed, since their drag was about the same as their weight. This meant they were destroyed by scavengers or by chemical processes before they reached the sea floor.\n\nMesozooplankton are plankton of a larger size. Early Cambrian specimens filtered microscopic plankton from the seawater. These larger organisms would have produced droppings and corpses that were large enough to fall fairly quickly. This provided a new supply of energy and nutrients to the mid-levels and bottoms of the seas, which opened up a huge range of new possible ways of life. If any of these remains sank uneaten to the sea floor they could be buried; this would have taken some carbon out of circulation, resulting in an increase in the concentration of breathable oxygen in the seas (carbon readily combines with oxygen).\n\nThe initial herbivorous mesozooplankton were probably larvae of benthic (seafloor) animals. A larval stage was probably an evolutionary innovation driven by the increasing level of predation at the seafloor during the Ediacaran period.\n\nMetazoans have an amazing ability to increase diversity through coevolution. This means that an organism's traits can lead to traits evolving in other organisms; a number of responses are possible, and a different species can potentially emerge from each one. As a simple example, the evolution of predation may have caused one organism to develop a defence, while another developed motion to flee. This would cause the predator lineage to split into two species: one that was good at chasing prey, and another that was good at breaking through defences. Actual coevolution is somewhat more subtle, but, in this fashion, great diversity can arise: three quarters of living species are animals, and most of the rest have formed by coevolution with animals.\n\nEvolving organisms inevitably change the environment they evolve in. The Devonian colonization of land had planet-wide consequences for sediment cycling and ocean nutrients, and was likely linked to the Devonian mass extinction. A similar process may have occurred on smaller scales in the oceans, with, for example, the sponges filtering particles from the water and depositing them in the mud in a more digestible form; or burrowing organisms making previously unavailable resources available for other organisms.\n\nThe explosion may not have been a significant evolutionary event. It may represent a threshold being crossed: for example a threshold in genetic complexity that allowed a vast range of morphological forms to be employed. This genetic threshold may have a correlation to the amount of oxygen available to organisms. Using oxygen for metabolism produces much more energy than anaerobic processes. Organisms that use more oxygen have the opportunity to produce more complex proteins, providing a template for further evolution. These proteins translate into larger, more complex structures that allow organisms better to adapt to their environments. With the help of oxygen, genes that code for these proteins could contribute to the expression of complex traits more efficiently. Access to a wider range of structures and functions would allow organisms to evolve in different directions, increasing the number of niches that could be inhabited. Furthermore, organisms had the opportunity to become more specialized in their own niches.\n\nThe \"Cambrian explosion\" can be viewed as two waves of metazoan expansion into empty niches: first, a coevolutionary rise in diversity as animals explored niches on the Ediacaran sea floor, followed by a second expansion in the early Cambrian as they became established in the water column. The rate of diversification seen in the Cambrian phase of the explosion is unparalleled among marine animals: it affected all metazoan clades of which Cambrian fossils have been found. Later radiations, such as those of fish in the Silurian and Devonian periods, involved fewer taxa, mainly with very similar body plans. Although the recovery from the Permian-Triassic extinction started with about as few animal species as the Cambrian explosion, the recovery produced far fewer significantly new types of animals.\n\nWhatever triggered the early Cambrian diversification opened up an exceptionally wide range of previously unavailable ecological niches. When these were all occupied, limited space existed for such wide-ranging diversifications to occur again, because strong competition existed in all niches and incumbents usually had the advantage. If a wide range of empty niches had continued, clades would be able to continue diversifying and become disparate enough for us to recognise them as different phyla; when niches are filled, lineages will continue to resemble one another long after they diverge, as limited opportunity exists for them to change their life-styles and forms.\n\nThere were two similar explosions in the evolution of land plants: after a cryptic history beginning about , land plants underwent a uniquely rapid adaptive radiation during the Devonian period, about . Furthermore, Angiosperms (flowering plants) originated and rapidly diversified during the Cretaceous period.\n\n\nTimeline References:\n\n"}
{"id": "7876225", "url": "https://en.wikipedia.org/wiki?curid=7876225", "title": "Compressed earth block", "text": "Compressed earth block\n\nA compressed earth block (CEB), also known as a \"pressed earth block\" or a \"compressed soil block\", is a building material made primarily from damp soil compressed at high pressure to form blocks. Compressed earth blocks use a mechanical press to form blocks out of an appropriate mix of fairly dry inorganic subsoil, non-expansive clay and aggregate. If the blocks are stabilized with a chemical binder such as Portland cement they are called \"compressed stabilized earth block\" (CSEB) or \"stabilized earth block\" (SEB). Typically, around is applied in compression, and the original soil volume is reduced by about half.\n\nCreating CEBs differs from rammed earth in that the latter uses a larger formwork into which earth is poured and manually tamped down, creating larger forms such as a whole wall or more at one time rather than building blocks. CEBs differ from mud bricks in that the latter are not compressed and solidify through chemical changes that take place as they air dry. The compression strength of properly made CEB can meet or exceed that of typical cement or mud brick. Building standards have been developed for CEB.\n\nCEBs are assembled onto walls using standard bricklaying and masonry techniques. The mortar may be a simple slurry made of the same soil/clay mix without aggregate, spread or brushed very thinly between the blocks for bonding, or cement mortar may also be used for high strength, or when construction during freeze-thaw cycles causes stability issues. Hydraform blocks are shaped to be interlocking.\n\nCEB technology has been developed for low-cost construction, as an alternative to adobe, and with some advantages. A commercial industry has been advanced by eco-friendly contractors, manufacturers of the mechanical presses, and by cultural acceptance of the method. In the United States, most general contractors building with CEB are in the Southwestern states: New Mexico, Colorado, Arizona, California, and to a lesser extent in Texas. The methods and presses have been used for many years in Mexico, and in developing countries.\n\nThe South African Department of Water Affairs and Forestry considers that CEB, locally called \"Dutch brick\" is an appropriate technology for a developing country, as are adobe, rammed earth and cob. All use natural building materials.\nIn 2002 the International Institute for Energy Conservation was one of the winners of a World Bank Development Marketplace Award for a project to make an energy-efficient Dutch brick-making machine for home construction in South Africa. By making cheaper bricks that use earth, the project would reduce housing costs while stimulating the building industry.\nThe machine would be mobile, allowing bricks to be made locally from earth.\n\nVarious types of CEB production machines exist, from manual to semi-automated and fully automated, with increasing capital-investment and production rates, and decreased labor. Automated machines are more common in the developed world, and manual machines in the developing world.\n\nThere are many advantages of the CEB system. On-site materials can be used, which reduces cost, minimizes shipping costs for materials, and increases efficiency and sustainability. The wait-time required to obtain materials is minimal, because after the blocks are pressed, materials are available very soon after a short drying period. The uniformity of the blocks simplifies construction, and minimizes or eliminates the need for mortar, thus reducing both the labor and materials costs. The blocks are strong, stable, water-resistant and long-lasting.\n\nCEB had very limited use prior to the 1980s. It was known in the 1950s in South America, where one of the most well-known presses, the Cinva Ram, was developed by Raul Ramirez in the Inter-American Housing Center (CINVA) in Bogota, Colombia. The Cinva Ram is a single-block, manual-press that uses a long, hand-operated lever to drive a cam, generating high pressure.\n\nIndustrial manufacturers produce much larger machines that run with diesel or gasoline engines and hydraulic presses that receive the soil/aggregate mixture through a hopper. This is fed into a chamber to create a block that is then ejected onto a conveyor.\n\nDuring the 1980s, soil-pressing technology became widespread. France, England, Germany, South Africa and Switzerland began to write standards. The Peace Corps, USAID, Habitat for Humanity and other programs began to implement it into housing projects.\n\nCompleted walls require either a reinforced bond beam or a ring beam on top or between floors and if the blocks are not stabilized, a plaster finish, usually stucco wire/stucco cement and/or lime plaster. Stabilized blocks can be left exposed with no outer plaster finish. In tropical environments, polycarbonate varnish is often used to provide an additional layer of wet-weather protection.\n\nStandards for foundations are similar to those for brick walls. A CEB wall is heavy. Footings must be at least 10 inches thick, with a minimum width that is 33 percent greater than the wall width. If a stem wall is used, it shall extend to an elevation not less than eight inches (203 mm) above the exterior finish grade. Rubble-filled foundation trench designs with a reinforced concrete grade beam above are allowed to support CEB construction.\n\nUsing the ASTM D1633-00 stabilization standard, a pressed and cured block must be submerged in water for four hours. It is then pulled from the water and immediately subjected to a compression test. The blocks must score at least a 300 pound-force per square inch (p.s.i) (2 MPa) minimum. This is a higher standard than for adobe, which must score an \"average\" of at least 300 p.s.i. (2 MPa)\n"}
{"id": "226741", "url": "https://en.wikipedia.org/wiki?curid=226741", "title": "Earth in science fiction", "text": "Earth in science fiction\n\nAn overwhelming majority of fiction is set on or features the Earth. However, authors of speculative fiction novels and writers and directors of science fiction film deal with Earth quite differently from authors of conventional fiction. Unbound from the same ties that bind authors of traditional fiction to the Earth, they can either completely ignore the Earth or use it as but one of many settings in a more complicated universe, exploring a number of common themes through examining outsiders' perceptions of and interactions with Earth.\n\n\n\n\n\nThe overarching plot in both the original and re-imagined \"Battlestar Galactica\" is the quest to find Earth, which is thought to be the location of the lost thirteenth colony of Kobol. Colonial history dictates that Kobol is the homeworld of all humanity, and that the Thirteen Tribes of Kobol fled that world thousands of years earlier, with twelve tribes founding the Twelve Colonies and the thirteenth heading to Earth. Both shows are similar in that the location of Earth is initially unknown, but clues to its location are gradually discovered by the refugee fleet from the Twelve Colonies. In both series, the exodus of the Thirteen Tribes took place so far in the past that most modern Colonials have come to assume that the stories of Earth are simply religious myths.\n\nIn the original series, several clues indicate that the existence of Earth is real. On the prison planet of Proteus, Starbuck encounters drawings of star systems on the wall of a cell once occupied by a mysterious prisoner. The star charts turn out to be that of the Solar System. Additionally, when the \"Galactica\" later reaches a planet called Terra, it is inhabited by humans who use Earth units of measurement (hours, minutes, etc.) rather than Colonial units of measurement, suggesting that it was settled by members of the lost Thirteenth Tribe thousands of years earlier on their way to Earth.\n\nIn \"Galactica 1980\", a continuation of the original series, the fleet did eventually discover Earth as it was in 1980.\n\nIn the Season Three finale of the re-imagined series, Kara Thrace returns to \"Galactica\" after her apparent death, claiming to have been to Earth and intending to lead the fleet there. The camera then pans out from the fleet to view the Milky Way galaxy, and then zooms back in to show Earth, confirming the existence of the planet. In the Season Four mid-season finale episode \"Revelations\", the fleet finally reaches Earth, only to discover that it is a lifeless, radioactive wasteland.\n\nIn \"Sometimes a Great Notion\", it is revealed that the Thirteenth Tribe consisted of humanoid and mechanical Cylons of a type previously unknown. It is also revealed that the final five Cylons had previously lived on Earth 2000 years in the past, when a nuclear war devastated the planet.\n\nIn the final episode, a twist ending shows \"Galactica\" reaching our Earth, 150,000 years ago. The Colonials and the Cylons they've made peace with decide to call their new world \"Earth\" due to the hope associated with the name of the now devastated planet the Thirteenth Tribe once inhabited. They then abandon their technology and live among the new Earth's native Hominini. 150,000 years later, in the present day, the remains of Mitochondrial Eve – a Colonial human/Cylon hybrid (named Hera Agathon) whose birth and destiny had been a major plot element of the series – are discovered.\n\nIn most variations on the \"Buck Rogers\" mythos (comic strip, TV series, feature film), Earth of the 25th century (where the action takes place) is recovering from various atomic wars, usually variations on World War III. In the original comic, Mongols have taken over the Earth; in the TV series, the Draconian Empire fills this role (although the Draconians are obviously based on Mongols). Most of Earth's cities lie in ruins, although rebuilding is in progress (Earth's capital is New Chicago; other cities include New Paris, New London etc.). The second season of the TV series revealed that much of Earth's population fled the planet in the wake of the atomic war and founded colonies in deep space; the Earth ship \"Searcher\" is dispatched to investigate.\n\nIn Joss Whedon's Buffyverse, established by \"Buffy the Vampire Slayer\" and \"Angel\", Earth is one of several dimensions; the term \"Earth\" is used both to refer to the specific planet and to the dimension the planet exists in as a whole. Born from the Seed of Wonder, the source of all magic, Earth originated as a world of Demons, and was ruled over by the Old Ones during a time known as the Primordium Age. Eventually, however, the human race rose up and fought back against the Old Ones, banishing them to other dimensions.\n\nIn Jerry Pournelle's \"CoDominium\" series (now largely alternate history) the Earth comes under the control of the CoDominium, an alliance between the United States and Soviet Union, in the year 1990. The CoDominium imposes its control over all other nations of the Earth, halting scientific development and warfare. The CoDominium is ruled by a Grand Senate located on the Moon, and eventually constructs interstellar colonies for the joint goal of economic gain and a means of exiling troublesome elements of society. Eventually in 2103, the CoDominium dissolves, with the US and USSR engaging in the nuclear \"Great Patriotic Wars\" which destroy almost all of Earth (it is mentioned that Jamaica and the Tyrolean Alps are untouched).\n\nThe CD Space Navy escapes to the planet Sparta, which eventually becomes the nucleus of the \"Empire of Man\". During the Empire's Formation Wars the Earth is once more hit hard, but is eventually incorporated into the Imperium as the \"honorary capital.\" When the Empire dissolves in the Secession Wars in the 27th century, Earth is once more subjected to nuclear attacks, but by the early 31st century has been reclaimed by the Second Empire. By that time, the Earth city of \"New Annapolis\" is a training center for the Imperial Space Navy. To inhabitants of planets newly contacted, such as Prince Samual's World in \"King David's Spaceship\", the condition of the still largely desolate Earth is presented as an object lesson for the prohibitive price of war and a justification for Empire's claim to universal rule.\n\nIn Akira Toriyama's \"Dragon Ball\" series, Earth is the primary setting and one of many planets in the North Galaxy. The planet is inhabited by humans, anthropomorphic animals, and demons, among others. \"Dragon Ball\"s Earth features heavy science fiction themes, such as humanoid robots and flying cars, as well as heavy magical influence.\n\nIn Frank Herbert's \"Dune\" series of novels, Earth is referred to as \"Old Earth / Old Terra\" by the time of the original novel \"Dune\" (at least 21,500 years in the future). The Sun is called Al-Lat, and humanity had populated many planets (among them Caladan, Giedi Prime and Salusa Secundus). In the time of Paul Atreides, the Earth is an uninhabited and largely forgotten land, shrouded in legend. In \"Dune Messiah\", Paul refers to Hitler and Genghis Khan, in comparing the destructiveness of his Jihad to their wars. It is a wilderness and recovering an ecosystem of its own as humans have abandoned it. The artifacts of \"Homo sapiens\" have for the most part crumbled back into the planet, though a more than casual observer can find many traces of the old civilizations.\n\nPaul's son, the God Emperor Leto II, refers to the Earth many times in his journals. The God Emperor seemed particularly fond of the ancestors he had from the Western sections of Eurasia. He makes references to Israel, Urartu, (also called Armenia), Edom, Damascus, Media, Babylon, Arpad, Umlias, the plains of Central Asia, and the Greeks; the family name refers to their descent from Atreus. He seems to have had ancestors among the Turks or the Mongols as he says that one of his memories involves a horse plain with felt yurts. Leto also has the memories of a famous politician from the United States whose name was Jacob Broom. In the book Children of Dune, Leto II mentions an ancestor named Agamemnon, and makes reference to Geoffrey Chaucer and the \"Canterbury Tales\".\n\nIn \"Heretics of Dune\", it is noted that the Bene Gesserit Mother Superior Taraza has the preserved Vincent van Gogh painting \"Cottages at Cordeville\" hanging in her room.\n\nIn the \"Legends of Dune\" series by Brian Herbert and Kevin J. Anderson, set in the Dune universe, it is revealed that at the beginning of mankind's war with the Machines, called the Butlerian Jihad, Earth had been devastated by humans themselves using atomics in an attack on the Machines. In the \"Prelude to Dune\" prequel series, also by Herbert and Anderson, it is mentioned that certain Monet and Gauguin paintings are owned by House Vernius, and hang in the Grand Palais at Ix.\n\nIn the Joss Whedon series \"Firefly\", Earth is long since abandoned. It is referred to with awe as \"Earth-that-Was\", having been abandoned centuries ago due to overpopulation and depletion of the planet's natural resources. After fleeing the planet, the remnants of humanity traveled in generation ships for decades (many humans lived their entire lives within a spaceship's walls) until finding a new star system. Collection of Earth-that-Was artifacts is a hobby for the rich, and ancient Earth artifacts are known to be very valuable.\n\nIt is unknown whether Earth has actually been destroyed, or if the planet still physically exists; in the feature film \"Serenity\", ancient starships are shown leaving a sickly brown Earth with gray oceans, but the fate of the planet is never fully revealed. A puppet show in the episode \"Heart of Gold\" implies that Earth has in fact been obliterated, but this was never actually confirmed on screen.\n\nIn much of Isaac Asimov's fiction, the future Earth is an underprivileged planet — impoverished, overcrowded and disease-ridden — which is regarded with disdain by the arrogant Spacers of the \"Outer Planets\" (at this stage, there are about fifty of them).\n\nIn the \"Robot\" series, the inhabitants of these planets are still aware that their ancestors came from Earth, but this does not make them fond of the place. Rather, they develop a racist theory by which \"the best strains\" had left Earth to colonize the other planets and left \"the inferior strains\" behind. However, they have no choice but to ask the help of the protagonist, a detective from the despised Earth, to solve murder mysteries which baffle their own police. Afterwards, Earth embarks on a major new campaign of space colonization, with the hope that the new colonists will prove more faithful to the Mother Planet than the earlier ones. However, in the end of the series, the Earth is doomed to a slow radioactive process that will leave the planet uninhabitable, causing a more rapid expansion of colonization from Earth.\n\nIn the \"Galactic Empire\" series, taking place thousands of years later (originally conceived as completely separate but made by Asimov in his later career into the direct sequel of the Robot Period), Earth and settlements from it are still clearly remembered in \"The Stars, Like Dust\". By the time of \"The Currents of Space\", Earth is ruled by Trantor, not yet a Galactic Empire. Its status as the original homeworld of humanity is now disputed.\n\nIn \"Pebble in the Sky\", we see Earth in the early days of the Empire of Trantor. Earth has a largely radioactive crust with only patches of habitable land in between, and its people have to undergo compulsory euthanasia at the age of 60. It is a backwater province, and among inhabitants of other planets there is a prevalent prejudice known as \"Anti-Terrestrialism\", (obviously modeled on antisemitism), with the main negative stereotype having to do with the radiation-induced diseases prevalent on Earth.\n\nBy this time, Earth people still believe themselves to be the original home of Humanity, but hardly anyone else shares this belief. Fanatical priests, based in a mysterious Temple erected on the ruins of Washington, D.C., cultivate the mystique of Earth's ancient glories and conceive a plot to spread a Terrestrial disease throughout the Galaxy and in this way take over the Empire (and incidentally, act out the stereotype). The plot is foiled by a middle-aged tailor from the Twentieth Century, who possess powerful psychic abilities as a result of experiments performed upon him when he arrived in the future. Schwartz, the tailor, is often described as being Jewish, though this is never stated within the novel.\n\nBy the time of the Galactic Empire's decline, Earth is vaguely remembered as 'Sol' in \"Foundation\", and only one candidate for being the Original World. In \"Foundation and Earth\", records of Earth are missing, so two citizens of the mature Foundation go looking for it, and eventually find that it is desolated by nuclear radiation. The only sentient being remaining in the Solar system is robot Daneel Olivaw, who resides in a small station on the moon, overseeing the progress of a humanity now spread throughout the galaxy.\n\nIn Ursula K. Le Guin's Hainish Cycle our Earth is referred to as Terra. Like all human worlds of the Hainish Cycle, Terra was populated by the humans of Hain in Earth's prehistory, but forgot our common ancestry after millennia of no contact from extraterrestrial humans after the collapse of the first Hainish interplanetary civilization.\n\nThe second period of contact with the interstellar Hainish community, now organized as \"The League Of All Worlds\" is described in \"The Word for World Is Forest\", \"The Dispossessed\", and \"Rocannon's World\". In \"The Dispossessed\", Terra's population is said to have fallen from 9 billion to only half a billion people due to a collapse of the Terran ecology, and that life has only survived there because of strict rationing of resources and help from the Hainish. In \"The Word for World Is Forest\", the people from Terra appear as reckless exploiters of other planets. Some time later, \"City of Illusions\" provides a detailed description of Terra in the depths of a second era of isolation, called \"The Age Of The Enemy\".\n\nThe post-apocalyptic Earth seen in \"City of Illusions\" shows signs of an advanced, abandoned civilization under a rewilded landscape. A small number of humans live in tiny, isolated settlements where they retain some technologies from the past but are completely cut off from any communication with neighboring regions or with other worlds. There is only one city with high technology and energy-intensive construction, and it is controlled by the alien conquerors of the League. The events of \"City of Illusions\" lead to the third period of Terran contact with other worlds, this time as the Ekumen, during which \"The Left Hand of Darkness\" takes place.\n\nIn the short story \"Dancing To Ganam,\" which takes place in the far future of the Hainish universe, it is said that an extreme religious movement called the Unists developed on Terra and engaged in mass slaughter of non-believers and then of rival Unists sects. It is described as \"the worst resurgence of theocratic violence since the Time of Pollution\". It unclear if this time of pollution refers to the ecological collapse described in \"The Dispossesed,\" the collapse seen in \"City Of Illusions\", or is another unexplored dark period on Terra. In any case, the inclusion of this story is meant to show that even after so many millennia in the League and the Ekumen, Terra is still in many ways culturally primitive and prone to violent self-destruction.\n\nVarious individuals from Terra play a part in other stories. In \"The Telling\", Terra's incorporation into the Ekumen is briefly explained. Also, the main character in \"The Left Hand of Darkness\" is from Terra.\n\nIn \"The Hitchhiker's Guide to the Galaxy\" series by Douglas Adams, the Earth is destroyed by a Vogon Constructor Fleet to make room for a hyperspace bypass. One of two surviving Earthmen, Arthur Dent, is affronted and dismayed to find that his planet's entry in the Guide consists of one word: \"Harmless\". Ford Prefect, a researcher for the Guide attempts (and fails) to placate him by informing him that he has written a more extensive article for the next edition, although the result of merciless editing of his original draft has reduced his version considerably, now reading \"Mostly harmless\". Dent also learns that the Earth was originally constructed by the inhabitants of the planet Magrathea, as a giant supercomputer built to find the Question to the Ultimate Answer of Life, The Universe and Everything. The computer was so large that it was often mistaken for a planet, and that it was destroyed five minutes before the program was due to complete (after ten million years of running). It also mentions that humans are descended from the passengers of an ark full of unwanted middlemen (hairdressers, telephone sanitizers, advertising executives and the like), tricked into leaving their own planet behind by spurious tales of impending destruction invented by the rest of the planet's civilization (it is mentioned that the population was then wiped out by a disease contracted from a dirty telephone). The Earth was located in Galactic sector ZZ9 Plural Z Alpha. An alternate version of Earth is the planet NowWhat, which occupies the same space as Earth, but not the same probability. In the 2005 film adaptation, a new Earth replaces the old one, and everything is restored to the moments leading up to its destruction, except for one thing: Arthur Dent is not part of the planet anymore, at his own request.\n\nIn the series \"Red Dwarf\", Earth is seen mainly as the goal of the crew's trip; Dave Lister is personally obsessed with revisiting it as his home world, especially since he is the only character to be from there as Arnold Rimmer was born on Io. The novel \"Infinity Welcomes Careful Drivers\" mentions the Earth, despite Lister's regard for it, as suffering from massive littering and environmental damage; with a giant toupee being installed to in order to cover up ozone depletion. The novel \"Better Than Life\", however, mentions Earth being voted out of inhabitability to be the solar system's chosen planet of refuse known as Garbage World. A methane build up \"farts\" the planet out of the system and sends it out into deep space where it becomes an ice planet; later moved and thawed by the actions of the crew of \"Red Dwarf\". The Earth is inhabited by giant cockroaches and the descendants of GELFs sent there as punishment for their rebellion and bred into the polymorph. Lister spends half a lifetime trapped there attempting to correct his species past actions before \"Red Dwarf\" can rescue him due to black hole time dilation. In \"Last Human\" a parallel Earth is doomed by the initiatives of Earth President John Milhous Nixon and humanity breeds GELFs and simulants to terraform a new home across the multi-dimensional omni-zone.\n\nIn the \"Stargate\" television series, Earth (designation: \"P2X-3YZ\") is described as one of countless inhabited worlds, and is revealed to be the original home world of humans all over the galaxy. In ancient history many groups of humans were kidnapped and enslaved by powerful alien races, primarily the Goa'uld. Others remained to form present day Earth societies, which interact covertly with other extraterrestrial races and civilizations, many of them human. Earth first became important in the scene after the Alterans occupied it as their new capital (its name was Terra at that point). When they were forced to relocate to Lantea in the Pegasus galaxy (several million years ago), they \"seeded\" the planet with a less advanced form of themselves. Eventually, the Goa'uld found the planet and determined that the human body is the ultimate host body for their parasitic race. Many humans were kidnapped through the Stargate the Supreme System Lord Ra brought to Earth (Earth already had its own Stargate in Antarctica but it was inoperable) but the leftover population wasn't touched; they eventually rebelled and drove the Goa'uld off the planet in 3000 BC. About 8000 BC, the Lantean remnants returned to the planet but the primitive civilization extinguished their last hope of rebuilding their once great civilization due to the presence of the Goa'uld; as such, the Ancients slowly died out or Ascended since their numbers were too small to survive, even by crossbreeding with regular humans.\n\nWith Earth largely left alone for millennia, its human population continued to advance until the rediscovery of the Stargate in 1928 and its subsequent reactivation in 1994 (since its DHD activation device was missing, they had been unable to determine its purpose until they were able to create a computer interface). Unlike an enormous majority of planets, the Stargate on Earth was kept secret from the general populace to prevent widespread panic because \"we are not alone\".\n\nHumans who are from Earth are referred to as the Tau'ri by most other life forms in the galaxy, including the Goa'uld. Earth is a relatively important player on account of the radical change it unwittingly brought about when troops under the command of Colonel Jack O'Neill killed Goa'uld Supreme System Lord Ra and started a guerrilla war against the Goa'uld. However, its importance pales in comparison to the power of the System Lords or the Free Jaffa Nation, even though after the extinction of the Asgard and the defeat of the Ori, the Tau'ri became the dominant race of known space — although they were initially at a huge technological disadvantage, they later managed to reverse-engineer Goa'uld technology to the point where they started building their own ships, though much of it was rendered obsolete when the Asgard granted a significant amount of non-weapon technology. The Tau'ri also created remarkable technological feats, such as fighters equipped with hyperdrives powered by an unstable isotope. Their power increased further when they discovered that, due to crossbreeding with Ancients before their extinction, some Earth-born humans actually possessed a unique gene required to operate some of the more advanced Ancient technology. The peak of their power occurred when the Asgard donated their entire technological knowledge to Earth prior to their extinction. With this and some Ancient technology, the Tau'ri actually surpassed their precursors and defeated the Ori.\n\nThe main interaction between Earth and the rest of the universe is via three organizations:\n\n\nIn the \"Star Trek\" universe, the unified human state based on Earth, was one of the founding members of the United Federation of Planets. Several major federal organizations are headquartered on Earth, such as the Federation Council which meets in Paris. The Federation President also keeps offices in Paris, and Starfleet Headquarters is located in San Francisco. Major events on Earth included first contact with the Vulcans (\"\"), barely averted attacks by the Borg (in \"\" and \"Star Trek: First Contact\"), Founder infiltration (\"\"), and numerous attempted coups. Like most other major Federation worlds, Earth is a near-utopia where poverty and war have been eradicated and environmental damage has been reversed. Earth was also the planet of origin for at least one other sentient species, the Voth, according to the \"\" episode \"Distant Origin\". Descendants of the hadrosauridæ, they are theorized to have fled Earth for the Delta Quadrant after an extinction event.\n\nIn the \"\" episode \"\", we learn that the name of the planet's actual government is named United Earth. Much of its early history is unknown, although recent Trek novels have revealed that Earth's governments founded United Earth by signing the historic \"Traite d'Unification\" in 2123. The episodes \"\" and \"Terra Prime\" imply that United Earth is a parliamentary system of government: we meet various government officials who are referred to as Ministers (such as Minister Nathan Samuels, played by Harry Groener). United Earth's leader is most likely a Prime Minister – possibly, but not necessarily, Samuels himself. In the novels, Earth's governmental structure is further developed. Earth is a parliamentary republic, with a separate head of state (the President) and head of government (the Prime Minister).\n\nIn the Mirror Universe, Earth is the capital of the despotic Terran Empire which rules over large portions of the Alpha and Beta Quadrants and is generally seen as the most powerful interstellar empire. \"\" revealed that the Empire had collapsed and fallen to a Klingon–Cardassian Alliance. \"Star Trek\" novels reveal that Earth was later liberated thanks to the efforts of anti-Alliance rebels and Memory Omega.\n\nEarth is the \"lost\" homeland of the terrans of the Koprulu Sector, often referred to as \"Old Earth\". Earth history is well known to us until the 21st century.\nHowever, by the time the 23rd century was reached, genetic engineering and cybernetics were in common use, and Earth's population had reached 23 billion. Consequently, a resource and overpopulation crisis was developing.\n\nEarth's corporate factions who supported the capitalization of genetic engineering and cybernetics were opposed by those who saw this as a degeneration of the human race. These groups included humanist factions as well as religious conservatives who resorted to terrorism in these turbulent times. The conflict was resolved by the creation of the United Powers League, which generally supported the humanist philosophy and controlled all nations except for a few volatile Latin American states. The UPL banned many religions and made English the worldwide language.\n\nThe UPL proceeded to arrest and kill many people who opposed its \"divinity of mankind\" philosophy (which included \"purity\" from cybernetics, mutations, and so forth). It was during this time that Doran Routhe set up the colonization of the Koprulu Sector. Contact between the colonists and Earth was seemingly lost, and the Koprulu Sector terrans could not have found their way back to Earth.\n\nWith the discovery of the protoss and zerg, the United Powers League reformed to become the United Earth Directorate and launched an invasion of the Koprulu Sector, ostensibly in an effort to protect itself from the distant aliens. The invasion was repelled by a tenuous alliance of the Sector's powers.\n\nIn the universe of the \"Babylon 5\" television series, Earth was located in a relatively uncontested and non-valuable portion of the Galaxy. As a result, the people of Earth were allowed to develop with relatively little outside interference or threat of invasion from alien races. Unified under the worldwide government of the Earth Alliance, first contact with the Centauri was made in the mid-22nd century, which led to trade with a number of different species.\n\nEarth remained a relatively minor power until the 2230s, when it intervened on behalf of a number of other races (which later became the League of Non-Aligned Worlds) during the Dilgar invasion. Following the Dilgar War, Earth began to expand its influence and was seen as a rising power in the galaxy. A disastrous first contact with the Minbari in the 2240s precipitated the Earth–Minbari War, in which Earth was nearly conquered: the military (EarthForce) was devastated and the planet's population was nearly annihilated. However, the Minbari mysteriously surrendered just prior to the final invasion of Earth.\n\nFollowing the war, Earth's major contribution on the galactic stage was the creation of the Babylon Stations, that are neutral trading posts and diplomatic havens. Earth turned inward and suffered from xenophobic tendencies in the late 2250s, early 2260s under the despotic regime of President Morgan Clark, until a military and civilian civil war, started by General William Hague and later concluded by Captain John Sheridan, overthrew the Clark regime and helped establish Earth as one of the major players in the Interstellar Alliance.\n\nIn the \"Robotech\" canon, based on \"Macross\", Earth is the homeworld of humanity and notable as one of the few places that \"The flower of life\" (the source of the powerful energy source Protoculture) can grow. In 1999 during a global war the (future) SDF-1 an alien warship crashed to Earth on Macross island. Discovering they were not alone in the universe (and in secret the fact that the SDF-1 was a warship for a giant sized alien race) the human race united and rebuilt the ship as well as using the technology to advance their own and to create a small defence fleet for earth.\n\nTen years later the ship was ready but as preparation for launch on a mission of exploration continued Zentradi warships arrived in orbit to search for the ship. Though humanity tried peaceful contact a booby trap in the SDF-1 fired the huge main gun at the Zentradi committing Earth to a devastating interstellar war. To lure the Zentradi away from Earth the SDF-1 attempted a space fold FTL jump. This went wrong transporting not only the SDF-1 but part of Macross Island, 70,000 civilians and two navy warships to an area near Pluto. Pressure held in sub-surface shelters long enough to evacuate the civilians while the ships were grafted on to the SDF-1 as flight decks. However the jump also caused the FTL drive to vanish (for unknown reasons) as such the SDF-1 had to return home under normal thrust fighting Zentradi all the way and unable to talk to earth due to jamming.\n\nDuring the conflict many Zentradi became fascinated by Earth culture and over a million ships eventually defected. The ship finally returned to Earth but was driven back into space to draw off the Zentradi again. However the Zentradi bought over four million ships to Earth and bombarded the planet. The SDF-1 took out most ships with an overload of its shield system but in the process the ship was left incapable of flight and most of the Earth's population was killed. Over the next two years the survivors tried to rebuild and at last the Earth began to green again.\n\nTwenty years later the Earth had recovered during the war with the Robotech masters, but even after Earth's victory the planet was then attacked and occupied by another set of aliens The Invid in 2031. The Invid collected what they could of the Protoculture on Earth but seem to have left the population (now millions once more) largely alone.\n\nIn 2042 the Robotech Mars expedition returned from deep space but was wiped out by the Invid during the battle to liberate the planet. The survivors were forced down to Earth where they hooked up with the local resistance groups.\n\nTwo years later a massive fleet arrived with more advanced technology given to them by the alien Hydenites. The Invid quickly left Earth rather than risk Earth's destruction by deadly weapons the Neutron_S missiles which were far more dangerous than man believed. As Humanity celebrated however the Hydenites were revealed as the Children of Shadow who had destroyed the Invid homeworld eons before. They launched a sneak attack on the human space station liberty. Another war then began.\n\nIn the various continuities of the \"Sonic the Hedgehog\" franchise, Earth is mentioned in one way or another:\n\n\n\nAt least a billion years ago in the Uplift Universe by David Brin, there was a semi-mythical species known as the Progenitors that started the Uplift cycle—adopting a pre-sentient race and over a period of a hundred thousand years of selective breeding and genetic engineering, raising them to full sentience. As a result, most sentient species in the universe are members of various clans and factions, often quite powerful, with varying beliefs.\n\nEarth was overlooked and humans evolved, evidently without a patron race. However, by the time of first contact with galactic civilization, humans had themselves raised chimpanzees and dolphins to sentience, giving the human race a claim to patron status. This claim is provisionally accepted by the major institutions of galactic civilization even as it is hotly contested by a number of senior patron races.\n\nGalactic civilization holds humanity's claim of having evolved to sentience independently to be highly controversial at best, and an offensive heresy at worst. Some clans holding the latter view have actively conspired to have humanity's patron status officially vacated and to adopt the \"wolfling\" race themselves, thus gaining three sentient races and control of \"fallow\" genetic material: Earth's wealth of species with uplift potential. Only the conservative and ponderous nature of galactic institutions and the rivalry of other clans reluctant to see the Earth's races claimed by another have prevented this.\n\nIn the Worldwar novels by Harry Turtledove, Earth is the human homeworld and is attacked by the aliens known as The Race in 1942. All sides in the Second World War are forced to unify to fight this threat, and despite superior technology the Lizards (a human racial epithet for the aliens) are fought to a draw by 1944.\n\nIn 1962 another Race fleet arrives carrying a civilian colony force of nearly 100 million, in 1965 the Race and the German Reich fight a major war which Germany loses.\n\nIn 1994 humanity has caught up to the Race enough to send a slower-than-light starship to the Race's home world, where it arrives in 2031. Soon after another ship arrives, an FTL-capable ship, indicating that humanity has now surpassed the Race in technology.\n\n\nIn the Shannara series of books by Terry Brooks, the setting is a post-apocalyptic earth that was destroyed after a nuclear holocaust wiped out most of humanity and mutated the survivors into Men, Gnomes, Dwarves, and Trolls. Elves are also there, but according to Allanon's recounting of history, the Elves always existed in our current world. Before the Great Wars, as the nuclear holocaust is referred to, humans had advanced to a utopian society.\n\nIn the Death gate cycle series of books by Margaret Weis and Tracy Hickman, a nuclear holocaust led to the creation of a two mutant strains of humans who developed fantastic magical powers. Other races, such as Dwarves and Elves were also present, even in pre-holocaust Earth, but were hidden from humanity. In this universe, the Earth was destroyed by the Sartan, one of the magical races that evolved from humans.\n\nAn incomplete list of what Earth natives have been referred to in various Sci-Fi worlds:\n"}
{"id": "212485", "url": "https://en.wikipedia.org/wiki?curid=212485", "title": "Earth religion", "text": "Earth religion\n\nEarth religion is a term used mostly in the context of neopaganism.\n\nEarth-centered religion or nature worship is a system of religion based on the veneration of natural phenomena. It covers any religion that worships the earth, nature, or fertility deity, such as the various forms of goddess worship or matriarchal religion. Some find a connection between earth-worship and the Gaia hypothesis. Earth religions are also formulated to allow one to utilize the knowledge of preserving the earth.\n\nAccording to Marija Gimbutas, pre-Indo-European societies lived in small-scale, family-based communities that practiced matrilineal succession and goddess-centered religion where creation comes from the woman. She is the Divine Mother who can give life and take it away. In Irish mythology she is Danu, in Slavic mythology she is Mat Zemlya, and in other cultures she is Pachamama, Ninsun, Terra Mater, Nüwa, Matres or Shakti.\n\nIn the late 1800s, James Weir wrote an article describing the beginnings and aspects of early religious feeling. According to Boyer, early man was forced to locate food and shelter in order to survive, while constantly being directed by his instincts and senses. Because man's existence depended on nature, men began to form their religion and beliefs on and around nature itself. It is evident that man's first religion would have had to develop from the material world, he argues, because man relied heavily on his senses and what he could see, touch, and feel. In this sense, the worship of nature formed, allowing man to further depend on nature for survival.\n\nNeopagans have tried to make claims that religion started in ways that correspond to earth religion. In one of their published works, \"The Urantia Book\", another reason for this worship of nature came from a fear of the world around primitive man. His mind lacked the complex function of processing and sifting through complex ideas. As a result, man worshiped the very entity that surrounded him every day. That entity was nature. Man experienced the different natural phenomena around him, such as storms, vast deserts, and immense mountains. Among the very first parts of nature to be worshiped were rocks and hills, plants and trees, animals, the elements, heavenly bodies, and even man himself. As primitive man worked his way through nature worship, he eventually moved on to incorporate spirits into his worship. Although these claims may have some merit, they are nonetheless presented from a biased position that cannot be authenticated by traditional and reliable sources. Therefore, their claims can not be relied upon.\n\nThe origins of religion can be looked at through the lens of the function and processing of the human mind. Pascal Boyer suggests that, for the longest period of time, the brain was thought of as a simple organ of the body. However, he claims that the more information collected about the brain indicates that the brain is indeed not a \"blank slate.\" Humans do not just learn any information from the environment and surroundings around them. They have acquired sophisticated cognitive equipment that prepares them to analyze information in their culture and determine which information is relevant and how to apply it. Boyer states that \"having a normal human brain does not imply that you have religion. All it implies is that people can acquire it, which is very different.\" He suggests that religions started for the reasons of providing answers to humans, giving comfort, providing social order to society, and satisfying the need of the illusion-prone nature of the human mind. Ultimately, religion came into existence because of our need to answer questions and hold together our societal order.\n\nAn additional idea on the origins of religion comes not from man's cognitive development, but from the ape. Barbara J. King argues that human beings have an emotional connection with those around them, and that that desire for a connection came from their evolution from apes. The closest relative to the human species is the African ape. At birth, the ape begins negotiating with its mother about what it wants and needs in order to survive. The world the ape is born into is saturated with close family and friends. Because of this, emotions and relationships play a huge role in the ape's life. Its reactions and responses to one another are rooted and grounded in a sense of belongingness, which is derived from its dependence on the ape's mother and family. Belongingness is defined as \"mattering to someone who matters to you ... getting positive feelings from our relationships.\" This sense and desire for belongingness, which started in apes, only grew as the hominid (a human ancestor) diverged from the lineage of the ape, which occurred roughly six to seven million years ago.\n\nAs severe changes in the environment, physical evolutions in the human body (especially in the development of the human brain), and changes in social actions occurred, humans went beyond trying to simply form bonds and relationships of empathy with others. As their culture and society became more complex, they began using practices and various symbols to make sense of the natural and spiritual world around them. Instead of simply trying to find belongingness and empathy from the relationships with others, humans created and evolved God and spirits in order to fulfil that need and exploration. King argued that \"an earthly need for belonging led to human religious imagination and thus to the otherworldly realm of relating to God, gods, and spirits.\"\n\nThe term \"earth religion\" encompasses any religion that worships the earth, nature or fertility gods or goddesses. There is an array of groups and beliefs that fall under earth religion, such as paganism, which is a polytheistic, nature based religion; animism, which is the worldview that all living entities (plants, animals, and humans) possess a spirit; Wicca, who hold the concept of an earth mother goddess as well as practice ritual magic; and druidism, which equates divinity with the natural world.\n\nAnother perspective of earth religion to consider is pantheism, which takes a varied approach to the importance and purpose of the earth, and man's relationship with the planet. Several of their core statements deal with the connectivity humans share with the planet, declaring that \"all matter, energy, and life are an interconnected unity of which we are an inseparable part\" and \"we are an integral part of Nature, which we should cherish, revere and preserve in all its magnificent beauty and diversity. We should strive to live in harmony with Nature locally and globally\".\n\nThe earth also plays a vital role to many Voltaic peoples, many of whom \"consider the Earth to be Heaven’s wife\", such as the Konkomba of northern Ghana, whose economic, social and religious life is heavily influenced by the earth. It is also important to consider various Native American religions, such as Peyote Religion, Longhouse Religion, and Earth Lodge Religion.\n\nApril 22 was established as International Mother Earth Day by the United Nations in 2009, but many cultures around the world have been celebrating the Earth for thousands of years. Winter solstice and Summer solstice are celebrated with holidays like Yule and Dongzhi in the winter and Tiregān and Kupala in the summer.\n\nAnimism is practiced among the Bantu peoples of Sub-Saharan Africa. The Dahomey mythology has deities like Nana Buluku, Gleti, Mawu, Asase Yaa, Naa Nyonmo and Xevioso.\n\nIn Baltic mythology, the sun is a female deity, Saule, a mother or a bride, and Mēness is the moon, father or husband, their children being the stars. In Slavic mythology Mokosh and Mat Zemlya together with Perun head up the pantheon. Celebrations and rituals are centered on nature and harvest seasons. Dragobete is a traditional Romanian spring holiday that celebrates \"the day when the birds are betrothed.\"\n\nIn Hindu philosophy, the yoni is the creative power of nature and the origin of life. In Shaktism, the yoni is celebrated and worshipped during the Ambubachi Mela, an annual fertility festival which celebrates the Earth's menstruation.\n\nAlthough the idea of earth religion has been around for thousands of years, it did not fully show up in popular culture until the early 1990s. \"The X-Files\" was one of the first nationally broadcast television programs to air witchcraft and Wicca (types of earth religion) content. On average, Wiccans - those who practice Wicca - were more or less pleased with the way the show had portrayed their ideals and beliefs. However, they still found it to be a little \"sensationalistic\". That same year, the movie \"The Craft\" was released - also depicting the art of Wicca. Unfortunately, this cinematic feature was not as happily accepted as \"The X-Files\" had been.\n\nA few years later, programs showcasing the aforementioned religious practices - such as \"Charmed\" and \"Buffy the Vampire Slayer\" - became widely popular. Although \"Charmed\" focused mostly on witchcraft, the magic they practiced very closely resembled Wicca. Meanwhile, \"Buffy\" was one of the first shows to actually cast a Wiccan character. However, since the shows focus was primarily on vampires, the Wiccan was depicted as having supernatural powers, rather than being in-tuned with the Earth.\n\nOther movies and shows throughout the last few decades have also been placed under the genre of Earth Religion. Among them are two of director Hayao Miyazaki's most well known films - \"Princess Mononoke\" and \"My Neighbor Totoro\". Both movies present human interaction with land, animal, and other nature spirits. Speakers for Earth Religion have said that these interactions suggest overtones of Earth Religion themes.\n\nSome popular Disney movies have also been viewed as Earth Religion films. Among them are \"The Lion King\" and \"Brother Bear\". Those who practice Earth Religion view \"The Lion King\" as an Earth Religion film mainly for the \"interconnectedness\" and \"Circle of Life\" it shows between the animals, plants, and life in general. When that link is broken, viewers see chaos and despair spread throughout the once bountiful land. Congruently, \"Brother Bear\" portrays interactions and consequences when humans disobey or go against the animal and Earth spirits.\n\nOther earth religion movies include \"The 13th Warrior\", \"The Deceivers (film)\", \"Sorceress (1982 film)\", \"Anchoress (film)\", \"Eye of the Devil\", \"Agora (film)\", and \"The Wicker Man (1973 film)\". These movies all contain various aspects of earth religion and nature worship in general.\n\nMany religions have negative stereotypes of earth religion and neo-paganism in general. A common critique of the worship of nature and resources of \"Mother Earth\" is that the rights of nature and ecocide movements are inhibitors of human progress and development. This argument is fueled by the fact that those people socialized into 'western' world views believe the earth itself is not a living being. Wesley Smith believes this is “anti-humanism with the potential to do real harm to the human family.” According to Smith, earth worshipers are hindering large-scale development, and they are viewed as inhibitors of advancement.\n\nA lot of criticism of earth religion comes from the negative actions of a few people who have been chastised for their actions. One such negative representative of earth religion is Aleister Crowley. He is believed to be \"too preoccupied with awakening magical powers\" instead of putting the well-being of others in his coven. Crowley allegedly looked up to \"Old George\" Pickingill, who was another worshipper of nature who was viewed negatively. Critics regarded Pickingill as a Satanist and \"England’s most notorious Witch\".\n\nCrowley himself was \"allegedly expelled from the Craft because he was a pervert.\" He became aroused by torture and pain, and enjoyed being \"punished\" by women. This dramatically damaged Crowley’s public image, because of his lifestyle and actions. Many people regarded all followers of earth religion as perverted Satanists.\n\nFollowers of earth religion have suffered major opprobrium over the years for allegedly being Satanists. Some religious adherents can be prone to viewing religions other than their religion as being wrong sometimes because they perceive those religions as characteristic of their concept of Satan worship. To wit, Witchcraft, a common practice of Wiccans, is sometimes misinterpreted as Satan worship by members of these groups, as well as less-informed persons who may not be specifically religious but who may reside within the sphere-of-influence of pagan-critical religious adherents. From the Wiccan perspective, however, earth religion and Wicca lie outside of the phenomenological world that encompasses Satanism. An all-evil being does not exist within the religious perspective of western earth religions. Devotees worship and celebrate earth resources and earth-centric deities. Satanism and Wicca \"have entirely different beliefs about deity, different rules for ethical behavior, different expectations from their membership, different views of the universe, different seasonal days of celebration, etc.\"\n\nNeo-pagans, or earth religion followers, often claim to be unaffiliated with Satanism. Neo-pagans, Wiccans, and earth religion believers do not acknowledge the existence of a deity that conforms to the common Semitic sect religious concept of Satan. Satanism stems from Christianity, while earth religion stems from older religious concepts.\n\nSome earth religion adherents take issue with the religious harassment that is inherent in the social pressure that necessitates their having to distance themselves from the often non-uniform, Semitic sect religious concept of Satan worship. Having to define themselves as \"other\" from a religious concept that is not within their worldview implies a certain degree of outsider-facilitated, informal, but functional religious restriction that is based solely on the metaphysical and mythological religious beliefs of those outsiders. This is problematic because outsider initiated comparisons to Satanism with the intent of condemnation, even when easily refuted, can have the effect of social pressure on earth religion adherents to conform to outsider perception of acceptable customs, beliefs, and modes of religious behavior.\n\nTo illustrate, a problem could arise with the \"other\" than Satanism argument if an earth centered belief system adopted a holiday that a critic considered to be similar or identical to a holiday that Satanists celebrate. Satanists have historically been prone to adopting holidays that have origins in various pagan traditions, ostensibly because these traditional holidays are amongst the last known vestiges of traditional pre-Semitic religious practice in the west. Satanists are, perhaps irrationally, prone to interpreting non-Semitic holidays as anti-Christian and therefore as implicitly representative of their worldview. This is not surprising given the fact that this is, in fact, how many Christians interpret holidays such as Samhain. In spite of any flawed perceptions or rationale held by any other group, earth centered religion adherents do not recognize misinterpretation of their customs made by outside religious adherents or critics inclusive of Satan worshippers.\n\nOrganized Satan worship, as defined by and anchored in the Semitic worldview, is characterized by a relatively disorganized and often disparate series of movements and groups that mostly emerged in the mid-20th century. Thus, their adopted customs have varied, continue to vary, and therefore this moving target of beliefs and customs can not be justifiably nor continuously accounted for by earth centered religious adherents. Once a Satanist group adopts a holiday, social stigma may unjustifiably taint the holiday and anyone who observes it without discrimination as to whence and for what purpose it was originally celebrated. Given these facts, many earth centered religion devotees find comparisons to Satanism intrinsically oppressive in nature. This logic transfers to any and all religious customs to include prayer, magic, ceremony, and any unintentional similarity in deity characteristics (an example is the horned traditional entity Pan having similar physical characteristics to common horned depictions of Satan).\n\nThe issue is further complicated by the theory that the intra and extra-biblical mythology of Satan that is present throughout various Semitic sects may have originally evolved to figuratively demonize the heathen religions of other groups. Thus, the concept of Satan, or \"the adversary\", would have been representative of all non-Semitic religions and, by extension, the people who believed in them. Although, at times, the concept of the \"other\" as demonic has also been used to characterize competing Semitic sects. Amongst other purposes, such belief would have been extraordinarily useful during the psychological and physical process of cleansing Europe of traditional tribal beliefs in favor of Christianity. This possibility would account for the historical tendency of Christian authorities, for example, to deem most pagan customs carried out in the pagan religious context as demonic. By any modern standard, such current beliefs would violate western concepts of religious tolerance as well as be inimical to the preservation of what remains of the culture of long-persecuted religious groups.\n\nBecause of the vast diversity of religions that fall under the title of \"earth religion\" there is no consensus of beliefs. However, the ethical beliefs of most religions overlap. The most well-known ethical code is the Wiccan Rede. Many of those who practice an earth religion choose to be environmentally active. Some perform activities such as recycling or composting while others feel it to be more productive to try and support the earth spiritually. These six beliefs about ethics seem to be universal.\n\n\"An [if] it harm none, do what ye will.\" Commonly worded in modern English as \"if it doesn't harm anyone, do what you want.\" This maxim was first printed in 1964, after being spoken by the priestess Doreen Valiente in the mid-20th century, and governs most ethical belief of Wiccans and some Pagans. There is no consensus of beliefs but this rede provides a starting point for most people's interpretation of what is ethical. The rede clearly states to do no harm but what constitutes as harm and what level of self-interest is acceptable is negotiable. Many Wiccans reverse the phrase into \"Do what ye will an it harm none,\" meaning \"Do what you want if it doesn't harm anyone.\" The difference may not seem significant but it is. The first implies that it is good to do no harm but does not say that it is necessarily unethical to do so, the second implies that all forms of harm are unethical. The second phrase is nearly impossible to follow. This shift occurred when trying to better adapt the phrase into modern English as well as to stress the \"harmlessness\" of Wiccans. The true nature of the rede simply implies that there is personal responsibility for your actions. You may do as you wish but there is a karma reaction from every action. Even though this is the most well-known rede of practice, it does not mean that those that choose not to follow it are unethical. There are many other laws of practice that other groups follow.\n\nThe Threefold Law is the belief that for all actions there is always a cause and effect. For every action taken either the good or ill intention will be returned to the action taker threefold. This is why the Wiccan Rede is typically followed because of fear of the threefold return from that harmful action.\n\nThis term is what Emma Restall Orr calls reverence for the earth in her book \"Living with Honour: A Pagan Ethics\". She separates the term into three sections: courage, generosity and loyalty, or honesty, respect and responsibility. There is no evil force in Nature. Nothing exists beyond the natural, therefore it is up to the individual to choose to be ethical not because of divine judgment. All beings are connected by the earth and so all should be treated fairly. There is a responsibility toward the environment and a harmony should be found with nature.\n\nThe following was written by the Church of All Worlds in 1988 and was affirmed by the Pagan Ecumenical Conferences of Ancient Ways (California, May 27–30) and Pagan Spirit Gathering (Wisconsin, June 17). The Pagan Community Council of Ohio then presented it to the Northeast Council of W.I.C.C.A.\n\n\"We, the undersigned, as adherents of Pagan and Old and Neo-Pagan Earth Religions, including Wicca or Witchcraft, practice a variety of positive, life affirming faiths that are dedicated to healing, both of ourselves and of the Earth. As such, we do not advocate or condone any acts that victimize others, including those proscribed by law. As one of our most widely accepted precepts is the Wiccan Rede's injunction to \"harm none,\" we absolutely condemn the practices of child abuse, sexual abuse and any other form of abuse that does harm to the bodies, minds or spirits of the victims of such abuses. We recognize and revere the divinity of Nature in our Mother the Earth, and we conduct our rites of worship in a manner that is ethical, compassionate and constitutionally protected. We neither acknowledge or worship the Christian devil, \"Satan,\" who is not in our Pagan pantheons. We will not tolerate slander or libel against our Temples, clergy or Temple Assemblers and we are prepared to defend our civil rights with such legal action as we deem necessary and appropriate.\"\n"}
{"id": "1800265", "url": "https://en.wikipedia.org/wiki?curid=1800265", "title": "Emergy", "text": "Emergy\n\nEmergy is the amount of energy that was consumed in direct and indirect transformations to make a product or service. Emergy is a measure of quality differences between different forms of energy. Emergy is an expression of all the energy used in the work processes that generate a product or service in units of one type of energy. Emergy is measured in units of \"emjoule\"s, a unit referring to the available energy consumed in transformations. Emergy accounts for different forms of energy and resources (e.g. sunlight, water, fossil fuels, minerals, etc.) Each form is generated by transformation processes in nature and each has a different ability to support work in natural and in human systems. The recognition of these quality differences is a key concept.\n\nThe theoretical and conceptual basis for the emergy methodology is grounded in thermodynamics, general system theory and systems ecology. Evolution of the theory by Howard T. Odum over the first thirty years is reviewed in \"Environmental Accounting\" and in the volume edited by C. A. S. Hall titled \"Maximum Power\".\n\nBeginning in the 1950s, Odum analyzed energy flow in ecosystems (\"e.g.\" Silver Springs, Florida; Enewetak atoll in the south Pacific; Galveston Bay, Texas and Puerto Rican rainforests, amongst others) where energies in various forms at various scales were observed. His analysis of energy flow in ecosystems, and the differences in the potential energy of sunlight, fresh water currents, wind and ocean currents led him to make the suggestion that when two or more different energy sources drive a system, they cannot be added without first converting them to a common measure that accounts for their differences in energy quality. This led him to introduce the concept of \"energy of one kind\" as a common denominator with the name \"energy cost\". He then expanded the analysis to model food production in the 1960s, and in the 1970s to fossil fuels.\n\nOdum's first formal statement of what would later be termed emergy was in 1973:\nEnergy is measured by calories, btu's, kilowatthours, and other intraconvertable units, but energy has a scale of quality which is not indicated by these measures. The ability to do work for man depends on the energy quality and quantity and this is measurable by the amount of energy of a lower quality grade required to develop the higher grade. The scale of energy goes from dilute sunlight up to plant matter, to coal, from coal to oil, to electricity and up to the high quality efforts of computer and human information processing.\n\nIn 1975, he introduced a table of \"Energy Quality Factors\", kilocalories of sunlight energy required to make a kilocalorie of a higher quality energy, the first mention of the energy hierarchy principle which states that \"energy quality is measured by the energy used in the transformations\" from one type of energy to the next.\n\nThese energy quality factors, were placed on a fossil-fuel basis and called \"Fossil Fuel Work Equivalents\" (FFWE), and the quality of energies were measured based on a fossil fuel standard with rough equivalents of 1 kilocalorie of fossil fuel equal to 2000 kilocalories of sunlight. \"Energy quality ratios\" were computed by evaluating the quantity of energy in a transformation process to make a new form and were then used to convert different forms of energy to a common form, in this case fossil fuel equivalents. FFWE's were replaced with coal equivalents (CE) and by 1977, the system of evaluating quality was placed on a solar basis and termed solar equivalents (SE).\n\nThe term \"embodied energy\" was used for a time in the early 1980s to refer to energy quality differences in terms of their costs of generation, and a ratio called a \"quality factor\" for the calories (or joules) of one kind of energy required to make those of another. However, since the term embodied energy was used by other groups who were evaluating the fossil fuel energy required to generate products and were not including all energies or using the concept to imply quality, embodied energy was dropped in favor of \"embodied solar calories\", and the quality factors became known as \"transformation ratios\".\n\nUse of the term \"embodied energy\" for this concept was modified in 1986 when David Scienceman, a visiting scholar at the University of Florida from Australia, suggested the term \"emergy\" and \"emjoule\" or \"emcalorie\" as the unit of measure to distinguish emergy units from units of available energy. The term transformation ratio was shortened to transformity in about the same time. It is important to note that throughout this twenty years the baseline or the basis for evaluating forms of energy and resources shifted from organic matter, to fossil fuels and finally to solar energy.\n\nAfter 1986, the emergy methodology continued to develop as the community of scientists expanded and as new applied research into combined systems of humans and nature presented new conceptual and theoretical questions. The maturing of the emergy methodology resulted in more rigorous definitions of terms and nomenclature and refinement of the methods of calculating transformities. The International Society for the Advancement of Emergy Research and a biennial International Conference at the University of Florida support this research.\n\nEmergy— amount of energy of one form that is used in transformations directly and indirectly to make a product or service. The unit of emergy is the emjoule or emergy joule. Using emergy, sunlight, fuel, electricity, and human service can be put on a common basis by expressing each of them in the emjoules of solar energy that is required to produce them. If solar emergy is the baseline, then the results are solar emjoules (abbreviated seJ). Although other baselines have been used, such as coal emjoules or electrical emjoules, in most cases emergy data are given in solar emjoules.\n\nUnit Emergy Values (UEVs) — the emergy required to generate one unit of output. Types of UEVs:\n\nEmergy accounting converts the thermodynamic basis of all forms of energy, resources and human services into equivalents of a single form of energy, usually solar. To evaluate a system, a system diagram organizes the evaluation and account for energy inputs and outflows. A table of the flows of resources, labor and energy is constructed from the diagram and all flows are evaluated. The final step involves interpreting the results.\n\nIn some cases, an evaluation is done to determine the fit of a development proposal within its environment. It also allows comparison of alternatives. Another purpose is to seek the best use of resources to maximize economic vitality.\n\nSystem diagrams show the inputs that are evaluated and summed to obtain the emergy of a flow. A diagram of a city and its regional support area is shown in Figure 1.\n\nA table (see example below) of resource flows, labor and energy is constructed from the diagram. Raw data on inflows that cross the boundary are converted into emergy units, and then summed to obtain total emergy supporting the system. Energy flows per unit time (usually per year) are presented in the table as separate line items.\n\nAll tables are followed by footnotes that show citations for data and calculations.\n\nThe table allows a unit emergy value to be calculated. The final, output row (row “O” in the example table above) is evaluated first in units of energy or mass. Then the input emergy is summed and the unit emergy value is calculated by dividing the emergy by the units of the output.\n\nFigure 2 shows non-renewable environmental contributions (N) as an emergy storage of materials, renewable environmental inputs (R), and inputs from the economy as purchased (F) goods and services. Purchased inputs are needed for the process to take place and include human service and purchased non-renewable energy and material brought in from elsewhere (fuels, minerals, electricity, machinery, fertilizer, etc.). Several ratios, or indices are given in Figure 2 that assess the global performance of a process.\n\nOther ratios are useful depending on the type and scale of the system under evaluation.\n\nThe recognition of the relevance of energy to the growth and dynamics of complex systems has resulted in increased emphasis on environmental evaluation methods that can account for and interpret the effects of matter and energy flows at all scales in systems of humanity and nature. The following table lists some general areas in which the emergy methodology has been employed.\n\nThe concept of emergy has been controversial within academe including ecology, thermodynamics and economy. Emergy theory has been criticized for allegedly offering an energy theory of value to replace other theories of value. The stated goal of emergy evaluations is to provide an \"ecocentric\" valuation of systems, processes. Thus it does not purport to replace economic values but to provide additional information, from a different point of view.\n\nThe idea that a calorie of sunlight is not equivalent to a calorie of fossil fuel or electricity strikes many as absurd, based on the 1st Law definition of energy units as measures of heat (i.e. Joule's mechanical equivalent of heat). Others have rejected the concept as impractical since from their perspective it is impossible to objectively quantify the amount of sunlight that is required to produce a quantity of oil. In combining systems of humanity and nature and evaluating environmental input to economies, mainstream economists criticize the emergy methodology for disregarding market values.\n\n"}
{"id": "46348501", "url": "https://en.wikipedia.org/wiki?curid=46348501", "title": "Evolution of the cochlea", "text": "Evolution of the cochlea\n\nCochlea is Latin for “snail, shell or screw” and originates from the Greek word κοχλίας \"kohlias\". The modern definition, the auditory portion of the inner ear, originated in the late 17th century. Within the mammalian cochlea exists the organ of Corti, which contains hair cells that are responsible for translating the vibrations it receives from surrounding fluid-filled ducts into electrical impulses that are sent to the brain to process sound. This spiral-shaped cochlea is estimated to have originated during the early Cretaceous Period, around 120 million years ago. Further, the auditory innervation of the spiral-shaped cochlea also traces back to the Cretaceous period. The evolution of the human cochlea is a major area of scientific interest because of its favourable representation in the fossil record. During the last century, many scientists such as evolutionary biologists and paleontologists strove to develop new methods and techniques to overcome the many obstacles associated with working with ancient, delicate artifacts. In the past, scientists were limited in their ability to fully examine specimens without causing damage to them. In more recent times, technologies such as micro-CT scanning became available. These technologies allow for the visual differentiation between fossilized animal materials and other sedimentary remains. With the use of X-ray technologies, it is possible to ascertain some information about the auditory capabilities of extinct creatures, giving insight to human ancestors as well as their contemporary species.\n\nWhile the basic structure of the inner ear in lepidosaurs (lizards and snakes), archosaurs (birds and crocodilians) and mammals is similar, and the organs are considered to be homologous, each group has a unique type of auditory organ. The hearing organ arose within the lagenar duct of stem reptiles, lying between the saccular and lagenar epithelia. In lepidosaurs, the hearing organ, the basilar papilla, is generally small, with at most 2000 hair cells, whereas in archosaurs the basilar papilla can be much longer (>10mm in owls) and contain many more hair cells that show two typical size extremes, the short and the tall hair cells. In mammals, the structure is known as the organ of Corti and shows a unique arrangement of hair cells and supporting cells. All mammalian organs of Corti contain a supporting tunnel made up of pillar cells, on the inner side of which there are inner hair cells and outer hair cells on the outer side. The definitive mammalian middle ear and the elongated cochlea allows for better sensitivity for higher frequencies.\n\nAs in all lepidosaurs and archosaurs, the single-ossicle (columellar) middle ear transmits sound to the footplate of the columella, which sends a pressure wave through the inner ear. In snakes, the basilar papilla is roughly 1mm long and only responds to frequencies below about 1 kHz. In contrast, lizards tend to have two areas of hair cells, one responding below and the other above 1 kHz. The upper frequency limit in most lizards is roughly 5–8 kHz. The longest lizard papillae are about 2mm long and contain 2000 hair cells and their afferent innervating fibers can be very sharply tuned to frequency.\n\nIn birds and crocodilians, the similarity of the structure of the basilar papilla betrays their close evolutionary relationship. The basilar papilla is up to about 10mm long and contains up to 16500 hair cells. While most birds have an upper hearing limit of only about 6 kHz, the barn owl can hear up to 12 kHz and thus close to the human upper limit.\n\nEgg-laying mammals, the monotremes (spiny anteater and platypus), do not have a spiral cochlea, but one shaped more like a banana, up to about 7 mm long. Like in lepidosaurs and archosaurs, it contains a lagena, a vestibular sensory epithelium, at its tip. Only in therian mammals (marsupials and placentals) is the cochlea truly coiled 1.5 to 3.5 times. Whereas in monotremes there are many rows of both inner and outer hair cells in the organ of Corti, in therian (marsupial and placental) mammals the number of inner hair-cell rows is one, and there are generally only three rows of outer hair cells.\n\nAmphibians have unique inner ear structures. There are two sensory papillae involved in hearing, the basilar (higher frequency) and amphibian (lower frequency) papillae, but it is uncertain whether either is homologous to the hearing organs of lepidosaurs, archosaurs and mammals and we have no idea when they arose.\n\nFish have no dedicated auditory epithelium, but use various vestibular sensory organs that respond to sound. In most teleost fishes it is the saccular macula that responds to sound. In some, such as goldfishes, there is also a special bony connection to the gas bladder that increases sensitivity allowing hearing up to about 4 kHz.\n\nThe size of cochlea has been measured throughout its evolution based on the fossil record. In one study, the basal turn of the cochlea was measured, and it was hypothesized that cochlear size correlates with body mass. The size of the basal turn of the cochlea was not different in Neanderthals and Holocene humans, however it became larger in early modern humans and Upper Paleolithic humans. Furthermore, the position and orientation of the cochlea is similar between Neanderthals and Holocene humans, relative to plane of the lateral canal, whereas early modern and upper Paleolithic humans have a more superiorly placed cochlea than Holocene humans. When comparing hominins of the Middle Pleistocene and Neanderthals and Holocene humans, the apex of the cochlea faces more inferiorly in the hominins than the latter two groups. Finally, the cochlea of European middle Pleistocene hominins faces more inferiorly than Neanderthals, modern humans, and Homo erectus.\nHuman beings, along with Apes, are the only mammals that do not have high frequency (>32 kHz) hearing. Humans have long cochleae, but the space devoted to each frequency range is quite large (2.5mm per octave), resulting in a comparatively reduced upper frequency limit. The human cochlea has approximately 2.5 turns around the modiolus (the axis). Humans, like many mammals and birds, are able to perceive auditory signals that displace the eardrum by a mere picometre.\n\nBecause of its prominence and preserved state in the fossil record, until recently, the ear had been used to determine phylogeny. The ear itself contains different portions, including the outer ear, the middle ear, and the inner ear and all of these show evolutionary changes that are often unique to each lineage [14]. It was the independent evolution of a tympanic middle ear in the Triassic era that produced strong selection pressures towards improved hearing organs in the separate lineages of land vertebrates.\n\nThe cochlea is the tri-chambered auditory detection portion of the ear, consisting of the scala media, the scala tympani, and the scala vestibuli. Regarding mammals, placental and marsupial cochleae have similar cochlear responses to auditory stimulation as well as DC resting potentials. This leads to the investigation of the relationship between these therian mammals and researching their ancestral species to trace the origin of the cochlea.\n\nThis spiral-shaped cochlea that is in both marsupial and placental mammals is traced back to approximately 120 million years ago. The development of the most basic basilar papilla (the auditory organ that later evolved into the Organ of Corti in mammals) happened at the same time as the water-to-land transition of vertebrates, approximately 380 million years ago. The actual coiling or spiral nature of the cochlea occurred to save space inside the skull. The longer the cochlea, the higher is the potential resolution of sound frequencies given the same hearing range. The oldest of the truly coiled mammalian cochleae were approximately 4 mm in length.\n\nThe earliest evidence available for primates depicts a short cochlea with prominent laminae, suggesting that they had good high-frequency sensitivity as opposed to low-frequency sensitivity. After this, over a period of around 60 million years, evidence suggests that primates developed longer cochleae and less prominent laminae, which means that they had an improvement in low-frequency sensitivity and a decrease in high-frequency sensitivity. By the early Miocene period, the cycle of the elongation of the cochleae and the deterioration of the laminae was completed. Evidence shows that primates have had an increasing cochlear volume to body mass ratio over time. These changes in the cochlear labyrinth volume negatively affect the highest and lowest audible frequencies, causing a downward shift. Non-primates appear to have smaller cochlear labyrinth volumes overall when compared to primates. Some evidence also suggests that selective forces for the larger cochlear labyrinth may have started after the basal primate node.\nMammals are the subject of a substantial amount of research not only because of the potential knowledge to be gained regarding humans, but also because of their rich and abundant representation in the fossil record. The spiral shape of the cochlea evolved later on in the evolutionary pathway of mammals than previously believed, just before the therians split into the two lineages marsupials and placentals, about 120 million years ago.\n\nParallel to the evolution of the cochlea, prestins show an increased rate of evolution in therian mammals. Prestins are located in the outer hair cells of mammalian cochlea and are considered motor proteins. They are found in the hair cells of all vertebrates, including fish, but are thought to have initially been membrane transporter molecules. A high concentration of prestins are found only in the lateral membranes of therian outer hair cells (there is uncertainty with regard to concentrations in monotremes). This high concentration is not found in inner hair cells, and is also lacking in all hair cell types of non-mammals. Prestin also has a role in motility, which evolved a greater importance in the motor function in land vertebrates, but this developed vastly differently in different lineages. In certain birds and mammals, prestins function as both transporters and motors, but the strongest evolution to robust motor dynamics only evolved in therian mammals. It is hypothesized that this motor system is significant to the therian cochlea at high frequencies because of the distinctive cellular and bony composition of the organ of Corti that allows the prestins to intensify movements of the whole structure.\nModern ultra-sound echolocating species such as bats and toothed whales show highly evolved prestins, and these prestins show identical sequence alterations over time. Unusually, the sequences thus apparently evolved independent from each other during different time periods. Furthermore, the evolution of neurotransmitter receptor systems (acetylcholine) that regulate the motor feedback of the outer hair cells coincides with prestin evolution in therians. This suggests that there was a parallel evolution of a control system and a motor system in the inner ear of therian mammals.\n\nLand vertebrates evolved middle ears independently in each major lineage, and are this the result of parallel evolution. The configurations of the middle ears of monotreme and therian mammals can thus be interpreted as convergent evolution or homoplasy. Thus evidence from fossils demonstrate homoplasies for the detachment of the ear from the jaw. Furthermore, it is apparent that the land-based eardrum, or tympanic membrane, and connecting structures such as the Eustachian tube evolved convergently in multiple different settings as opposed to being a defining morphology.\n"}
{"id": "4329772", "url": "https://en.wikipedia.org/wiki?curid=4329772", "title": "Exertion", "text": "Exertion\n\nExertion is the physical or perceived use of energy. Exertion traditionally connotes a strenuous or costly \"effort,\"resulting in generation of force, initiation of motion, or in the performance of work. It often relates to muscularactivity and can be quantified, empirically and by measurable metabolic response.\n\nIn physics, \"exertion\" is the expenditure of energy against, or inductive of, inertia as described by Isaac Newton's third law of motion. In physics, force exerted equivocates work done. The ability to do work can be either positive or negative depending on the direction of exertion relative to gravity. For example, a force exerted upwards, like lifting an object, creates positive work done on that object.\n\nExertion often results in force generated, a contributing dynamic of general motion. In mechanics it describes the use of force against a body in the direction of its motion (see vector).\n\nExertion, physiologically, can be described by the initiation of exercise, or, intensive and exhaustive physical activity that causes cardiovascular stress or a sympathetic nervous response. This can be continuous or intermittent exertion.\n\nExertion requires, of the body, modified oxygen uptake, increased heart rate, and autonomic monitoring of blood lactate concentrations. Mediators of physical exertion include cardio-respiratory and musculoskeletal strength, as well as metabolic capability. This often correlates to an output of force followed by a refractory period of recovery. Exertion is limited by cumulative load and repetitive motions.\n\nMuscular energy reserves, or stores for biomechanical exertion, stem from metabolic, immediate production of ATP and increased O2 consumption. Muscular exertion generated depends on the muscle length and the velocity at which it is able to shorten, or contract.\n\nPerceived exertion can be explained as subjective, perceived experience that mediates response to somatic sensations and mechanisms. A rating of perceived exertion, as measured by the \"RPE-scale\", or Borg scale, is a quantitative measure of physical exertion.\n\nOften in health, exertion of oneself resulting in cardiovascular stress showed reduced physiological responses, like cortisol levels and mood, to stressors. Therefore, biological exertion is effective in mediating psychological exertion, responsive to environmental stress.\n\nOverexertion causes more than 3.5 million injuries a year. An overexertion injury can include sprains or strains, the stretching and tear of ligaments, tendons, or muscles caused by a load that exceeds the human ability to perform the work. Overexertion, besides causing acute injury, implies physical exertion beyond the person's capacity which leads to symptoms such as dizziness, irregular breathing and heart rate, and fatigue. Preventative measures can be taken based on biomechanical knowledge to limit possible overexertion injuries.\n\n\n"}
{"id": "4014603", "url": "https://en.wikipedia.org/wiki?curid=4014603", "title": "Flying and gliding animals", "text": "Flying and gliding animals\n\nA number of animals have evolved aerial locomotion, either by powered flight or by gliding. Flying and gliding animals (\"volant\" animals) have evolved separately many times, without any single ancestor. Flight has evolved at least four times, in the insects, pterosaurs, birds, and bats. Gliding has evolved on many more occasions. Usually the development is to aid canopy animals in getting from tree to tree, although there are other possibilities. Gliding, in particular, has evolved among rainforest animals, especially in the rainforests in Asia (most especially Borneo) where the trees are tall and widely spaced. Several species of aquatic animals, and a few amphibians and reptiles have also evolved to acquire this gliding flight ability, typically as a means of evading predators.\n\nAnimal aerial locomotion can be divided into two categories—powered and unpowered. In unpowered modes of locomotion, the animal uses aerodynamics forces exerted on the body due to wind or falling through the air. In powered flight, the animal uses muscular power to generate aerodynamic forces. Animals using unpowered aerial locomotion cannot maintain altitude and speed due to unopposed drag, while animals using powered flight can maintain steady, level flight as long as their muscles are capable of doing so.\n\nThese modes of locomotion typically require an animal start from a raised location, converting that potential energy into kinetic energy and using aerodynamic forces to control trajectory and angle of descent. Energy is continually lost to drag without being replaced, thus these methods of locomotion have limited range and duration.\n\nPowered flight has evolved only four times (first in insects, then in pterosaurs, birds and bats). It uses muscular power to generate aerodynamic forces and to replace energy lost to drag.\n\nBallooning and soaring are not powered by muscle, but rather by external aerodynamic sources of energy: the wind and rising thermals, respectively. Both can continue as long as the source of external power is present. Soaring is typically only seen in species capable of powered flight, as it requires extremely large wings.\n\nMany species will use multiple of these modes at various times; a hawk will use powered flight to rise, then soar on thermals, then descend via free-fall to catch its prey.\n\nWhile gliding occurs independently from powered flight, it has some ecological advantages of its own. Gliding is a very energy-efficient way of travelling from tree to tree. An argument made is that many gliding animals eat low energy foods such as leaves and are restricted to gliding because of this, whereas flying animals eat more high energy foods such as fruits, nectar, and insects. In contrast to flight, gliding has evolved independently many times (more than a dozen times among extant vertebrates); however these groups have not radiated nearly as much as have groups of flying animals.\n\nWorldwide, the distribution of gliding animals is uneven as most inhabit rain forests in Southeast Asia. (Despite seemingly suitable rain forest habitats, few gliders are found in India or New Guinea and none in Madagascar.) Additionally, a variety of gliding vertebrates are found in Africa, a family of hylids (flying frogs) lives in South America and several species of gliding squirrels are found in the forests of northern Asia and North America. Various factors produce these disparities. In the forests of Southeast Asia, the dominant canopy trees (usually dipterocarps) are taller than the canopy trees of the other forests. A higher start provides a competitive advantage of further glides and farther travel. Gliding predators may more efficiently search for prey. The lower abundance of insect and small vertebrate prey for carnivorous animals (such as lizards) in Asian forests may be a factor. In Australia, many mammals (and all mammalian gliders) possess, to some extent, prehensile tails.\n\nPowered flight has evolved unambiguously only four times—birds, bats, pterosaurs, and insects. In contrast to gliding, which has evolved more frequently but typically gives rise to only a handful of species, all three extant groups of powered flyers have a huge number of species, suggesting that flight is a very successful strategy once evolved. Bats, after rodents, have the most species of any mammalian order, about 20% of all mammalian species. Birds have the most species of any class of terrestrial vertebrates. Finally, insects (most of which fly at some point in their life cycle) have more species than all other animal groups combined.\n\nThe evolution of flight is one of the most striking and demanding in animal evolution, and has attracted the attention of many prominent scientists and generated many theories. Additionally, because flying animals tend to be small and have a low mass (both of which increase the surface-area-to-mass ratio), they tend to fossilize infrequently and poorly compared to the larger, heavier-boned terrestrial species they share habitat with. Fossils of flying animals tend to be confined to exceptional fossil deposits formed under highly specific circumstances, resulting in a generally poor fossil record, and a particular lack of transitional forms. Furthermore, as fossils do not preserve behavior or muscle, it can be difficult to discriminate between a poor flyer and a good glider.\n\nInsects were the first to evolve flight, approximately 350 million years ago. The developmental origin of the insect wing remains in dispute, as does the purpose prior to true flight. One suggestion is that wings initially were used to catch the wind for small insects that live on the surface of the water, while another is that they functioned in parachuting, then gliding, then flight for originally arboreal insects.\n\nPterosaurs were the next to evolve flight, approximately 228 million years ago. These reptiles were close relatives of the dinosaurs (and sometimes mistakenly considered dinosaurs by laymen), and reached enormous sizes, with some of the last forms being the largest flying animals ever to inhabit the Earth, having wingspans of over 9.1 m (30 ft). However, they spanned a large range of sizes, down to a 250 mm (10 in) wingspan in \"Nemicolopterus\".\n\nBirds have an extensive fossil record, along with many forms documenting both their evolution from small theropod dinosaurs and the numerous bird-like forms of theropod which did not survive the mass extinction at the end of the Cretaceous. Indeed, \"Archaeopteryx\" is arguably the most famous transitional fossil in the world, both due to its mix of reptilian and avian anatomy and the luck of being discovered only two years after Darwin's publication of \"On the Origin of Species\". However, the ecology and this transition is considerably more contentious, with various scientists supporting either a \"trees down\" origin (in which an arboreal ancestor evolved gliding, then flight) or a \"ground up\" origin (in which a fast-running terrestrial ancestor used wings for a speed boost and to help catch prey).\n\nBats are the most recent to evolve (about 60 million years ago), most likely from a fluttering ancestor, though their poor fossil record has hindered more detailed study.\n\nOnly a few animals are known to have specialised in soaring: the larger of the extinct pterosaurs, and some large birds. Powered flight is very energetically expensive for large animals, but for soaring their size is an advantage, as it allows them a low wing loading, that is a large wing areas relative to their weight, which maximizes lift. Soaring is very energetically efficient.\n\nDuring a free-fall with no aerodynamic forces, the object accelerates due to gravity, resulting in increasing velocity as the object descends. During parachuting, animals use the aerodynamic forces on their body to counteract the force or gravity. Any object moving through air experiences a drag force that is proportion to surface area and to velocity squared, and this force will partially counter the force of gravity, slowing the animal's descent to a safer speed. If this drag is oriented at an angle to the vertical, the animal's trajectory will gradually become more horizontal, and it will cover horizontal as well as vertical distance. Smaller adjustments can allow turning or other maneuvers. This can allow a parachuting animal to move from a high location on one tree to a lower location on another tree nearby.\n\nDuring gliding, lift plays an increased role. Like drag, lift is proportional to velocity squared. Gliding animals will typically leap or drop from high locations such as trees, just as in parachuting, and as gravitational acceleration increases their speed, the aerodynamic forces also increase. Because the animal can utilize lift and drag to generate greater aerodynamic force, it can glide at a shallower angle than parachuting animals, allowing it to cover greater horizontal distance in the same loss of altitude, and reach trees further away.\n\nUnlike most air vehicles, in which the objects that generate lift (wings) and thrust (engine/propeller) are separate and the wings remained fixed, flying animals use their wings to generate both lift and thrust by moving them relative to the body. This has made the flight of organisms considerably harder to understand than that of vehicles, as it involves varying speeds, angles, orientations, areas, and flow patterns over the wings.\n\nA bird or bat flying through the air at a constant speed moves its wings up and down (usually with some fore-aft movement as well). Because the animal is in motion, there is some airflow relative to its body which, combined with the velocity of its wings, generates a faster airflow moving over the wing. This will generate lift force vector pointing forwards and upwards, and a drag force vector pointing rearwards and upwards. The upwards components of these counteract gravity, keeping the body in the air, while the forward component provides thrust to counteract both the drag from the wing and from the body as a whole. Pterosaur flight likely worked in a similar manner, though no living pterosaurs remain for study.\n\nInsect flight is considerably different, due to their small size, rigid wings, and other anatomical differences. Turbulence and vortices play a much larger role in insect flight, making it even more complex and difficult to study than the flight of vertebrates. There are two basic aerodynamic models of insect flight. Most insects use a method that creates a spiralling leading edge vortex. Some very small insects use the fling-and-clap or Weis-Fogh mechanism in which the wings clap together above the insect's body and then fling apart. As they fling open, the air gets sucked in and creates a vortex over each wing. This bound vortex then moves across the wing and, in the clap, acts as the starting vortex for the other wing. Circulation and lift are increased, at the price of wear and tear on the wings.\n\n\n\n\n\n\n\nGliding has evolved independently in two families of tree frogs, the Old World Rhacophoridae and the New World Hylidae. Within each lineage there are a range of gliding abilities from non-gliding, to parachuting, to full gliding.\nSeveral lizards and snakes are capable of gliding:\n\n\n\nBats are the only mammal with flapping or powered flight. A few other mammals glide or parachute; the best known are flying squirrels and flying lemurs.\n\n\n\n\n\n\n\n"}
{"id": "1686779", "url": "https://en.wikipedia.org/wiki?curid=1686779", "title": "Fusion energy gain factor", "text": "Fusion energy gain factor\n\nThe fusion energy gain factor, usually expressed with the symbol Q, is the ratio of fusion power produced in a nuclear fusion reactor to the power required to maintain the plasma in steady state. The condition of \"Q\" = 1, when the power being released by the fusion reactions is equal to the required heating power, is referred to as breakeven.\n\nThe power given off by the fusion reactions may be captured within the fuel, leading to \"self-heating\". Most fusion reactions release at least some of their energy in a form that cannot be captured within the plasma, so a system at \"Q\" = 1 will cool without external heating. With typical fuels, self-heating in fusion reactors is not expected to match the external sources until at least \"Q\" = 5. If \"Q\" increases past this point, increasing self-heating eventually removes the need for external heating. At this point the reaction becomes self-sustaining, a condition called ignition. Ignition corresponds to infinite \"Q\", and is generally regarded as highly desirable for a practical reactor design.\n\nOver time, several related terms have entered the fusion lexicon. As a reactor does not cover its own heating losses until about \"Q\" = 5, the term engineering breakeven is sometimes used to describe a reactor that produces enough electricity to provide that heating. Above engineering breakeven a machine would produce more electricity than it uses, and could sell that excess. A machine that can sell enough electricity to cover its operating costs, estimated to require at least \"Q\" = 20, is sometimes known as economic breakeven.\n\n, the record for \"Q\" is held by the JET tokamak in the UK, at \"Q\" = (16 MW)/(24 MW) ≈ 0.67, first attained in 1997. ITER was originally designed to reach ignition, but is currently designed to reach \"Q\" = 10, producing 500 MW of fusion power from 50 MW of injected thermal power.\n\n\"Q\" is simply the comparison of the power being released by the fusion reactions in a reactor, \"P\", to the constant heating power being supplied, \"P\". However, there are several definitions of breakeven that consider additional power losses.\n\nIn 1955, John Lawson was the first to explore the energy balance mechanisms in detail, initially in classified works but published openly in a now-famous 1957 paper. In this paper he considered and refined work by earlier researchers, notably Hans Thirring, Peter Thonemann, and a review article by Richard Post. Expanding on all of these, Lawson's paper made detailed predictions for the amount of power that would be lost through various mechanisms, and compared that to the energy needed to sustain the reaction. This balance is today known as the Lawson criterion.\n\nIn a successful fusion reactor design, the fusion reactions generate an amount of power designated \"P\". Some amount of this energy, \"P\", is lost through a variety of mechanisms, mostly convection of the fuel to the walls of the reactor chamber and various forms of radiation that cannot be captured to generate power. In order to keep the reaction going, the system has to provide heating to make up for these losses, where \"P\" = \"P\" to maintain thermal equilibrium.\n\nThe most basic definition of breakeven is when \"Q\" = 1, that is, \"P\" = \"P\".\n\nSome works refer to this definition as scientific breakeven, to contrast it with similar terms. However, this usage is rare outside certain areas, specifically the inertial confinement fusion field, where the term is much more widely used.\n\nSince the 1950s, most commercial fusion reactor designs have been based on a mix of deuterium and tritium as their primary fuel; others fuels have been studied for a variety of reasons but are much harder to ignite. As tritium is radioactive, highly bioactive and highly mobile, it represents a significant safety concern and adds to the cost of designing and operating such a reactor.\n\nIn order to lower costs, many experimental machines are designed to run on test fuels of hydrogen or deuterium alone, leaving out the tritium. In this case, the term extrapolated breakeven is used to define the expected performance of the machine running on D-T fuel based on the performance when running on hydrogen or deuterium alone.\n\nThe records for extrapolated breakeven are slightly higher than the records for scientific breakeven. Both JET and JT-60 have reached values around 1.25 (see below for details) while running on D-D fuel. When running on D-T, only possible in JET, the maximum performance is about half the extrapolated value.\n\nAnother related term, engineering breakeven, considers the need to extract the energy from the reactor, turn that into electrical energy, and feed that back into the heating system. This closed loop is known as \"recirculation\". In this case, the basic definition changes by adding additional terms to the \"P\" side to consider the efficiencies of these processes.\n\nMost fusion reactions release energy in a variety of forms, mostly neutrons and a variety of charged particles like alpha particles. Neutrons are electrically neutral and will travel out of any magnetic confinement fusion (MFE) design, and in spite of the very high densities found in inertial confinement fusion (ICF) designs, they tend to easily escape the fuel mass in these designs as well. This means that only the charged particles from the reactions can be captured within the fuel mass and give rise to self-heating. If the fraction of the energy being released in the charged particles is \"f\", then the power in these particles is \"P\" = \"f\"\"P\". If this self-heating process is perfect, that is, all of \"P\" is captured in the fuel, that means the power available for generating electricity is the power that is not released in that form, or (1 − \"f\")\"P\".\n\nIn the case of neutrons carrying most of the practical energy, as is the case in the D-T fuel studied in most designs, this neutron energy is normally captured in a \"blanket\" of lithium that produces more tritium that is used to fuel the reactor. Due to various exothermic and endothermic reactions, the blanket may have a power gain factor a few percent higher or lower than 100%, but that will be neglected here. The blanket is then cooled and the cooling fluid used in a heat exchanger driving conventional steam turbines. These have an efficiency η which is around 35 to 40%.\n\nConsider a system that uses external heaters to heat the fusion fuel, then extracts the power from those reactions to generate electrical power. Some fraction of that power, \"f\", is needed to recirculate back into the heaters to close the loop. This is not the same as the \"P\" because the self-heating processes are providing some of the required energy. While the system as a whole requires additional power for building climate control, lighting, and the confinement system, these are generally much smaller than the plasma heating system requirements.\nConsidering all of these factors, the heating power can thus be related to the fusion power by the following equation:\n\nformula_1\n\nwhere formula_2 is the efficiency that power supplied to the heating systems is turned into heat in the fuel, as opposed to lost in the equipment itself, and formula_3 is the efficiency achieved when turning the heat into electrical power, for instance, through the Rankine cycle.\n\nThe fusion energy gain factor is then defined as:\n\nformula_4\n\nAs the temperature of the plasma increases, the rate of fusion reactions grows rapidly, and with it, the rate of self heating. In contrast, the non-capturable energy losses like x-rays do not grow at the same rate. Thus, in overall terms, the self-heating process becomes more efficient as the temperature increases, and less energy is needed from external sources to keep it hot.\n\nEventually \"P\" reaches zero, that is, all of the energy needed to keep the plasma at the operational temperature is being supplied by self-heating, and the amount of external energy that needs to be added drops to zero. This point is known as ignition.\n\nIgnition, by definition, corresponds to an infinite \"Q\", but it does not mean that \"f\" drops to zero as the other power sinks in the system, like the magnets and cooling systems, still need to be powered. Generally, however, these are much smaller than the energy in the heaters, and require a much smaller \"f\". More importantly, this number is more likely to be near constant, meaning that further improvements in plasma performance will result in more energy that can be directly used for commercial generation, as opposed to recirculation.\n\nThe final definition of breakeven is commercial breakeven, which occurs when the economic value of any net energy left over after recirculation is enough to finance the construction of the reactor. This value depends both on the reactor and the spot price of electrical power.\n\nCommercial breakeven relies on factors outside the technology of the reactor itself, and it is possible that even a reactor with a fully ignited plasma will not generate enough energy to pay for itself. Whether any of the mainline concepts like ITER can reach this goal is being debated in the field.\n\nMost fusion reactor designs being studied are based on the D-T reaction, as this is by far the easiest to ignite, and is energy dense. However, this reaction also gives off most of its energy in the form of a single highly energetic neutron, and only 20% of the energy in the form of an alpha. Thus, for the D-T reaction, \"f\" = 0.2. This means that self-heating does not become equal to the external heating until at least \"Q\" = 5. \n\nEfficiency values depend on design details but may be in the range of η = 0.7 (70%) and η = 0.4 (40%). The purpose of a fusion reactor is to produce power, not to recirculate it, so a practical reactor must have \"f\" = 0.2 approximately. Lower would be better but will be hard to achieve. Using these values we find for a practical reactor \"Q\" = 22.\n\nMany early fusion devices operated for microseconds, using some sort of pulsed power source to feed their magnetic confinement system and used the confinement as the heating source. Lawson defined breakeven in this context as the total energy released by the entire reaction cycle compared to the total energy supplied to the machine during the same cycle.\n\nOver time, as performance increased by orders of magnitude, the reaction times have extended from microseconds to seconds, and in ITER, on the order of minutes. In this case definition of \"the entire reaction cycle\" becomes blurred. In the case of an ignited plasma, for instance, P may be quite high while the system is being set up, and then drop to zero when it is fully developed, so one may be tempted to pick an instant in time when it is operating at its best to determine \"Q\". A better solution in these cases is to use the original Lawson definition averaged over the reaction to produce a similar value as the original definition.\n\nHowever, there is a complication. During the heating phase when the system is being brought up to operational conditions, some of the energy released by the fusion reactions will be used to heat the surrounding fuel, and thus not be released. This is no longer true when the plasma reaches its operational temperature and enters thermal equilibrium. Thus, if one averages over the entire cycle, this energy will be included as part of the heating term, that is, some of the energy that was captured for heating would otherwise have been released in P and is therefore not indicative of an operational \"Q\".\n\nOperators of the JET reactor argued that this input should be removed from the total:\n\nformula_5\n\nwhere:\n\nformula_6\n\nThat is, P is the amount of energy needed to raise the internal energy of the plasma. It is this definition that was used when reporting JET's record 0.67 value.\n\nSome debate over this definition continues. In 1998, the operators of the JT-60 claimed to have reached \"Q\" = 1.25 running on D-D fuel, thus reaching extrapolated breakeven. However, this measurement was based on the JET definition of Q*. Using this definition, JET had also reached extrapolated breakeven some time earlier. If one considers the energy balance in these conditions, and the analysis of previous machines, it is argued the original definition should be used, and thus both machines remain well below break-even of any sort.\n\nAlthough most fusion experiments use some form of magnetic confinement, another major branch is inertial confinement fusion (ICF) that mechanically presses together the fuel mass (the \"target\") to increase its density. This greatly increases the rate of fusion events and lowers the need to confine the fuel for long periods. This compression is accomplished by heating a lightweight capsule holding the fuel so rapidly that it explodes outwards, driving the fuel mass on the inside inward in accordance with Newton's third law. There are a variety of proposed \"drivers\" to cause the implosion process, but to date most experiments have used lasers. \n\nUsing the traditional definition of \"Q\", \"P\" / \"P\", ICF devices have extremely low \"Q\". This is because the laser is extremely inefficient; whereas formula_2 for the heaters used in magnetic systems might be on the order of 70%, lasers are on the order of 1.5%. For this reason, Lawrence Livermore National Laboratory (LLNL), the leader in ICF research, has proposed another modification of \"Q\" that defines \"P\" as the energy delivered by the driver, as opposed to the energy put into the driver. This definition produces much higher \"Q\" values, and changes the definition of breakeven to be \"P\" / \"P\" = 1. On occasion, they referred to this definition as \"scientific breakeven\". This term was not universally used, other groups adopted the redefinition of \"Q\" but continued to refer to \"P\" = \"P\" simply as breakeven. \n\nOn 7 October 2013, the BBC announced that LLNL had achieved scientific breakeven in the National Ignition Facility (NIF) on 29 September. In this experiment, \"P\" was approximately 14 kJ, while the laser output was 1.8 MJ. By their previous definition, this would be a \"Q\" of 0.0077. However, for this press release, they re-defined \"Q\" once again, this time equating \"P\" to be only the amount energy delivered to \"the hottest portion of the fuel\", calculating that only 10 kJ of the original laser energy reached the part of the fuel that was undergoing fusion reactions. This release has been heavily criticized in the field.\n\n"}
{"id": "24179592", "url": "https://en.wikipedia.org/wiki?curid=24179592", "title": "Future of Earth", "text": "Future of Earth\n\nThe biological and geological future of Earth can be extrapolated based upon the estimated effects of several long-term influences. These include the chemistry at Earth's surface, the rate of cooling of the planet's interior, the gravitational interactions with other objects in the Solar System, and a steady increase in the Sun's luminosity. An uncertain factor in this extrapolation is the ongoing influence of technology introduced by humans, such as climate engineering, which could cause significant changes to the planet. The current Holocene extinction is being caused by technology and the effects may last for up to five million years. In turn, technology may result in the extinction of humanity, leaving the planet to gradually return to a slower evolutionary pace resulting solely from long-term natural processes.\n\nOver time intervals of hundreds of millions of years, random celestial events pose a global risk to the biosphere, which can result in mass extinctions. These include impacts by comets or asteroids, and the possibility of a massive stellar explosion, called a supernova, within a 100-light-year radius of the Sun. Other large-scale geological events are more predictable. Milankovitch theory predicts that the planet will continue to undergo glacial periods at least until the Quaternary glaciation comes to an end. These periods are caused by variations in eccentricity, axial tilt, and precession of the Earth's orbit. As part of the ongoing supercontinent cycle, plate tectonics will probably result in a supercontinent in 250–350 million years. Some time in the next 1.5–4.5 billion years, the axial tilt of the Earth may begin to undergo chaotic variations, with changes in the axial tilt of up to 90°.\n\nThe luminosity of the Sun will steadily increase, resulting in a rise in the solar radiation reaching the Earth. This will result in a higher rate of weathering of silicate minerals, which will cause a decrease in the level of carbon dioxide in the atmosphere. In about 600 million years from now, the level of carbon dioxide will fall below the level needed to sustain C carbon fixation photosynthesis used by trees. Some plants use the C carbon fixation method, allowing them to persist at carbon dioxide concentrations as low as 10 parts per million. However, the long-term trend is for plant life to die off altogether. The extinction of plants will be the demise of almost all animal life, since plants are the base of the food chain on Earth.\n\nIn about one billion years, the solar luminosity will be 10% higher than at present. This will cause the atmosphere to become a \"moist greenhouse\", resulting in a runaway evaporation of the oceans. As a likely consequence, plate tectonics will come to an end, and with them the entire carbon cycle. Following this event, in about 2−3 billion years, the planet's magnetic dynamo may cease, causing the magnetosphere to decay and leading to an accelerated loss of volatiles from the outer atmosphere. Four billion years from now, the increase in the Earth's surface temperature will cause a runaway greenhouse effect, heating the surface enough to melt it. By that point, all life on the Earth will be extinct. The most probable fate of the planet is absorption by the Sun in about 7.5 billion years, after the star has entered the red giant phase and expanded beyond the planet's current orbit.\n\nHumans play a key role in the biosphere, with the large human population dominating many of Earth's ecosystems. This has resulted in a widespread, ongoing mass extinction of other species during the present geological epoch, now known as the Holocene extinction. The large-scale loss of species caused by human influence since the 1950s has been called a biotic crisis, with an estimated 10% of the total species lost as of 2007. At current rates, about 30% of species are at risk of extinction in the next hundred years. The Holocene extinction event is the result of habitat destruction, the widespread distribution of invasive species, hunting, and climate change. In the present day, human activity has had a significant impact on the surface of the planet. More than a third of the land surface has been modified by human actions, and humans use about 20% of global primary production. The concentration of carbon dioxide in the atmosphere has increased by close to 30% since the start of the Industrial Revolution.\n\nThe consequences of a persistent biotic crisis have been predicted to last for at least five million years. It could result in a decline in biodiversity and homogenization of biotas, accompanied by a proliferation of species that are opportunistic, such as pests and weeds. Novel species may also emerge; in particular taxa that prosper in human-dominated ecosystems may rapidly diversify into many new species. Microbes are likely to benefit from the increase in nutrient-enriched environmental niches. No new species of existing large vertebrates are likely to arise and food chains will probably be shortened.\n\nThere are multiple scenarios for known risks that can have a global impact on the planet. From the perspective of humanity, these can be subdivided into survivable risks and terminal risks. Risks that humanity pose to itself include climate change, the misuse of nanotechnology, a nuclear holocaust, warfare with a programmed superintelligence, a genetically engineered disease, or a disaster caused by a physics experiment. Similarly, several natural events may pose a doomsday threat, including a highly virulent disease, the impact of an asteroid or comet, runaway greenhouse effect, and resource depletion. There may also be the possibility of an infestation by an extraterrestrial lifeform. The actual odds of these scenarios occurring are difficult if not impossible to deduce.\n\nShould the human race become extinct, then the various features assembled by humanity will begin to decay. The largest structures have an estimated decay half-life of about 1,000 years. The last surviving structures would most likely be open pit mines, large landfills, major highways, wide canal cuts, and earth-fill flank dams. A few massive stone monuments like the pyramids at the Giza Necropolis or the sculptures at Mount Rushmore may still survive in some form after a million years.\n\nAs the Sun orbits the Milky Way, wandering stars may approach close enough to have a disruptive influence on the Solar System. A close stellar encounter may cause a significant reduction in the perihelion distances of comets in the Oort cloud—a spherical region of icy bodies orbiting within half a light year of the Sun. Such an encounter can trigger a 40-fold increase in the number of comets reaching the inner Solar System. Impacts from these comets can trigger a mass extinction of life on Earth. These disruptive encounters occur at an average of once every 45 million years. The mean time for the Sun to collide with another star in the solar neighborhood is approximately , which is much longer than the estimated age of the Milky Way galaxy, at . This can be taken as an indication of the low likelihood of such an event occurring during the lifetime of the Earth.\n\nThe energy release from the impact of an asteroid or comet with a diameter of or larger is sufficient to create a global environmental disaster and cause a statistically significant increase in the number of species extinctions. Among the deleterious effects resulting from a major impact event is a cloud of fine dust ejecta blanketing the planet, blocking some direct sunlight from reaching the Earth's surface thus lowering land temperatures by about within a week and halting photosynthesis for several months (similar to a nuclear winter). The mean time between major impacts is estimated to be at least 100 million years. During the last 540 million years, simulations demonstrated that such an impact rate is sufficient to cause 5–6 mass extinctions and 20–30 lower severity events. This matches the geologic record of significant extinctions during the Phanerozoic Eon. Such events can be expected to continue into the future.\n\nA supernova is a cataclysmic explosion of a star. Within the Milky Way galaxy, supernova explosions occur on average once every 40 years. During the history of the Earth, multiple such events have likely occurred within a distance of 100 light years; known as a near-Earth supernova. Explosions inside this distance can contaminate the planet with radioisotopes and possibly impact the biosphere. Gamma rays emitted by a supernova react with nitrogen in the atmosphere, producing nitrous oxides. These molecules cause a depletion of the ozone layer that protects the surface from ultraviolet (UV) radiation from the Sun. An increase in UV-B radiation of only 10–30% is sufficient to cause a significant impact to life; particularly to the phytoplankton that form the base of the oceanic food chain. A supernova explosion at a distance of 26 light years will reduce the ozone column density by half. On average, a supernova explosion occurs within 32 light years once every few hundred million years, resulting in a depletion of the ozone layer lasting several centuries. Over the next two billion years, there will be about 20 supernova explosions and one gamma ray burst that will have a significant impact on the planet's biosphere.\n\nThe incremental effect of gravitational perturbations between the planets causes the inner Solar System as a whole to behave chaotically over long time periods. This does not significantly affect the stability of the Solar System over intervals of a few million years or less, but over billions of years the orbits of the planets become unpredictable. Computer simulations of the Solar System's evolution over the next five billion years suggest that there is a small (less than 1%) chance that a collision could occur between Earth and either Mercury, Venus, or Mars. During the same interval, the odds that the Earth will be scattered out of the Solar System by a passing star are on the order of one part in 10. In such a scenario, the oceans would freeze solid within several million years, leaving only a few pockets of liquid water about underground. There is a remote chance that the Earth will instead be captured by a passing binary star system, allowing the planet's biosphere to remain intact. The odds of this happening are about one chance in three million.\n\nThe gravitational perturbations of the other planets in the Solar System combine to modify the orbit of the Earth and the orientation of its spin axis. These changes can influence the planetary climate. Despite such interactions, highly accurate simulations show that overall, Earth's orbit is likely to remain dynamically stable for billions of years into the future. In all 1,600 simulations, the planet's semimajor axis, eccentricity, and inclination remained nearly constant.\n\nHistorically, there have been cyclical ice ages in which glacial sheets periodically covered the higher latitudes of the continents. Ice ages may occur because of changes in ocean circulation and continentality induced by plate tectonics. The Milankovitch theory predicts that glacial periods occur during ice ages because of astronomical factors in combination with climate feedback mechanisms. The primary astronomical drivers are a higher than normal orbital eccentricity, a low axial tilt (or obliquity), and the alignment of summer solstice with the aphelion. Each of these effects occur cyclically. For example, the eccentricity changes over time cycles of about 100,000 and 400,000 years, with the value ranging from less than 0.01 up to 0.05. This is equivalent to a change of the semiminor axis of the planet's orbit from 99.95% of the semimajor axis to 99.88%, respectively.\n\nThe Earth is passing through an ice age known as the quaternary glaciation, and is presently in the Holocene interglacial period. This period would normally be expected to end in about 25,000 years. However, the increased rate of carbon dioxide release into the atmosphere by humans may delay the onset of the next glacial period until at least 50,000–130,000 years from now. On the other hand, a global warming period of finite duration (based on the assumption that fossil fuel use will cease by the year 2200) will probably only impact the glacial period for about 5,000 years. Thus, a brief period of global warming induced through a few centuries worth of greenhouse gas emission would only have a limited impact in the long term.\n\nThe tidal acceleration of the Moon slows the rotation rate of the Earth and increases the Earth-Moon distance. Friction effects—between the core and mantle and between the atmosphere and surface—can dissipate the Earth's rotational energy. These combined effects are expected to increase the length of the day by more than 1.5 hours over the next 250 million years, and to increase the obliquity by about a half degree. The distance to the Moon will increase by about 1.5 Earth radii during the same period.\n\nBased on computer models, the presence of the Moon appears to stabilize the obliquity of the Earth, which may help the planet to avoid dramatic climate changes. This stability is achieved because the Moon increases the precession rate of the Earth's spin axis, thereby avoiding resonances between the precession of the spin and precession of the planet's orbital plane (that is, the precession motion of the ecliptic). However, as the semimajor axis of the Moon's orbit continues to increase, this stabilizing effect will diminish. At some point, perturbation effects will probably cause chaotic variations in the obliquity of the Earth, and the axial tilt may change by angles as high as 90° from the plane of the orbit. This is expected to occur between 1.5 and 4.5 billion years from now.\n\nA high obliquity would probably result in dramatic changes in the climate and may destroy the planet's habitability. When the axial tilt of the Earth exceeds 54°, the yearly insolation at the equator is less than that at the poles. The planet could remain at an obliquity of 60° to 90° for periods as long as 10 million years.\n\nTectonics-based events will continue to occur well into the future and the surface will be steadily reshaped by tectonic uplift, extrusions, and erosion. Mount Vesuvius can be expected to erupt about 40 times over the next 1,000 years. During the same period, about five to seven earthquakes of magnitude 8 or greater should occur along the San Andreas Fault, while about 50 magnitude 9 events may be expected worldwide. Mauna Loa should experience about 200 eruptions over the next 1,000 years, and the Old Faithful Geyser will likely cease to operate. The Niagara Falls will continue to retreat upstream, reaching Buffalo in about 30,000–50,000 years.\n\nIn 10,000 years, the post-glacial rebound of the Baltic Sea will have reduced the depth by about . The Hudson Bay will decrease in depth by 100 m over the same period. After 100,000 years, the island of Hawaii will have shifted about to the northwest. The planet may be entering another glacial period by this time.\n\nThe theory of plate tectonics demonstrates that the continents of the Earth are moving across the surface at the rate of a few centimeters per year. This is expected to continue, causing the plates to relocate and collide. Continental drift is facilitated by two factors: the energy generation within the planet and the presence of a hydrosphere. With the loss of either of these, continental drift will come to a halt. The production of heat through radiogenic processes is sufficient to maintain mantle convection and plate subduction for at least the next 1.1 billion years.\n\nAt present, the continents of North and South America are moving westward from Africa and Europe. Researchers have produced several scenarios about how this will continue in the future. These geodynamic models can be distinguished by the subduction flux, whereby the oceanic crust moves under a continent. In the introversion model, the younger, interior, Atlantic ocean becomes preferentially subducted and the current migration of North and South America is reversed. In the extroversion model, the older, exterior, Pacific ocean remains preferentially subducted and North and South America migrate toward eastern Asia.\n\nAs the understanding of geodynamics improves, these models will be subject to revision. In 2008, for example, a computer simulation was used to predict that a reorganization of the mantle convection will occur over the next 100 million years, causing a supercontinent composed of Africa, Eurasia, Australia, Antarctica and South America to form around Antarctica.\n\nRegardless of the outcome of the continental migration, the continued subduction process causes water to be transported to the mantle. After a billion years from the present, a geophysical model gives an estimate that 27% of the current ocean mass will have been subducted. If this process were to continue unmodified into the future, the subduction and release would reach an equilibrium after 65% of the current ocean mass has been subducted.\n\nChristopher Scotese and his colleagues have mapped out the predicted motions several hundred million years into the future as part of the Paleomap Project. In their scenario, 50 million years from now the Mediterranean sea may vanish and the collision between Europe and Africa will create a long mountain range extending to the current location of the Persian Gulf. Australia will merge with Indonesia, and Baja California will slide northward along the coast. New subduction zones may appear off the eastern coast of North and South America, and mountain chains will form along those coastlines. To the south, the migration of Antarctica to the north will cause all of its ice sheets to melt. This, along with the melting of the Greenland ice sheets, will raise the average ocean level by . The inland flooding of the continents will result in climate changes.\n\nAs this scenario continues, by 100 million years from the present the continental spreading will have reached its maximum extent and the continents will then begin to coalesce. In 250 million years, North America will collide with Africa while South America will wrap around the southern tip of Africa. The result will be the formation of a new supercontinent (sometimes called Pangaea Ultima), with the Pacific Ocean stretching across half the planet. The continent of Antarctica will reverse direction and return to the South Pole, building up a new ice cap.\n\nThe first scientist to extrapolate the current motions of the continents was Canadian geologist Paul F. Hoffman of Harvard University. In 1992, Hoffman predicted that the continents of North and South America would continue to advance across the Pacific Ocean, pivoting about Siberia until they begin to merge with Asia. He dubbed the resulting supercontinent, Amasia. Later, in the 1990s, Roy Livermore calculated a similar scenario. He predicted that Antarctica would start to migrate northward, and east Africa and Madagascar would move across the Indian Ocean to collide with Asia.\n\nIn an extroversion model, the closure of the Pacific Ocean would be complete in about 350 million years. This marks the completion of the current supercontinent cycle, wherein the continents split apart and then rejoin each other about every 400–500 million years. Once the supercontinent is built, plate tectonics may enter a period of inactivity as the rate of subduction drops by an order of magnitude. This period of stability could cause an increase in the mantle temperature at the rate of every 100 million years, which is the minimum lifetime of past supercontinents. As a consequence, volcanic activity may increase.\n\nThe formation of a supercontinent can dramatically affect the environment. The collision of plates will result in mountain building, thereby shifting weather patterns. Sea levels may fall because of increased glaciation. The rate of surface weathering can rise, resulting in an increase in the rate that organic material is buried. Supercontinents can cause a drop in global temperatures and an increase in atmospheric oxygen. This, in turn, can affect the climate, further lowering temperatures. All of these changes can result in more rapid biological evolution as new niches emerge.\n\nThe formation of a supercontinent insulates the mantle. The flow of heat will be concentrated, resulting in volcanism and the flooding of large areas with basalt. Rifts will form and the supercontinent will split up once more. The planet may then experience a warming period, as occurred during the Cretaceous period.\n\nThe iron-rich core region of the Earth is divided into a radius solid inner core and a radius liquid outer core. The rotation of the Earth creates convective eddies in the outer core region that cause it to function as a dynamo. This generates a magnetosphere about the Earth that deflects particles from the solar wind, which prevents significant erosion of the atmosphere from sputtering. As heat from the core is transferred outward toward the mantle, the net trend is for the inner boundary of the liquid outer core region to freeze, thereby releasing thermal energy and causing the solid inner core to grow. This iron crystallization process has been ongoing for about a billion years. In the modern era, the radius of the inner core is expanding at an average rate of roughly per year, at the expense of the outer core. Nearly all of the energy needed to power the dynamo is being supplied by this process of inner core formation.\n\nThe growth of the inner core may be expected to consume most of the outer core by some 3–4 billion years from now, resulting in a nearly solid core composed of iron and other heavy elements. The surviving liquid envelope will mainly consist of lighter elements that will undergo less mixing. Alternatively, if at some point plate tectonics comes to an end, the interior will cool less efficiently, which may end the growth of the inner core. In either case, this can result in the loss of the magnetic dynamo. Without a functioning dynamo, the magnetic field of the Earth will decay in a geologically short time period of roughly 10,000 years. The loss of the magnetosphere will cause an increase in erosion of light elements, particularly hydrogen, from the Earth's outer atmosphere into space, resulting in less favorable conditions for life.\n\nThe energy generation of the Sun is based upon thermonuclear fusion of hydrogen into helium. This occurs in the core region of the star using the proton–proton chain reaction process. Because there is no convection in the solar core, the helium concentration builds up in that region without being distributed throughout the star. The temperature at the core of the Sun is too low for nuclear fusion of helium atoms through the triple-alpha process, so these atoms do not contribute to the net energy generation that is needed to maintain hydrostatic equilibrium of the Sun.\n\nAt present, nearly half the hydrogen at the core has been consumed, with the remainder of the atoms consisting primarily of helium. As the number of hydrogen atoms per unit mass decreases, so too does their energy output provided through nuclear fusion. This results in a decrease in pressure support, which causes the core to contract until the increased density and temperature bring the core pressure into equilibrium with the layers above. The higher temperature causes the remaining hydrogen to undergo fusion at a more rapid rate, thereby generating the energy needed to maintain the equilibrium.\nThe result of this process has been a steady increase in the energy output of the Sun. When the Sun first became a main sequence star, it radiated only 70% of the current luminosity. The luminosity has increased in a nearly linear fashion to the present, rising by 1% every 110 million years. Likewise, in three billion years the Sun is expected to be 33% more luminous. The hydrogen fuel at the core will finally be exhausted in five billion years, when the Sun will be 67% more luminous than at present. Thereafter the Sun will continue to burn hydrogen in a shell surrounding its core, until the luminosity reaches 121% above the present value. This marks the end of the Sun's main sequence lifetime, and thereafter it will pass through the subgiant stage and evolve into a red giant.\n\nBy this time, the collision of the Milky Way and Andromeda galaxies should be underway. Although this could result in the Solar System being ejected from the newly combined galaxy, it is considered unlikely to have any adverse effect on the Sun or its planets.\n\nThe rate of weathering of silicate minerals will increase as rising temperatures speed up chemical processes. This in turn will decrease the level of carbon dioxide in the atmosphere, as these weathering processes convert carbon dioxide gas into solid carbonates. Within the next 600 million years from the present, the concentration of carbon dioxide will fall below the critical threshold needed to sustain C photosynthesis: about 50 parts per million. At this point, trees and forests in their current forms will no longer be able to survive, the last living trees being evergreen conifers. However, C carbon fixation can continue at much lower concentrations, down to above 10 parts per million. Thus plants using photosynthesis may be able to survive for at least 0.8 billion years and possibly as long as 1.2 billion years from now, after which rising temperatures will make the biosphere unsustainable. Currently, plants represent about 5% of Earth's plant biomass and 1% of its known plant species. For example, about 50% of all grass species (Poaceae) use the photosynthetic pathway, as do many species in the herbaceous family Amaranthaceae.\n\nWhen the levels of carbon dioxide fall to the limit where photosynthesis is barely sustainable, the proportion of carbon dioxide in the atmosphere is expected to oscillate up and down. This will allow land vegetation to flourish each time the level of carbon dioxide rises due to tectonic activity and respiration from animal life. However, the long term trend is for the plant life on land to die off altogether as most of the remaining carbon in the atmosphere becomes sequestered in the Earth. Some microbes are capable of photosynthesis at concentrations of carbon dioxide of a few parts per million, so these life forms would probably disappear only because of rising temperatures and the loss of the biosphere.\n\nPlants—and, by extension, animals—could survive longer by evolving other strategies such as requiring less carbon dioxide for photosynthetic processes, becoming carnivorous, adapting to desiccation, or associating with fungi. These adaptations are likely to appear near the beginning of the moist greenhouse (see further).\n\nThe loss of plant life will also result in the eventual loss of oxygen as well as ozone due to the respiration of animals, chemical reactions in the atmosphere, and volcanic eruptions. This will result in less attenuation of DNA-damaging UV, as well as the death of animals; the first animals to disappear would be large mammals, followed by small mammals, birds, amphibians and large fish, reptiles and small fish, and finally invertebrates. Before this happens, it's expected that life would concentrate at refugia of lower temperature such as high elevations where less land surface area is available, thus restricting population sizes. Smaller animals would survive better than larger ones because of lesser oxygen requirements, while birds would fare better than mammals thanks to their ability to travel large distances looking for colder temperatures.\n\nIn their work \"The Life and Death of Planet Earth\", authors Peter D. Ward and Donald Brownlee have argued that some form of animal life may continue even after most of the Earth's plant life has disappeared. Ward and Brownlee use fossil evidence from the Burgess Shale in British Columbia, Canada, to determine the climate of the Cambrian Explosion, and use it to predict the climate of the future when rising global temperatures caused by a warming Sun and declining oxygen levels result in the final extinction of animal life. Initially, they expect that some insects, lizards, birds and small mammals may persist, along with sea life. However, without oxygen replenishment by plant life, they believe that animals would probably die off from asphyxiation within a few million years. Even if sufficient oxygen were to remain in the atmosphere through the persistence of some form of photosynthesis, the steady rise in global temperature would result in a gradual loss of biodiversity.\n\nAs temperatures continue to rise, the last of animal life will be driven toward the poles, and possibly underground. They would become primarily active during the polar night, aestivating during the polar day due to the intense heat. Much of the surface would become a barren desert and life would primarily be found in the oceans. However, due to a decrease in the amount of organic matter entering the oceans from land as well as a decrease in dissolved oxygen, sea life would disappear too following a similar path to that on Earth's surface. This process would start with the loss of freshwater species and conclude with invertebrates, particularly those that do not depend on living plants such as termites or those near hydrothermal vents such as worms of the genus \"Riftia\". As a result of these processes, multicellular life forms may be extinct in about 800 million years, and eukaryotes in 1.3 billion years, leaving only the prokaryotes.\n\nOne billion years from now, about 27% of the modern ocean will have been subducted into the mantle. If this process were allowed to continue uninterrupted, it would reach an equilibrium state where 65% of the current surface reservoir would remain at the surface. Once the solar luminosity is 10% higher than its current value, the average global surface temperature will rise to . The atmosphere will become a \"moist greenhouse\" leading to a runaway evaporation of the oceans. At this point, models of the Earth's future environment demonstrate that the stratosphere would contain increasing levels of water. These water molecules will be broken down through photodissociation by solar UV, allowing hydrogen to escape the atmosphere. The net result would be a loss of the world's sea water by about 1.1 billion years from the present. This will be a simple dramatic step in annihilating all life on Earth.\n\nThere will be two variations of this future warming feedback: the \"moist greenhouse\" where water vapor dominates the troposphere while water vapor starts to accumulate in the stratosphere (if the oceans evaporate very quickly), and the \"runaway greenhouse\" where water vapor becomes a dominant component of the atmosphere (if the oceans evaporate too slowly). The Earth will undergo rapid warming that could send its surface temperature to over as the atmosphere will be totally overwhelmed by water vapor, causing its entire surface to melt and killing all life, perhaps in about 3 billion years. In this ocean-free era, there will continue to be surface reservoirs as water is steadily released from the deep crust and mantle, where it is estimated that there is an amount of water equivalent to several times that currently present in the Earth's oceans. Some water may be retained at the poles and there may be occasional rain storms, but for the most part the planet would be a dry desert with large dunefields covering its equator, and a few salt flats on what was once the ocean floor, similar to the ones in the Atacama Desert in Chile.\n\nWith no water to serve as a lubricant, plate tectonics would very likely stop and the most visible signs of geological activity would be shield volcanoes located above mantle hotspots. In these arid conditions the planet may retain some microbial and possibly even multicellular life. Most of these microbes will be halophiles and life could find refuge in the atmosphere as has been proposed to have happened on Venus. However, the increasingly extreme conditions will likely lead to the extinction of the prokaryotes between 1.6 billion years and 2.8 billion years from now, with the last of them living in residual ponds of water at high latitudes and heights or in caverns with trapped ice. However, underground life could last longer. What proceeds this depends on the level of tectonic activity. A steady release of carbon dioxide by volcanic eruption could cause the atmosphere to enter a \"super-greenhouse\" state like that of the planet Venus. But, as stated above, without surface water, plate tectonics would probably come to a halt and most of the carbonates would remain securely buried until the Sun becomes a red giant and its increased luminosity heated the rock to the point of releasing the carbon dioxide.\n\nThe loss of the oceans could be delayed until 2 billion years in the future if the atmospheric pressure were to decline. A lower atmospheric pressure would reduce the greenhouse effect, thereby lowering the surface temperature. This could occur if natural processes were to remove the nitrogen from the atmosphere. Studies of organic sediments has shown that at least of nitrogen has been removed from the atmosphere over the past four billion years; enough to effectively double the current atmospheric pressure if it were to be released. This rate of removal would be sufficient to counter the effects of increasing solar luminosity for the next two billion years.\n\nBy 2.8 billion years from now, the surface temperature of the Earth will have reached , even at the poles. At this point, any remaining life will be extinguished due to the extreme conditions. If the Earth loses its surface water by this point, the planet will stay in the same conditions until the Sun becomes a red giant. If this scenario does not happen, then in about 3–4 billion years the amount of water vapour in the lower atmosphere will rise to 40% and a \"moist greenhouse\" effect will commence once the luminosity from the Sun reaches 35–40% more than its present-day value. A \"runaway greenhouse\" effect will ensue, causing the atmosphere to heat up and raising the surface temperature to around . This is sufficient to melt the surface of the planet. However, most of the atmosphere will be retained until the Sun has entered the red giant stage.\n\nWith the extinction of life, 2.8 billion years from now it is also expected that Earth biosignatures will disappear, to be replaced by signatures caused by non-biological processes.\n\nOnce the Sun changes from burning hydrogen at its core to burning hydrogen around its shell, the core will start to contract and the outer envelope will expand. The total luminosity will steadily increase over the following billion years until it reaches 2,730 times the Sun's current luminosity at the age of 12.167 billion years. Most of Earth's atmosphere will be lost to space and its surface will consist of a lava ocean with floating continents of metals and metal oxides as well as icebergs of refractory materials, with its surface temperature reaching more than . The Sun will experience more rapid mass loss, with about 33% of its total mass shed with the solar wind. The loss of mass will mean that the orbits of the planets will expand. The orbital distance of the Earth will increase to at most 150% of its current value.\n\nThe most rapid part of the Sun's expansion into a red giant will occur during the final stages, when the Sun will be about 12 billion years old. It is likely to expand to swallow both Mercury and Venus, reaching a maximum radius of . The Earth will interact tidally with the Sun's outer atmosphere, which would serve to decrease Earth's orbital radius. Drag from the chromosphere of the Sun would also reduce the Earth's orbit. These effects will act to counterbalance the effect of mass loss by the Sun, and the Earth will probably be engulfed by the Sun.\n\nThe drag from the solar atmosphere may cause the orbit of the Moon to decay. Once the orbit of the Moon closes to a distance of , it will cross the Earth's Roche limit. This means that tidal interaction with the Earth would break apart the Moon, turning it into a ring system. Most of the orbiting ring will then begin to decay, and the debris will impact the Earth. Hence, even if the Earth is not swallowed up by the Sun, the planet may be left moonless. The ablation and vaporization caused by its fall on a decaying trajectory towards the Sun may remove Earth's mantle, leaving just its core, which will finally be destroyed after at most 200 years. Following this event, Earth's sole legacy will be a very slight increase (0.01%) of the solar metallicity.\n\nAfter fusing helium in its core to carbon, the Sun will begin to collapse again, evolving into a compact white dwarf star after ejecting its outer atmosphere as a planetary nebula. In 50 billion years, if the Earth and Moon are not engulfed by the Sun, they will become tidelocked, with each showing only one face to the other. Thereafter, the tidal action of the Sun will extract angular momentum from the system, causing the lunar orbit to decay and the Earth's spin to accelerate.\n\nIn about 65 billion years, it is estimated that the Moon may end up colliding with the Earth, assuming they are not destroyed by the red giant Sun, due to the remaining energy of the Earth–Moon system being sapped by the remnant Sun, causing the Moon to slowly move inwards toward the Earth.\n\nIf Earth is not destroyed by the expanding red giant Sun in 7.6 billion years, then on a time scale of 10 (10 quintillion) years the remaining planets in the Solar System will be ejected from the system by violent relaxation. If this does not occur to the Earth, the ultimate fate of the planet will be that it collides with the black dwarf Sun due to the decay of its orbit via gravitational radiation, in 10 years. On a time scale of 10 years, the Sun will collide with another black dwarf, producing a Type Ia supernova explosion that destroys both objects.\n\n\n"}
{"id": "23793839", "url": "https://en.wikipedia.org/wiki?curid=23793839", "title": "Global Earthquake Model", "text": "Global Earthquake Model\n\nThe Global Earthquake Model (GEM) is a public–private partnership initiated in 2006 by the Global Science Forum of the OECD to develop global, open-source risk assessment software and tools. With committed backing from academia, governments and industry, GEM contributes to achieving profound, lasting reductions in earthquake risk worldwide by following the priorities of the Hyogo Framework for Action. From 2009 to 2013 GEM is constructing its first working global earthquake model and will provide an authoritative standard for calculating and communicating earthquake risk worldwide.\n\nSince March 2009, GEM is a legal entity in the form of a non-profit foundation based in Pavia, Italy. The GEM Secretariat is hosted at the European Centre for Training and Research in Earthquake Engineering (EUCENTRE). The current secretary general is John Schneider.\n\nBetween 2000 and 2010 over half a million people died due to earthquakes and tsunamis, most of these in the developing world, where risks increase due to rapid population growth and urbanization. However, in many earthquake-prone regions no risk models exist, and even where models do exist, they are inaccessible. Better risk-awareness can reduce the toll that earthquakes take by leading to better construction, improved emergency response, and greater access to insurance.\n\nGEM will provide a basis for comparing earthquake risks across regions and across borders, and thereby take the necessary first step towards increased awareness and actions that reduce earthquake risk. GEM tools will be usable at the community, national and international level for uniform earthquake risk-evaluation and as a defensible basis for risk-mitigation plans. GEM results will be disseminated all over the world. GEM will build technical capacity and carry out awareness-raising activities.\n\nThe GEM scientific framework serves as the underlying basis for constructing the global earthquake model, and is organised in three principal integrated modules: seismic hazard, seismic risk and socio-economic impact.\n\n\nIt will take five years to build the first working global earthquake model – including corresponding tools, software and datasets. The work started in 2009 and will be finished at the end of 2013. Construction occurs in various stages that are partly overlapping in time. The pilot project GEM1 (January 2009 – March 2010) generates GEM’s first products and initial model building infrastructure, Global components will establish a common set of definitions, strategies, standards, quality criteria and formats for the compilation of databases that serve as an input to the global earthquake model. They are addressed by international consortia that respond to calls for proposals on hazard, risk and socio-economic impact. Global components will provide preliminary data on a global scale, but on a local scale, regional and national programmes will provide more detailed and reliable data. One Global component was the Global GMPEs project that proposed a set of ground motion prediction equations for use when calculating seismic hazard. Regional Programmes are projects with targeted funding taking place in various regions of the world; currently in the Middle East and Europe programs have already been completed. The data produced on a regional and national scale will be carefully quality-controlled and integrated into the global models. The actual development of the model will occur using a common, web-based platform for dynamic sharing of tools and resources, in order to create software and online tools as end-products. The global earthquake model will be tested and evaluated before its official release; the testing procedure will involve the establishment of scientific experiments that are reproducible, transparent, and set up within a controlled environment.\n\nGEM is however more than the creation and release of this first version of the model. GEM strives for continuous improvement of the model and will ensure that results are disseminated, technology is transferred through training and workshops and that awareness raising activities are deployed in order to contribute to risk reduction worldwide.\n\n\n"}
{"id": "48120666", "url": "https://en.wikipedia.org/wiki?curid=48120666", "title": "Grayite", "text": "Grayite\n\nGrayite, ThPO • (HO), is a thorium phosphate mineral of the Rabdophane group first discovered in 1957 by S.H.U. Bowie in Rhodesia. It is of moderate hardness occurring occasionally in aggregates of hexagonal crystals occasionally but more commonly in microgranular/cryptocrystalline masses. Due to its thorium content, grayite displays some radioactivity although it is only moderate and the mineral displays powder XRD peaks without any metamict-like effects. The color of grayite is most commonly observed as a light to dark reddish brown but has also been observed as lighter yellows with grayish tints. It has a low to moderate hardness with a Mohs hardness of 3-4 and has a specific gravity of 3.7-4.3. It has been found in both intrusive igneous and sedimentary environments.\n\nFormations including grayite were originally documented in Rhodesia (now Zimbabwe) in 1957 and subsequently around the globe. Some of these locales include the states of Wyoming and Colorado as well as Madagascar. Grayite has often been found in pegmatitic environments amongst other thorium minerals, particularly monazite ((Ce,La)PO). Recent work has shown widespread occurrences in Wisconsin pegmatitic environments. Other notable finds of pegmatitic grayite occur in Bulgaria. Grayite has also been found in sedimentary environments with an observation of high concentrations in cracks raising the possibility of the mineral as a precipitate from fluid mobilized ions. Formation of grayite and other rhabdophane minerals in this context has been documented in literature.\n\nGrayite is isostructural with members of the Rhabdophane group such as brockite and rhabdrophane. While previous work has identified grayite as a pseudohexagonal orthorhombic member of the rhabdophane group along with ningyoite, more contemporary work seems to maintain a hexagonal crystal structure. These hydrated phosphate minerals often include radioactive elements such as thorium, uranium, and cerium. Powder XRD analysis produces peaks matching those of rhabdophane.\n\nIn the identification of new hydrated phosphate minerals related to rhabdophane XRD peak information is usually recorded through different sample preparation methods. Besides standard powder XRD, samples are often heated to ~850 °C so that the structure changes. The peak information is analyzed again and upon doing this hydrated thorium phosphate minerals will show a monazite-like structure indicating a possible alteration relationship.\n\n"}
{"id": "4277031", "url": "https://en.wikipedia.org/wiki?curid=4277031", "title": "Index of gardening articles", "text": "Index of gardening articles\n\nThis is a list of gardening topics.\n\nAesthetics\n- African Violet Society of America\n- Allotment\n- Aquascaping\n- Arboretum\n- Architectural theory\n\nBonsai\n- Botanical gardens\n\nCalifornia native plants\n- Chelsea Flower Show\n- Community garden\n- Companion planting\n- Compost\n- Composting\n- Conservation\n\nDesign\n\nEnglish garden\n- Environmental design\n\nFlowerbed\n- Fountains\n- French intensive gardening\n- French landscape garden\n\nGarden\n- Garden designer\n- Gardener\n- Gardening\n- Garden buildings\n- List of gardens in fiction\n- Garden tool\n- Garden Gnome Liberation Front\n- Garden à la française\n- Gardens of the French Renaissance\n- Gardens of Versailles\n- \n- Giardino all'italiana \n- Grandi Giardini Italiani\n- Growbag\n- Guerrilla gardening\n\nHistory of gardening\n- History of gardens\n- History of landscape architecture\n- Hedge\n- Herbaceous border\n- Home economics\n- Horticulture\n\nInvasive species\n- Italian Renaissance garden\n\nJapanese garden\n- Japanese rock garden\n\n- \n- \n- List of notable historical gardens\n- Land Arts of the American West\n- Landscape architecture\n- Landscape design\n- Landscape detailing\n- Landscape garden\n- Landscape manager\n- Landscape products\n- Lawn\n- Lawnmower\n- List of botanical gardens in Italy\n- List of botanical gardens in the United States\n- List of Chinese gardens\n- List of garden and horticulture books\n- List of garden features\n- List of garden plants\n- List of gardens in Italy\n- List of invasive species in North America\n- List of organic gardening and farming topics\n- List of professional gardeners\n- Local food\n\nNative plant gardening\n- Natural landscaping\n- Nature and Culture\n- Never Ending Gardens\n- No-dig gardening\n\nOrganic gardening\n- Arboreta\n\nPatio garden\n- Pergola\n- Parterre\n- Permaculture\n- Plant community\n- Planting design\n- Flower pot\n- Pruning\n\nRain gardens\n- Raised bed gardening\n- Remarkable Gardens of France\n- Rock garden\n- Roof garden\n- Roman garden\n\nSculpture garden\n- Sheet mulching\n- Shrub\n- Spanish garden\n- Spanish gardens\n- Square foot gardening\n- Statuary\n- Sustainable art\n- Sustainable design\n- Sustainability\n\nterrace\n- Topiary\n- Tree\n- Tropical garden\n\nVegetable farming\n\nWater garden\n- water feature\n- Wildlife corridor\n- Wildlife garden\n\nXeriscaping\n\nZen garden\n\n"}
{"id": "23158223", "url": "https://en.wikipedia.org/wiki?curid=23158223", "title": "Jack Collom", "text": "Jack Collom\n\nJohn Aldridge \"Jack\" Collom (November 8, 1931 – July 2, 2017) was an American poet, essayist, and creative writing pedagogue. Included among the twenty-five books he published during his lifetime were \"Red Car Goes By: Selected Poems 1955–2000\"; \"Poetry Everywhere: Teaching Poetry Writing in School and in the Community\"; and \"Second Nature\", which won the 2013 Colorado Book Award for Poetry. In the fields of education and pedagogy, he was involved in eco-literature, ecopoetics, and creative writing instruction for children.\n\nJack Collom was born John Aldridge Collom in Chicago on November 8, 1931. He grew up in the small town of Western Springs, Illinois, spent much of his time birdwatching, and over the years became an inveterate bird-watcher. Collom moved to Fraser, Colorado, in 1947. He studied Forestry at Colorado A&M College where he earned a B.S. in 1952. Afterwards, he spent four years in the U.S. Air Force, and he started writing poetry in 1955 while stationed in Tripoli, Libya. His unit was next stationed at Neubiberg, a base just south of Munich, in Bavaria. It is there he met his first wife (a native German), in 1956. After his discharge from the military, he moved back to the US after a brief time living in Germany, and worked in factories for twenty years while writing poetry.\n\nHe received his B.A. in English (1972) and M.A. in English literature (1974) from the University of Colorado, where he had studied on the G.I. Bill. In 1974, he began teaching in the \"Poetry-in-the-Schools\" programs in Colorado, Wyoming, and Nebraska. In 1980, he began teaching poetry in the public schools of New York City, by way of the \"Poets In Public Service\" and \"Teachers & Writers\" programs. Collom continued to teach creative writing to children for the next 35 years, in both elementary and secondary schools, where he developed a pedagogy for this type of educational approach.\n\nSubsequently, Teachers & Writers Collaborative published three books of Collom's essays and commentary on this experience (which included the young students' poems), notably \"Poetry Everywhere\" and \"Moving Windows\".\n\nFrom 1966 to 1977, he published the work of many writers in a little magazine called \"The\". He was twice awarded Poetry Fellowships from the National Endowment for the Arts, and received a Foundation for Contemporary Arts Grants to Artists award (2012). From 1986 until his death in 2017, Collom taught at Naropa University's Jack Kerouac School of Disembodied Poetics as an adjunct professor, where he shaped Writing Outreach, a community creative-writing project, into a course. In 1989, he pioneered Eco-Lit, one of the first ecology literature courses ever offered in the United States. Some of his accomplishments as an environmentalist-poet are documented in \"American Environmental Leaders: From Colonial Times to the Present\". His nature writings and essays about the environment were published in various venues, including \"ecopoetics\", \"The Alphabet of Trees: A Guide to Writing Nature Poetry\", and \"ISLE\", the journal of Interdisciplinary Studies in Literature and the Environment.\n\nHe read and taught throughout the United States, in Mexico, Costa Rica, Austria, Belgium, and Germany. In 2008, he was the plenary speaker at the \"Poetic Ecologies\" conference at the Université Libre de Bruxelles. In 2009, he led a three-week Creativity and Aging Program at Woodland Pattern in Milwaukee, Wisconsin.\n\nHe worked with numerous dancers, visual artists and musician/composers, and recorded three CDs: \"Calluses of Poetry\" and \"Colors Born of Shadow\", with Ken Bernstein, and \"Blue Yodel Blue Heron\", with Dan Hankin and Sierra Collom.\n\nIn 2001, his adopted hometown of Boulder, Colorado, declared and celebrated a \"Jack Collom Day\".\n\nCollom was married three times. He had three sons by his first marriage: Nathaniel, Christopher, and Franz. He had a daughter, Sierra, through a second marriage.\n\nJack Collom died in Boulder, Colorado on July 2, 2017. He is survived by his wife, Jennifer Heath, his four grown children, and a grandson.\n\n\n\n"}
{"id": "103194", "url": "https://en.wikipedia.org/wiki?curid=103194", "title": "Leidenfrost effect", "text": "Leidenfrost effect\n\nThe Leidenfrost effect is a physical phenomenon in which a liquid, in near contact with a mass significantly hotter than the liquid's boiling point, produces an insulating vapor layer keeping that liquid from boiling rapidly. Because of this 'repulsive force', a droplet hovers over the surface rather than making physical contact with it. This is most commonly seen when cooking: one sprinkles drops of water in a pan to gauge its temperature: if the pan's temperature is at or above the Leidenfrost point, the water skitters across the pan and takes longer to evaporate than in a pan with a temperature below the Leidenfrost point but still above boiling. It is named after Johann Gottlob Leidenfrost, who discussed it in \"A Tract About Some Qualities of Common Water\" in 1751.\n\nThe effect is responsible for the ability of liquid nitrogen to skitter across floors. It also allows a person to dip a wet finger in molten lead, or blow out a mouthful of liquid nitrogen, without injury. The latter is potentially lethal, particularly should one accidentally swallow the liquid nitrogen.\n\nThe effect can be seen as drops of water are sprinkled onto a pan at various times as it heats up. Initially, as the temperature of the pan is just below , the water flattens out and slowly evaporates, or if the temperature of the pan is well below , the water stays liquid. As the temperature of the pan goes above , the water droplets hiss when touching the pan and these droplets evaporate quickly. Later, as the temperature exceeds the Leidenfrost point, the Leidenfrost effect comes into play. On contact with the pan, the water droplets bunch up into small balls of water and skitter around, lasting much longer than when the temperature of the pan was lower. This effect works until a much higher temperature causes any further drops of water to evaporate too quickly to cause this effect.\n\nThis is because at temperatures above the Leidenfrost point, the bottom part of the water droplet vaporizes immediately on contact with the hot pan. The resulting gas suspends the rest of the water droplet just above it, preventing any further direct contact between the liquid water and the hot pan. As steam has much poorer thermal conductivity than the metal pan, further heat transfer between the pan and the droplet is slowed down dramatically. This also results in the drop being able to skid around the pan on the layer of gas just under it.\n\nThe temperature at which the Leidenfrost effect begins to occur is not easy to predict. Even if the volume of the drop of liquid stays the same, the Leidenfrost point may be quite different, with a complicated dependence on the properties of the surface, as well as any impurities in the liquid. Some research has been conducted into a theoretical model of the system, but it is quite complicated. As a very rough estimate, the Leidenfrost point for a drop of water on a frying pan might occur at .\n\nThe effect was also described by the eminent Victorian steam boiler designer, Sir William Fairbairn, in reference to its effect on massively reducing heat transfer from a hot iron surface to water, such as within a boiler. In a pair of lectures on boiler design, he cited the work of Pierre Hippolyte Boutigny (1798-1884) and Professor Bowman of King's College, London in studying this. A drop of water that was vaporized almost immediately at persisted for 152 seconds at . Lower temperatures in a boiler firebox might evaporate water more quickly as a result; compare Mpemba effect. An alternative approach was to increase the temperature beyond the Leidenfrost point. Fairbairn considered this too, and may have been contemplating the flash steam boiler, but considered the technical aspects insurmountable for the time.\n\nThe Leidenfrost point may also be taken to be the temperature for which the hovering droplet lasts longest.\n\nIt has been demonstrated that it is possible to stabilize the Leidenfrost vapour layer of water by exploiting superhydrophobic surfaces. In this case, once the vapour layer is established, cooling never collapses the layer, and no nucleate boiling occurs; the layer instead slowly relaxes until the surface is cooled.\n\nLeidenfrost effect has been used for the development of high sensitivity ambient mass spectrometry. Under the influence of Leidenfrost condition the Levitating droplet does not release molecules out and the molecules are enriched inside the droplet. At the last moment of droplet evaporation all of the enriched molecules release in a short time domain and thus increase the sensitivity.\n\nA heat engine based on the Leidenfrost effect has been prototyped. It has the advantage of extremely low friction.\n\nThe Leidenfrost point signifies the onset of stable film boiling. It represents the point on the boiling curve where the heat flux is at the minimum and the surface is completely covered by a vapor blanket. Heat transfer from the surface to the liquid occurs by conduction and radiation through the vapor. In 1756, Leidenfrost observed that water droplets supported by the vapor film slowly evaporate as they move about on the hot surface. As the surface temperature is increased, radiation through the vapor film becomes more significant and the heat flux increases with increasing excess temperature.\n\nThe minimum heat flux for a large horizontal plate can be derived from Zuber's equation,\n\n<math>_{min}}=C\n"}
{"id": "37355128", "url": "https://en.wikipedia.org/wiki?curid=37355128", "title": "List of English animal nouns", "text": "List of English animal nouns\n\nThe following is a list of English animal nouns, (the common names of kinds of animals). This list includes the common names used for the animal in general; names for the male animal and the female animal where such names exist; the name used for the young or juveniles of the animal; the common name given for the sound the animal makes, if any; the group noun where applicable; and the name of both the natural shelter and (if applicable) an artificial shelter for the animals.\n\n\n"}
{"id": "2099301", "url": "https://en.wikipedia.org/wiki?curid=2099301", "title": "List of common household pests", "text": "List of common household pests\n\nThis is a list of common household pests, undesired animal species or genera that have a history of living, invading, causing damage, eating human foods, acting as disease vectors, or causing other harms in human habitation structures.\n\n\n\n"}
{"id": "2395906", "url": "https://en.wikipedia.org/wiki?curid=2395906", "title": "List of pine barrens", "text": "List of pine barrens\n\nThe following is a list of pine barrens.\n\n\nPennsylvania\n"}
{"id": "15911800", "url": "https://en.wikipedia.org/wiki?curid=15911800", "title": "List of reserves for waterbirds and migratory birds in Switzerland", "text": "List of reserves for waterbirds and migratory birds in Switzerland\n\nThis is a list of reserves for waterbirds and migratory birds in Switzerland. The nature reserves on this inventory are protected by Swiss federal legislation (\"Federal Inventory of Water and Migratory Birds Reserves of National and International Importance\").\n\n\n"}
{"id": "15522138", "url": "https://en.wikipedia.org/wiki?curid=15522138", "title": "Marine counterparts of land creatures", "text": "Marine counterparts of land creatures\n\nThe idea that there are specific marine counterparts to land creatures, inherited from the writers on natural history in Antiquity, was firmly believed in Islam and in Medieval Europe, and is exemplified by the creatures represented in the medieval animal encyclopedias called bestiaries and in the parallels drawn in the moralising attributes attached to each. \"The creation was a mathematical diagram drawn in parallel lines,\" T. H. White said a propos the bestiary he translated. \"Things did not only have a moral they often had physical counterparts in other strata. There was a horse in the land and a sea-horse in the sea. For that matter there was probably a Pegasus in heaven\". The idea of perfect analogies in the fauna of land and sea was considered part of the perfect symmetry of the Creator's plan, offered as the \"book of nature\" to mankind, for which a text could be found in \"Job\":\nBut ask the animals, and they will teach you, or the birds of the air, and they will tell you; or speak to the earth, and it will teach you, or let the fish of the sea inform you. Which of all these does not know that the hand of the Lord has done this? In his hand is the life of every creature and the breath of all mankind.\n\nThe idea appears in the Jewish Tannaic sources as well, as brought down in Babylonian Talmud, Chulin 127a. Rashi (Psalms 49:2) traces this to a biblical source – the land is referred to as \"Chaled\", from the weasel (chulda), because the weasel is the only animal on dry land that does not have its counterpart in the sea.\nAll of Creation was considered to reflect the Creator, and Man could learn about the Creator through studying the Creation, an assumption that underlies the \"watchmaker analogy\" offered as a proof of God's existence.\n\nThe correspondence between the realms of earth and sea, extending to its denizens, offers examples of the taste for allegory engendered by Christian and Islamic methods of exegesis, which also encouraged the doctrine of signatures, a \"key\" to the meaning and use of herbs.\n\nThe source text that was most influential in compiling the bestiaries of the 12th and 13th centuries was the \"Physiologus\", one of the most widely read and copied secular texts of the Middle Ages. Written in Greek in Alexandria the 2nd century CE and accumulating further \"exemplary\" beasts in the next three centuries and more, \"Physiologus\" was transmitted in the West in Latin, and eventually translated into many vernacular languages: many manuscripts in various languages survive.\nAelian, \"On the Characteristics of Animals\" (A. F. Scholfield, in Loeb Classical Library, 1958).\n\nChristian writers, trained in anagogical thinking and expecting to find spiritual instruction inherent in the processes of Nature, disregarded the caveat in Pliny's Natural History, where the idea is presented as a \"vulgar opinion\": \n\nHence it is that the vulgar notion may very possibly be true, that whatever is produced in any other department of Nature, is to be found in the sea as well; while, at the same time, many other productions are there to be found which nowhere else exist. That there are to be found in the sea the forms, not only of terrestrial animals, but of inanimate objects even, is easily to be understood by all who will take the trouble to examine the grape-fish, the sword-fish, the sawfish, and the cucumber-fish, which last so strongly resembles the real cucumber both in colour and in smell.\nPliny points out that many more things are found in the sea than on the land, and also mentions the correspondences that may be discovered between many non-living objects of the land and living creatures in the sea. \n\nSaint Augustine of Hippo reasons based on analogy, that since there is a serpent in the grass, there must be an eel in the sea; because there is a Leviathan in the sea, there must be a Behemoth on the land. (\"City of God\"? xi.15?)\nThe reaction to such anagogical thinking set in with the unfolding of critical scientific thought in the 17th century. Sir Thomas Browne devoted a chapter of his \"Pseudodoxia Epidemica\" to dispelling such a belief: Chapter XXIV: \"That all Animals in the land are in their kinde in the Sea.\" During the Enlightenment the ancient conception was given an innovative and rationalized cast by Benoît de Maillet in describing the transformations and metamorphoses undergone by creatures of the sea to render them fit for life on land, a proto-evolutionist concept, though it was based on superficial morphological similarities:\nThere are in the Sea, Fish of almost all the Figures of Land-Animals, and even of Birds. She includes Plants, Flowers, and some Fruits; the Nettle, the Rose, the Pink, the Melon and the Grape, are to be found there.<br>\n<br>\nAs for the Quadrupeds, we not only find in the Sea, Species of the same Figure and Inclinations, and in the Waves living on the same Aliments by which they are nourished on Land, we have also Examples of those Species living equally in the Air and in the Water. Have not the Sea-Apes precisely the same figure with those of the Land?\n\nThough in \"Moby-Dick\" Ishmael, with a nod to Sir Thomas Browne's wording, denies the claim that land animals find their counterparts in the sea,For though some old naturalists have maintained that all creatures of the land are of their kind in the sea; and though taking a broad general view of the thing, this may very well be; yet coming to specialties, where, for example, does the ocean furnish any fish that in disposition answers to the sagacious kindness of the dog? The accursed shark alone can in any generic respect be said to bear comparative analogy to him.\nin discussing dolphins trained to aid scuba divers, a 1967 \"Popular Mechanics\" article could still casually state: \"It's hoped that the marine counterparts of some land animals can be trained to become useful members of the Man-in-the-Sea program.\"\n"}
{"id": "229104", "url": "https://en.wikipedia.org/wiki?curid=229104", "title": "Matter wave", "text": "Matter wave\n\nMatter waves are a central part of the theory of quantum mechanics, being an example of wave–particle duality. All matter can exhibit wave-like behavior. For example, a beam of electrons can be diffracted just like a beam of light or a water wave. The concept that matter behaves like a wave was proposed by Louis de Broglie () in 1924. It is also referred to as the \"de Broglie hypothesis\". Matter waves are referred to as \"de Broglie waves\".\n\nThe \"de Broglie wavelength\" is the wavelength, , associated with a massive particle and is related to its momentum, , through the Planck constant, :\n\nWave-like behavior of matter was first experimentally demonstrated by George Paget Thomson's thin metal diffraction experiment, and independently in the Davisson–Germer experiment both using electrons, and it has also been confirmed for other elementary particles, neutral atoms and even molecules. Recently, it was also found that investigating the elementary process of diffusion gives the theoretical evidence of the relation of matter wave, regardless of the photon energy. It is thus revealed that the relation of matter wave is now not a hypothesis but an actual equation relevant to a characteristic of micro particle. The wave-like behavior of matter is crucial to the modern theory of atomic structure and particle physics.\n\nAt the end of the 19th century, light was thought to consist of waves of electromagnetic fields which propagated according to Maxwell's equations, while matter was thought to consist of localized particles (See history of wave and particle viewpoints). In 1900, this division was exposed to doubt, when, investigating the theory of black body thermal radiation, Max Planck proposed that light is emitted in discrete quanta of energy. It was thoroughly challenged in 1905. Extending Planck's investigation in several ways, including its connection with the photoelectric effect, Albert Einstein proposed that light is also propagated and absorbed in quanta. Light quanta are now called photons. These quanta would have an energy given by the Planck–Einstein relation:\nand a momentum\nwhere (lowercase Greek letter nu) and (lowercase Greek letter lambda) denote the frequency and wavelength of the light, the speed of light, and the Planck constant. In the modern convention, frequency is symbolized by \"f\" as is done in the rest of this article. Einstein’s postulate was confirmed experimentally by Robert Millikan and Arthur Compton over the next two decades.\n\nDe Broglie, in his 1924 PhD thesis, proposed that just as light has both wave-like and particle-like properties, electrons also have wave-like properties. By rearranging the momentum equation stated in the above section, we find a relationship between the wavelength, associated with an electron and its momentum, , through the Planck constant, :\n\nThe relationship is now known to hold for all types of matter: all matter exhibits properties of both particles and waves.\n\nIn 1926, Erwin Schrödinger published an equation describing how a matter wave should evolve—the matter wave analogue of Maxwell’s equations—and used it to derive the energy spectrum of hydrogen.\n\nMatter waves were first experimentally confirmed to occur in George Paget Thomson's cathode ray diffraction experiment and the Davisson-Germer experiment for electrons, and the de Broglie hypothesis has been confirmed for other elementary particles. Furthermore, neutral atoms and even molecules have been shown to be wave-like.\n\nIn 1927 at Bell Labs, Clinton Davisson and Lester Germer fired slow-moving electrons at a crystalline nickel target. The angular dependence of the diffracted electron intensity was measured, and was determined to have the same diffraction pattern as those predicted by Bragg for x-rays. At the same time George Paget Thomson at the University of Aberdeen was independently firing electrons at very thin metal foils to demonstrate the same effect. Before the acceptance of the de Broglie hypothesis, diffraction was a property that was thought to be exhibited only by waves. Therefore, the presence of any diffraction effects by matter demonstrated the wave-like nature of matter. When the de Broglie wavelength was inserted into the Bragg condition, the observed diffraction pattern was predicted, thereby experimentally confirming the de Broglie hypothesis for electrons.\n\nThis was a pivotal result in the development of quantum mechanics. Just as the photoelectric effect demonstrated the particle nature of light, the Davisson–Germer experiment showed the wave-nature of matter, and completed the theory of wave–particle duality. For physicists this idea was important because it meant that not only could any particle exhibit wave characteristics, but that one could use wave equations to describe phenomena in matter if one used the de Broglie wavelength.\n\nExperiments with Fresnel diffraction and an atomic mirror for specular reflection of neutral atoms confirm the application of the de Broglie hypothesis to atoms, i.e. the existence of atomic waves which undergo diffraction, interference and allow quantum reflection by the tails of the attractive potential. Advances in laser cooling have allowed cooling of neutral atoms down to nanokelvin temperatures. At these temperatures, the thermal de Broglie wavelengths come into the micrometre range. Using Bragg diffraction of atoms and a Ramsey interferometry technique, the de Broglie wavelength of cold sodium atoms was explicitly measured and found to be consistent with the temperature measured by a different method.\n\nThis effect has been used to demonstrate atomic holography, and it may allow the construction of an atom probe imaging system with nanometer resolution. The description of these phenomena is based on the wave properties of neutral atoms, confirming the de Broglie hypothesis.\n\nThe effect has also been used to explain the spatial version of the quantum Zeno effect, in which an otherwise unstable object may be stabilised by rapidly repeated observations.\n\nRecent experiments even confirm the relations for molecules and even macromolecules that otherwise might be supposed too large to undergo quantum mechanical effects. In 1999, a research team in Vienna demonstrated diffraction for molecules as large as fullerenes. The researchers calculated a De Broglie wavelength of the most probable C velocity as 2.5 pm.\nMore recent experiments prove the quantum nature of molecules made of 810 atoms and with a mass of 10,123 amu.\n\nStill one step further than Louis De Broglie go theories which in quantum mechanics eliminate the concept of a pointlike classical particle and explain the observed facts by means of wavepackets of matter waves alone.\n\nThe de Broglie equations relate the wavelength to the momentum , and frequency to the total energy of a particle:\n\nformula_5\n\nwhere \"h\" is the Planck constant. The equations can also be written as\n\nformula_6\n\nor \n\nformula_7\n\nwhere is the reduced Planck constant, is the wave vector, is the phase constant, and is the angular frequency.\nIn each pair, the second equation is also referred to as the Planck–Einstein relation, since it was also proposed by Planck and Einstein.\n\nUsing two formulas from special relativity, one for the relativistic momentum and one for the relativistic mass energy\n\nallows the equations to be written as\n\nwhere formula_11 denotes the particle's rest mass, formula_12 its velocity, formula_13 the Lorentz factor, and formula_14 the speed of light in a vacuum. See below for details of the derivation of the de Broglie relations. Group velocity (equal to the particle's speed) should not be confused with phase velocity (equal to the product of the particle's frequency and its wavelength). In the case of a non-dispersive medium, they happen to be equal, but otherwise they are not.\n\nAlbert Einstein first explained the wave–particle duality of light in 1905. Louis de Broglie hypothesized that any particle should also exhibit such a duality. The velocity of a particle, he concluded, should always equal the group velocity of the corresponding wave. The magnitude of the group velocity is equal to the particle's speed.\n\nBoth in relativistic and non-relativistic quantum physics, we can identify the group velocity of a particle's wave function with the particle velocity. Quantum mechanics has very accurately demonstrated this hypothesis, and the relation has been shown explicitly for particles as large as molecules.\n\nDe Broglie deduced that if the duality equations already known for light were the same for any particle, then his hypothesis would hold. This means that\n\nwhere is the total energy of the particle, is its momentum, is the reduced Planck constant. For a free non-relativistic particle it follows that\n\nwhere is the mass of the particle and its velocity.\n\nAlso in special relativity we find that\n\nwhere is the rest mass of the particle and is the speed of light in a vacuum. But (see below), using that the phase velocity is , therefore\n\nwhere is the velocity of the particle regardless of wave behavior.\n\nIn quantum mechanics, particles also behave as waves with complex phases. The phase velocity is equal to the product of the frequency multiplied by the wavelength.\n\nBy the de Broglie hypothesis, we see that\n\nUsing relativistic relations for energy and momentum, we have\n\nwhere \"E\" is the total energy of the particle (i.e. rest energy plus kinetic energy in the kinematic sense), \"p\" the momentum, formula_13 the Lorentz factor, \"c\" the speed of light, and β the speed as a fraction of \"c\". The variable \"v\" can either be taken to be the speed of the particle or the group velocity of the corresponding matter wave. Since the particle speed formula_22 for any particle that has mass (according to special relativity), the phase velocity of matter waves always exceeds \"c\", i.e.\n\nand as we can see, it approaches \"c\" when the particle speed is in the relativistic range. The superluminal phase velocity does not violate special relativity, because phase propagation carries no energy. See the article on \"Dispersion (optics)\" for details.\n\nUsing four-vectors, the De Broglie relations form a single equation:\n\nformula_24\n\nwhich is frame-independent.\n\nLikewise, the relation between group/particle velocity and phase velocity is given in frame-independent form by:\n\nformula_25\n\nwhere\n\nThe physical reality underlying de Broglie waves is a subject of ongoing debate. Some theories treat either the particle or the wave aspect as its fundamental nature, seeking to explain the other as an emergent property. Some, such as the hidden variable theory, treat the wave and the particle as distinct entities. Yet others propose some intermediate entity that is neither quite wave nor quite particle but only appears as such when we measure one or the other property. The Copenhagen interpretation states that the nature of the underlying reality is unknowable and beyond the bounds of scientific inquiry.\n\nSchrödinger's quantum mechanical waves are conceptually different from ordinary physical waves such as water or sound. Ordinary physical waves are characterized by undulating real-number 'displacements' of dimensioned physical variables at each point of ordinary physical space at each instant of time. Schrödinger's \"waves\" are characterized by the undulating value of a dimensionless complex number at each point of an abstract multi-dimensional space, for example of configuration space.\n\nAt the Fifth Solvay Conference in 1927, Max Born and Werner Heisenberg reported as follows:\n\nAt the same conference, Erwin Schrödinger reported likewise.\n\nIn 1955, Heisenberg reiterated this:\n\nIt is mentioned above that the \"displaced quantity\" of the Schrödinger wave has values that are dimensionless complex numbers. One may ask what is the physical meaning of those numbers. According to Heisenberg, rather than being of some ordinary physical quantity such as, for example, Maxwell's electric field intensity, or mass density, the Schrödinger-wave packet's \"displaced quantity\" is probability amplitude. He wrote that instead of using the term 'wave packet', it is preferable to speak of a probability packet. The probability amplitude supports calculation of probability of location or momentum of discrete particles. Heisenberg recites Duane's account of particle diffraction by probabilistic quantal translation momentum transfer, which allows, for example in Young's two-slit experiment, each diffracted particle probabilistically to pass discretely through a particular slit. Thus one does not need necessarily think of the matter wave, as it were, as 'composed of smeared matter'.\n\nThese ideas may be expressed in ordinary language as follows. In the account of ordinary physical waves, a 'point' refers to a position in ordinary physical space at an instant of time, at which there is specified a 'displacement' of some physical quantity. But in the account of quantum mechanics, a 'point' refers to a configuration of the system at an instant of time, every particle of the system being in a sense present in every 'point' of configuration space, each particle at such a 'point' being located possibly at a different position in ordinary physical space. There is no explicit definite indication that, at an instant, this particle is 'here' and that particle is 'there' in some separate 'location' in configuration space. This conceptual difference entails that, in contrast to de Broglie's pre-quantum mechanical wave description, the quantum mechanical probability packet description does not directly and explicitly express the Aristotelian idea, referred to by Newton, that causal efficacy propagates through ordinary space by contact, nor the Einsteinian idea that such propagation is no faster than light. In contrast, these ideas are so expressed in the classical wave account, through the Green's function, though it is inadequate for the observed quantal phenomena. The physical reasoning for this was first recognized by Einstein.\n\nDe Broglie's thesis started from the hypothesis, \"that to each portion of energy with a proper mass one may associate a periodic phenomenon of the frequency , such that one finds: . The frequency is to be measured, of course, in the rest frame of the energy packet. This hypothesis is the basis of our theory.\"\n\nDe Broglie followed his initial hypothesis of a periodic phenomenon, with frequency  , associated with the energy packet. He used the special theory of relativity to find, in the frame of the observer of the electron energy packet that is moving with velocity formula_12, that its frequency was apparently reduced to\n\nThen\n\nusing the same notation as above. The quantity formula_32 is the velocity of what de Broglie called the \"phase y wave\". Its wavelength is formula_33 and frequency formula_34. De Broglie reasoned that his hypothetical intrinsic particle periodic phenomenon is in phase with that phase wave. This was his basic matter wave conception. He noted, as above, that formula_35, and the phase wave does not transfer energy.\n\nWhile the concept of waves being associated with matter is correct, de Broglie did not leap directly to the final understanding of quantum mechanics with no missteps. There are conceptual problems with the approach that de Broglie took in his thesis that he was not able to resolve, despite trying a number of different fundamental hypotheses in different papers published while working on, and shortly after publishing, his thesis.\nThese difficulties were resolved by Erwin Schrödinger, who developed the wave mechanics approach, starting from a somewhat different basic hypothesis.\n\n\n\n"}
{"id": "52555162", "url": "https://en.wikipedia.org/wiki?curid=52555162", "title": "Metal vapor synthesis", "text": "Metal vapor synthesis\n\nIn chemistry, metal vapor synthesis (MVS) is a method for preparing metal complexes by combining freshly produced metal atoms or small particles with ligands. In contrast to the high reactivity of such freshly produced metal atoms, bulk metals typically are unreactive toward neutral ligands. The method has been used to prepare compounds that cannot be prepared by traditional synthetic methods, e.g. Ti(η-toluene). The technique relies on a reactor that evaporates the metal, allowing the vapor to impinge on a cold reactor wall that is coated with the organic ligand. The metal evaporates upon being heated resistively or irradiated with an electron beam. The apparatus operates under high vacuum. In a common implementation, the metal vapor and the organic ligand are co-condensed at liquid nitrogen temperatures.\n\nIn several case where compounds are prepared by MVS, related preparations employ conventional routes. Thus, tris(butadiene)molybdenum was first prepared by co-condensation of butadiene and Mo vapor, but yields are higher for the reduction of molybdenum(V) chloride in the presence of the diene.\n"}
{"id": "19555", "url": "https://en.wikipedia.org/wiki?curid=19555", "title": "Molecule", "text": "Molecule\n\nA molecule is an electrically neutral group of two or more atoms held together by chemical bonds. Molecules are distinguished from ions by their lack of electrical charge. However, in quantum physics, organic chemistry, and biochemistry, the term \"molecule\" is often used less strictly, also being applied to polyatomic ions.\n\nIn the kinetic theory of gases, the term \"molecule\" is often used for any gaseous particle regardless of its composition. According to this definition, noble gas atoms are considered molecules as they are monatomic molecules.\n\nA molecule may be homonuclear, that is, it consists of atoms of one chemical element, as with oxygen (O); or it may be heteronuclear, a chemical compound composed of more than one element, as with water (HO). Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are generally not considered single molecules.\n\nMolecules as components of matter are common in organic substances (and therefore biochemistry). They also make up most of the oceans and atmosphere. However, the majority of familiar solid substances on Earth, including most of the minerals that make up the crust, mantle, and core of the Earth, contain many chemical bonds, but are \"not\" made of identifiable molecules. Also, no typical molecule can be defined for ionic crystals (salts) and covalent crystals (network solids), although these are often composed of repeating unit cells that extend either in a plane (such as in graphene) or three-dimensionally (such as in diamond, quartz, or sodium chloride). The theme of repeated unit-cellular-structure also holds for most condensed phases with metallic bonding, which means that solid metals are also not made of molecules. In glasses (solids that exist in a vitreous disordered state), atoms may also be held together by chemical bonds with no presence of any definable molecule, nor any of the regularity of repeating units that characterizes crystals.\n\nThe science of molecules is called \"molecular chemistry\" or \"molecular physics\", depending on whether the focus is on chemistry or physics. Molecular chemistry deals with the laws governing the interaction between molecules that results in the formation and breakage of chemical bonds, while molecular physics deals with the laws governing their structure and properties. In practice, however, this distinction is vague. In molecular sciences, a molecule consists of a stable system (bound state) composed of two or more atoms. Polyatomic ions may sometimes be usefully thought of as electrically charged molecules. The term \"unstable molecule\" is used for very reactive species, i.e., short-lived assemblies (resonances) of electrons and nuclei, such as radicals, molecular ions, Rydberg molecules, transition states, van der Waals complexes, or systems of colliding atoms as in Bose–Einstein condensate.\n\nAccording to Merriam-Webster and the Online Etymology Dictionary, the word \"molecule\" derives from the Latin \"moles\" or small unit of mass.\n\nThe definition of the molecule has evolved as knowledge of the structure of molecules has increased. Earlier definitions were less precise, defining molecules as the smallest particles of pure chemical substances that still retain their composition and chemical properties. This definition often breaks down since many substances in ordinary experience, such as rocks, salts, and metals, are composed of large crystalline networks of chemically bonded atoms or ions, but are not made of discrete molecules.\n\nMolecules are held together by either covalent bonding or ionic bonding. Several types of non-metal elements exist only as molecules in the environment. For example, hydrogen only exists as hydrogen molecule. A molecule of a compound is made out of two or more elements.\n\nA covalent bond is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are termed \"shared pairs\" or \"bonding pairs\", and the stable balance of attractive and repulsive forces between atoms, when they share electrons, is termed \"covalent bonding\".\n\nIonic bonding is a type of chemical bond that involves the electrostatic attraction between oppositely charged ions, and is the primary interaction occurring in ionic compounds. The ions are atoms that have lost one or more electrons (termed cations) and atoms that have gained one or more electrons (termed anions). This transfer of electrons is termed \"electrovalence\" in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complicated nature, e.g. molecular ions like NH or SO. Basically, an ionic bond is the transfer of electrons from a metal to a non-metal for both atoms to obtain a full valence shell.\n\nMost molecules are far too small to be seen with the naked eye, but there are exceptions. DNA, a macromolecule, can reach macroscopic sizes, as can molecules of many polymers. Molecules commonly used as building blocks for organic synthesis have a dimension of a few angstroms (Å) to several dozen Å, or around one billionth of a meter. Single molecules cannot usually be observed by light (as noted above), but small molecules and even the outlines of individual atoms may be traced in some circumstances by use of an atomic force microscope. Some of the largest molecules are macromolecules or supermolecules.\n\nThe smallest molecule is the diatomic hydrogen (H), with a bond length of 0.74 Å.\n\nEffective molecular radius is the size a molecule displays in solution.\nThe table of permselectivity for different substances contains examples.\n\nThe chemical formula for a molecule uses one line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, and \"plus\" (+) and \"minus\" (−) signs. These are limited to one typographic line of symbols, which may include subscripts and superscripts.\n\nA compound's empirical formula is a very simple type of chemical formula. It is the simplest integer ratio of the chemical elements that constitute it. For example, water is always composed of a 2:1 ratio of hydrogen to oxygen atoms, and ethyl alcohol or ethanol is always composed of carbon, hydrogen, and oxygen in a 2:6:1 ratio. However, this does not determine the kind of molecule uniquely – dimethyl ether has the same ratios as ethanol, for instance. Molecules with the same atoms in different arrangements are called isomers. Also carbohydrates, for example, have the same ratio (carbon:hydrogen:oxygen= 1:2:1) (and thus the same empirical formula) but different total numbers of atoms in the molecule.\n\nThe molecular formula reflects the exact number of atoms that compose the molecule and so characterizes different molecules. However different isomers can have the same atomic composition while being different molecules.\n\nThe empirical formula is often the same as the molecular formula but not always. For example, the molecule acetylene has molecular formula CH, but the simplest integer ratio of elements is CH.\n\nThe molecular mass can be calculated from the chemical formula and is expressed in conventional atomic mass units equal to 1/12 of the mass of a neutral carbon-12 (C isotope) atom. For network solids, the term formula unit is used in stoichiometric calculations.\n\nFor molecules with a complicated 3-dimensional structure, especially involving atoms bonded to four different substituents, a simple molecular formula or even semi-structural chemical formula may not be enough to completely specify the molecule. In this case, a graphical type of formula called a structural formula may be needed. Structural formulas may in turn be represented with a one-dimensional chemical name, but such chemical nomenclature requires many words and terms which are not part of chemical formulas.\n\nMolecules have fixed equilibrium geometries—bond lengths and angles— about which they continuously oscillate through vibrational and rotational motions. A pure substance is composed of molecules with the same average geometrical structure. The chemical formula and the structure of a molecule are the two important factors that determine its properties, particularly its reactivity. Isomers share a chemical formula but normally have very different properties because of their different structures. Stereoisomers, a particular type of isomer, may have very similar physico-chemical properties and at the same time different biochemical activities.\n\nMolecular spectroscopy deals with the response (spectrum) of molecules interacting with probing signals of known energy (or frequency, according to Planck's formula). Molecules have quantized energy levels that can be analyzed by detecting the molecule's energy exchange through absorbance or emission.\nSpectroscopy does not generally refer to diffraction studies where particles such as neutrons, electrons, or high energy X-rays interact with a regular arrangement of molecules (as in a crystal).\n\nMicrowave spectroscopy commonly measures changes in the rotation of molecules, and can be used to identify molecules in outer space. Infrared spectroscopy measures changes in vibration of molecules, including stretching, bending or twisting motions. It is commonly used to identify the kinds of bonds or functional groups in molecules. Changes in the arrangements of electrons yield absorption or emission lines in ultraviolet, visible or near infrared light, and result in colour. Nuclear resonance spectroscopy actually measures the environment of particular nuclei in the molecule, and can be used to characterise the numbers of atoms in different positions in a molecule.\n\nThe study of molecules by molecular physics and theoretical chemistry is largely based on quantum mechanics and is essential for the understanding of the chemical bond. The simplest of molecules is the hydrogen molecule-ion, H, and the simplest of all the chemical bonds is the one-electron bond. H is composed of two positively charged protons and one negatively charged electron, which means that the Schrödinger equation for the system can be solved more easily due to the lack of electron–electron repulsion. With the development of fast digital computers, approximate solutions for more complicated molecules became possible and are one of the main aspects of computational chemistry.\n\nWhen trying to define rigorously whether an arrangement of atoms is \"sufficiently stable\" to be considered a molecule, IUPAC suggests that it \"must correspond to a depression on the potential energy surface that is deep enough to confine at least one vibrational state\". This definition does not depend on the nature of the interaction between the atoms, but only on the strength of the interaction. In fact, it includes weakly bound species that would not traditionally be considered molecules, such as the helium dimer, He, which has one vibrational bound state and is so loosely bound that it is only likely to be observed at very low temperatures.\n\nWhether or not an arrangement of atoms is \"sufficiently stable\" to be considered a molecule is inherently an operational definition. Philosophically, therefore, a molecule is not a fundamental entity (in contrast, for instance, to an elementary particle); rather, the concept of a molecule is the chemist's way of making a useful statement about the strengths of atomic-scale interactions in the world that we observe.\n\n\n"}
{"id": "14389994", "url": "https://en.wikipedia.org/wiki?curid=14389994", "title": "Natural landscape", "text": "Natural landscape\n\nA natural landscape is the original landscape that exists before it is acted upon by human culture. The natural landscape and the cultural landscape are separate parts of the landscape. However, in the twenty-first century landscapes that are totally untouched by human activity no longer exist, so that reference is sometimes now made to degrees of naturalness within a landscape.\n\nIn \"Silent Spring\" (1962) Rachel Carson describes a roadside verge as it used to look: \"Along the roads, laurel, viburnum and alder, great ferns and wildflowers delighted the traveler’s eye through much of the year\" and then how it looks now following the use of herbicides: \"The roadsides, once so attractive, were now lined with browned and withered vegetation as though swept by fire\". Even though the landscape before it is sprayed is biologically degraded, and may well contains alien species, the concept of what might constitute a natural landscape can still be deduced from the context.\n\nThe phrase \"natural landscape\" was first used in connection with landscape painting, and landscape gardening, to contrast a formal style with a more natural one, closer to nature. Alexander von Humboldt (1769 – 1859) was to further conceptualize this into the idea of a natural landscape \"separate\" from the cultural landscape. Then in 1908 geographer Otto Schlüter developed the terms original landscape (\"Urlandschaft\") and its opposite cultural landscape (\"Kulturlandschaft\") in an attempt to give the science of geography a subject matter that was different from the other sciences. An early use of the actual phrase \"natural landscape\" by a geographer can be found in Carl O. Sauer's paper \"The Morphology of Landscape\" (1925).\n\nThe concept of a natural landscape was first developed in connection with landscape painting, though the actual term itself was first used in relation to landscape gardening. In both cases it was used to contrast a formal style with a more natural one, that is closer to nature. Chunglin Kwa suggests, \"that a seventeenth-century or early-eighteenth-century person could experience natural scenery ‘just like on a painting,’ and so, with or without the use of the word itself, designate it as a landscape.\" With regard to landscape gardening John Aikin, commented in 1794: \"Whatever, therefore, there be of \"novelty\" in the singular scenery of an artificial garden, it is soon exhausted, whereas the infinite diversity of a natural landscape presents an inexhaustible flore of new forms\". Writing in 1844 the prominent American landscape gardener Andrew Jackson Downing comments: \"straight canals, round or oblong pieces of water, and all the regular forms of the geometric mode ... would evidently be in violent opposition to the whole character and expression of natural landscape\".\n\nIn his extensive travels in South America, Alexander von Humboldt became the first to conceptualize a natural landscape separate from the cultural landscape, though he does not actually use these terms. Andrew Jackson Downing was aware of, and sympathetic to, Humboldt's ideas, which therefore influenced American landscape gardening.\n\nSubsequently, the geographer Otto Schlüter, in 1908, argued that by defining geography as a \"Landschaftskunde\" (landscape science) would give geography a logical subject matter shared by no other discipline. He defined two forms of landscape: the \"Urlandschaft\" (original landscape) or landscape that existed before major human induced changes and the \"Kulturlandschaft\" (cultural landscape) a landscape created by human culture. Schlüter argued that the major task of geography was to trace the changes in these two landscapes.\n\nThe term natural landscape is sometimes used as a synonym for wilderness, but for geographers natural landscape is a scientific term which refers to the biological, geological, climatological and other aspects of a landscape, not the cultural values that are implied by the word wilderness.\n\nMatters are complicated by the fact that the words nature and natural have more than one meaning. On the one hand there is the main dictionary meaning for nature: \"The phenomena of the physical world collectively, including plants, animals, the landscape, and other features and products of the earth, as opposed to humans or human creations\". On the other hand, there is the growing awareness, especially since Charles Darwin, of humanities biological affinity with nature.\n\nThe dualism of the first definition has its roots is an \"ancient concept\", because early people viewed \"nature, or the nonhuman world […] as a divine \"Other\", godlike in its separation from humans\". In the West, Christianity's myth of the fall, that is the expulsion of humankind from the Garden of Eden, where all creation lived in harmony, into an imperfect world, has been the major influence. Cartesian dualism, from the seventeenth century on, further reinforced this dualistic thinking about nature. \nWith this dualism goes value judgement as to the superiority of the natural over the artificial. Modern science, however, is moving towards a holistic view of nature.\n\nWhat is meant by natural, within the American conservation movement, has been changing over the last century and a half.\n\nIn the mid-nineteenth century American began to realize that the land was becoming more and more domesticated and wildlife was disappearing. This led to the creation of American National Parks and other conservation sites. Initially it was believed that all that was needed to do was to separate what was seen as natural landscape and \"avoid disturbances such as logging, grazing, fire and insect outbreaks\". This, and subsequent environmental policy, until recently, was influenced by ideas of the wilderness. However, this policy was not consistently applied, and in Yellowstone Park, to take one example, the existing ecology was altered, firstly by the exclusion of Native Americans and later with the virtual extermination of the wolf population.\n\nA century later, in the mid-twentieth century, it began to be believed that the earlier policy of \"protection from disturbance was inadequate to preserve park values\", and that is that direct human intervention was necessary to restore the landscape of National Parks to its ‘’natural’’ condition. In 1963 the Leopold Report argued that \"A national park should represent a vignette of primitive America\". This policy change eventually led to the restoration of wolves in Yellowstone Park in the 1990s.\n\nHowever, recent research in various disciplines indicates that a pristine natural or \"primitive\" landscape is a myth, and it now realised that people have been changing the natural into a cultural landscape for a long while, and that there are few places untouched in some way from human influence. The earlier conservation policies were now seen as cultural interventions. The idea of what is natural and what artificial or cultural, and how to maintain the natural elements in a landscape, has been further complicated by the discovery of global warming and how it is changing natural landscapes.\n\nAlso important is a reaction recently amongst scholars against dualistic thinking about nature and culture. Maria Kaika comments: \"Nowadays, we are beginning to see nature and culture as intertwined once again – not ontologically separated anymore […].What I used to perceive as a compartmentalized world, consisting of neatly and tightly sealed, autonomous ‘space envelopes’ (the home, the city, and nature) was, in fact, a messy socio-spatial continuum”. And William Cronon argues against the idea of wilderness because it \"involves a dualistic vision in which the human is entirely outside the natural\" and affirms that \"wildness (as opposed to wilderness) can be found anywhere\" even \"in the cracks of a Manhattan sidewalk\". According to Cronon we have to \"abandon the dualism that sees the tree in the garden as artificial […] and the tree in the wilderness as natural […] Both in some ultimate sense are wild.\" Here he bends somewhat the regular dictionary meaning of wild, to emphasise that nothing natural, even in a garden, is fully under human control.\n\nThe landscape of Europe has considerably altered by people and even in an area, like the Cairngorm Mountains of Scotland, with a low population density, only \" the high summits of the Cairngorm Mountains, consist entirely of natural elements. These \"high summits\" are of course only part of the Cairngorms, and there are no longer wolves, bears, wild boar or lynx in Scotland's wilderness. The Scots pine in the form of the Caledonian forest also covered much more of the Scottish landscape than today.\n\nThe Swiss National Park, however, represent a more natural landscape. It was founded in 1914, and is one of the earliest national parks in Europe.\nVisitors are not allowed to leave the motor road, or paths through the park, make fire or camp. The only building within the park is Chamanna Cluozza, mountain hut. It is also forbidden to disturb the animals or the plants, or to take home anything found in the park. Dogs are not allowed. Due to these strict rules, the Swiss National Park is the only park in the Alps who has been categorized by the IUCN as a strict nature reserve, which is the highest protection level.\n\nNo place on the Earth is unaffected by people and their culture. People are part of biodiversity, but human activity affects biodiversity, and this alters the natural landscape. Mankind have altered landscape to such an extent that few places on earth remain pristine, but once free of human influences, the landscape can return to a natural or near natural state.\nEven the remote Yukon and Alaskan wilderness, the bi-national Kluane-Wrangell-St. Elias-Glacier Bay-Tatshenshini-Alsek park system comprising Kluane, Wrangell-St Elias, Glacier Bay and Tatshenshini-Alsek parks, a UNESCO World Heritage Site, is not free from human influence, because the Kluane National Park lies within the traditional territories of the Champagne and Aishihik First Nations and Kluane First Nation who have a long history of living in this region. Through their respective Final Agreements with the Canadian Government, they have made into law their rights to harvest in this region.\n\nCultural forces intentionally or unintentionally, have an influence upon the landscape. Cultural landscapes are places or artifacts created and maintained by people. Examples of cultural intrusions into a landscape are: fences, roads, parking lots, sand pits, buildings, hiking trails, management of plants, including the introduction of invasive species, extraction or removal of plants, management of animals, mining, hunting, natural landscaping, farming and forestry, pollution. Areas that might be confused with a natural landscape include public parks, farms, orchards, artificial lakes and reservoirs, managed forests, golf courses, nature center trails, gardens.\n\n"}
{"id": "1870708", "url": "https://en.wikipedia.org/wiki?curid=1870708", "title": "Near-Earth supernova", "text": "Near-Earth supernova\n\nA near-Earth supernova is an explosion resulting from the death of a star that occurs close enough to the Earth (roughly less than 10 to 300 parsecs (30 to 1000 light-years) away) to have noticeable effects on Earth's biosphere.\n\nOn average, a supernova explosion occurs within of the Earth every 240 million years. Gamma rays are responsible for most of the adverse effects a supernova can have on a living terrestrial planet. In Earth's case, gamma rays induce a chemical reaction in the upper atmosphere, converting molecular nitrogen into nitrogen oxides, depleting the ozone layer enough to expose the surface to harmful solar and cosmic radiation (mainly ultra-violet). Phytoplankton and reef communities would be particularly affected, which could severely deplete the base of the marine food chain.\n\nSpeculation as to the effects of a nearby supernova on Earth often focuses on large stars as Type II supernova candidates. Several prominent stars within a few hundred light years of the Sun are candidates for becoming supernovae in as little as a millennium. Although they would be spectacular to look at, were these \"predictable\" supernovae to occur, they are thought to have little potential to affect Earth.\n\nIt is estimated that a Type II supernova closer than eight parsecs (26 light-years) would destroy more than half of the Earth's ozone layer. Such estimates are based on atmospheric modeling and the measured radiation flux from SN 1987A, a Type II supernova in the Large Magellanic Cloud. Estimates of the rate of supernova occurrence within 10 parsecs of the Earth vary from 0.05–0.5 per Ga to 10 per Ga. Several studies assume that supernovae are concentrated in the spiral arms of the galaxy, and that supernova explosions near the Sun usually occur during the ~10 million years that the Sun takes to pass through one of these regions. Examples of relatively near supernovae are the Vela Supernova Remnant (~800 ly, ~12,000 years ago) and Geminga (~550 ly, ~300,000 years ago).\n\nType Ia supernovae are thought to be potentially the most dangerous if they occur close enough to the Earth. Because Type Ia supernovae arise from dim, common white dwarf stars, it is likely that a supernova that could affect the Earth will occur unpredictably and take place in a star system that is not well studied. The closest known candidate is IK Pegasi. It is currently estimated, however, that by the time it could become a threat, its velocity in relation to the Solar System would have carried IK Pegasi to a safe distance.\n\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system. Supernova production of heavy elements over astronomic periods of time ultimately made the chemistry of life on Earth possible.\n\nPast supernovae might be detectable on Earth in the form of metal isotope signatures in rock strata. Subsequently, iron-60 enrichment has been reported in deep-sea rock of the Pacific Ocean by researchers from the Technical University of Munich. Twenty-three atoms of this iron isotope were found in the top 2 cm of crust (this layer corresponds to times from 13.4 to 0 million years ago). It is estimated that the supernova must have occurred in the last 5 million years or else it would have had to happen very close to the solar system to account for so much iron-60 still being here. A supernova occurring so close would have probably caused a mass extinction, which did not happen in that time frame. The quantity of iron seems to indicate that the supernova was less than 30 parsecs away. On the other hand, the authors estimate the frequency of supernovae at a distance less than \"D\" (for reasonably small \"D\") as around (\"D\"/10 pc) per Ga, which gives a probability of only around 5% for a supernova within 30 pc in the last 5 million years. They point out that the probability may be higher because the Solar System is entering the Orion Arm of the Milky Way.\n\nGamma ray bursts from \"dangerously close\" supernova explosions occur two or more times per billion years, and this has been proposed as the cause of the end Ordovician extinction, which resulted in the death of nearly 60% of the oceanic life on Earth.\n\nIn 1998 a supernova remnant, RX J0852.0-4622, was found in front (apparently) of the larger Vela Supernova Remnant. Gamma rays from the decay of titanium-44 (half-life about 60 years) were independently discovered emanating from it, showing that it must have exploded fairly recently (perhaps around 1200 CE), but there is no historical record of it. The flux of gamma rays and x-rays indicates that the supernova was relatively close to us (perhaps 200 parsecs or 660 ly). If so, this is an unexpected event because supernovae less than 200 parsecs away are estimated to occur less than once per 100,000 years.\n\n"}
{"id": "44862806", "url": "https://en.wikipedia.org/wiki?curid=44862806", "title": "Outline of evolution", "text": "Outline of evolution\n\nThe following outline is provided as an overview of and topical guide to evolution:\n\nEvolution – change in heritable traits of biological organisms over generations due to natural selection, mutation, gene flow, and genetic drift. Also known as descent with modification. Over time these evolutionary processes lead to formation of new species (speciation), changes within lineages (anagenesis), and loss of species (extinction). \"Evolution\" is also another name for evolutionary biology, the subfield of biology concerned with studying evolutionary processes that produced the diversity of life on Earth.\n\n\n\n\n\n\"See also Basic principles (above)\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "15654603", "url": "https://en.wikipedia.org/wiki?curid=15654603", "title": "Peterson Identification System", "text": "Peterson Identification System\n\nThe Peterson Identification System is a practical method for the field identification of animals, plants and other natural phenomena. It was devised by ornithologist Roger Tory Peterson in 1934 for the first of his series of \"Field Guide\"s (See Peterson Field Guides.) Peterson devised his system \"so that live birds could be identified readily at a distance by their 'field marks' without resorting to the bird-in-hand characters that the early collectors relied on. During the last half century the binocular and the spotting scope have replaced the shotgun.\" As such, it both reflected and contributed to awareness of the emerging early environmental movement.\n\nCreated for use by amateur naturalists and laymen, rather than specialists, the \"Peterson System\" is essentially a pictorial key based upon readily noticed visual impressions rather than on the technical features of interest to scientists. The technique involves patternistic drawings with arrows that pinpoint the key field comparisons between similar species.\n\nSince the first Peterson \"Field Guide\", the system has been expanded to about three dozen volumes in the series as well as being emulated by many other publishers and authors of field guides. It has become the near-universally accepted standard, first in the United States and Europe and then around the world.\n"}
{"id": "33316231", "url": "https://en.wikipedia.org/wiki?curid=33316231", "title": "Pouillet effect", "text": "Pouillet effect\n\nIn physics, the term Pouillet effect refers to an exothermic reaction that takes place when a liquid is added to a powder. It was first observed by Leslie in 1802 when dry sawdust was wetted with water. Claude Pouillet later described this phenomenon in 1822 when it came to be known as the Pouillet effect in France.\n"}
{"id": "30635101", "url": "https://en.wikipedia.org/wiki?curid=30635101", "title": "Regius Professor of Natural History (Aberdeen)", "text": "Regius Professor of Natural History (Aberdeen)\n\nThe Regius Professor of Natural History is a Regius Professorship at the University of Aberdeen in Scotland. It was originally called the Regius Professor of Civil and Natural History at Marischal College until in 1860 Marischal College and King's Colleges merged to form the University of Aberdeen, and the title changed to Natural History.\n"}
{"id": "58206657", "url": "https://en.wikipedia.org/wiki?curid=58206657", "title": "Self-sealing suction cup", "text": "Self-sealing suction cup\n\nThe self-sealing suction cup is a suction cup that exerts a suction force only when it is in physical contact with an object. Unlike most other suction cups, it does not exert any suction force when it is not in contact with an object. Its grasping ability is achieved entirely through passive means without the use of sensors, valves, or actuators.\n\nIt was designed so that, when used as part of a suction cup array, the suction cups that don’t come in contact with the object remain sealed. By having only the suction cups that are in direct contact of the object to exhibit suction force, the researchers were able to minimize leak points where air could enter and increase the pressure that each active cup receives, maximizing the suction force. As a result, an array of self-sealing suction cups can grasp and pick up a wide range of object sizes and shapes. This comes in contrast to conventional suction cups that are typically designed for one specific object size and geometry. In addition, suction cups of various sizes have been manufactured, ranging from the palm of a hand to the point of a fingertip.\n\nThe self-sealing suction cup was first developed in 2010 by a collaboration of researchers from the U.S. Army Research Laboratory (ARL), the Edgewood Chemical Biological Center at Aberdeen Proving Ground, and the University of Maryland.\n\nThe design of the self-sealing suction cup was initially inspired by the suckers of the octopus and its ability to pick up different sized items by individually actuating its suction cups based on the item’s size and physical features.\n\nThe internal geometry of the self-sealing suction cup was designed to the smallest possible size and features a minimum wall thickness of 1.02 mm, a tube diameter of 1.59 mm, and minimum part spacing of 0.13 mm. The suction cup incorporates a mix of rubber and plastic components, where the cup lip, base, tube, springs, and plug are made out of soft rubber while the cup side, collar, hinges, and flange are made out of plastic. As part of its design, a central vacuum pump can be used to maximize the suction force of the suction cup. A multi-material 3D printer was used to create the prototype of the self-sealing suction cup in about 20 minutes.\n\nInside the self-sealing suction cup, the plug is positioned close to the tube opening so that it can get sucked into the tube seal the hole when the central suction line is powered. A pair of springs connected to the suction cup’s base helps maintain the plug’s position, restoring the plug seal in the absence of object forces. If the cup makes contact with an object, a hinge action raises the plug away from the suction tube. The moment the cup’s lips are pushed against the object, the passive reaction forces from the cup lips are transferred to the rubber base of the cup, which stretches over the collar and allow the structure to compress. Acting as a pivot for the hinges, the collar causes the hinges to rotate and the edges of the hinges slide along the underside of the flange and raise the plug away from the suction tube opening. As a result, the suction cup self-seals when not in contact with an object and self-opens the cup’s lips makes contacts with an object.\n\nIn 2015, several improvements were made to the design of the self-sealing suction cup to improve its grasping capabilities. The previous design demonstrated the following flaws:\n\n\nTo address these flaws, researchers from ARL decreased the number of components by consolidating the functions of several parts, which reduced the uncompressed height of the suction cup by almost 50% to 0.72 cm. The cup diameter was also reduced to 1.07 cm. A lever system was added to the base of the cup, which pivots the collar to lift the plug. In addition, the tube doubles as a spring, which helps restore the levers and the plug to their closed position. A plastic restraint was added around the cup to aid with handling the hyper-extension, shear, and torsional forces.\n\nThe self-sealing suction cup has been subjected to a series of tests to determine the quality of its performance. A flexible test rig with four dime-sized suction cups and plastic ribs connected with rubber tubes was created for force-displacement and testing.\n\nA force-displacement test that compared the performance between the self-sealing suction cup, an identical suction cup, and a commercially available suction cup found that the internal structures of the self-sealing cup allowed more force to be exerted for the same displacement compared to the other cups. However, under identical conditions, the self-sealing cup achieved a maximum force of 12.5 N while the commercially available cup achieved a maximum force of 12.9 N.\n\nA seal quality test measured the pressure generated from each self-sealing suction cup. The results showed that an array of four cups maintained a pressure of 93.8% atmospheric. The test also demonstrated that not all the cups were equally efficient at sealing after object contact. However, this could be the result of variation in the cups’ prior usage.\n\nDuring object grasping testing where the grasping range was examined, the test rig successfully grasped about 80% of the objects attempted. These items consisted of the following: TV remote, pill bottle, glue stick, eyeglasses, fork, disposable bottle, toothpaste, coffee mug, bowl, plate, book, cell phone, bar of soap, paper money, mail, keys, show, table knife, medicine box, credit card, coin, pillow, hairbrush, non-disposable bottle, wallet, magazine, soda can, newspaper, scissors, wrist watch, purse, lighter, compact disc, telephone receiver, full wine bottle, full wine glass, light bulb, lock, padded volleyball, wooden block. (4) As a demonstration of the cups’ strength, the ARL researchers were able to pick up a full bottle of wine using only four of the dime-sized suction cups.\n\nThe self-sealing suction cups have been incorporated in robots to improve their passive grasping capabilities. Due to the design of the suction cups, a central vacuum source can be used to effectively generate suction force from the cups and reduce the number of actuators and sensors for the robot.\n\nResearchers from ARL designed and developed a three-finger hand actuator system using a 3D printer in order for the robot to properly utilize the self-sealing suction cups. Four suction cups run along the bottom of each finger, which contains a narrow vacuum channel running through the center. A central vacuum pump serves to power the suction cups and facilitate grasping. The fingers can also curl around the object to better grasp it and release any object in its hold by feeding back the output of the vacuum pump and emitting a burst of positive pressure.\n\nThe three-finger hand has been used by aerial systems and has demonstrated considerable success in grasping objects on the ground while maintaining flight. According to ARL researchers, the self-sealing suction cups may exhibit higher rates of success underwater due to the extra pressure from the sea depths surrounding and pressing against the object and grasper. However, they noted that an underwater environment would require different manufacturing materials that would allow the suction cups to perform well in salt water, such as a thermal plastic.\n"}
{"id": "1576445", "url": "https://en.wikipedia.org/wiki?curid=1576445", "title": "Sociological naturalism", "text": "Sociological naturalism\n\nSociological naturalism is a theory that states that the natural world and social world are roughly identical and governed by similar principles. Sociological naturalism, in sociological texts simply referred to as naturalism, can be traced back to the philosophical thinking of Auguste Comte in the 19th century, closely connected to positivism, which advocates use of the scientific method of the natural sciences in studying social sciences. It should not be identified too closely with Positivism, however, since whilst the latter advocates the use of controlled situations like experiments as sources of scientific information, naturalism insists that social processes should only be studied in their \"natural\" setting. A similar form of naturalism was applied to the scientific study of art and literature by Hippolyte Taine (see Race, milieu, and moment).\n\nContemporary sociologists do not generally dispute that social phenomena take place within the natural universe and, as such, are subject to natural constraints, such as the laws of physics. Up for debate is the nature of the distinctiveness of social phenomena as a subset of natural phenomena. Broad support exists for the antipositivist claim that crucial qualitative differences mean that one cannot explain social phenomena effectively using investigative tools or even standards of validity derived from other natural sciences. From this point of view, naturalism does not imply scientism. \n\nHowever, a classically positivist conflation of naturalism with scientism has not disappeared; this view is still dominant in some old and prestigious schools, such as the sociology departments at the University of Chicago in the United States, and McGill University in Montréal, Canada.\n\nMore recently, actor-network theory has analyzed the social construction of the nature/society distinction itself.\n\n"}
{"id": "7917758", "url": "https://en.wikipedia.org/wiki?curid=7917758", "title": "Software evolution", "text": "Software evolution\n\nSoftware evolution is the term used in software engineering (specifically software maintenance) to refer to the process of developing software initially, then repeatedly updating it for various reasons. \n\nFred Brooks, in his key book \"The Mythical Man-Month\", states that over 90% of the costs of a typical system arise in the maintenance phase, and that any successful piece of software will inevitably be maintained.\n\nIn fact, Agile methods stem from maintenance-like activities in and around web based technologies, where the bulk of the capability comes from frameworks and standards.\n\nSoftware maintenance address bug fixes and minor enhancements and software evolution focus on adaptation and migration.\nSoftware technologies will continue to develop. These changes will require new laws and theories to be created and justified. Some models as well would require additional aspects in developing future programs. Innovations and improvements do increase unexpected form of software development. The maintenance issues also would probably change as to adapt to the evolution of the future software. Software processes are themselves evolving, after going through learning and refinements, it is always improve their efficiency and effectiveness.\n\nThe need for software evolution comes from the fact that no one is able to predict how user requirements will evolve \"a priori\" . \nIn other words, the existing systems are never complete and continue to evolve. As they evolve, the complexity of the systems will grow unless there is a better solution available to solve these issues. The main objectives of software evolution are ensuring functional relevance, reliability and flexibility of the system. Software evolution can be fully manual (based on changes by software engineers), partially automated (e.g. using refactoring tools) or fully automated (with autonomous configuration or evolution). \n\nSoftware evolution has been greatly impacted by the Internet:\n\n\n\nE.B. Swanson initially identified the \nthree categories of maintenance: corrective, adaptive, and perfective. Four categories of software were then catalogued by Lientz and Swanson (1980).\nThese have since been updated and normalized internationally in the ISO/IEC 14764:2006:\n\n\nAll of the preceding take place when there is a known requirement for change.\n\nAlthough these categories were supplemented by many authors like Warren et al. (1999) and Chapin (2001), the ISO/IEC 14764:2006 international standard has kept the basic four categories.\n\nMore recently the description of software maintenance and evolution has been done using ontologies (Kitchenham et al. (1999), Deridder (2002), Vizcaíno (2003), Dias (2003), and Ruiz (2004)), which enrich the description of the many evolution activities.\n\nCurrent trends and practices are projected forward using a new model of software evolution called the staged model [1]. Staged model was introduced to replace conventional analysis which is less suitable for modern software development is rapid changing due to its difficulties of hard to contribute in software evolution. There are five distinct stages contribute in simple staged model (Initial development, Evolution, Servicing, Phase-out, and Close-down). \n\nProf. Meir M. Lehman, who worked at Imperial College London from 1972 to 2002, and his colleagues have identified a set of behaviours in the evolution of proprietary software. These behaviours (or observations) are known as Lehman's Laws, and there are eight of them:\n\n\nIt is worth mentioning that the applicability of all of these laws for all types of software systems has been studied by several researchers. For example, see a presentation by Nanjangud C Narendra where he describes a case study of an enterprise Agile project in the light of Lehman’s laws of software evolution. Some empirical observations coming from the study of open source software development appear to challenge some of the laws .\n\nThe laws predict that the need for functional change in a software system is inevitable, and not a consequence of incomplete or incorrect analysis of requirements or bad programming. They state that there are limits to what a software development team can achieve in terms of safely implementing changes and new functionality.\n\nMaturity Models specific to software evolution have been developed to improve processes, and help to ensure continuous rejuvenation of the software as it evolves iteratively.\n\nThe \"global process\" that is made by the many stakeholders (e.g. developers, users, their managers) has many feedback loops. The evolution speed is a function of the feedback loop structure and other characteristics of the global system. Process simulation techniques, such as system dynamics can be useful in understanding and managing such global process.\n\nSoftware evolution is not likely to be Darwinian, Lamarckian or Baldwinian, but an important phenomenon on its own. Given the increasing dependence on software at all levels of society and economy, the successful evolution of software is becoming increasingly critical. This is an important topic of research that hasn't received much attention.\n\nThe evolution of software, because of its rapid path in comparison to other man-made entities, was seen by Lehman as the \"fruit fly\" of the study of the evolution of artificial systems.\n\n\n\n"}
{"id": "235576", "url": "https://en.wikipedia.org/wiki?curid=235576", "title": "Suction", "text": "Suction\n\nSuction is the flow of a fluid into a partial vacuum, or region of low pressure. The pressure gradient between this region and the ambient pressure will propel matter toward the low pressure area. Dust is sucked into a vacuum cleaner when it is pushed in by the higher pressure air on the outside of the cleaner.\n\nThis is similar to what happens when humans breathe or drink through a straw. Both breathing and using a straw involve contracting the diaphragm and muscles around the rib cage. The increased volume in the chest cavity or thoracic cavity decreases the pressure inside, creating an imbalance with the ambient air pressure, or atmospheric pressure. This imbalance results in air pushing into the lungs or liquid pushing up through a straw and into the mouth.\n\nPumps typically have an inlet where the fluid (or air) enters the pump and an outlet where the fluid/air comes out. The inlet location is said to be at the suction side of the pump. The outlet location is said to be at the discharge side of the pump. Operation of the pump creates suction (a lower pressure) at the suction side so that fluid/air can enter the pump through the inlet. Pump operation also causes higher pressure at the discharge side by forcing the fluid/air out at the outlet. There may be pressure sensing devices at the pump's suction and/or discharge sides which control the operation of the pump. For example, if the suction pressure of a centrifugal pump is too high, a device may trigger the fluid pump to shut off to keep it from running dry; i. e. with no fluid entering.\n\nUnder normal conditions of atmospheric pressure suction can draw pure water up to a maximum height of approximately 10.3 m (33.9 feet). \n\nIn medicine, suction devices are used to clear airways of materials that would like to impede breathing or cause infections, to aid in surgery, and for other purposes.\n\n\n"}
{"id": "712222", "url": "https://en.wikipedia.org/wiki?curid=712222", "title": "Transit of Earth from Mars", "text": "Transit of Earth from Mars\n\nA transit of Earth across the Sun as seen from Mars takes place when the planet Earth passes directly between the Sun and Mars, obscuring a small part of the Sun's disc for an observer on Mars. During a transit, Earth would be visible from Mars as a small black disc moving across the face of the Sun. They occur every 26, 79 and 100 years, and every 1,000 years or so there is an extra 53rd-year transit.\n\nTransits of Earth from Mars usually occur in pairs, with one following the other after 79 years; rarely, there are three in the series. The transits also follow a 284-year cycle, occurring at intervals of 100.5, 79, 25.5, and 79 years; a transit falling on a particular date is usually followed by another transit 284 years later. Transits occurring when Mars is at its ascending node are in May, those at descending node happen in November. This cycle corresponds fairly closely to 151 Mars orbits, 284 Earth orbits, and 133 synodic periods, and is analogous to the cycle of transits of Venus from Earth, which follow a cycle of 243 years (121.5, 8, 105.5, 8). There are currently four such active series, containing from 8 to 25 transits. A new one is set to begin in 2394. The last series ending was in 1211.\n\nNo one has ever seen a transit of Earth from Mars, but the next transit will take place on November 10, 2084. The last such transit took place on May 11, 1984.\n\nDuring the event, the Moon could almost always also be seen in transit, although due to the distance between Earth and Moon, sometimes one body completes the transit before the other begins (this last occurred in the 1800 transit, and will happen again in 2394).\n\nA transit of Earth from Mars corresponds to Mars being perfectly uniformly illuminated at opposition from Earth, its phase being 180.0° without any defect of illumination. During the 1879 event, this permitted Charles Augustus Young to attempt a careful measurement of the oblateness (polar compression) of Mars. He obtained the value 1/219, or 0.0046. This is close to the modern value of 1/154 (many sources will cite somewhat different values, such as 1/193, because even a difference of only a couple of kilometers in the values of Mars' polar and equatorial radii gives a considerably different result).\n\nMuch more recently, better measurements of the oblateness of Mars have been made by using radar from the Earth. Also, better measurements have been made by using artificial satellites that have been put into orbit around Mars, including \"Mariner 9\", \"Viking 1\", \"Viking 2\", and Soviet orbiters, and the more recent orbiters that have been sent from the Earth to Mars.\n\nA science fiction short story published in 1971 by Arthur C. Clarke, called \"Transit of Earth\", depicts a doomed astronaut on Mars observing the transit in 1984. This short story was first published in the January 1971 issue of \"Playboy\" magazine.\n\nSometimes Earth only grazes the Sun during a transit. In this case it is possible that in some areas of Mars a full transit can be seen while in other regions there is only a partial transit (no second or third contact). The last transit of this type was on 30 April 1211, and the next such transit will occur on 27 November 4356. It is also possible that a transit of Earth can be seen in some parts of Mars as a partial transit, while in others Earth misses the Sun. Such a transit last occurred on 26 October 664, and the next transit of this type will occur on 14 December 5934.\n\nThe simultaneous occurrence of a transit of Venus and a transit of Earth is extremely rare, and will next occur in the year 571,471.\n\n\n\n"}
{"id": "7879916", "url": "https://en.wikipedia.org/wiki?curid=7879916", "title": "Vacuum cementing", "text": "Vacuum cementing\n\nVacuum cementing or vacuum welding is the natural process of solidifying small objects in a hard vacuum. The most notable example is dust on the surface of the Moon.\n\nThis effect was reported to be a problem with the first American and Soviet satellites, as small moving parts would seize together.\n\nIn 2009 the European Space Agency published a peer-reviewed paper detailing why cold welding is a significant issue that spacecraft designers need to carefully consider. The conclusions of this appropriately titled study can be found on page 25 of \"Assessment of Cold Welding between Separable Contact Surfaces due to Impact and Fretting under Vacuum\". The paper also cites a documented example from 1991 with the Galileo spacecraft high-gain antenna (see page 2; the technical source document from NASA regarding the Galileo spacecraft is also provided in a link here).\n\nOne source of difficulty is that vacuum (aka cold) welding does not exclude relative motion between the surfaces that are to be joined. This allows the broadly defined notions of galling, fretting, sticking, stiction and adhesion to overlap in some instances. For example, it is possible for a joint to be the result of both vacuum welding and galling (and/or fretting and/or impact). Galling and vacuum welding, therefore, are not mutually exclusive.\n\n\n"}
{"id": "469990", "url": "https://en.wikipedia.org/wiki?curid=469990", "title": "Whirlpool", "text": "Whirlpool\n\nA whirlpool is a body of rotating water produced by the meeting of opposing currents. The vast majority of whirlpools are not very powerful and very small whirlpools can be easily seen when a bath or a sink is draining. More powerful ones in seas or oceans may be termed maelstroms. \"Vortex\" is the proper term for any whirlpool that has a downdraft.\n\nIn oceans, in narrow straits with fast flowing water, whirlpools are normally caused by tides; there are few stories of large ships ever being sucked into such a maelstrom, although smaller craft are in danger. Smaller whirlpools also appear at the base of many waterfalls and can also be observed downstream from manmade structures such as weirs and dams. In the case of powerful waterfalls, like Niagara Falls, these whirlpools can be quite strong.\n\nThe Maelstrom of Saltstraumen is the Earth's strongest maelstrom, and is located close to the Arctic Circle, round the bay on the Highway 17, south-east of the city of Bodø, Norway. The strait at its narrowest is in width and water \"funnels\" through the channel four times a day. It is estimated that of water passes the narrow strait during this event. The water is creamy in colour and most turbulent during high tide, which is witnessed by thousands of tourists. It reaches speeds of , with mean speed of about . As navigation is dangerous in this strait only a small slot of time is available for large ships to pass through. Its impressive strength is caused by the world's strongest tide occurring in the same location during the new and full moon. A narrow channel of length connects the outer Saltfjord with its extension, the large Skjerstadfjord, causing a colossal tide which in turn produces the Saltstraumen maelstrom.\n\nMoskstraumen is an unusual system of whirlpools in the open seas in the Lofoten Islands off the Norwegian coast. It is the second strongest whirlpool in the world with flow currents reaching speeds as high as . It finds mention in several books and movies.\n\nThe Moskstraumen is formed by the combination of powerful semi-diurnal tides and the unusual shape of the seabed, with a shallow ridge between the Moskenesøya and Værøy islands which amplifies and whirls the tidal currents.\n\nThe fictional depictions of the Maelstrom by Edgar Allan Poe, Jules Verne, and Cixin Liu describe it as a gigantic circular vortex that reaches the bottom of the ocean, when in fact it is a set of currents and crosscurrents with a rate of . Poe described this phenomenon in his short story \"A Descent into the Maelstrom,\" which in 1841 was the first to use the word \"maelstrom\" in the English language; in this story related to the Lofoten Maelstrom, two fishermen are swallowed by the maelstrom while one survives miraculously.\n\nThe Corryvreckan is a narrow strait between the islands of Jura and Scarba, in Argyll and Bute, on the northern side of the Gulf of Corryvreckan, Scotland. It is the third-largest whirlpool in the world. Flood tides and inflow from the Firth of Lorne to the west can drive the waters of Corryvreckan to waves of over , and the roar of the resulting maelstrom, which reaches speeds of , can be heard away. Though it was initially classified as non-navigable by the British navy it was later categorized as \"extremely dangerous\".\n\nA documentary team from Scottish independent producers Northlight Productions once threw a mannequin into the Corryvreckan (\"the Hag\") with a life jacket and depth gauge. The mannequin was swallowed and spat up far down current with a depth gauge reading of with evidence of being dragged along the bottom for a great distance.\n\nOld Sow whirlpool is located between Deer Island, New Brunswick, Canada, and Moose Island, Eastport, Maine, USA. It is given the epithet \"pig-like\" as it makes a screeching noise when the vortex is at its full fury and reaches speeds of up to . The smaller whirlpools around this Old Sow are known as \"Piglets.\n\nThe Naruto whirlpools are located in the Naruto Strait near Awaji Island in Japan, which have speeds of .\n\nSkookumchuck Narrows is a tidal rapids that develops whirlpools, on the Sunshine Coast, Canada with current speeds exceeding .\n\nFrench Pass () is a narrow and treacherous stretch of water that separates D'Urville Island from the north end of the South Island of New Zealand. In 2000 a whirlpool there caught student divers, resulting in fatalities.\n\nThere was a short-lived whirlpool that sucked in a portion of the 1300 acre (~530 hectares) Lake Peigneur in Louisiana, United States after a drilling mishap in November 1980. This was not a naturally occurring whirlpool, but a man-made disaster caused by underwater drillers breaking through the roof of a salt mine. The lake then drained into the mine until the mine filled and the water levels equalized but the ten-foot deep lake was now 1,300 feet deep. This mishap resulted in destruction of five houses, loss of nineteen barges and eight tug boats, oil rigs, a mobile home, and most of a botanical garden. The adjacent settlement of Jefferson Island was reduced in area by 10%. A crater 0.5-mile (~1km) across was left behind. Nine of the barges which had sunk floated back.\n\nA more recent example of a man-made whirlpool that received significant media coverage was in early June 2015, when an intake vortex formed in Lake Texoma, on the Oklahoma–Texas border, near the floodgates of the dam that forms the lake. At the time of the whirlpool's formation, the lake was being drained after reaching its highest level ever. The Army Corps of Engineers, which operates the dam and lake, expected that the whirlpool would last until the lake reached normal seasonal levels by late July.\n\nPowerful whirlpools have killed unlucky seafarers, but their power tends to be exaggerated by laymen. There are virtually no stories of large ships ever being sucked into a whirlpool. Tales like those by Paul the Deacon, Edgar Allan Poe, and Jules Verne are entirely fictional.\n\nHowever, temporary whirlpools caused by major engineering disasters are capable of submerging large ships. A prominent example is the drilling disaster that occurred on November 20, 1980, in Lake Peigneur. A drilling platform, eleven barges, several trees, and multiple acres of the surrounding terrain were submerged by the resulting whirlpool. Days after the disaster, once the water pressure equalized, nine of the eleven sunken barges popped out of the whirlpool and refloated on the lake's surface.\n\nApart from Poe and Verne other literary source is of the 1500s, of Olaus Magnus, a Swedish Bishop, who had stated that the maelstrom which was more powerful than \"The Odyssey\" destroyed ships which sank to the bottom of the sea, and even whales were sucked in. Pytheas, the Greek historian, also mentioned that maelstroms swallowed ships and threw them up again.\n\nCharybdis in Greek mythology was later rationalized as a whirlpool, which sucked entire ships into its fold in the narrow coast of Sicily, a disaster faced by navigators.\n\nIn the 8th century, Paul the Deacon, who had lived among the Belgii, described tidal bores and the maelstrom for a Mediterranean audience unused to such violent tidal surges:\n\nThree of the most notable literary references to the Lofoten Maelstrom date from the nineteenth century. The first is the Edgar Allan Poe short story \"A Descent into the Maelström\" (1841). The second is \"20,000 Leagues Under the Sea\" (1870), the famous novel by Jules Verne. At the end of this novel, Captain Nemo seems to commit suicide, sending his \"Nautilus\" submarine into the Maelstrom (although in Verne's sequel Nemo and the Nautilus were seen to have survived). The \"Norway maelstrom\" is also mentioned in Herman Melville's \"Moby-Dick\".\n\nIn the 'Life of St Columba', the author, Adomnan of Iona', attributes to the saint miraculous knowledge of a particular bishop who ran into a whirlpool off the coast of Ireland. In Adomnan's narrative, he quotes Columba saying\n\nOne of the earliest uses in English of the Scandinavian word (\"malström\" or \"malstrøm\") was by Edgar Allan Poe in his short story \"A Descent into the Maelström\" (1841). In turn, the Nordic word is derived from the Dutch \"maelstrom\", modern spelling \"maalstroom\", from \"malen\" (\"to grind\") and \"stroom\" (\"stream\"), to form the meaning \"grinding current\" or literally \"mill-stream\", in the sense of milling (grinding) grain.\n\n\n\n"}
{"id": "18365403", "url": "https://en.wikipedia.org/wiki?curid=18365403", "title": "Wildland fire emission", "text": "Wildland fire emission\n\nWildland fire and wildland fire atmospheric emissions have been a part of the global biosphere for millennia. The major wildland fire emissions include greenhouse gasses and several criteria pollutants that impact human health and welfare.:\nCompared to the preindustrial era, wildland land fire in the conterminous U.S. has been reduced 90 percent with proportional reductions in wildland fire emissions. Land use changes (agriculture and urbanization) are responsible for roughly 50 percent of this decrease, and land management decisions (land fragmentation, suppression actions, etc.) are responsible for the remainder. Anthropogenic activities (e.g., industrial production, transportation, agriculture, etc.) today have more than replaced the lost preindustrial wildland fire atmospheric emissions.\n\nThe following charts compare preindustrial wildland fire emissions with contemporary emissions.\n"}
{"id": "33959", "url": "https://en.wikipedia.org/wiki?curid=33959", "title": "Witchcraft", "text": "Witchcraft\n\nWitchcraft or witchery broadly means the practice of and belief in magical skills and abilities exercised by solitary practitioners and groups. \"Witchcraft\" is a broad term that varies culturally and societally, and thus can be difficult to define with precision, and cross-cultural assumptions about the meaning or significance of the term should be applied with caution. Witchcraft often occupies a religious divinatory or medicinal role, and is often present within societies and groups whose cultural framework includes a magical world view.\n\nThe concept of witchcraft and the belief in its existence have persisted throughout recorded history. They have been present or central at various times and in many diverse forms among cultures and religions worldwide, including both \"primitive\" and \"highly advanced\" cultures, and continue to have an important role in many cultures today. Scientifically, the existence of magical powers and witchcraft are generally believed to lack credence and to be unsupported by high-quality experimental testing, although individual witchcraft practices and effects may be open to scientific explanation or explained via mentalism and psychology.\n\nHistorically, the predominant concept of witchcraft in the Western world derives from Old Testament laws against witchcraft, and entered the mainstream when belief in witchcraft gained Church approval in the Early Modern Period. It posits a theosophical conflict between good and evil, where witchcraft was generally evil and often associated with the Devil and Devil worship. This culminated in deaths, torture and scapegoating (casting blame for human misfortune), and many years of large scale witch-trials and witch hunts, especially in Protestant Europe, before largely ceasing during the European Age of Enlightenment. Christian views in the modern day are diverse and cover the gamut of views from intense belief and opposition (especially from Christian fundamentalists) to non-belief, and in some churches even approval. From the mid-20th century, witchcraft – sometimes called contemporary witchcraft to clearly distinguish it from older beliefs – became the name of a branch of modern paganism. It is most notably practiced in the Wiccan and modern witchcraft traditions, and no longer practices in secrecy.\n\nThe Western mainstream Christian view is far from the only societal perspective about witchcraft. Many cultures worldwide continue to have widespread practices and cultural beliefs that are loosely translated into English as \"witchcraft\", although the English translation masks a very great diversity in their forms, magical beliefs, practices, and place in their societies. During the Age of Colonialism, many cultures across the globe were exposed to the modern Western world via colonialism, usually accompanied and often preceded by intensive Christian missionary activity \"(see \"Christianization\")\". Beliefs related to witchcraft and magic in these cultures were at times influenced by the prevailing Western concepts. Witch hunts, scapegoating, and killing or shunning of suspected witches still occurs in the modern era, with killings both of victims for their supposedly magical body parts, and of suspected witchcraft practitioners.\n\nSuspicion of modern medicine due to beliefs about illness being due to witchcraft also continues in many countries to this day, with tragic healthcare consequences. HIV/AIDS and Ebola virus disease are two examples of often-lethal infectious disease epidemics whose medical care and containment has been severely hampered by regional beliefs in witchcraft. Other severe medical conditions whose treatment is hampered in this way include tuberculosis, leprosy, epilepsy and the common severe bacterial Buruli ulcer. Public healthcare often requires considerable education work related to epidemology and modern health knowledge in many parts of the world where belief in witchcraft prevails, to encourage effective preventive health measures and treatments, to reduce victim blaming, shunning and stigmatization, and to prevent the killing of people and endangering of animal species for body parts believed to convey magical abilities.\n\nThe word \"witch\" is of uncertain origin. There are numerous etymologies that it could be derived from. One popular belief is that it is \"related to the English words wit, wise, wisdom [Germanic root *weit-, *wait-, *wit-; Indo-European root *weid-, *woid-, *wid-],\" so \"craft of the wise.\" Another is from the Old English wiccecræft, a compound of \"wicce\" (\"witch\") and \"cræft\" (\"craft\").\n\nIn anthropological terminology, witches differ from sorcerers in that they don't use physical tools or actions to curse; their maleficium is perceived as extending from some intangible inner quality, and one may be unaware of being a witch, or may have been convinced of his/her nature by the suggestion of others. This definition was pioneered in a study of central African magical beliefs by E. E. Evans-Pritchard, who cautioned that it might not correspond with normal English usage.\n\nHistorians of European witchcraft have found the anthropological definition difficult to apply to European witchcraft, where witches could equally use (or be accused of using) physical techniques, as well as some who really had attempted to cause harm by thought alone. European witchcraft is seen by historians and anthropologists as an ideology for explaining misfortune; however, this ideology has manifested in diverse ways, as described below.\n\nHistorically the witchcraft label has been applied to practices people believe influence the mind, body, or property of others against their will—or practices that the person doing the labeling believes undermine social or religious order. Some modern commentators believe the malefic nature of witchcraft is a Christian projection. The concept of a magic-worker influencing another person's body or property against their will was clearly present in many cultures, as traditions in both folk magic and religious magic have the purpose of countering malicious magic or identifying malicious magic users. Many examples appear in early texts, such as those from ancient Egypt and Babylonia. Malicious magic users can become a credible cause for disease, sickness in animals, bad luck, sudden death, impotence and other such misfortunes. Witchcraft of a more benign and socially acceptable sort may then be employed to turn the malevolence aside, or identify the supposed evil-doer so that punishment may be carried out. The folk magic used to identify or protect against malicious magic users is often indistinguishable from that used by the witches themselves.\n\nThere has also existed in popular belief the concept of white witches and white witchcraft, which is strictly benevolent. Many neopagan witches strongly identify with this concept, and profess ethical codes that prevent them from performing magic on a person without their request.\n\nWhere belief in malicious magic practices exists, such practitioners are typically forbidden by law as well as hated and feared by the general populace, while beneficial magic is tolerated or even accepted wholesale by the people – even if the orthodox establishment opposes it.\n\nProbably the most widely known characteristic of a witch was the ability to cast a spell, \"spell\" being the word used to signify the means employed to carry out a magical action. A spell could consist of a set of words, a formula or verse, or a ritual action, or any combination of these. Spells traditionally were cast by many methods, such as by the inscription of runes or sigils on an object to give it magical powers; by the immolation or binding of a wax or clay image (poppet) of a person to affect him or her magically; by the recitation of incantations; by the performance of physical rituals; by the employment of magical herbs as amulets or potions; by gazing at mirrors, swords or other specula (scrying) for purposes of divination; and by many other means.\n\nIn Christianity and Islam, sorcery came to be associated with heresy and apostasy and to be viewed as evil. Among the Catholics, Protestants, and secular leadership of the European Late Medieval/Early Modern period, fears about witchcraft rose to fever pitch and sometimes led to large-scale witch-hunts. The key century was the fifteenth, which saw a dramatic rise in awareness and terror of witchcraft, culminating in the publication of the \"Malleus Maleficarum\" but prepared by such fanatical popular preachers as Bernardino of Siena. Throughout this time, it was increasingly believed that Christianity was engaged in an apocalyptic battle against the Devil and his secret army of witches, who had entered into a diabolical pact. In total, tens or hundreds of thousands of people were executed, and others were imprisoned, tortured, banished, and had lands and possessions confiscated. The majority of those accused were women, though in some regions the majority were men. In early modern Scots, the word Warlock came to be used as the male equivalent of witch (which can be male or female, but is used predominantly for females). From this use, the word passed into Romantic literature and ultimately 20th-century popular culture. Accusations of witchcraft were often combined with other charges of heresy against such groups as the Cathars and Waldensians.\n\nThe \"Malleus Maleficarum,\" (Latin for \"Hammer of The Witches\") was a witch-hunting manual written in 1486 by two German monks, Heinrich Kramer and Jacob Sprenger. It was used by both Catholics and Protestants for several hundred years, outlining how to identify a witch, what makes a woman more likely than a man to be a witch, how to put a witch on trial, and how to punish a witch. The book defines a witch as evil and typically female. The book became the handbook for secular courts throughout Renaissance Europe, but was not used by the Inquisition, which even cautioned against relying on the work, and was later officially condemned by the Catholic Church in 1490.\n\nIn the modern Western world, witchcraft accusations have often accompanied the satanic ritual abuse moral panic. Such accusations are a counterpart to blood libel of various kinds, which may be found throughout history across the globe.\n\nThroughout the early modern period, the English term \"witch\" was not exclusively negative in meaning, and could also indicate cunning folk. As Alan McFarlane noted, \"There were a number of interchangeable terms for these practitioners, 'white', 'good', or 'unbinding' witches, blessers, wizards, sorcerers, however 'cunning-man' and 'wise-man' were the most frequent.\" The contemporary Reginald Scot explained, \"At this day it is indifferent to say in the English tongue, 'she is a witch' or 'she is a wise woman'\". Folk magicians throughout Europe were often viewed ambivalently by communities, and were considered as capable of harming as of healing, which could lead to their being accused as \"witches\" in the negative sense. Many English \"witches\" convicted of consorting with demons seem to have been cunning folk whose fairy familiars had been demonised; many French \"devins-guerisseurs\" (\"diviner-healers\") were accused of witchcraft, and over one half the accused witches in Hungary seem to have been healers.\n\nSome of the healers and diviners historically accused of witchcraft have considered themselves mediators between the mundane and spiritual worlds, roughly equivalent to shamans. Such people described their contacts with fairies, spirits often involving out-of-body experiences and travelling through the realms of an \"other-world\".\nBeliefs of this nature are implied in the folklore of much of Europe, and were explicitly described by accused witches in central and southern Europe. Repeated themes include participation in processions of the dead or large feasts, often presided over by a horned male deity or a female divinity who teaches magic and gives prophecies; and participation in battles against evil spirits, \"vampires\", or \"witches\" to win fertility and prosperity for the community.\n\nÉva Pócs states that reasons for accusations of witchcraft fall into four general categories:\n\nShe identifies three varieties of witch in popular belief:\n\"Neighbourhood witches\" are the product of neighbourhood tensions, and are found only in self-sufficient serf village communities where the inhabitants largely rely on each other. Such accusations follow the breaking of some social norm, such as the failure to return a borrowed item, and any person part of the normal social exchange could potentially fall under suspicion. Claims of \"sorcerer\" witches and \"supernatural\" witches could arise out of social tensions, but not exclusively; the supernatural witch in particular often had nothing to do with communal conflict, but expressed tensions between the human and supernatural worlds; and in Eastern and Southeastern Europe such supernatural witches became an ideology explaining calamities that befell entire communities.\n\nBelief in witchcraft continues to be present today in some societies and accusations of witchcraft are the trigger of serious forms of violence, including murder. Such incidents are common in places such as Burkina Faso, Ghana, India, Kenya, Malawi, Nepal and Tanzania. Accusations of witchcraft are sometimes linked to personal disputes, jealousy, and conflicts between neighbors or family over land or inheritance. Witchcraft-related violence is often discussed as a serious issue in the broader context of violence against women.\n\nIn Tanzania, about 500 older women are murdered each year following accusations against them of witchcraft or of being a witch. Apart from extrajudicial violence, there is also state-sanctioned violence in some jurisdictions. For instance, in Saudi Arabia practicing witchcraft and sorcery is a crime punishable by death and the country has executed people for this crime in 2011, 2012 and 2014.\n\nChildren in some regions of the world, such as parts of Africa, are also vulnerable to violence related to witchcraft accusations. Such incidents have also occurred in immigrant communities in the UK, including the much publicized case of the murder of Victoria Climbié.\n\nModern practices identified by their practitioners as \"witchcraft\" have grown dramatically since the early 20th century. Generally portrayed as revivals of pre-Christian European ritual and spirituality, they are understood to involve varying degrees of magic, shamanism, folk medicine, spiritual healing, calling on elementals and spirits, veneration of ancient deities and archetypes, and attunement with the forces of nature.\n\nThe first Neopagan groups to publicly appear, during the 1950s and 60s, were Gerald Gardner's Bricket Wood coven and Roy Bowers' Clan of Tubal Cain. They operated as initiatory secret societies. Other individual practitioners and writers such as Paul Huson also claimed inheritance to surviving traditions of witchcraft.\n\nDuring the 20th century, interest in witchcraft in English-speaking and European countries began to increase, inspired particularly by Margaret Murray's theory of a pan-European witch-cult originally published in 1921, since discredited by further careful historical research. Interest was intensified, however, by Gerald Gardner's claim in 1954 in \"Witchcraft Today\" that a form of witchcraft still existed in England. The truth of Gardner's claim is now disputed too, with different historians offering evidence for or against the religion's existence prior to Gardner.\n\nThe Wicca that Gardner initially taught was a witchcraft religion having a lot in common with Margaret Murray's hypothetically posited cult of the 1920s. Indeed, Murray wrote an introduction to Gardner's \"Witchcraft Today\", in effect putting her stamp of approval on it. Wicca is now practised as a religion of an initiatory secret society nature with positive ethical principles, organised into autonomous covens and led by a High Priesthood. There is also a large \"Eclectic Wiccan\" movement of individuals and groups who share key Wiccan beliefs but have no initiatory connection or affiliation with traditional Wicca. Wiccan writings and ritual show borrowings from a number of sources including 19th and 20th-century ceremonial magic, the medieval grimoire known as the Key of Solomon, Aleister Crowley's Ordo Templi Orientis and pre-Christian religions. Both men and women are equally termed \"witches.\" They practice a form of duotheistic universalism.\n\nSince Gardner's death in 1964, the Wicca that he claimed he was initiated into has attracted many initiates, becoming the largest of the various witchcraft traditions in the Western world, and has influenced other Neopagan and occult movements.\n\nWiccan literature has been described as aiding the empowerment of young women through its lively portrayal of female protagonists. Part of the recent growth in Neo-Pagan religions has been attributed to the strong media presence of fictional works such as the Buffy the Vampire Slayer and Harry Potter series with their depictions of witchcraft. Widespread accessibility to related material through internet media such as chat rooms and forums is also thought to be driving this development. Wiccan beliefs are currently often found to be compatible with liberal ideals such as the Green movement, and particularly with feminism by providing young women with means for empowerment and for control of their own lives. This is the case particularly in North America due to the strong presence of feminist ideals. The 2002 study Enchanted Feminism: The Reclaiming Witches of San Francisco suggests that Wiccan religion represents the second wave of feminism that has also been redefined as a religious movement.\n\nStregheria is an Italian witchcraft religion popularised in the 1980s by Raven Grimassi, who claims that it evolved within the ancient Etruscan religion of Italian peasants who worked under the Catholic upper classes.\n\nModern Stregheria closely resembles Charles Leland's controversial late-19th-century account of a surviving Italian religion of witchcraft, worshipping the Goddess Diana, her brother Dianus/Lucifer, and their daughter Aradia. Leland's witches do not see Lucifer as the evil Satan that Christians see, but a benevolent god of the Sun and Moon).\n\nThe ritual format of contemporary Stregheria is roughly similar to that of other Neopagan witchcraft religions such as Wicca. The pentagram is the most common symbol of religious identity. Most followers celebrate a series of eight festivals equivalent to the Wiccan Wheel of the Year, though others follow the ancient Roman festivals. An emphasis is placed on ancestor worship.\n\nTraditional witchcraft is a term used to refer to a variety of contemporary forms of witchcraft. Pagan studies scholar Ethan Doyle White described it as \"a broad movement of aligned magico-religious groups who reject any relation to Gardnerianism and the wider Wiccan movement, claiming older, more \"traditional\" roots. Although typically united by a shared aesthetic rooted in European folklore, the Traditional Craft contains within its ranks a rich and varied array of occult groups, from those who follow a contemporary Pagan path that is suspiciously similar to Wicca to those who adhere to Luciferianism\". According to British Traditional Witch Michael Howard, the term refers to \"any non-Gardnerian, non-Alexandrian, non-Wiccan or pre-modern form of the Craft, especially if it has been inspired by historical forms of witchcraft and folk magic\". Another definition was offered by Daniel A. Schulke, the current Magister of the Cultus Sabbati, when he proclaimed that \"traditional witchcraft\" \"refers to a coterie of initiatory lineages of ritual magic, spellcraft and devotional mysticism\". Some forms of traditional witchcraft are the Feri Tradition, Cochrane's Craft and the Sabbatic craft.\n\nSatanism is a broad term referring to diverse beliefs that share a symbolic association with, or admiration for, Satan, who is seen as a liberating figure. While it is heir to the same historical period and pre-Enlightenment beliefs that gave rise to modern witchcraft, it is generally seen as completely separate from modern witchcraft and Wicca, and has little or no connection to them.\n\nModern witchcraft considers Satanism to be the \"dark side of Christianity\" rather than a branch of Wicca: – the character of Satan referenced in Satanism exists only in the theology of the three Abrahamic religions, and Satanism arose as, and occupies the role of, a rebellious counterpart to Christianity, in which all is permitted and the self is central. (Christianity can be characterized as having the diametrically opposite views to these.) Such beliefs become more visibly expressed in Europe after the Enlightenment, when works such as Milton's \"Paradise Lost\" were described anew by romantics who suggested that they presented the biblical Satan as an allegory representing crisis of faith, individualism, free will, wisdom and enlightenment; a few works from that time also begin to directly present Satan in a less negative light, such as \"Letters from the Earth\". The two major trends are theistic Satanism and atheistic Satanism; the former venerates Satan as a supernatural patriarchal deity, while the latter views Satan as merely a symbolic embodiment of certain human traits.\n\nOrganized groups began to emerge in the mid 20th century, including the Ophite Cultus Satanas (1948) and The Church of Satan (1966). After seeing Margaret Murray's book \"The God of the Witches\" the leader of Ophite Cultus Satanas, Herbert Arthur Sloane, said he realized that the horned god was Satan (\"Sathanas\"). Sloane also corresponded with his contemporary Gerald Gardner, founder of the Wicca religion, and implied that his views of Satan and the horned god were not necessarily in conflict with Gardner's approach. However, he did believe that, while \"gnosis\" referred to knowledge, and \"Wicca\" referred to wisdom, modern witches had fallen away from the true knowledge, and instead had begun worshipping a fertility god, a reflection of the creator god. He wrote that \"the largest existing body of witches who are true Satanists would be the Yezedees\". Sloane highly recommended the book \"The Gnostic Religion\", and sections of it were sometimes read at ceremonies. It was estimated that there were up to 100,000 Satanists worldwide by 2006, twice the number estimated in 1990. Satanistic beliefs have been largely permitted as a valid expression of religious belief in the West. For example, they were allowed in the British Royal Navy in 2004, and an appeal was considered in 2005 for religious status as a right of prisoners by the Supreme Court of the United States. Contemporary Satanism is mainly an American phenomenon, although it began to reach Eastern Europe in the 1990s around the time of the fall of the Soviet Union.\n\nLuciferianism, on the other hand, is a belief system and does not revere the devil figure or most characteristics typically affixed to Satan. Rather, Lucifer in this context is seen as one of many morning stars, a symbol of enlightenment, independence and human progression. Madeline Montalban was an English witch who adhered to a specific form of luciferianism which revolved around the veneration of Lucifer, or Lumiel, whom she considered to be a benevolent angelic being who had aided humanity's development. Within her Order, she emphasised that her followers discover their own personal relationship with the angelic beings, including Lumiel. Although initially seeming favourable to Gerald Gardner, by the mid-1960s she had become hostile towards him and his Gardnerian tradition, considering him to be \"a 'dirty old man' and sexual pervert.\" She also expressed hostility to another prominent Pagan Witch of the period, Charles Cardell, although in the 1960s became friends with the two Witches at the forefront of the Alexandrian Wiccan tradition, Alex Sanders and his wife, Maxine Sanders, who adopted some of her Luciferian angelic practices. In contemporary times luciferian witches exist within traditional witchcraft.\n\nThe belief in sorcery and its practice seem to have been widespread in the Ancient Near East and Nile Valley. It played a conspicuous role in the cultures of ancient Egypt and in Babylonia. The latter tradition included an Akkadian anti-witchcraft ritual, the Maqlû. A section from the Code of Hammurabi (about 2000 B.C.) prescribes:\n\nAccording to the New Advent Catholic Encyclopedia: \n\nThe King James Version uses the words \"witch\", \"witchcraft\", and \"witchcrafts\" to translate the Masoretic \"kāsháf\" () and (\"qésem\"); these same English terms are used to translate \"pharmakeia\" in the Greek New Testament. Verses such as and (\"Thou shalt not suffer a witch to live\") thus provided scriptural justification for Christian witch hunters in the early modern period (see Christian views on magic).\n\nThe precise meaning of the Hebrew , usually translated as \"witch\" or \"sorceress\", is uncertain. In the Septuagint, it was translated as \"pharmakeía\" or \"pharmakous\". In the 16th century, Reginald Scot, a prominent critic of the witch trials, translated , φαρμακεία, and the Vulgate's Latin equivalent \"veneficos\" as all meaning \"poisoner\", and on this basis, claimed that \"witch\" was an incorrect translation and poisoners were intended. His theory still holds some currency, but is not widely accepted, and in is listed alongside other magic practitioners who could interpret dreams: magicians, astrologers, and Chaldeans. Suggested derivations of include \"mutterer\" (from a single root) or \"herb user\" (as a compound word formed from the roots \"kash\", meaning \"herb\", and \"hapaleh\", meaning \"using\"). The Greek φαρμακεία literally means \"herbalist\" or one who uses or administers drugs, but it was used virtually synonymously with \"mageia\" and \"goeteia\" as a term for a sorcerer.\n\nThe Bible provides some evidence that these commandments against sorcery were enforced under the Hebrew kings:\nNote that the Hebrew word \"ob\", translated as \"familiar spirit\" in the above quotation, has a different meaning than the usual English sense of the phrase; namely, it refers to a spirit that the woman is familiar with, rather than to a spirit that physically manifests itself in the shape of an animal.\n\nThe New Testament condemns the practice as an abomination, just as the Old Testament had (Galatians 5:20, compared with Revelation 21:8; 22:15; and Acts 8:9; 13:6). The word in most New Testament translations is \"sorcerer\"/\"sorcery\" rather than \"witch\"/\"witchcraft\".\n\nJewish law views the practice of witchcraft as being laden with idolatry and/or necromancy; both being serious theological and practical offenses in Judaism. Although Maimonides vigorously denied the efficacy of all methods of witchcraft, and claimed that the Biblical prohibitions regarding it were precisely to wean the Israelites from practices related to idolatry. It is acknowledged that while magic exists, it is forbidden to practice it on the basis that it usually involves the worship of other gods. Rabbis of the Talmud also condemned magic when it produced something other than illusion, giving the example of two men who use magic to pick cucumbers (Sanhedrin 67a). The one who creates the illusion of picking cucumbers should not be condemned, only the one who actually picks the cucumbers through magic.\n\nHowever, some of the rabbis practiced \"magic\" themselves or taught the subject. For instance, Rabbah created a person and sent him to Rav Zeira, and Hanina and Hoshaiah studied every Friday together and created a small calf to eat on Shabbat (Sanhedrin 67b). In these cases, the \"magic\" was seen more as divine miracles (i.e., coming from God rather than \"unclean\" forces) than as witchcraft.\n\nJudaism does make it clear that Jews shall not try to learn about the ways of witches (Book of Deuteronomy 18: 9–10) and that witches are to be put to death (Exodus 22:17).\n\nJudaism's most famous reference to a medium is undoubtedly the Witch of Endor whom Saul consults, as recounted in 1 Samuel 28.\n\nDivination, and magic in Islam, encompass a wide range of practices, including black magic, warding off the evil eye, the production of amulets and other magical equipment, evocation, casting lots, and astrology. Muslims do commonly believe in magic (\"sihr\") and explicitly forbid its practice. \"Sihr\" translates from Arabic as sorcery or black magic. The best known reference to magic in Islam is surah al-Falaq of the Qur'an, which is known as a prayer to God to ward off black magic:\nAlso according to the Qur'an:\nIslam distinguishes between God-given gifts, which can heal sickness, and possession, and sorcery. Good supernatural powers are therefore a \"special gift from God\", whereas sorcery or black magic is achieved through help of jinn and demons. In the Qurʾānic narrative, the Prophet Sulayman had the power to speak with animals and command jinn, and he thanks God for this نعمة (i.e. gift, privilege, favour, bounty), which is only given to him with God’s permission. The Prophet Muhammad was accused of being a magician by his opponents.\n\nIt is a common belief that jinn can possess a human, thus requiring exorcism (\"ruqya\") derived from the Prophet's \"sunnah\" to cast off the jinn or devils from the body of the possessed. The practice of seeking help from the jinn is prohibited and can lead to possession. The \"ruqya\" contains verses of the Qur'an as well as prayers specifically targeted against demons. The knowledge of which verses of the Qur'an to use in what way is what is considered \"magic knowledge.\"\n\nA \"hadith\" recorded in states: \"Seventy thousand people of my followers will enter Paradise without accounts, and they are those who do not practice Ar-Ruqya and do not see an evil omen in things, and put their trust in their Lord.\" Ibn Qayyim al-Jawziyya, a scholar, commented on this \"hadith\", stating: That is because these people will enter Paradise without being called to account because of the perfection of their Tawheed, therefore he described them as people who did not ask others to perform ruqyah for them. Hence he said \"and they put their trust in their Lord.\" Because of their complete trust in their Lord, their contentment with Him, their faith in Him, their being pleased with Him and their seeking their needs from Him, they do not ask people for anything, be it ruqyah or anything else, and they are not influenced by omens and superstitions that could prevent them from doing what they want to do, because superstition detracts from and weakens Tawheed\".\n\nIbn al-Nadim hold, exorcists gain their power by their obedience to God, while sorcerers please the demons by acts of disobedience and sacrifices and they in return do him a favor. Being pious and strictly following the teachings of the Qur'an can increase the probability to perform magic or miracles, that is distinguished from witchcraft, the latter practised in aid with demons.\n\nA \"hadith\" recorded in narrates that one who has eaten seven Ajwa dates in the morning will not be adversely affected by magic in the course of that day.\n\nStudents of the history of religion have linked several magical practises in Islam with pre-Islamic Turkish and East African customs. Most notable of these customs is the Zār.\n\nIn Southern African traditions, there are three classifications of somebody who uses magic. The \"tagati\" is usually improperly translated into English as \"witch\", and is a spiteful person who operates in secret to harm others. The \"sangoma\" is a diviner, somewhere on a par with a fortune teller, and is employed in detecting illness, predicting a person's future (or advising them on which path to take), or identifying the guilty party in a crime. She also practices some degree of medicine. The \"inyanga\" is often translated as \"witch doctor\" (though many Southern Africans resent this implication, as it perpetuates the mistaken belief that a \"witch doctor\" is in some sense a \"practitioner\" of malicious magic). The \"inyanga\"s job is to heal illness and injury and provide customers with magical items for everyday use. Of these three categories the \"tagati\" is almost exclusively female, the \"sangoma\" is usually female, and the \"inyanga\" is almost exclusively male.\n\nMuch of what witchcraft represents in Africa has been susceptible to misunderstandings and confusion, thanks in no small part to a tendency among western scholars since the time of the now largely discredited Margaret Murray to approach the subject through a comparative lens vis-a-vis European witchcraft. Okeja argues that witchcraft in Africa today plays a very different social role than in Europe of the past—or present—and should be understood through an African rather than post-colonial Western lens.\n\nComplimentary remarks about witchcraft by a native Congolese initiate: \"From witchcraft ... may be developed the remedy (\"kimbuki\") that will do most to raise up our country.\" \"Witchcraft ... deserves respect ... it can embellish or redeem (\"ketula evo vuukisa\").\" \"The ancestors were equipped with the protective witchcraft of the clan (\"kindoki kiandundila kanda\"). ... They could also gather the power of animals into their hands ... whenever they needed. ... If we could make use of these kinds of witchcraft, our country would rapidly progress in knowledge of every kind.\" \"You witches (\"zindoki\") too, bring your science into the light to be written down so that ... the benefits in it ... endow our race.\"\n\nIn eastern Cameroon, the term used for witchcraft among the Maka is \"djambe\" and refers to a force inside a person; its powers may make the proprietor more vulnerable. It encompasses the occult, the transformative, killing and healing.\n\nIn some Central African areas, malicious magic users are believed by locals to be the source of terminal illness such as AIDS and cancer. In such cases, various methods are used to rid the person from the bewitching spirit, occasionally physical and psychological abuse. Children may be accused of being witches, for example a young niece may be blamed for the illness of a relative. Most of these cases of abuse go unreported since the members of the society that witness such abuse are too afraid of being accused of being accomplices. It is also believed that witchcraft can be transmitted to children by feeding. Parents discourage their children from interacting with people believed to be witches.\n\nEvery year, hundreds of people in the Central African Republic are convicted of witchcraft.\n\nChristian militias in the Central African Republic have also kidnapped, burnt and buried alive women accused of being 'witches' in public ceremonies.\n\n, between 25,000 and 50,000 children in Kinshasa, Democratic Republic of the Congo, had been accused of witchcraft and thrown out of their homes. These children have been subjected to often-violent abuse during exorcisms, sometimes supervised by self-styled religious pastors. Other pastors and Christian activists strongly oppose such accusations and try to rescue children from their unscrupulous colleagues. The usual term for these children is \"enfants sorciers\" (child witches) or \"enfants dits sorciers\" (children accused of witchcraft). In 2002, USAID funded the production of two short films on the subject, made in Kinshasa by journalists Angela Nicoara and Mike Ormsby.\n\nIn April 2008, in Kinshasa, the police arrested 14 suspected victims (of penis snatching) and sorcerers accused of using black magic or witchcraft to steal (make disappear) or shrink men's penises to extort cash for cure, amid a wave of panic.\n\nAccording to one study, the belief in magical warfare technologies (such as \"bulletproofing\") in the Eastern Democratic Republic of the Congo serves a group-level function, as it increases group efficiency in warfare, even if it is suboptimal at the individual level. The authors of the study argue that this is one reason why the belief in witchcraft persists.\n\nIn Ghana, women are often accused of witchcraft and attacked by neighbours. Because of this, there exist six witch camps in the country where women suspected of being witches can flee for safety. The witch camps, which exist solely in Ghana, are thought to house a total of around 1000 women. Some of the camps are thought to have been set up over 100 years ago. The Ghanaian government has announced that it intends to close the camps.\n\nArrests were made in an effort to avoid bloodshed seen in Ghana a decade ago, when 12 alleged penis snatchers were beaten to death by mobs. While it is easy for modern people to dismiss such reports, Uchenna Okeja argues that a belief system in which such magical practices are deemed possible offer many benefits to Africans who hold them. For example, the belief that a sorcerer has \"stolen\" a man's penis functions as an anxiety-reduction mechanism for men suffering from impotence while simultaneously providing an explanation that is consistent with African cultural beliefs rather than appealing to Western scientific notions that are tainted by the history of colonialism (at least for many Africans).\n\nIt was reported on May 21, 2008 that in Kenya, a mob had burnt to death at least 11 people accused of witchcraft.\n\nIn Malawi it is also common practice to accuse children of witchcraft and many children have been abandoned, abused and even killed as a result. As in other African countries both African traditional healers and their Christian counterparts are trying to make a living out of exorcising children and are actively involved in pointing out children as witches. Various secular and Christian organizations are combining their efforts to address this problem.\n\nAccording to William Kamkwamba, witches and wizards are afraid of money, which they consider a rival evil. Any contact with cash will snap their spell and leave the wizard naked and confused. So placing cash, such as kwacha around a room or bed mat will protect the resident from their malevolent spells.\n\nIn Nigeria, several Pentecostal pastors have mixed their evangelical brand of Christianity with African beliefs in witchcraft to benefit from the lucrative witch finding and exorcism business—which in the past was the exclusive domain of the so-called witch doctor or traditional healers. These pastors have been involved in the torturing and even killing of children accused of witchcraft. Over the past decade, around 15,000 children have been accused, and around 1,000 murdered. Churches are very numerous in Nigeria, and competition for congregations is hard. Some pastors attempt to establish a reputation for spiritual power by \"detecting\" child witches, usually following a death or loss of a job within a family, or an accusation of financial fraud against the pastor. In the course of \"exorcisms\", accused children may be starved, beaten, mutilated, set on fire, forced to consume acid or cement, or buried alive. While some church leaders and Christian activists have spoken out strongly against these abuses, many Nigerian churches are involved in the abuse, although church administrations deny knowledge of it.\n\nAmong the Mende (of Sierra Leone), trial and conviction for witchcraft has a beneficial effect for those convicted. \"The witchfinder had warned the whole village to ensure the relative prosperity of the accused and sentenced ... old people. ... Six months later all of the people ... accused, were secure, well-fed and arguably happier than at any [previous] time; they had hardly to beckon and people would come with food or whatever was needful. ... Instead of such old and widowed people being left helpless or (as in Western society) institutionalized in old people's homes, these were reintegrated into society and left secure in their old age ... . ... Old people are 'suitable' candidates for this kind of accusation in the sense that they are isolated and vulnerable, and they are 'suitable' candidates for 'social security' for precisely the same reasons.\"\n\nIn Kuranko language, the term for witchcraft is \"suwa'ye\" referring to \"extraordinary powers\".\n\nIn Tanzania in 2008, President Kikwete publicly condemned witchdoctors for killing albinos for their body parts, which are thought to bring good luck. 25 albinos have been murdered since March 2007. In Tanzania, albinos are often murdered for their body parts on the advice of witch doctors in order to produce powerful amulets that are believed to protect against witchcraft and make the owner prosper in life.\n\nBrua is an Afro-Caribbean religion and healing tradition that originates in Aruba, Bonaire, and Curaçao, in the Dutch Caribbean. A healer in this culture is called a \"kurioso\" or \"kuradó\", a man or woman who performs \"trabou chikí\" (little works) and \"trabou grandi\" (large treatments) to promote or restore health, bring fortune or misfortune, deal with unrequited love, and more serious concerns, in which sorcery is involved. Sorcery usually involves reference to the \"almasola\" or \"homber chiki\", a devil-like entity. \"Transcultural Psychiatry\" published a paper called \"Traditional healing practices originating in Aruba, Bonaire, and Curaçao: A review of the literature on psychiatry and Brua\" by Jan Dirk Blom, Igmar T. Poulina, Trevor L. van Gellecum and Hans W. Hoek of the Parnassia Psychiatric Institute.\n\nIn 1645, Springfield, Massachusetts, experienced America's first accusations of witchcraft when husband and wife Hugh and Mary Parsons accused each other of witchcraft. At America's first witch trial, Hugh was found innocent, while Mary was acquitted of witchcraft but sentenced to be hanged for the death of her child. She died in prison. From 1645–1663, about eighty people throughout England's Massachusetts Bay Colony were accused of practicing witchcraft. Thirteen women and two men were executed in a witch-hunt that lasted throughout New England from 1645–1663.\n\nThe Salem witch trials followed in 1692–93. These witch trials were the most famous in British North America and took place in the coastal settlements near Salem, Massachusetts. Prior to the witch trials, nearly 300 men and women had been suspected of partaking in witchcraft, and 19 of these people were hanged, and one was “pressed to death”. The Salem witch trials were a series of hearings before local magistrates followed by county court trials to prosecute people accused of witchcraft in Essex, Suffolk and Middlesex Counties of colonial Massachusetts, between February 1692 and May 1693. Over 150 people were arrested and imprisoned, with even more accused who were not formally pursued by the authorities. The two courts convicted 29 people of the capital felony of witchcraft. Nineteen of the accused, 14 women and 5 men, were hanged. One man who refused to enter a plea was crushed to death under heavy stones in an attempt to force him to do so. At least five more of the accused died in prison.\n\nDespite being generally known as the \"Salem\" witch trials, the preliminary hearings in 1692 were conducted in a variety of towns across the province: Salem Village (now Danvers), Salem Town, Ipswich, and Andover. The best known trials were conducted by the Court of Oyer and Terminer in 1692 in Salem Town. All 26 who went to trial before this court were convicted. The four sessions of the Superior Court of Judicature in 1693, held in Salem Town, but also in Ipswich, Boston, and Charlestown, produced only 3 convictions in the 31 witchcraft trials it conducted. Likewise, alleged witchcraft was not isolated to New England. In 1706 Grace Sherwood the \"Witch of Pungo\" was imprisoned for the crime in Princess Anne County, Virginia.\n\nIn Maryland, there is a legend of Moll Dyer, who escaped a fire set by fellow colonists only to die of exposure in December 1697. The historical record of Dyer is scant as all official records were burned in a courthouse fire, though the county courthouse has on display the rock where her frozen body was found. A letter from a colonist of the period describes her in most unfavourable terms. A local road is named after Dyer, where her homestead was said to have been. Many local families have their own version of the Moll Dyer affair, and her name is spoken with care in the rural southern counties.\n\nAccusations of witchcraft and wizardry led to the prosecution of a man in Tennessee as recently as 1833. \"The Crucible\" by Arthur Miller is a dramatized and partially fictionalized story of the Salem witch trials that took place in the Massachusetts Bay Colony during 1692–93.\n\nIn Diné culture, witches are seen as the polar opposite of ceremonial people. While spiritual leaders perform \"sings\" for healing, protection and other beneficial purposes, all practices referred to as \"witchcraft\" are intended to hurt and curse. Witches are associated with harm to the community and transgression of societal standards, especially those relating to family and the dead.\n\nThe \"yee naaldlooshii\" is the type of witch known in English as a \"skin-walker\". They are believed to take the forms of animals in order to travel in secret and do harm to the innocent. In the Navajo language, ' translates to \"with it, he goes on all fours\". While perhaps the most common variety seen in horror fiction by non-Navajo people, the ' is one of several varieties of Navajo witch, specifically a type of \"\".\n\nCorpse powder or corpse poison (, literally \"witchery\" or \"harming\") is a substance made from powdered corpses. The powder is used by witches to curse their victims. The effect of the \"\" is a curse and disease, usually indicated by an immediate action to administration of the poison, like fainting, swelling of the tongue, or lockjaw. Sometimes, however, the victims simply wastes away, as from a normal disease.\n\nTraditional Navajos usually hesitate to discuss things like witches and witchcraft with non-Navajos.\n\nWitchcraft was also an important part of the social and cultural history of late-Colonial Mexico, during the Mexican Inquisition. Spanish Inquisitors viewed witchcraft as a problem that could be cured simply through confession. Yet, as anthropologist Ruth Behar writes, witchcraft, not only in Mexico but in Latin America in general, was a \"conjecture of sexuality, witchcraft, and religion, in which Spanish, indigenous, and African cultures converged.\" Furthermore, witchcraft in Mexico generally required an interethnic and interclass network of witches. Yet, according to anthropology professor Laura Lewis, witchcraft in colonial Mexico ultimately represented an \"affirmation of hegemony\" for women, Indians, and especially Indian women over their white male counterparts as a result of the casta system.\n\nIn modern history, notoriety has been awarded to a place called Catemaco, in the state of Veracruz, which has a history of witchcraft, and where the practice of witchcraft by contemporary \"brujos\" and \"brujas\" thrives.\n\nIn Mexico City, people who practice brujería, Santería, voodoo, ocultism and magic may find items, herbs and supplies at the \"mercado de Sonora\".\n\nIn Chile there is a tradition of the Kalku in Mapuche religion; and the Warlocks of Chiloé in the folklore and Chilote mythology.\n\nThe presence of the witch is a constant in the ethnographic history of colonial Brazil, especially during the several denunciations and confessions given to the Congregation for the Doctrine of the Faith of Bahia (1591–1593), Pernambuco and Paraíba (1593–1595).\n\nBelief in the supernatural is strong in all parts of India, and lynchings for witchcraft are reported in the press from time to time. Around 750 people were killed as witches in Assam and West Bengal between 2003 and 2008. Officials in the state of Chhattisgarh reported in 2008 that at least 100 women are maltreated annually as suspected witches. A local activist stated that only a fraction of cases of abuse are reported. In Indian mythology, a common perception of a witch is a being with her feet pointed backwards.\n\nApart from other types of Violence against women in Nepal, the malpractice of abusing women in the name of witchcraft is also really prominent. According to the statistics in 2013, there was a total of 69 reported cases of abuse to women due to accusation of performing witchcraft. The perpetrators of this malpractice are usually neighbors, so-called witch doctors and family members. The main causes of these malpractices are lack of education, lack of awareness and superstition. According to the statistics by INSEC, the age group of women who fall victims to the witchcraft violence in Nepal is 20–40.\n\nIn Japanese folklore, the most common types of witch can be separated into two categories: those who employ snakes as familiars, and those who employ foxes.\n\nThe fox witch is, by far, the most commonly seen witch figure in Japan. Differing regional beliefs set those who use foxes into two separate types: the \"kitsune-mochi\", and the \"tsukimono-suji\". The first of these, the \"kitsune-mochi\", is a solitary figure who gains his fox familiar by bribing it with its favourite foods. The \"kitsune-mochi\" then strikes up a deal with the fox, typically promising food and daily care in return for the fox's magical services. The fox of Japanese folklore is a powerful trickster in and of itself, imbued with powers of shape changing, possession, and illusion. These creatures can be either nefarious; disguising themselves as women in order to trap men, or they can be benign forces as in the story of \"The Grateful foxes\". However, once a fox enters the employ of a human it almost exclusively becomes a force of evil to be feared. A fox under the employ of a human can provide many services. The fox can turn invisible and find secrets its master desires. It can apply its many powers of illusion to trick and deceive its master's enemies. The most feared power of the \"kitsune-mochi\" is the ability to command his fox to possess other humans. This process of possession is called Kitsunetsuki.\n\nBy far, the most commonly reported cases of fox witchcraft in modern Japan are enacted by \"tsukimono-suji\" families, or \"hereditary witches\". The \"Tsukimono-suji\" is traditionally a family who is reported to have foxes under their employ. These foxes serve the family and are passed down through the generations, typically through the female line. \"Tsukimono-suji\" foxes are able to supply much in the way of the same mystical aid that the foxes under the employ of a \"kitsune-mochi\" can provide its more solitary master with. In addition to these powers, if the foxes are kept happy and well taken care of, they bring great fortune and prosperity to the \"Tsukimono-suji\" house. However, the aid in which these foxes give is often overshadowed by the social and mystical implications of being a member of such a family. In many villages, the status of local families as \"tsukimono-suji\" is often common, everyday knowledge. Such families are respected and feared, but are also openly shunned. Due to its hereditary nature, the status of being \"Tsukimono-suji\" is considered contagious. Because of this, it is often impossible for members of such a family to sell land or other properties, due to fear that the possession of such items will cause foxes to inundate one's own home. In addition to this, because the foxes are believed to be passed down through the female line, it is often nearly impossible for women of such families to find a husband whose family will agree to have him married to a \"tsukimono-suji\" family. In such a union the woman's status as a \"Tsukimono-suji\" would transfer to any man who married her.\n\nWitchcraft in the Philippines is often classified as malevolent, with practitioners of black magic called \"Mangkukulam\" in Tagalog and \"Mambabarang\" in Cebuano; there are also practitioners of benevolent, white magic, in addition to some who practise both. \"Mambabarang\" in particular are noted for their ability to command insects and other invertebrates to accomplish a task, such as delivering a curse to a target.\n\nMagic and witchcraft in the Philippines varies considerably across the different ethnic groups, and is commonly a modern manifestation of pre-Colonial spirituality interwoven with Catholic religious elements such as the invocation of saints and the use of pseudo-Latin prayers (\"oración\") in spells, and \"anting-anting\" (amulets).\n\nPractitioners of traditional herbal-based medicine and divination called \"albularyo\" are not considered witches. They are perceived to be either quack doctors or a quasi-magical option when western medicine fails to identify or cure an ailment that is thus suspected to be of supernatural, often malevolent, origin. Feng shui, an influence of Filipino Chinese culture, is also not classified as witchcraft as it is considered a separate realm of belief altogether.\n\nSaudi Arabia continues to use the death penalty for sorcery and witchcraft. In 2006 Fawza Falih Muhammad Ali was condemned to death for practicing witchcraft. There is no legal definition of sorcery in Saudi, but in 2007 an Egyptian pharmacist working there was accused, convicted, and executed. Saudi authorities also pronounced the death penalty on a Lebanese television presenter, Ali Hussain Sibat, while he was performing the \"hajj\" (Islamic pilgrimage) in the country.\n\nIn 2009 the Saudi authorities set up the Anti-Witchcraft Unit of their Committee for the Promotion of Virtue and the Prevention of Vice police.\n\nIn April 2009, a Saudi woman Amina Bint Abdulhalim Nassar was arrested and later sentenced to death for practicing witchcraft and sorcery. In December 2011, she was beheaded. A Saudi man has been beheaded on charges of sorcery and witchcraft in June 2012. A beheading for sorcery occurred in 2014.\n\nIn June 2015, Yahoo reported: \"The Islamic State group has beheaded two women in Syria on accusations of \"sorcery,\" the first such executions of female civilians in Syria, the Syrian Observatory for Human Rights said Tuesday.\"\nISIS decapitated a man in Iraq over sorcery.\n\nAn expedition sent to what is now the Xinjiang region of western China by the PBS documentary series \"Nova\" found a fully clothed female Tocharian mummy wearing a black conical hat of the type now associated with witches in Europe in the storage area of a small local museum, indicative of an Indo-European priestess.\n\nWitchcraft in Europe between 500–1750 was believed to be a combination of sorcery and heresy. While sorcery attempts to produce negative supernatural effects through formulas and rituals, heresy is the Christian contribution to witchcraft in which an individual makes a pact with the Devil. In addition, heresy denies witches the recognition of important Christian values such as baptism, salvation, Christ and sacraments. The beginning of the witch accusations in Europe took place in the 14th and 15th centuries; however as the social disruptions of the 16th century took place, witchcraft trials intensified. \nIn Early Modern European tradition, witches were stereotypically, though not exclusively, women. European pagan belief in witchcraft was associated with the goddess Diana and dismissed as \"diabolical fantasies\" by medieval Christian authors. Witch-hunts first appeared in large numbers in southern France and Switzerland during the 14th and 15th centuries. The peak years of witch-hunts in southwest Germany were from 1561 to 1670.\n\nIt was commonly believed that individuals with power and prestige were involved in acts of witchcraft and even cannibalism. Because Europe had a lot of power over individuals living in West Africa, Europeans in positions of power were often accused of taking part in these practices. Though it is not likely that these individuals were actually involved in these practices, they were most likely associated due to Europe’s involvement in things like the slave trade, which negatively affected the lives of many individuals in the Atlantic World throughout the fifteenth through seventeenth centuries.\n\nThe familiar witch of folklore and popular superstition is a combination of numerous influences. The characterization of the witch as an evil magic user developed over time.\n\nEarly converts to Christianity looked to Christian clergy to work magic more effectively than the old methods under Roman paganism, and Christianity provided a methodology involving saints and relics, similar to the gods and amulets of the Pagan world. As Christianity became the dominant religion in Europe, its concern with magic lessened.\n\nThe Protestant Christian explanation for witchcraft, such as those typified in the confessions of the Pendle witches, commonly involves a diabolical pact or at least an appeal to the intervention of the spirits of evil. The witches or wizards engaged in such practices were alleged to reject Jesus and the sacraments; observe \"the witches' sabbath\" (performing infernal rites that often parodied the Mass or other sacraments of the Church); pay Divine honour to the Prince of Darkness; and, in return, receive from him preternatural powers. It was a folkloric belief that a Devil's Mark, like the brand on cattle, was placed upon a witch's skin by the devil to signify that this pact had been made. Witches were most often characterized as women. Witches disrupted the societal institutions, and more specifically, marriage. It was believed that a witch often joined a pact with the devil to gain powers to deal with infertility, immense fear for her children's well-being, or revenge against a lover. They were also depicted as lustful and perverted, and it was thought that they copulated with the devil at the Sabbath.\n\nThe Church and European society were not always so zealous in hunting witches or blaming them for misfortunes. Saint Boniface declared in the 8th century that belief in the existence of witches was un-Christian. The emperor Charlemagne decreed that the burning of supposed witches was a pagan custom that would be punished by the death penalty. In 820 the Bishop of Lyon and others repudiated the belief that witches could make bad weather, fly in the night, and change their shape. This denial was accepted into Canon law . Other rulers such as King Coloman of Hungary declared that witch-hunts should cease because witches (more specifically, strigas) do not exist.\n\nThe Church did not invent the idea of witchcraft as a potentially harmful force whose practitioners should be put to death. This idea is commonplace in pre-Christian religions. According to the scholar Max Dashu, the concept of medieval witchcraft contained many of its elements even before the emergence of Christianity. These can be found in Bacchanalias, especially in the time when they were led by priestess Paculla Annia (188BC–186BC).\n\nPowers typically attributed to European witches include turning food poisonous or inedible, flying on broomsticks or pitchforks, casting spells, cursing people, making livestock ill and crops fail, and creating fear and local chaos.\n\nHowever, even at a later date, not all witches were assumed to be harmful practicers of the craft. In England, the provision of this curative magic was the job of a witch doctor, also known as a cunning man, white witch, or wise man. The term \"witch doctor\" was in use in England before it came to be associated with Africa. Toad doctors were also credited with the ability to undo evil witchcraft. (Other folk magicians had their own purviews. Girdle-measurers specialised in diagnosing ailments caused by fairies, while magical cures for more mundane ailments, such as burns or toothache, could be had from charmers.)\n\nHistorians Keith Thomas and his student Alan Macfarlane study witchcraft by combining historical research with concepts drawn from anthropology. They argued that English witchcraft, like African witchcraft, was endemic rather than epidemic. Older women were the favorite targets because they were marginal, dependent members of the community and therefore more likely to arouse feelings of both hostility and guilt, and less likely to have defenders of importance inside the community. Witchcraft accusations were the village's reaction to the breakdown of its internal community, coupled with the emergence of a newer set of values that was generating psychic stress.\nIn Wales, fear of witchcraft mounted around the year 1500. There was a growing alarm of women's magic as a weapon aimed against the state and church. The Church made greater efforts to enforce the canon law of marriage, especially in Wales where tradition allowed a wider range of sexual partnerships. There was a political dimension as well, as accusations of witchcraft were levied against the enemies of Henry VII, who was exerting more and more control over Wales.\n\nThe records of the Courts of Great Sessions for Wales, 1536–1736 show that Welsh custom was more important than English law. Custom provided a framework of responding to witches and witchcraft in such a way that interpersonal and communal harmony was maintained, Showing to regard to the importance of honour, social place and cultural status. Even when found guilty, execution did not occur.\n\nBecoming king in 1603, James I Brought to England and Scotland continental explanations of witchcraft. His goal was to divert suspicion away from male homosociality among the elite, and focus fear on female communities and large gatherings of women. He thought they threatened his political power so he laid the foundation for witchcraft and occultism policies, especially in Scotland. The point was that a widespread belief in the conspiracy of witches and a witches' Sabbath with the devil deprived women of political influence. Occult power was supposedly a womanly trait because women were weaker and more susceptible to the devil.\n\nIn 1944 Helen Duncan was the last person in Britain to be imprisoned for fraudulently claiming to be a witch.\n\nIn the United Kingdom children believed to be witches or seen as possessed by evil spirits can be subject to severe beatings, traumatic exorcism, and/or other abuse. There have even been child murders associated with witchcraft beliefs. The problem is particularly serious among immigrant or former immigrant communities of African origin but other communities, such as those of Asian origin are also involved. Step children and children seen as different for a wide range of reasons are particularly at risk of witchcraft accusations. Children may be beaten or have chilli rubbed into their eyes during exorcisms. This type of abuse is frequently hidden and can include torture. A 2006 recommendation to record abuse cases linked to witchcraft centrally has not yet been implemented. Lack of awareness among social workers, teachers and other professionals dealing with at risk children hinders efforts to combat the problem.\n\nThere is a 'money making scam' involved. Pastors accuse a child of being a witch and later the family pays for exorcism. If a child at school says that his/her pastor called the child a witch that should become a child safeguarding issue.\n\nAs in most European countries, women in Italy were more likely suspected of witchcraft than men. Women were considered dangerous due to their supposed sexual instability, such as when being aroused, and also due to the powers of their menstrual blood.\n\nIn the 16th century, Italy had a high portion of witchcraft trials involving love magic. The country had a large number of unmarried people due to men marrying later in their lives during this time. This left many women on a desperate quest for marriage leaving them vulnerable to the accusation of witchcraft whether they took part in it or not. Trial records from the Inquisition and secular courts discovered a link between prostitutes and supernatural practices. Professional prostitutes were considered experts in love and therefore knew how to make love potions and cast love related spells. Up until 1630, the majority of women accused of witchcraft were prostitutes. A courtesan was questioned about her use of magic due to her relationship with men of power in Italy and her wealth. The majority of women accused were also considered \"outsiders\" because they were poor, had different religious practices, spoke a different language, or simply from a different city/town/region. Cassandra from Ferrara, Italy, was still considered a foreigner because not native to Rome where she was residing. She was also not seen as a model citizen because her husband was in Venice.\n\nFrom the 16th-18th centuries, the Catholic Church enforced moral discipline throughout Italy. With the help of local tribunals, such as in Venice, the two institutions investigated a woman's religious behaviors when she was accused of witchcraft.\n\nFranciscan friars from New Spain introduced Diabolism, belief in the devil, to the indigenous people after their arrival in 1524.\nBartolomé de las Casas believed that human sacrifice was not diabolic, in fact far off from it, and was a natural result of religious expression.\nMexican Indians gladly took in the belief of Diabolism and still managed to keep their belief in creator-destroyer deities.\n\nIn pre-Christian times, witchcraft was a common practice in the Cook Islands. The native name for a sorcerer was \"tangata purepure\" (a man who prays). The prayers offered by the \"ta'unga\" (priests) to the gods worshiped on national or tribal \"marae\" (temples) were termed \"karakia\"; those on minor occasions to the lesser gods were named \"pure\". All these prayers were metrical, and were handed down from generation to generation with the utmost care. There were prayers for every such phase in life; for success in battle; for a change in wind (to overwhelm an adversary at sea, or that an intended voyage be propitious); that his crops may grow; to curse a thief; or wish ill-luck and death to his foes. Few men of middle age were without a number of these prayers or charms. The succession of a sorcerer was from father to son, or from uncle to nephew. So too of sorceresses: it would be from mother to daughter, or from aunt to niece. Sorcerers and sorceresses were often slain by relatives of their supposed victims.\n\nA singular enchantment was employed to kill off a husband of a pretty woman desired by someone else. The expanded flower of a Gardenia was stuck upright—a very difficult performance—in a cup (i.e., half a large coconut shell) of water. A prayer was then offered for the husbands speedy death, the sorcerer earnestly watching the flower. Should it fall the incantation was successful. But if the flower still remained upright, he will live. The sorcerer would in that case try his skill another day, with perhaps better success.\n\nAccording to Beatrice Grimshaw, a journalist who visited the Cook Islands in 1907, the uncrowned Queen Makea was believed to have possessed the mystic power called \"mana\", giving the possessor the power to slay at will. It also included other gifts, such as second sight to a certain extent, the power to bring good or evil luck, and the ability already mentioned to deal death at will.\n\nA local newspaper informed that more than 50 people were killed in two Highlands provinces of Papua New Guinea in 2008 for allegedly practicing witchcraft. An estimated 50–150 alleged witches are killed each year in Papua New Guinea.\n\nAmong the Russian words for \"witch\", ведьма (ved'ma) literally means \"one who knows\", from Old Slavic вѣдъ \"to know\"). Another frequent term is колдунья (koldun'ya), \"sorcerer\" being колдун (koldun).\n\nPagan practices formed a part of Russian and Eastern Slavic culture; the Russian people were deeply superstitious. The witchcraft practiced consisted mostly of earth magic and herbology; it was not so significant which herbs were used in practices, but how these herbs were gathered. Ritual centered on harvest of the crops and the location of the sun was very important. One source, pagan author Judika Illes, tells that herbs picked on Midsummer's Eve were believed to be most powerful, especially if gathered on Bald Mountain near Kiev during the witches' annual revels celebration. Botanicals should be gathered, \"During the seventeenth minute of the fourteenth hour, under a dark moon, in the thirteenth field, wearing a red dress, pick the twelfth flower on the right.\"\n\nSpells also served for midwifery, shape-shifting, keeping lovers faithful, and bridal customs. Spells dealing with midwifery and childbirth focused on the spiritual wellbeing of the baby. Shape-shifting spells involved invocation of the wolf as a spirit animal. To keep men faithful, lovers would cut a ribbon the length of his erect penis and soak it in his seminal emissions after sex while he was sleeping, then tie seven knots in it; keeping this talisman of knot magic ensured loyalty. Part of an ancient pagan marriage tradition involved the bride taking a ritual bath at a bathhouse before the ceremony. Her sweat would be wiped from her body using raw fish, and the fish would be cooked and fed to the groom.\n\nDemonism, or black magic, was not prevalent. Persecution for witchcraft, mostly involved the practice of simple earth magic, founded on herbology, by solitary practitioners with a Christian influence. In one case investigators found a locked box containing something bundled in a kerchief and three paper packets, wrapped and tied, containing crushed grasses. Most rituals of witchcraft were very simple—one spell of divination consists of sitting alone outside meditating, asking the earth to show one's fate.\n\nWhile these customs were unique to Russian culture, they were not exclusive to this region. Russian pagan practices were often akin to paganism in other parts of the world. The Chinese concept of \"chi\", a form of energy that often manipulated in witchcraft, is known as bioplasma in Russian practices. The western concept of an \"evil eye\" or a \"hex\" was translated to Russia as a \"spoiler\". A spoiler was rooted in envy, jealousy and malice. Spoilers could be made by gathering bone from a cemetery, a knot of the target's hair, burned wooden splinters and several herb Paris berries (which are very poisonous). Placing these items in sachet in the victim's pillow completes a spoiler. The Sumerians, Babylonians, Assyrians, and the ancient Egyptians recognized the evil eye from as early as 3,000 BCE; in Russian practices it is seen as a sixteenth-century concept.\n\nThe dominant societal concern those practicing witchcraft was not whether paganism was effective, but whether it could cause harm. Peasants in Russian and Ukrainian societies often shunned witchcraft, unless they needed help against supernatural forces. Impotence, stomach pains, barrenness, hernias, abscesses, epileptic seizures, and convulsions were all attributed to evil (or witchcraft). This is reflected in linguistics; there are numerous words for a variety of practitioners of paganism-based healers. Russian peasants referred to a witch as a \"chernoknizhnik\" (a person who plied his trade with the aid of a black book), \"sheptun\"/\"sheptun'ia\" (a \"whisperer\" male or female), \"lekar\"/\"lekarka\" or \"znakhar\"/\"znakharka\" (a male or female healer), or \"zagovornik\" (an incanter).\n\nIronically enough, there was universal reliance on folk healers – but clients often turned them in if something went wrong. According to Russian historian Valerie A. Kivelson, witchcraft accusations were normally thrown at lower-class peasants, townspeople and Cossacks. People turned to witchcraft as a means to support themselves. The ratio of male to female accusations was 75% to 25%. Males were targeted more, because witchcraft was associated with societal deviation. Because single people with no settled home could not be taxed, males typically had more power than women in their dissent.\n\nThe history of Witchcraft had evolved around society. More of a psychological concept to the creation and usage of Witchcraft can create the assumption as to why women are more likely to follow the practices behind Witchcraft. Identifying with the soul of an individual’s self is often deemed as \"feminine\" in society. There is analyzed social and economic evidence to associate between witchcraft and women.\n\nWitchcraft trials occurred frequently in seventeenth-century Russia, although the \"great witch-hunt\" is believed to be a predominately Western European phenomenon. However, as the witchcraft-trial craze swept across Catholic and Protestant countries during this time, Orthodox Christian Europe indeed partook in this so-called \"witch hysteria.\" This involved the persecution of both males and females who were believed to be practicing paganism, herbology, the black art, or a form of sorcery within and/or outside their community. Very early on witchcraft legally fell under the jurisdiction of the ecclesiastical body, the church, in Kievan Rus' and Muscovite Russia. Sources of ecclesiastical witchcraft jurisdiction date back as early as the second half of the eleventh century, one being Vladimir the Great's first edition of his State Statute or \"Ustav\", another being multiple references in the \"Primary Chronicle\" beginning in 1024.\n\nThe sentence for an individual found guilty of witchcraft or sorcery during this time, and in previous centuries, typically included either burning at the stake or being tested with the \"ordeal of cold water\" or \"judicium aquae frigidae\". The cold-water test was primarily a Western European phenomenon, but was used as a method of truth in Russia prior to, and post, seventeenth-century witchcraft trials in Muscovy. Accused persons who submerged were considered innocent, and ecclesiastical authorities would proclaim them \"brought back,\" but those who floated were considered guilty of practicing witchcraft, and burned at the stake or executed in an unholy fashion. The thirteenth-century bishop of Vladimir, Serapion Vladimirskii, preached sermons throughout the Muscovite countryside, and in one particular sermon revealed that burning was the usual punishment for witchcraft, but more often the cold water test was used as a precursor to execution.\n\nAlthough these two methods of torture were used in the west and the east, Russia implemented a system of fines payable for the crime of witchcraft during the seventeenth century. Thus, even though torture methods in Muscovy were on a similar level of harshness as Western European methods used, a more civil method was present. In the introduction of a collection of trial records pieced together by Russian scholar Nikolai Novombergsk, he argues that Muscovite authorities used the same degree of cruelty and harshness as Western European Catholic and Protestant countries in persecuting witches. By the mid-sixteenth century the manifestations of paganism, including witchcraft, and the black arts—astrology, fortune telling, and divination—became a serious concern to the Muscovite church and state.\n\nTsar Ivan IV (reigned 1547–1584) took this matter to the ecclesiastical court and was immediately advised that individuals practicing these forms of witchcraft should be excommunicated and given the death penalty. Ivan IV, as a true believer in witchcraft, was deeply convinced that sorcery accounted for the death of his wife, Anastasiia in 1560, which completely devastated and depressed him, leaving him heartbroken. Stemming from this belief, Ivan IV became majorly concerned with the threat of witchcraft harming his family, and feared he was in danger. So, during the Oprichnina (1565–1572), Ivan IV succeeded in accusing and charging a good number of boyars with witchcraft whom he did not wish to remain as nobles. Rulers after Ivan IV, specifically during the Time of Troubles (1598–1613), increased the fear of witchcraft among themselves and entire royal families, which then led to further preoccupation with the fear of prominent Muscovite witchcraft circles.\n\nAfter the Time of Troubles, seventeenth-century Muscovite rulers held frequent investigations of witchcraft within their households, laying the ground, along with previous tsarist reforms, for widespread witchcraft trials throughout the Muscovite state. Between 1622 and 1700 ninety-one people were brought to trial in Muscovite courts for witchcraft. Although Russia did partake in the witch craze that swept across Western Europe, the Muscovite state did not persecute nearly as many people for witchcraft, let alone execute a number of individuals anywhere close to the number executed in the west during the witch hysteria.\n\nWitches have a long history of being depicted in art, although most of their earliest artistic depictions seem to originate in Early Modern Europe, particularly the Medieval and Renaissance periods. Many scholars attribute their manifestation in art as inspired by texts such as \"Canon Episcopi\", a demonology-centered work of literature, and \"Malleus Maleficarum\", a \"witch-craze\" manual published in 1487, by Heinrich Kramer and Jacob Sprenger.\n\n\"Canon Episcopi\", a ninth-century text that explored the subject of demonology, initially introduced concepts that would continuously be associated with witches, such as their ability to fly or their believed fornication and sexual relations with the devil. The text refers to two women, Diana the Huntress and Herodias, who both express the duality of female sorcerers. Diana was described as having a heavenly body and as the \"protectress of childbirth and fertility\" while Herodias symbolized \"unbridled sensuality\". They thus represent the mental powers and cunning sexuality that witches used as weapons to trick men into performing sinful acts which would result in their eternal punishment. These characteristics were distinguished as Medusa-like or Lamia-like traits when seen in any artwork (Medusa's mental trickery was associated with Diana the Huntress's psychic powers and Lamia was a rumored female figure in the Medieval ages sometimes used in place of Herodias).\nOne of the first individuals to regularly depict witches after the witch-craze of the medieval period was Albrecht Dürer, a German Renaissance artist. His famous 1497 engraving \"The Four Witches\", portrays four physically attractive and seductive nude witches. Their supernatural identities are emphasized by the skulls and bones lying at their feet as well as the devil discreetly peering at them from their left. The women's sensuous presentation speaks to the overtly sexual nature they were attached to in early modern Europe. Moreover, this attractiveness was perceived as a danger to ordinary men who they could seduce and tempt into their sinful world. Some scholars interpret this piece as utilizing the logic of the \"Canon Episcopi\", in which women used their mental powers and bodily seduction to enslave and lead men onto a path of eternal damnation, differing from the unattractive depiction of witches that would follow in later Renaissance years.\nDürer also employed other ideas from the Middle Ages that were commonly associated with witches. Specifically, his art often referred to former 12th- to 13th-century Medieval iconography addressing the nature of female sorcerers. In the Medieval period, there was a widespread fear of witches, accordingly producing an association of dark, intimidating characteristics with witches, such as cannibalism (witches described as \"[sucking] the blood of newborn infants\") or described as having the ability to fly, usually on the back of black goats. As the Renaissance period began, these concepts of witchcraft were suppressed, leading to a drastic change in the sorceress' appearances, from sexually explicit beings to the 'ordinary' typical housewives of this time period. This depiction, known as the 'Waldensian' witch became a cultural phenomenon of early Renaissance art. The term originates from the 12th-century monk Peter Waldo, who established his own religious sect which explicitly opposed the luxury and commodity-influenced lifestyle of the Christian church clergy, and whose sect was excommunicated before being persecuted as \"practitioners of witchcraft and magic\".\n\nSubsequent artwork exhibiting witches tended to consistently rely on cultural stereotypes about these women. These stereotypes were usually rooted in early Renaissance religious discourse, specifically the Christian belief that an \"earthly alliance\" had taken place between Satan's female minions who \"conspired to destroy Christendom\".\n\nAnother significant artist whose art consistently depicted witches was Dürer's apprentice, Hans Baldung Grien, a 15th-century German artist. His chiaroscuro woodcut, \"Witches\", created in 1510, visually encompassed all the characteristics that were regularly assigned to witches during the Renaissance. Social beliefs labeled witches as supernatural beings capable of doing great harm, possessing the ability to fly, and as cannibalistic. The urn in \"Witches\" seems to contain pieces of the human body, which the witches are seen consuming as a source of energy. Meanwhile, their nudity while feasting is recognized as an allusion to their sexual appetite, and some scholars read the witch riding on the back of a goat-demon as representative of their \"flight-inducing [powers]\". This connection between women's sexual nature and sins was thematic in the pieces of many Renaissance artists, especially Christian artists, due to cultural beliefs which characterized women as overtly sexual beings who were less capable (in comparison to men) of resisting sinful temptation.\n\n\n\n"}
{"id": "33550", "url": "https://en.wikipedia.org/wiki?curid=33550", "title": "Wood", "text": "Wood\n\nWood is a porous and fibrous structural tissue found in the stems and roots of trees and other woody plants. It is an organic material, a natural composite of cellulose fibers that are strong in tension and embedded in a matrix of lignin that resists compression. Wood is sometimes defined as only the secondary xylem in the stems of trees, or it is defined more broadly to include the same type of tissue elsewhere such as in the roots of trees or shrubs. In a living tree it performs a support function, enabling woody plants to grow large or to stand up by themselves. It also conveys water and nutrients between the leaves, other growing tissues, and the roots. Wood may also refer to other plant materials with comparable properties, and to material engineered from wood, or wood chips or fiber.\n\nWood has been used for thousands of years for fuel, as a construction material, for making tools and weapons, furniture and paper. More recently it emerged as a feedstock for the production of purified cellulose and its derivatives, such as cellophane and cellulose acetate.\n\nAs of 2005, the growing stock of forests worldwide was about 434 billion cubic meters, 47% of which was commercial. As an abundant, carbon-neutral renewable resource, woody materials have been of intense interest as a source of renewable energy. In 1991 approximately 3.5 billion cubic meters of wood were harvested. Dominant uses were for furniture and building construction.\n\nA 2011 discovery in the Canadian province of New Brunswick yielded the earliest known plants to have grown wood, approximately 395 to 400 million years ago.\n\nWood can be dated by carbon dating and in some species by dendrochronology to determine when a wooden object was created.\n\nPeople have used wood for thousands of years for many purposes, including as a fuel or as a construction material for making houses, tools, weapons, furniture, packaging, artworks, and paper. Known constructions using wood date back ten thousand years. Buildings like the European Neolithic long house were made primarily of wood.\n\nRecent use of wood has been enhanced by the addition of steel and bronze into construction.\n\nThe year-to-year variation in tree-ring widths and isotopic abundances gives clues to the prevailing climate at the time a tree was cut.\n\nWood, in the strict sense, is yielded by trees, which increase in diameter by the formation, between the existing wood and the inner bark, of new woody layers which envelop the entire stem, living branches, and roots. This process is known as secondary growth; it is the result of cell division in the vascular cambium, a lateral meristem, and subsequent expansion of the new cells. These cells then go on to form thickened secondary cell walls, composed mainly of cellulose, hemicellulose and lignin.\n\nWhere the differences between the four seasons are distinct, e.g. New Zealand, growth can occur in a discrete annual or seasonal pattern, leading to growth rings; these can usually be most clearly seen on the end of a log, but are also visible on the other surfaces. If the distinctiveness between seasons is annual (as is the case in equatorial regions, e.g. Singapore), these growth rings are referred to as annual rings. Where there is little seasonal difference growth rings are likely to be indistinct or absent. If the bark of the tree has been removed in a particular area, the rings will likely be deformed as the plant overgrows the scar.\n\nIf there are differences within a growth ring, then the part of a growth ring nearest the center of the tree, and formed early in the growing season when growth is rapid, is usually composed of wider elements. It is usually lighter in color than that near the outer portion of the ring, and is known as earlywood or springwood. The outer portion formed later in the season is then known as the latewood or summerwood. However, there are major differences, depending on the kind of wood (see below).\n\nAs a tree grows, lower branches often die, and their bases may become overgrown and enclosed by subsequent layers of trunk wood, forming a type of imperfection known as a knot. The dead branch may not be attached to the trunk wood except at its base, and can drop out after the tree has been sawn into boards. Knots affect the technical properties of the wood, usually reducing the local strength and increasing the tendency for splitting along the wood grain, but may be exploited for visual effect. In a longitudinally sawn plank, a knot will appear as a roughly circular \"solid\" (usually darker) piece of wood around which the grain of the rest of the wood \"flows\" (parts and rejoins). Within a knot, the direction of the wood (grain direction) is up to 90 degrees different from the grain direction of the regular wood.\n\nIn the tree a knot is either the base of a side branch or a dormant bud. A knot (when the base of a side branch) is conical in shape (hence the roughly circular cross-section) with the inner tip at the point in stem diameter at which the plant's vascular cambium was located when the branch formed as a bud.\n\nIn grading lumber and structural timber, knots are classified according to their form, size, soundness, and the firmness with which they are held in place. This firmness is affected by, among other factors, the length of time for which the branch was dead while the attaching stem continued to grow.\n\nKnots do not necessarily influence the stiffness of structural timber, this will depend on the size and location. Stiffness and elastic strength are more dependent upon the sound wood than upon localized defects. The breaking strength is very susceptible to defects. Sound knots do not weaken wood when subject to compression parallel to the grain.\n\nIn some decorative applications, wood with knots may be desirable to add visual interest. In applications where wood is painted, such as skirting boards, fascia boards, door frames and furniture, resins present in the timber may continue to 'bleed' through to the surface of a knot for months or even years after manufacture and show as a yellow or brownish stain. A knot primer paint or solution (knotting), correctly applied during preparation, may do much to reduce this problem but it is difficult to control completely, especially when using mass-produced kiln-dried timber stocks.\n\nHeartwood (or duramen) is wood that as a result of a naturally occurring chemical transformation has become more resistant to decay. Heartwood formation is a genetically programmed process that occurs spontaneously. Some uncertainty exists as to whether the wood dies during heartwood formation, as it can still chemically react to decay organisms, but only once.\n\nHeartwood is often visually distinct from the living sapwood, and can be distinguished in a cross-section where the boundary will tend to follow the growth rings. For example, it is sometimes much darker. However, other processes such as decay or insect invasion can also discolor wood, even in woody plants that do not form heartwood, which may lead to confusion.\n\nSapwood (or alburnum) is the younger, outermost wood; in the growing tree it is living wood, and its principal functions are to conduct water from the roots to the leaves and to store up and give back according to the season the reserves prepared in the leaves. However, by the time they become competent to conduct water, all xylem tracheids and vessels have lost their cytoplasm and the cells are therefore functionally dead. All wood in a tree is first formed as sapwood. The more leaves a tree bears and the more vigorous its growth, the larger the volume of sapwood required. Hence trees making rapid growth in the open have thicker sapwood for their size than trees of the same species growing in dense forests. Sometimes trees (of species that do form heartwood) grown in the open may become of considerable size, or more in diameter, before any heartwood begins to form, for example, in second-growth hickory, or open-grown pines.\n\nThe term \"heartwood\" derives solely from its position and not from any vital importance to the tree. This is evidenced by the fact that a tree can thrive with its heart completely decayed. Some species begin to form heartwood very early in life, so having only a thin layer of live sapwood, while in others the change comes slowly. Thin sapwood is characteristic of such species as chestnut, black locust, mulberry, osage-orange, and sassafras, while in maple, ash, hickory, hackberry, beech, and pine, thick sapwood is the rule. Others never form heartwood.\n\nNo definite relation exists between the annual rings of growth and the amount of sapwood. Within the same species the cross-sectional area of the sapwood is very roughly proportional to the size of the crown of the tree. If the rings are narrow, more of them are required than where they are wide. As the tree gets larger, the sapwood must necessarily become thinner or increase materially in volume. Sapwood is relatively thicker in the upper portion of the trunk of a tree than near the base, because the age and the diameter of the upper sections are less.\n\nWhen a tree is very young it is covered with limbs almost, if not entirely, to the ground, but as it grows older some or all of them will eventually die and are either broken off or fall off. Subsequent growth of wood may completely conceal the stubs which will however remain as knots. No matter how smooth and clear a log is on the outside, it is more or less knotty near the middle. Consequently, the sapwood of an old tree, and particularly of a forest-grown tree, will be freer from knots than the inner heartwood. Since in most uses of wood, knots are defects that weaken the timber and interfere with its ease of working and other properties, it follows that a given piece of sapwood, because of its position in the tree, may well be stronger than a piece of heartwood from the same tree.\n\nIt is remarkable that the inner heartwood of old trees remains as sound as it usually does, since in many cases it is hundreds, and in a few instances thousands, of years old. Every broken limb or root, or deep wound from fire, insects, or falling timber, may afford an entrance for decay, which, once started, may penetrate to all parts of the trunk. The larvae of many insects bore into the trees and their tunnels remain indefinitely as sources of weakness. Whatever advantages, however, that sapwood may have in this connection are due solely to its relative age and position.\n\nIf a tree grows all its life in the open and the conditions of soil and site remain unchanged, it will make its most rapid growth in youth, and gradually decline. The annual rings of growth are for many years quite wide, but later they become narrower and narrower. Since each succeeding ring is laid down on the outside of the wood previously formed, it follows that unless a tree materially increases its production of wood from year to year, the rings must necessarily become thinner as the trunk gets wider. As a tree reaches maturity its crown becomes more open and the annual wood production is lessened, thereby reducing still more the width of the growth rings. In the case of forest-grown trees so much depends upon the competition of the trees in their struggle for light and nourishment that periods of rapid and slow growth may alternate. Some trees, such as southern oaks, maintain the same width of ring for hundreds of years. Upon the whole, however, as a tree gets larger in diameter the width of the growth rings decreases.\n\nDifferent pieces of wood cut from a large tree may differ decidedly, particularly if the tree is big and mature. In some trees, the wood laid on late in the life of a tree is softer, lighter, weaker, and more even-textured than that produced earlier, but in other trees, the reverse applies. This may or may not correspond to heartwood and sapwood. In a large log the sapwood, because of the time in the life of the tree when it was grown, may be inferior in hardness, strength, and toughness to equally sound heartwood from the same log. In a smaller tree, the reverse may be true.\n\nIn species which show a distinct difference between heartwood and sapwood the natural color of heartwood is usually darker than that of the sapwood, and very frequently the contrast is conspicuous (see section of yew log above). This is produced by deposits in the heartwood of chemical substances, so that a dramatic color variation does not imply a significant difference in the mechanical properties of heartwood and sapwood, although there may be a marked biochemical difference between the two.\n\nSome experiments on very resinous longleaf pine specimens indicate an increase in strength, due to the resin which increases the strength when dry. Such resin-saturated heartwood is called \"fat lighter\". Structures built of fat lighter are almost impervious to rot and termites; however they are very flammable. Stumps of old longleaf pines are often dug, split into small pieces and sold as kindling for fires. Stumps thus dug may actually remain a century or more since being cut. Spruce impregnated with crude resin and dried is also greatly increased in strength thereby.\n\nSince the latewood of a growth ring is usually darker in color than the earlywood, this fact may be used in visually judging the density, and therefore the hardness and strength of the material. This is particularly the case with coniferous woods. In ring-porous woods the vessels of the early wood often appear on a finished surface as darker than the denser latewood, though on cross sections of heartwood the reverse is commonly true. Otherwise the color of wood is no indication of strength.\n\nAbnormal discoloration of wood often denotes a diseased condition, indicating unsoundness. The black check in western hemlock is the result of insect attacks. The reddish-brown streaks so common in hickory and certain other woods are mostly the result of injury by birds. The discoloration is merely an indication of an injury, and in all probability does not of itself affect the properties of the wood. Certain rot-producing fungi impart to wood characteristic colors which thus become symptomatic of weakness; however an attractive effect known as spalting produced by this process is often considered a desirable characteristic. Ordinary sap-staining is due to fungal growth, but does not necessarily produce a weakening effect.\n\nWater occurs in living wood in three locations, namely:\n\nIn heartwood it occurs only in the first and last forms. Wood that is thoroughly air-dried retains 8–16% of the water in the cell walls, and none, or practically none, in the other forms. Even oven-dried wood retains a small percentage of moisture, but for all except chemical purposes, may be considered absolutely dry.\n\nThe general effect of the water content upon the wood substance is to render it softer and more pliable. A similar effect occurs in the softening action of water on rawhide, paper, or cloth. Within certain limits, the greater the water content, the greater its softening effect.\n\nDrying produces a decided increase in the strength of wood, particularly in small specimens. An extreme example is the case of a completely dry spruce block 5 cm in section, which will sustain a permanent load four times as great as a green (undried) block of the same size will.\n\nThe greatest strength increase due to drying is in the ultimate crushing strength, and strength at elastic limit in endwise compression; these are followed by the modulus of rupture, and stress at elastic limit in cross-bending, while the modulus of elasticity is least affected.\n\nWood is a heterogeneous, hygroscopic, cellular and anisotropic material. It consists of cells, and the cell walls are composed of micro-fibrils of cellulose (40–50%) and hemicellulose (15–25%) impregnated with lignin (15–30%).\n\nIn coniferous or softwood species the wood cells are mostly of one kind, tracheids, and as a result the material is much more uniform in structure than that of most hardwoods. There are no vessels (\"pores\") in coniferous wood such as one sees so prominently in oak and ash, for example.\n\nThe structure of hardwoods is more complex. The water conducting capability is mostly taken care of by vessels: in some cases (oak, chestnut, ash) these are quite large and distinct, in others (buckeye, poplar, willow) too small to be seen without a hand lens. In discussing such woods it is customary to divide them into two large classes, \"ring-porous\" and \"diffuse-porous\".\n\nIn ring-porous species, such as ash, black locust, catalpa, chestnut, elm, hickory, mulberry, and oak, the larger vessels or pores (as cross sections of vessels are called) are localized in the part of the growth ring formed in spring, thus forming a region of more or less open and porous tissue. The rest of the ring, produced in summer, is made up of smaller vessels and a much greater proportion of wood fibers. These fibers are the elements which give strength and toughness to wood, while the vessels are a source of weakness.\n\nIn diffuse-porous woods the pores are evenly sized so that the water conducting capability is scattered throughout the growth ring instead of being collected in a band or row. Examples of this kind of wood are alder, basswood, birch, buckeye, maple, willow, and the \"Populus\" species such as aspen, cottonwood and poplar. Some species, such as walnut and cherry, are on the border between the two classes, forming an intermediate group.\n\nIn temperate softwoods, there often is a marked difference between latewood and earlywood. The latewood will be denser than that formed early in the season. When examined under a microscope, the cells of dense latewood are seen to be very thick-walled and with very small cell cavities, while those formed first in the season have thin walls and large cell cavities. The strength is in the walls, not the cavities. Hence the greater the proportion of latewood, the greater the density and strength. In choosing a piece of pine where strength or stiffness is the important consideration, the principal thing to observe is the comparative amounts of earlywood and latewood. The width of ring is not nearly so important as the proportion and nature of the latewood in the ring.\n\nIf a heavy piece of pine is compared with a lightweight piece it will be seen at once that the heavier one contains a larger proportion of latewood than the other, and is therefore showing more clearly demarcated growth rings. In white pines there is not much contrast between the different parts of the ring, and as a result the wood is very uniform in texture and is easy to work. In hard pines, on the other hand, the latewood is very dense and is deep-colored, presenting a very decided contrast to the soft, straw-colored earlywood.\n\nIt is not only the proportion of latewood, but also its quality, that counts. In specimens that show a very large proportion of latewood it may be noticeably more porous and weigh considerably less than the latewood in pieces that contain less latewood. One can judge comparative density, and therefore to some extent strength, by visual inspection.\n\nNo satisfactory explanation can as yet be given for the exact mechanisms determining the formation of earlywood and latewood. Several factors may be involved. In conifers, at least, rate of growth alone does not determine the proportion of the two portions of the ring, for in some cases the wood of slow growth is very hard and heavy, while in others the opposite is true. The quality of the site where the tree grows undoubtedly affects the character of the wood formed, though it is not possible to formulate a rule governing it. In general, however, it may be said that where strength or ease of working is essential, woods of moderate to slow growth should be chosen.\n\nIn ring-porous woods, each season's growth is always well defined, because the large pores formed early in the season abut on the denser tissue of the year before.\n\nIn the case of the ring-porous hardwoods, there seems to exist a pretty definite relation between the rate of growth of timber and its properties. This may be briefly summed up in the general statement that the more rapid the growth or the wider the rings of growth, the heavier, harder, stronger, and stiffer the wood. This, it must be remembered, applies only to ring-porous woods such as oak, ash, hickory, and others of the same group, and is, of course, subject to some exceptions and limitations.\n\nIn ring-porous woods of good growth, it is usually the latewood in which the thick-walled, strength-giving fibers are most abundant. As the breadth of ring diminishes, this latewood is reduced so that very slow growth produces comparatively light, porous wood composed of thin-walled vessels and wood parenchyma. In good oak, these large vessels of the earlywood occupy from 6 to 10 percent of the volume of the log, while in inferior material they may make up 25% or more. The latewood of good oak is dark colored and firm, and consists mostly of thick-walled fibers which form one-half or more of the wood. In inferior oak, this latewood is much reduced both in quantity and quality. Such variation is very largely the result of rate of growth.\n\nWide-ringed wood is often called \"second-growth\", because the growth of the young timber in open stands after the old trees have been removed is more rapid than in trees in a closed forest, and in the manufacture of articles where strength is an important consideration such \"second-growth\" hardwood material is preferred. This is particularly the case in the choice of hickory for handles and spokes. Here not only strength, but toughness and resilience are important.\n\nThe results of a series of tests on hickory by the U.S. Forest Service show that:\n\nThe effect of rate of growth on the qualities of chestnut wood is summarized by the same authority as follows:\n\nIn the diffuse-porous woods, the demarcation between rings is not always so clear and in some cases is almost (if not entirely) invisible to the unaided eye. Conversely, when there is a clear demarcation there may not be a noticeable difference in structure within the growth ring.\n\nIn diffuse-porous woods, as has been stated, the vessels or pores are even-sized, so that the water conducting capability is scattered throughout the ring instead of collected in the earlywood. The effect of rate of growth is, therefore, not the same as in the ring-porous woods, approaching more nearly the conditions in the conifers. In general it may be stated that such woods of medium growth afford stronger material than when very rapidly or very slowly grown. In many uses of wood, total strength is not the main consideration. If ease of working is prized, wood should be chosen with regard to its uniformity of texture and straightness of grain, which will in most cases occur when there is little contrast between the latewood of one season's growth and the earlywood of the next.\n\nStructural material that resembles ordinary, \"dicot\" or conifer timber in its gross handling characteristics is produced by a number of monocot plants, and these also are colloquially called wood. Of these, bamboo, botanically a member of the grass family, has considerable economic importance, larger culms being widely used as a building and construction material and in the manufacture of engineered flooring, panels and veneer. Another major plant group that produces material that often is called wood are the palms. Of much less importance are plants such as \"Pandanus,\" \"Dracaena\" and \"Cordyline.\" With all this material, the structure and composition of the processed raw material is quite different from ordinary wood.\n\nThe single most revealing property of wood as an indicator of wood quality is specific gravity (Timell 1986), as both pulp yield and lumber strength are determined by it. Specific gravity is the ratio of the mass of a substance to the mass of an equal volume of water; density is the ratio of a mass of a quantity of a substance to the volume of that quantity and is expressed in mass per unit substance, e.g., grams per milliliter (g/cm or g/ml). The terms are essentially equivalent as long as the metric system is used. Upon drying, wood shrinks and its density increases. Minimum values are associated with green (water-saturated) wood and are referred to as \"basic specific gravity\" (Timell 1986).\n\nWood density is determined by multiple growth and physiological factors compounded into “one fairly easily measured wood characteristic” (Elliott 1970).\n\nAge, diameter, height, radial (trunk) growth, geographical location, site and growing conditions, silvicultural treatment, and seed source all to some degree influence wood density. Variation is to be expected. Within an individual tree, the variation in wood density is often as great as or even greater than that between different trees (Timell 1986). Variation of specific gravity within the bole of a tree can occur in either the horizontal or vertical direction.\n\nIt is common to classify wood as either softwood or hardwood. The wood from conifers (e.g. pine) is called softwood, and the wood from dicotyledons (usually broad-leaved trees, (e.g. oak) is called hardwood. These names are a bit misleading, as hardwoods are not necessarily hard, and softwoods are not necessarily soft. The well-known balsa (a hardwood) is actually softer than any commercial softwood. Conversely, some softwoods (e.g. yew) are harder than many hardwoods.\n\nThere is a strong relationship between the properties of wood and the properties of the particular tree that yielded it. The density of wood varies with species. The density of a wood correlates with its strength (mechanical properties). For example, mahogany is a medium-dense hardwood that is excellent for fine furniture crafting, whereas balsa is light, making it useful for model building. One of the densest woods is black ironwood.\n\nThe chemical composition of wood varies from species to species, but is approximately 50% carbon, 42% oxygen, 6% hydrogen, 1% nitrogen, and 1% other elements (mainly calcium, potassium, sodium, magnesium, iron, and manganese) by weight. Wood also contains sulfur, chlorine, silicon, phosphorus, and other elements in small quantity.\n\nAside from water, wood has three main components. Cellulose, a crystalline polymer derived from glucose, constitutes about 41–43%. Next in abundance is hemicellulose, which is around 20% in deciduous trees but near 30% in conifers. It is mainly five-carbon sugars that are linked in an irregular manner, in contrast to the cellulose. Lignin is the third component at around 27% in coniferous wood vs. 23% in deciduous trees. Lignin confers the hydrophobic properties reflecting the fact that it is based on aromatic rings. These three components are interwoven, and direct covalent linkages exist between the lignin and the hemicellulose. A major focus of the paper industry is the separation of the lignin from the cellulose, from which paper is made.\n\nIn chemical terms, the difference between hardwood and softwood is reflected in the composition of the constituent lignin. Hardwood lignin is primarily derived from sinapyl alcohol and coniferyl alcohol. Softwood lignin is mainly derived from coniferyl alcohol.\n\nAside from the lignocellulose, wood consists of a variety of low molecular weight organic compounds, called \"extractives\". The wood extractives are fatty acids, resin acids, waxes and terpenes. For example, rosin is exuded by conifers as protection from insects. The extraction of these organic materials from wood provides tall oil, turpentine, and rosin.\n\nWood has a long history of being used as fuel, which continues to this day, mostly in rural areas of the world. Hardwood is preferred over softwood because it creates less smoke and burns longer. Adding a woodstove or fireplace to a home is often felt to add ambiance and warmth.\n\nWood has been an important construction material since humans began building shelters, houses and boats. Nearly all boats were made out of wood until the late 19th century, and wood remains in common use today in boat construction. Elm in particular was used for this purpose as it resisted decay as long as it was kept wet (it also served for water pipe before the advent of more modern plumbing).\n\nWood to be used for construction work is commonly known as \"lumber\" in North America. Elsewhere, \"lumber\" usually refers to felled trees, and the word for sawn planks ready for use is \"timber\". In Medieval Europe oak was the wood of choice for all wood construction, including beams, walls, doors, and floors. Today a wider variety of woods is used: solid wood doors are often made from poplar, small-knotted pine, and Douglas fir.\nNew domestic housing in many parts of the world today is commonly made from timber-framed construction. Engineered wood products are becoming a bigger part of the construction industry. They may be used in both residential and commercial buildings as structural and aesthetic materials.\n\nIn buildings made of other materials, wood will still be found as a supporting material, especially in roof construction, in interior doors and their frames, and as exterior cladding.\n\nWood is also commonly used as shuttering material to form the mold into which concrete is poured during reinforced concrete construction.\n\nA solid wood floor is a floor laid with planks or battens created from a single piece of timber, usually a hardwood. Since wood is hydroscopic (it acquires and loses moisture from the ambient conditions around it) this potential instability effectively limits the length and width of the boards.\n\nSolid hardwood flooring is usually cheaper than engineered timbers and damaged areas can be sanded down and refinished repeatedly, the number of times being limited only by the thickness of wood above the tongue.\n\nSolid hardwood floors were originally used for structural purposes, being installed perpendicular to the wooden support beams of a building (the joists or bearers) and solid construction timber is still often used for sports floors as well as most traditional wood blocks, mosaics and parquetry.\n\nEngineered wood products, glued building products \"engineered\" for application-specific performance requirements, are often used in construction and industrial applications. Glued engineered wood products are manufactured by bonding together wood strands, veneers, lumber or other forms of wood fiber with glue to form a larger, more efficient composite structural unit.\n\nThese products include glued laminated timber (glulam), wood structural panels (including plywood, oriented strand board and composite panels), laminated veneer lumber (LVL) and other structural composite lumber (SCL) products, parallel strand lumber, and I-joists. Approximately 100 million cubic meters of wood was consumed for this purpose in 1991. The trends suggest that particle board and fiber board will overtake plywood.\n\nWood unsuitable for construction in its native form may be broken down mechanically (into fibers or chips) or chemically (into cellulose) and used as a raw material for other building materials, such as engineered wood, as well as chipboard, hardboard, and medium-density fiberboard (MDF). Such wood derivatives are widely used: wood fibers are an important component of most paper, and cellulose is used as a component of some synthetic materials. Wood derivatives can be used for kinds of flooring, for example laminate flooring.\n\nWood has always been used extensively for furniture, such as chairs and beds. It is also used for tool handles and cutlery, such as chopsticks, toothpicks, and other utensils, like the wooden spoon and pencil.\n\nFurther developments include new lignin glue applications, recyclable food packaging, rubber tire replacement applications, anti-bacterial medical agents, and high strength fabrics or composites.\nAs scientists and engineers further learn and develop new techniques to extract various components from wood, or alternatively to modify wood, for example by adding components to wood, new more advanced products will appear on the marketplace. Moisture content electronic monitoring can also enhance next generation wood protection.\n\nWood has long been used as an artistic medium. It has been used to make sculptures and carvings for millennia. Examples include the totem poles carved by North American indigenous people from conifer trunks, often Western Red Cedar (\"Thuja plicata\").\n\nOther uses of wood in the arts include:\n\nMany types of sports equipment are made of wood, or were constructed of wood in the past. For example, cricket bats are typically made of white willow. The baseball bats which are legal for use in Major League Baseball are frequently made of ash wood or hickory, and in recent years have been constructed from maple even though that wood is somewhat more fragile. NBA courts have been traditionally made out of parquetry.\n\nMany other types of sports and recreation equipment, such as skis, ice hockey sticks, lacrosse sticks and archery bows, were commonly made of wood in the past, but have since been replaced with more modern materials such as aluminium, titanium or composite materials such as fiberglass and carbon fiber. One noteworthy example of this trend is the family of golf clubs commonly known as the \"woods\", the heads of which were traditionally made of persimmon wood in the early days of the game of golf, but are now generally made of metal or (especially in the case of drivers) carbon-fiber composites.\n\nLittle is known about the bacteria that degrade cellulose. Symbiotic bacteria in \"Xylophaga\" may play a role in the degradation of sunken wood; while bacteria such as \"Alphaproteobacteria\", \"Flavobacteria\", \"Actinobacteria\", \"Clostridia\", and \"Bacteroidetes\" have been detected in wood submerged over a year.\n\n"}
{"id": "13873874", "url": "https://en.wikipedia.org/wiki?curid=13873874", "title": "Zeta potential titration", "text": "Zeta potential titration\n\nZeta potential titration is a titration of heterogeneous systems, for example colloids and emulsions. Solids in such systems have very high surface area. This type of titration is used to study the zeta potential of these surfaces under different conditions.\n\nThe iso-electric point is one such property. The iso-electric point is the pH value at which the zeta potential is approximately zero. At a pH near the iso-electric point (± 2 pH units), colloids are usually unstable; the particles tend to coagulate or flocculate. Such titrations use acids or bases as titration reagents. Tables of iso-electric points for different materials are available. The attached figure illustrates results of such titrations for concentrated dispersions of alumina (4% v/v) and rutile (7% v/v). It is seen that iso-electric point of alumina is around pH 9.3, whereas for rutile it is around pH 4. Alumina is unstable in the pH range from 7 to 11. Rutile is unstable in the pH range from 2 to 6.\n\nAnother purpose of this titration is determination of the optimum dose of surfactant for achieving stabilization or flocculation of a heterogeneous system. Examples can be found in the book by Dukhin and Goetz.\n\nIn a zeta-potential titration, the Zeta potential is the indicator. Measurement of the zeta potential can be performed using microelectrophoresis, or electrophoretic light scattering, or electroacoustic phenomena. The last method makes possible to perform titrations in concentrated systems, with no dilution. The book by Dukhin and Goetz provides a detailed description of such titrations.\n\n"}
