{"id": "71318", "url": "https://en.wikipedia.org/wiki?curid=71318", "title": "100-year flood", "text": "100-year flood\n\nA one-hundred-year flood is a flood event that has a 1% probability of occurring in any given year.\n\nThe 100-year flood is also referred to as the 1% flood, since its annual exceedance probability is 1%. For river systems, the 100-year flood is generally expressed as a flowrate. Based on the expected 100-year flood flow rate, the flood water level can be mapped as an area of inundation. The resulting floodplain map is referred to as the 100-year floodplain. Estimates of the 100-year flood flowrate and other streamflow statistics for any stream in the United States are available. In the UK The Environment Agency publishes a comprehensive map of all areas at risk of a 1 in 100 year flood. Areas near the coast of an ocean or large lake also can be flooded by combinations of tide, storm surge, and waves. Maps of the riverine or coastal 100-year floodplain may figure importantly in building permits, environmental regulations, and flood insurance.\n\nA common misunderstanding is that a 100-year flood is likely to occur only once in a 100-year period. In fact, there is approximately a 63.4% chance of one or more 100-year floods occurring in any 100-year period. On the Danube River at Passau, Germany, the actual intervals between 100-year floods during 1501 to 2013 ranged from 37 to 192 years. The probability P that one or more floods occurring during any period will exceed a given flood threshold can be expressed, using the binomial distribution, as\n\nformula_1\n\nwhere T is the threshold return period (e.g. 100-yr, 50-yr, 25-yr, and so forth), and n is the number of years in the period. The probability of exceedance P is also described as the natural, inherent, or hydrologic risk of failure. However, the expected value of the number of 100-year floods occurring in any 100-year period is 1.\n\nTen-year floods have a 10% chance of occurring in any given year (P =0.10); 500-year have a 0.2% chance of occurring in any given year (P =0.002); etc. The percent chance of an X-year flood occurring in a single year is 100/X. A similar analysis is commonly applied to coastal flooding or rainfall data. The recurrence interval of a storm is rarely identical to that of an associated riverine flood, because of rainfall timing and location variations among different drainage basins.\n\nThe field of extreme value theory was created to model rare events such as 100-year floods for the purposes of civil engineering. This theory is most commonly applied to the maximum or minimum observed stream flows of a given river. In desert areas where there are only ephemeral washes, this method is applied to the maximum observed rainfall over a given period of time (24-hours, 6-hours, or 3-hours). The extreme value analysis only considers the most extreme event observed in a given year. So, between the large spring runoff and a heavy summer rain storm, whichever resulted in more runoff would be considered the extreme event, while the smaller event would be ignored in the analysis (even though both may have been capable of causing terrible flooding in their own right).\n\nThere are a number of assumptions which are made to complete the analysis which determines the 100-year flood. First, the extreme events observed in each year must be independent from year-to-year. In other words, the maximum river flow rate from 1984 cannot be found to be significantly correlated with the observed flow rate in 1985. 1985 cannot be correlated with 1986, and so forth. The second assumption is that the observed extreme events must come from the same probability distribution function. The third assumption is that the probability distribution relates to the largest storm (rainfall or river flow rate measurement) that occurs in any one year. The fourth assumption is that the probability distribution function is stationary, meaning that the mean (average), standard deviation and max/min values are not increasing or decreasing over time. This concept is referred to as stationarity.\n\nThe first assumption is often but not always valid and should be tested on a case by case basis. The second assumption is often valid if the extreme events are observed under similar climate conditions. For example, if the extreme events on record all come from late summer thunder storms (as is the case in the southwest U.S.), or from snow pack melting (as is the case in north-central U.S.), then this assumption should be valid. If, however, there are some extreme events taken from thunder storms, others from snow pack melting, and others from hurricanes, then this assumption is most likely not valid. The third assumption is only a problem when trying to forecast a low, but maximum flow event (for example, an event smaller than a 2-year flood). Since this is not typically a goal in extreme analysis, or in civil engineering design, then the situation rarely presents itself. The final assumption about stationarity is difficult to test from data for a single site because of the large uncertainties in even the longest flood records (see next section). More broadly, substantial evidence of climate change strongly suggests that the probability distribution is also changing and that managing flood risks in the future will become even more difficult. The simplest implication of this is that not all of the historical data are, or can be, considered valid as input into the extreme event analysis.\n\nWhen these assumptions are violated there is an \"unknown\" amount of uncertainty introduced into the reported value of what the 100-year flood means in terms of rainfall intensity, or flood depth. When all of the inputs are known the uncertainty can be measured in the form of a confidence interval. For example, one might say there is a 95% chance that the 100-year flood is greater than X, but less than Y.\n\nDirect statistical analysis to estimate the 100-year riverine flood is possible only at the relatively few locations where an annual series of maximum instantaneous flood discharges has been recorded. In the United States as of 2014, taxpayers have supported such records for at least 60 years at fewer than 2,600 locations, for at least 90 years at fewer than 500, and for at least 120 years at only 11. For comparison, the total area of the nation is about , so there are perhaps 3,000 stream reaches that drain watersheds of and 300,000 reaches that drain . In urban areas, 100-year flood estimates are needed for watersheds as small as . For reaches without sufficient data for direct analysis, 100-year flood estimates are derived from indirect statistical analysis of flood records at other locations in a hydrologically similar region or from other hydrologic models. Similarly for coastal floods, tide gauge data exist for only about 1,450 sites worldwide, of which only about 950 added information to the global data center between January 2010 and March 2016.\nMuch longer records of flood elevations exist at a few locations around the world, such as the Danube River at Passau, Germany, but they must be evaluated carefully for accuracy and completeness before any statistical interpretation.\n\nFor an individual stream reach, the uncertainties in any analysis can be large, so 100-year flood estimates have large individual uncertainties for most stream reaches. For the largest recorded flood at any specific location, or any potentially larger event, the recurrence interval always is poorly known. Spatial variability adds more uncertainty, because a flood peak observed at different locations on the same stream during the same event commonly represents a different recurrence interval at each location. If an extreme storm drops enough rain on one branch of a river to cause a 100-year flood, but no rain falls over another branch, the flood wave downstream from their junction might have a recurrence interval of only 10 years. Conversely, a storm that produces a 25-year flood simultaneously in each branch might form a 100-year flood downstream. During a time of flooding, news accounts necessarily simplify the story by reporting the greatest damage and largest recurrence interval estimated at any location. The public can easily and incorrectly conclude that the recurrence interval applies to all stream reaches in the flood area.\n\nPeak elevations of 14 floods as early as 1501 on the Danube River at Passau, Germany, reveal great variability in the actual intervals between floods. Flood events greater than the 50-year flood occurred at intervals of 4 to 192 years since 1501, and the 50-year flood of 2002 was followed only 11 years later by a 500-year flood. Only half of the intervals between 50- and 100-year floods were within 50 percent of the nominal average interval. Similarly, the intervals between 5-year floods during 1955 to 2007 ranged from 5 months to 16 years, and only half were within 2.5 to 7.5 years.\nIn the United States, the 100-year flood provides the risk basis for flood insurance rates. Complete information on the National Flood Insurance Program (NFIP) is available here. A \"regulatory flood\" or \"base flood\" is routinely established for river reaches through a science-based rule-making process targeted to a 100-year flood at the historical average recurrence interval. In addition to historical flood data, the process accounts for previously established regulatory values, the effects of flood-control reservoirs, and changes in land use in the watershed. Coastal flood hazards have been mapped by a similar approach that includes the relevant physical processes. Most areas where serious floods can occur in the United States have been mapped consistently in this manner. On average nationwide, those 100-year flood estimates are well sufficient for the purposes of the NFIP and offer reasonable estimates of future flood risk, if the future is like the past. Approximately 3% of the U.S. population lives in areas subject to the 1% annual chance coastal flood hazard.\n\nIn theory, removing homes and businesses from areas that flood repeatedly can protect people and reduce insurance losses, but in practice it is difficult for people to retreat from established neighborhoods.\n\n\n"}
{"id": "55257518", "url": "https://en.wikipedia.org/wiki?curid=55257518", "title": "Amity-enmity complex", "text": "Amity-enmity complex\n\nThe amity-enmity complex was a term introduced by Sir Arthur Keith. His work, \"A New Theory of Human Evolution\" (1948), posited that humans evolved as differing races, tribes, and cultures, exhibiting patriotism, morality, leadership and nationalism. Those who belong are part of the in-group, and tolerated; all others are classed as out-group, and subject to hostility; 'The code of enmity is a necessary part of the machinery of evolution. He who feels generous towards his enemy... has given up his place in the turmoil of evolutionary competition.' Conscience in humans evolved a duality; to protect and save friends,\nand also to hate and fight enemies. \nKeith's work summarized earlier opinions on human tribalism by Charles Darwin, Alfred Russel Wallace, and Herbert Spencer.\n\n\nThe amity-enmity complex maintains 'tribal spirit' and thus unity, of the community, 'as long as personal contact between its members is possible.' If the community grows beyond this limitation, then disruption, swarming and disintegration occur. Modern mass communication enables communities 'of 100 million' to remain intact.\n\nKeith expressed regret that this phenomenon, which explains so much, had not become common knowledge: \"[W]e eternally experience the misery... of each new manifestation of the complex, then invent some new 'ism' to categorise this behavior as an evil, dealing with a common behavioural trait piecemeal [instead of] finally grasping and understanding the phenomenon.\"\n\nColleges, sports teams, churches, trades unions, female fashions and political parties enable people to exhibit tribal loyalty within large, mass-communicating nations. 'In politics we have to take sides.' But all these 'petty manifestations' are cast aside in time of war.\nBismarck, Abraham Lincoln and Lloyd George are cited as statesmen who knew how to exploit the tribal spirit for political ends.\n\nRobert Ardrey pointed out that similar behavior can be observed in most primates, especially baboons and chimps. \"Nationalism as such is no more than a human expression of the animal drive to maintain and defend a territory... the mentality of the single Germanic tribe under Hitler differed in no way from that of early man or late baboon.\"\n\nThe amity-enmity complex is a serious obstacle to world peace and world government, and may even lead to nuclear holocaust: \"How can we get along without war?... if we fail to get along without war, the future will be as lacking in human problems as it will be remarkably lacking in men.\"\n\nDesmond Morris makes a prescriptive point: \"We must try to step outside our groups and look down on human battlefields with the unbiased eye of a hovering Martian.\" And he warns that \"the truly violent species all appear to have exterminated themselves, a lesson we should not overlook.\" The inherited aggression of the amity-enmity rivalry between communities is rationalized under a \"persistent cloak of ideology... a matter of ideals, moral principles, social philosophies or religious beliefs... [O]nly an immense amount of intellectual restraint will save the situation.\"\n\nAfter World War Two, a debate about the place of instinct and learning (the nature-versus-nurture debate) has occurred. According to Steven Pinker, the \"bitter lessons of lynchings, world wars, and the Holocaust\" have caused \"prevailing theories of mind\" to be \"refashioned to make racism and sexism as untenable as possible. The doctrine of the blank slate became entrenched in intellectual life.\"\n\nPinker makes the point that \"conflicts of interest are inherent to the human condition.\" Man is a product of nature, as much as malarial mosquitoes; both \"are doing exactly what evolution designed them to do, even if the outcome makes people suffer... [We] cannot call their behavior pathological... [T]he belief that violence is an aberration is dangerous.\"\n\n"}
{"id": "36939819", "url": "https://en.wikipedia.org/wiki?curid=36939819", "title": "Analog observation", "text": "Analog observation\n\nAnalog observation is, in contrast to naturalistic observation, a research tool by which a subject is observed in an artificial setting. Typically, types of settings in which analog observation is utilized include clinical offices or research laboratories, but, by definition, analog observations can be made in any artificial environment, even if the environment is one which the subject is likely to encounter naturally.\n\nAnalog observation is typically divided into two iteration of application: The first iteration primarily studies the effect of manipulation of variables in the subject's environment, including setting and events, on the subject's behavior. The second iteration primarily seeks to observe the subject's behavior in quasi-experimental social situations.\n\n"}
{"id": "251456", "url": "https://en.wikipedia.org/wiki?curid=251456", "title": "Brights movement", "text": "Brights movement\n\nThe Brights Movement is an international intellectual movement, whose members refer to themselves as Brights and hold a naturalist worldview.\n\nMost Brights believe that public policies should be based on science (a body of knowledge obtained and tested by use of the scientific method). Brights are likely to oppose the practice of basing public policies on supernatural doctrines. Brights may therefore be described as secularists.\n\nThe most politically active Brights frequently and openly advocate scientocracy, the practice of basing public policies on science. They look forward to living in an era when the best available scientific evidence provides a foundation for the humane and compassionate operation of human societies.\n\nThe Bright movement has proposed the following terminology:\n\nPaul Geisert, who coined the term bright and co-founded the bright movement is a one-time Chicago biology teacher, professor, entrepreneur and writer of learning materials. In deciding to attend the Godless Americans March on Washington in 2002, Geisert disliked the label \"godless\" as he saw it as synonymous with evil, he thought this would alienate the public. He sought a new, positive word that might become well-accepted and improve the image of those who did not believe in the supernatural, in the same way that the term \"gay\" did. A few weeks later, Geisert came up with the noun \"bright\" after brainstorming lots of ideas. He then ran into another room and told his wife: \"I've got the word, and this is going to be big!\".\n\nIt was also co-founded by his wife, Mynga Futrell. Futrell and Geisert remain co-directors of the organization to this day.\n\nAfter coming up with the term they pitched their idea to friends and decided to unveil their idea at an Atheist Alliance International conference in Tampa, Florida. They called the organizers and got permission to present the idea.\n\nGeisert and Futrell made their proposal in Spring 2003 at an atheist conference in Florida which was attended by Richard Dawkins. They launched the Brights' Net website on June 4, 2003. The movement gained early publicity through articles by Richard Dawkins in \"The Guardian\" and \"Wired\"; and by Daniel Dennett in \"The New York Times\".\n\nThe movement continued to grow and experienced accelerated registrations following media debate around New Atheism prompted by a series of book releases in late 2006 including \"The God Delusion\", \"\", \"God Is Not Great\", \"The End of Faith\" and \"Letter to a Christian Nation\". The movement has grown to be a constituency of over 78,000 Brights in 204 nations and territories. \n\nMany, but not all, brights also identify as atheist, antitheist, humanist (specifically secular humanist), freethinker, Objectivist, irreligionist, naturalist, materialist or physicalist, agnostic, skeptic, or even naturalistic pantheist. Even so, the \"movement is not associated with any defined beliefs\". The website Brights' Net says its goal is to include the umbrella term bright in the vocabulary of this existing \"community of reason\".\n\nHowever, \"the broader intent is inclusive of the many-varied persons whose worldview is naturalistic\", but are in the \"general population\" as opposed to associating solely with the \"community of reason\". Thus, persons who can declare their naturalistic worldview using the term bright extend beyond the familiar secularist categories as long as they do not hold theistic worldviews. Registrations even include some members of the clergy, such as Presbyterian ministers and a Church History Professor and ordained priest.\n\nDawkins compares the coining of bright to the \"triumph of consciousness-raising\" from the term gay:\nGay is succinct, uplifting, positive: an \"up\" word, where homosexual is a down word, and queer, faggot and pooftah are insults. Those of us who subscribe to no religion; those of us whose view of the universe is natural rather than supernatural; those of us who rejoice in the real and scorn the false comfort of the unreal, we need a word of our own, a word like \"gay\"[,] [...] a noun hijacked from an adjective, with its original meaning changed but not too much. Like gay, it should be catchy: a potentially prolific meme. Like gay, it should be positive, warm, cheerful, bright.\n\nDespite the explicit difference between the noun and adjective, there have been comments on the comparison. In his \"Wired\" article, Dawkins stated: \"Whether there is a statistical tendency for brights [noun] to be bright [adjective] is a matter for research\".\n\nNotable people who have self-identified as brights at one time or another include: biologists Richard Dawkins and Richard J. Roberts; cognitive scientist Steven Pinker; philosophers Daniel Dennett and Massimo Pigliucci; stage magicians and debunkers James Randi and Penn & Teller; Amy Alkon; Sheldon Lee Glashow; Babu Gogineni; Edwin Kagin; Mel Lipman; Piergiorgio Odifreddi; and Air America Radio talk show host Lionel.\n\nDaniel Dennett, in his book \"\", suggests that if non-naturalists are concerned with connotations of the word bright, then they should invent an equally positive sounding word for themselves, like supers (i.e. one whose world view contains supernaturalism). He also suggested this during his presentation at the Atheist Alliance International '07 convention. Geisert and Futrell maintain that the neologism has always had a kinship with the Enlightenment, an era which celebrated the possibilities of science and a certain amount of free inquiry. They have endorsed the use of super as the antonym to bright, although this term makes the assumption that anyone not a bright necessarily subscribes to notions of supernaturalism.\n\nThe Brights' avatar represents a celestial body viewed from space. As there is no up or down or right or left in outer space, the arrangement of planet and darkness and starlight is changeable. Although the symbol is open to the viewer's interpretation, it is generally meant to invoke transition and a sense of gradual illumination. The intentional ambiguity of the avatar is meant to symbolically reflect an important question: Is the future of humankind becoming luminous or more dim? The Brights aspire \"to take the promising route, whereby the imagery brings to mind a gradually increasing illumination for this earth of ours, an escalation of enlightenment\". This optimistic interpretation of the Brights' symbol is summarized by the motto \"Embrightenment Now!\".\n\nThe movement has been criticised by some (both religious and non-religious) who have objected to the adoption of the title \"bright\" because they believe it suggests that the individuals with a naturalistic worldview are more intelligent (\"brighter\") than non-naturalists, such as philosophical skeptics or idealists, believers in the paranormal, philosophical theists, or the religious. For example, the Committee for Skeptical Inquiry published an article by Chris Mooney titled \"Not Too 'Bright in which he stated that although he agreed with the movement, Richard Dawkins's and Daniel Dennett's \"campaign to rename religious unbelievers 'brights' could use some rethinking\" because of the possibility that the term would be misinterpreted. The journalist and noted atheist Christopher Hitchens likewise found it a \"cringe-making proposal that atheists should conceitedly nominate themselves to be called 'brights.\n\nIn response to this, Daniel Dennett has stated:\n"}
{"id": "25508508", "url": "https://en.wikipedia.org/wiki?curid=25508508", "title": "Building Safer Communities. Risk Governance, Spatial Planning and Responses to Natural Hazards", "text": "Building Safer Communities. Risk Governance, Spatial Planning and Responses to Natural Hazards\n\nBuilding Safer Communities. Risk Governance, Spatial Planning and Responses to Natural Hazards is a 2009 book edited by Urbano Fra Paleo, published by IOS Press.\n\nThis textbook examines the central principles of enhanced risk governance, whose implementation might help to mitigate the increasing losses caused by natural hazards. It promotes the adoption of proactive, preventive approaches in public policies, particularly through land use planning, by influencing on the occupation of hazard-prone areas.\nIt serves both as a comprehensive introduction to the formulation and implementation at the strategic level of policies that address risk, and as an advancement in the integration of current practices, including emergency management, environmental management, community development and spatial planning. \nThe authors study and construe solutions that review integrated strategies of the various levels of government considering:\n\nUrbano Fra Paleo is a geographer, and an Associate Professor of Human Geography at the University of Santiago de Compostela, Spain.\n\n"}
{"id": "44262036", "url": "https://en.wikipedia.org/wiki?curid=44262036", "title": "Coleridge's theory of life", "text": "Coleridge's theory of life\n\nRomanticism grew largely out of an attempt to understand not just inert nature, but also vital nature. Romantic works in the realm of art and Romantic medicine were a response to the general failure of the application of method of inertial science to reveal the foundational laws and operant principles of vital nature. German romantic science and medicine sought to understand the nature of the life principle identified by John Hunter as distinct from matter itself via Johan Friedrich Blumenbach's \"Bildungstrieb\" and Romantic medicine's \"Lebenskraft\", as well as Röschlaub's development of the Brunonian system of medicine system of John Brown, in his \"excitation theory\" of life (German:\"Erregbarkeit theorie\"), working also with Schelling's \"Naturphilosophie\", the work of Goethe regarding morphology, and the first dynamic conception of physiology of Richard Saumarez. But it is in Samuel Taylor Coleridge that we find the question of life and vital nature most intensely and comprehensively examined, particularly in his Hints towards the Formation of a more Comprehensive Theory of Life (1818), providing the foundation for Romantic philosophy, science and medicine. The work is key to understanding the relationship of Romantic literature and science.\n\nThe Enlightenment had developed a philosophy and science supported by formidable twin pillars: the first the Cartesian split of mind and matter, the second Newtonian physics, with its conquest of inert nature, both of which focused the mind's gaze on things or objects. For Cartesian philosophy, life existed on the side of matter, not mind; and for the physical sciences, the method that had been so productive for revealing the secrets of inert nature, should be equally productive in examining vital nature. The initial attempt to seek the cause and principle of life in matter was challenged by John Hunter, who held that the principle of life was not to be found nor confined within matter, but existed independently of matter itself, and informed or animated it, that is, he implied, it was the unifying or antecedent cause of the things or what Aristotelean philosophy termed \"natura naturata\" (the outer appearances of nature).\nThis reduction of the question of life to matter, and the corollary, that the method of the inertial sciences was the way to understand the very phenomenon of life, that is, its very nature and essence as a power (\"natura naturans\"), not as manifestations through sense-perceptible appearances (\"natura naturata\"), also reduced the individual to a material-mechanical 'thing' and seemed to render human freedom an untenable concept. It was this that Romanticism challenged, seeking instead to find an approach to the essence of nature as being also vital not simply inert, through a systematic method involving not just physics, but physiology (living functions). For Coleridge, quantitative analysis was anti-realist and needed to be grounded in qualitative analysis ('-ologies') (as was the case with Goethe's approach).\nAt the same time, the Romantics had to deal with the idealistic view that life was a 'somewhat' outside of things, such that the things themselves lost any real existence, a stream coming through Hume and Kant, and also infusing the German natural philosophical stream, German idealism, and in particular \"naturphilosophie\", eventuating scientifically in the doctrine of 'vitalism'. For the Romantics, life is independent of and antecedent to nature, but also infused and suspended in nature, not apart from it, As David Bohm expresses it in more modern terms \"In nature nothing remains constant…everything comes from other things and gives rise to other things. This principle is…at the foundation of the possibility of our understanding nature in a rational way.\"\n\nAnd as Coleridge explained, 'this antecedent unity, or cause and principle of each union, it has since the time of Bacon and Kepler been customary to call a law.\" And as law, \"we derive from it a progressive insight into the necessity and generation of the phenomena of which it is the law.\"\n\nColeridge's was the dominant mind on many issues involving the philosophy and science in his time, as John Stuart Mill acknowledged, along with others since who have studied the history of Romanticism.\n\nFor Coleridge, as for many of his romantic contemporaries, the idea that matter itself can begat life only dealt with the various changes in the arrangement of particles and did not explain life itself as a principle or power that lay behind the material manifestations, \"natura naturans\" or \"the productive power suspended and, as it were, quenched in the product\" Until this were addressed, according to Coleridge, \"we have not yet attained to a science of nature.\"\n\nThis productive power is above sense experience, but not above nature herself, that is, supersensible, but not supernatural, and, thus, not 'occult' as was the case with vitalism. Vitalism failed to distinguish between spirit and nature, and then within nature, between the visible appearances and the invisible, yet very real and not simply hypothesized notion, essence or motivating principle (\"natura naturans\"). Even Newton spoke of things invisible in themselves (though not in their manifestations), such as force, though Comte, the thorough materialist, complained of the use of such terms as the 'force of gravity' as being relics of animism.\n\nMatter was not a 'datum' or thing in and of itself, but rather a product or effect, and for Coleridge, looking at life in its broadest sense, it was the product of a polarity of forces and energies, but derived from a unity which is itself a power, not an abstract or nominal concept, that is Life, and this polar nature of forces within the power of Life is the very law or 'Idea' (in the Platonic sense) of Creation.\n\nFor Coleridge the essence of the universe is motion and motion is driven by a dynamic polarity of forces that is both inherent in the world as potential and acting inherently in all manifestations. This polarity is the very dynamic that acts throughout all of nature, including into the more particular form of 'life biological', as well as of mind and consciousness.\n\nAnd this polarity is dynamic, that is real, though not visible, and not simply logical or abstract. Thus, the polarity results in manifestations that are real, as the opposite powers are not contradictory, but counteracting and inter-penetrating.\n\nThus, then, Life itself is not a thing—a self-subsistent hypostasis—but an act and process...\n\nAnd in that sense Coleridge re-phrases the question \"What is Life?\" to \"What is not Life that really is?\"\n\nThis dynamic polar essence of nature in all its functions and manifestations is a universal law in the order of the law of gravity and other physical laws of inert nature. And, critically, this dynamic polarity of constituent powers of life at all levels is not outside or above nature, but is within nature (\"natura naturans\"), not as a part of the visible product, but as the ulterior natural functions that produce such products or things.\n\nIt is these functions that provided the bridge being sought by Romantic science and medicine, in particular by Andreas Röschlaub and the Brunonian system of medicine, between the inertial science of inert nature (physics) and the vital science of vital nature (physiology) and its therapeutic application or physic (the domain of the physician).\n\nColeridge was influenced by German philosophy, in particular Kant, Fichte and Schelling (Naturphilosophie), as well as the physiology of Blumenbach and the dynamic excitation theory of life of the Brunonian system. He sought a path that was neither the mystical tendency of the earlier vitalists nor the materialistic reductionist approach to natural science, but a dynamic one.\n\nColeridge's challenge was to describe something that was dynamic neither in mystical terms not materialistic ones, but via analogy, drawing from the examples of inertial science. As one writer explains, he uses the examples of electricity, magnetism and gravity not because they are like life, but because they offer a way of understanding powers, forces and energies, which lie at the heart of life. And using these analogies, Coleridge seeks to demonstrate that life is not a material force, but a product of relations amongst forces. Life is not linear and static, but dynamic process of self-regulation and Emergent evolution that results in increasing complexity and individuation. This spiral, upward movement (cf. Goethe's ideas) creates a force for organization that unifies, and is most intense and powerful in that which is most complex and most individual - the self-regulating, enlightened, developed individual mind. But at the same time, this process of life increases interdependence (like the law of comparative advantage in economics) and associational powers of the mind. Thus, he is not talking about an isolated, individual subjective mind, but about the evolution of a higher level of consciousness and thought at the core of the process of life.\n\nAnd the direction of this motion is towards increasing individuation, that is the creation of specific, individual units of things. At the same time, given the dynamic polarity of the world, there must always be an equal and opposite tendency, in this case, that of connection. So, a given of our experience is that man is both an individual, tending in each life and in history generally to greater and greater individualization, and a social creature seeking interaction and connection. It is the dynamic interplay between the individuation and connecting forces that leads to higher and higher individuation.\n\nColeridge makes a further distinction between mathematics and life, the latter being productive or creative, that is, living, and the former ideal. Thus, the mathematical approach that works so well with inert nature, is not suitable for vital nature.\n\nThe counteraction then of the two assumed forces does not depend on their meeting from opposite directions; the power which acts in them is indestructible; it is therefore inexhaustibly re-ebullient; and as something must be the result of these two forces, both alike infinite, and both alike indestructible; and as rest or neutralization cannot be this result; no other conception is possible, but that the product must be a tertium aliquid, [a third thing] or finite generation. Consequently this conception is necessary. Now this tertium aliquid can be no other than an inter-penetration of the counteracting powers, partaking of both… Consequently the 'constituent powers', that have given rise to a body, may then reappear in it as its function: \"a Power, acting in and by its Product or Representative to a predetermined purpose is a Function...the first product of its energy is the thing itself: \"ipsa se posuit et iam facta est ens positum\". Still, however, its productive energy is not exhausted in this product, but overflows, or is effluent, as the specific forces, properties, faculties, of the product. It reappears, in short, as the function of the body...The vital functions are consequents of the \"Vis Vitae Principium Vitale\", and presuppose the Organs, as the Functionaries.\n\nLife, that is, the essential polarity in unity (multeity in unity) in Coleridge’s sense also has a four beat cycle, different from the arid dialectics of abstraction - namely the tension of the polar forces themselves, the charge of their synthesis, the discharge of their product (indifference) and the resting state of this new form (predominance). The product is not a neutralization, but a new form of the essential forces, these forces remaining within, though now as the functions of the form.\n\nTo make it adequate, we must substitute the idea of positive production for that of rest, or mere neutralization. To the fancy alone it is the null-point, or zero, but to the reason it is the punctum saliens, and the power itself in its eminence.\n\nThis dynamic polarity that is Life is expressed at different levels. At its most basic it is Space-Time, with its product - motion. The interplay of both gives us either a line or a circle, and then there are different degrees possible within a given form or “predominance” of forces. Geometry is not conceivable except as the dynamic interplay of space (periphery) and time (point). Space, time and motion are also geometrically represented by width, length (breadth) and depth. And this correspondence is repeated throughout the scale of Life.\n\nMatter, then, is the product of the dynamic forces - repulsion (centrifugal), and attraction (centripetal); it is not itself a productive power. It is also the mass of a given body.\n\nColeridge’s understanding of life is contrasted with the materialist view which is essentially reduced to defining life as that which is the opposite of not-life, or that which resists death, that is, that which is life.\n\nThe problem for Coleridge and the Romantics was that the intellect, 'left to itself' as Bacon stated, was capable of apprehending only the outer forms of nature (natura naturata) and not the inmost, living functions (natura naturans) giving rise to these forms. Thus, effects can only be 'explained' in terms of other effects, not causes. It takes a different capacity to 'see' these living functions, which is an imaginative activity. For Coleridge, there is an innate, primitive or 'primary' imagination that configures invisibly sense-experience into perception, but a rational perception, that is, one raised into consciousness and awareness and then rationally presentable, requires a higher level, what he termed 'secondary imagination', which is able to connect with the thing being experienced, penetrate to its essence in terms of the living dynamics upholding its outer form, and then present the phenomena as and within its natural law, and further, using reason, develop the various principles of its operation.\n\nThis cognitive capacity involved what Coleridge termed the 'inmost sense' or what Goethe termed the Gemüt. It also involved the reactivation of the old Greek noetic capacity, and the ability to 'see' or produce the theory (Greek \"theoria\" from the verb 'to see') dynamic polarities, or natural Laws, the dynamic transcendent (foundational) entities that Plato termed 'Ideas' (\"eidos\").\n\nSince \"natura naturata\" is sustained by \"natura naturans\", and the creative power of \"natura naturans\" is one of a kind with the human mind, itself creative, then there must be a correspondence or connection between the mind and the things we perceive, such that we can overcome the apparent separation between the object and the representation in the mind of the object that came to bedevil Enlightenment thought (Hume, Kant). As one commentator noted \"to speak at all of the unity of intelligence and nature is of course flatly to contradict Descartes.\"\n\nFor Coleridge the power of life lies in every seed as a potential to be unfolded as a result of interaction with the environment (heat, light, air, moisture, etc.), an insight which allowed him to see in the Brunonian system a dynamic polarity in excitation theory.\nColeridge also saw that there was a progressive movement through time and space of life or the law of polarity, from the level of physics (space and time) and the mineral or inert nature (law of gravity, operating through forces of attraction and repulsion), up to man, with his law of resonance in terms of his innate desire to be himself (force of individuation) and to also connect with like-minded (force of connection), as Goethe expressed in his novel \"Elective Affinities\" (\"Wahlverwandschaften\") as well as in his own life's experience.\nEvolution occurred because the original polarity of creation, the very 'Law of Creation', itself gives birth to subsequent polarities, as each pole is itself a unity that can be further polarized (what Wilhelm Reich later termed 'orgonomic functionalism' and what at the biological level constitutes physiology), an insight that would later be taken up by the concept of emergent evolution, including the emergence of mind and consciousness.\n\nAnd that this is so, is also an intimate and shared experience of all humans, as is set out in Reid's Common Sense philosophy. As Coleridge states\n\nThat nature evolves towards a purpose, and that is the unfolding of the human mind and consciousness in all its levels and degrees, is not teleological but a function of the very nature of the law of polarity or creation itself, namely that of increasing individuation of an original unity, what Coleridge termed 'multeity in unity'. As he states, \"without assigning to nature as nature, a conscious purpose\" we must still \"distinguish her agency from a blind and lifeless mechanism.\"\n\nWhile man contains and is subject to the various laws of nature, man as a self-conscious being is also the summa of a process of creation leading to greater mind and consciousness, that is, a creative capacity of imagination. Instead of being a creature of circumstance, man is the creator of them, or at least has that potential.\n\n"}
{"id": "1065253", "url": "https://en.wikipedia.org/wiki?curid=1065253", "title": "Darwin machine", "text": "Darwin machine\n\nA Darwin machine (a 1987 coinage by William H. Calvin, by analogy to a Turing machine) is a machine that, like a Turing machine, involves an iteration process that yields a high-quality result, but, whereas a Turing machine uses logic, the Darwin machine uses rounds of variation, selection, and inheritance.\nIn its original connotation, a Darwin machine is any process that bootstraps quality by utilizing all of the six essential features of a Darwinian process: A \"pattern\" is \"copied\" with \"variations\", where populations of one variant pattern \"compete\" with another population, their relative success biased by a \"multifaceted environment\" (natural selection) so that winners predominate in producing the further variants of the next generation (Darwin's \"inheritance principle\").\n\nMore loosely, a Darwin machine is a process that utilizes some subset of the Darwinian essentials, typically natural selection to create a non-reproducing pattern, as in neural Darwinism. Many aspects of neural development utilize overgrowth followed by pruning to a pattern, but the resulting pattern does not itself create further copies.\n\n\"Darwin machine\" has been used multiple times to name computer programs after Charles Darwin.\n\n\n"}
{"id": "1045142", "url": "https://en.wikipedia.org/wiki?curid=1045142", "title": "Debris", "text": "Debris\n\nDebris or débris (, ) is rubble, wreckage, ruins, litter and discarded garbage/refuse/trash, scattered remains of something destroyed, discarded, or as in geology, large rock fragments left by a melting glacier etc. Depending on context, \"debris\" can refer to a number of different things. The first apparent use of the French word in English is in a 1701 description of the army of Prince Rupert upon its retreat from a battle with the army of Oliver Cromwell, in England.\n\nIn disaster scenarios, tornados leave behind large pieces of houses and mass destruction overall. This debris also flies around the tornado itself when it is in progress. The tornado's winds capture debris it kicks up in its wind orbit, and spins it inside its vortex. The tornado's wind radius is larger than the funnel itself. tsunamis and hurricanes also bring large amounts of debris, such as Hurricane Katrina in 2005 and Hurricane Sandy in 2012. Earthquakes rock cities to rubble debris.\n\nIn geology, debris usually applies to the remains of geological activity including landslides, volcanic explosions, avalanches, mudflows or Glacial lake outburst floods (Jökulhlaups) and moraine, lahars, and lava eruptions. Geological debris sometimes moves in a stream called a debris flow. When it accumulates at the base of hillsides, it can be called \"talus\" or \"scree\".\n\nIn mining, debris called \"attle\" usually consists of rock fragments which contain little or no ore.\n\n\"Marine debris\" applies to floating garbage such as bottles, cans, styrofoam, cruise ship waste, offshore oil and gas exploration and production facilities pollution, and fishing paraphernalia from professional and recreational boaters. Marine debris is also called litter or flotsam and jetsam. Objects that can constitute marine debris include used automobile tires, detergent bottles, medical wastes, discarded fishing line and nets, soda cans, and bilge waste solids.\n\nIn addition to being unsightly, it can pose a serious threat to marine life, boats, swimmers, divers, and others. For example, each year millions of seabirds, sea turtles, fish, and marine mammals become entangled in marine debris, or ingest plastics which they have mistaken for food. As many as 30,000 northern fur seals per year get caught in abandoned fishing nets and either drown or suffocate. Whales mistake plastic bags for squid, and birds may mistake plastic pellets for fish eggs. At other times, animals accidentally eat the plastic while feeding on natural food.\n\nThe largest concentration of marine debris is the Great Pacific Garbage Patch.\n\nMarine debris most commonly originates from land-based sources. Various international agencies are currently working to reduce marine debris levels around the world.\n\nIn meteorology, debris usually applies to the remains of human habitation and natural flora after storm related destruction. This debris is also commonly referred to as storm debris. Storm debris commonly consists of roofing material, downed tree limbs, downed signs, downed power lines and poles, and wind-blown garbage. Storm debris can become a serious problem immediately after a storm, in that it often blocks access to individuals and communities that may require emergency services. This material frequently exists in such large quantities that disposing of it becomes a serious issue for a community. In addition, storm debris is often hazardous by its very nature, since, for example, downed power lines annually account for storm-related deaths.\n\n\"Space debris\" usually refers to the remains of spacecraft that have either fallen to Earth or are still orbiting Earth. Space debris may also consist of natural components such as chunks of rock and ice. The problem of space debris has grown as various space programs have left legacies of launches, explosions, repairs, and discards in both low Earth orbit and more remote orbits. These orbiting fragments have reached a great enough proportion to constitute a hazard to future space launches of both satellite and manned vehicles. Various government agencies and international organizations are beginning to track space debris and also research possible solutions to the problem. While many of these items, ranging in size from nuts and bolts to entire satellites and spacecraft, may fall to Earth, other items located in more remote orbits may stay aloft for centuries. The velocity of some of these pieces of space junk have been clocked in excess of 17,000 miles per hour (27,000 km/h). A piece of space debris falling to Earth leaves a fiery trail, just like a meteor.\n\nA debris disk is a circumstellar disk of dust and debris in orbit around a star.\n\nIn medicine, debris usually refers to biological matter that has accumulated or lodged in surgical instruments and is referred to as surgical debris. The presence of surgical debris can result in cross-infections or nosocomial infections if not removed and the affected surgical instruments or equipment properly disinfected.\n\nIn the aftermath of a war, large areas of the region of conflict are often strewn with \"war debris\" in the form of abandoned or destroyed hardware and vehicles, mines, unexploded ordnance, bullet casings and other fragments of metal.\n\nMuch war debris has the potential to be lethal and continues to kill and maim civilian populations for years after the end of a conflict. The risks from war debris may be sufficiently high to prevent or delay the return of refugees. In addition war debris may contain hazardous chemicals or radioactive components that can contaminate the land or poison civilians who come into contact with it. Many Mine clearance agencies are also involved in the clearance of war debris.\n\nLand mines in particular are very dangerous as they can remain active for decades after a conflict, which is why they have been banned by international war regulations.\n\nIn November 2006 the Protocol on Explosive Remnants of War\ncame into effect with 92 countries subscribing to the treaty that requires the parties involved in a conflict to assist with the removal of unexploded ordnance following the end of hostilities.\n\nSome of the countries most affected by war debris are Afghanistan, Angola, Cambodia, Iraq and Laos.\n\nSimilarly \"military debris\" may be found in and around firing ranges and military training areas.\n\nDebris can also be used as cover for military purposes, depending on the situation.\n\nIn South Louisiana's Creole and Cajun cultures, debris (pronounced \"DAY-bree\") refers to chopped organs such as liver, heart, kidneys, tripe, spleen, brain, lungs and pancreas.\n\n\n"}
{"id": "48048662", "url": "https://en.wikipedia.org/wiki?curid=48048662", "title": "Dessauite-(Y)", "text": "Dessauite-(Y)\n\nDessauite-(Y) is a mineral member of the crichtonite group with the formula (Sr,Pb)(Y,U)(Ti,Fe)O. It is associated with derbylite, hematite, rutile, karelianite, siderite, and calcite. Founded in the Buca della Vena Mine, Tuscany, Italy, the mineral was called dessauite in honor of professor Gabor Dessau (1907–1983). \n\nDessauite occurs as small, flattened rhombohedral crystals, tabular {001} with hexagonal outline. Members of the Crichtonite group may be confused with ilmenite or hematite. The difference between dessauite and other minerals in the crichonite group is the occurrence of three additional octahedral sites and of a site in square pyramidal coordination, all with low occupancies. The mineral is black and opaque, presents a metallic luster, and it is brittle. Dessauite presents dimensions of diameter up to 1mm and thickness up to 0.2mm. In reflected plane-polarized light the color is ash-grey with pale bluish tones. The calculated density is 4.68 g/cm. The habit is tabular, forming thin dimensions in one direction and hardness of 6.5 and 7. Dessauite differs from other elements of the crichtonite group because of the quantity of cations and X-ray diffraction pattern.\n\nDessauite was found in the Buca della Vena Mine, Apuan Alps, northern Tuscany, Italy, with many other minerals, coming from hydrothermal fluids circulating through a small hematite-barite ore deposit within dolomite, during an alpine metamorphic event. It occurs in calcite veins hosted within dolomite and is associated with calcite, rutile, hematite, siderite, and derbylite.\n\n"}
{"id": "35353877", "url": "https://en.wikipedia.org/wiki?curid=35353877", "title": "Deyrolle", "text": "Deyrolle\n\nDuring the 20th century, Deyrolle was a Parisian institution for natural sciences and pedagogy. It is one of the best known companies of entomology and taxidermy of Paris. Today, Deyrolle is a shop and a cabinet of curiosities open to the public, a reference in the field of taxidermy, entomology and natural sciences, whose vocation is to show the beauty of Nature. Deyrolle is also involved in pedagogy and art.\n\nDeyrolle was created in 1831 by Jean-Baptiste Deyrolle, who was soon succeeded by his son Achille, at 46 rue du Bac in a building constructed in 1697-1699 by Jean-Baptiste Voille for a member of the Bruand family (Libéral Bruand). It was deeply transformed in 1739 by Samuel-Jacques Bernard, son of the banker of Louis XIV, Samuel Bernard (7 arrondissement). Beyond its scientific material, minerals collections, seashells, fossils, mounted animals and prehistoric tools, Deyrolle provides pedagogical charts to schools and universities in France, made to illustrate teacher’s lessons. (\"Musée scolaire Deyrolle\").\n\nIn 2001 Louis Albert de Broglie bought Deyrolle and he restored the shop.\n\nOn February the 1st 2008, the Cabinet of Curiosities was destroyed by a big fire. The cause was probably a short-circuit. A big part of the rooms and of the collections has been destroyed: butterflies, insects, and animals (zebras, alligators, gazelles, bears, lions, shellfish and turtles). On May the 15th 2008, the building was already cleaned and the two rooms of the first floor reopened.\n\nSome artists that contributed to save Deyrolle:\n\nJan Fabre - Nan Goldin - Jacques Grange - Karen Knorr - Marie-Jo Lafontaine - Claude Lalanne - François-Xavier Lalanne - Pierre Alechinsky - Yann Arthus-Bertrand - Miquel Barcelo - Pascal Bernier - Laurent Bochet - Sophie Calle - Johan Creten - Marc Dantan - Nicolas Darrot - Mark Dion - Bettina Rheims - Bernar Venet - Huang Yong Ping.\n\nDeyrolle is well known for its pedagogical charts. It all starts around 1871, when Emile Deyrolle developed everything that concerns the educational material, anatomical models in staff, biology pieces, and most of all, the creation of coloured wall charts, published under the name \"Musée scolaire Deyrolle\". They are meant to teach the \"Leçons de choses\" (\"Lessons of things\") but also Botany, Zoology, Entomology, Geography, Anatomy, Civics, Physics, Chemistry, Geology, Mineralogy, Biology, etc.\n\n« \"Visual instruction is the least tiring for the mind, but this education can have good results only if the ideas engraved in the children’s mind are rigorously exact.\" » Émile Deyrolle\n\nIn 2007, Louis Albert de Broglie restarts the publishing activity with the creation of new educational charts, to tackle contemporary environmental and societal issues. It is the start of a new collection of educational charts published under the name of Deyrolle pour l’Avenir (DPA). There are charts on sustainable development, climate changes, endangered species, renewable energy, etc.\n\nDeyrolle is a reference in the field of taxidermy. We can find birds, beasts and mammals from all over the world. At Deyrolle, with only a few exceptions, no animal was killed to be mounted: the non-domestic species come from zoos, parks, where they died of old age or illness. They are traceable, and protected species are held and delivered in accordance with the Washington Convention (CITES).\n\nDeyrolle is also known for its entomological collections. The drawers of the entomological room are filled with colourful butterflies, beetles, and other insects. It is possible to see the experts of the entomology team working on the mounting of insects.\n\nThe first aim of Deyrolle was to teach natural sciences to children and students, but Deyrolle was a point of interest also for artists: the surrealists André Breton and Salvador Dalí, the painters Jean Dubuffet and Mathieu, the writers Louise de Vilmorin and Théodore Monod, Raymond Queneau and many others stopped regularly at the shop.\n\nToday, Deyrolle continues its proximity with artists and the shop welcomes a lot of exhibitions and events during the year: Bettina Rheims, Éric Sander or also Charwei Tsai was exhibited at Deyrolle. Woody Allen used the rooms of Deyrolle in July 2010 for his movie \"Midnight in Paris\", and Wes Anderson is a huge fan of the shop.\n\nDeyrolle also develops collaborations with artists. We can mention Aurèle or Damien Hirst, for example.\n\nIn 2005, French singer Nolwenn Leroy shot the artwork for her album \"Histoires Naturelles\" at Deyrolle, as well as the music video for the single \"Histoire Naturelle\".\n\nSome exhibitions:\n\n\n"}
{"id": "944638", "url": "https://en.wikipedia.org/wiki?curid=944638", "title": "Earth's energy budget", "text": "Earth's energy budget\n\nEarth's energy budget accounts for the balance between the energy Earth receives from the Sun, the energy Earth radiates back into outer space after having been distributed throughout the five components of Earth's climate system and having thus powered the so-called Earth’s heat engine. This system is made up of earth's water, ice, atmosphere, rocky crust, and all living things.\n\nQuantifying changes in these amounts is required to accurately model the Earth's climate. \n\nReceived radiation is unevenly distributed over the planet, because the Sun heats equatorial regions more than polar regions. The atmosphere and ocean work non-stop to even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation. Earth is very close to being in radiative equilibrium, the situation where the incoming solar energy is balanced by an equal flow of heat to space; under that condition, global temperatures will be \"relatively\" stable. Globally, over the course of the year, the Earth system—land surfaces, oceans, and atmosphere—absorbs and then radiates back to space an average of about 240 watts of solar power per square meter. Anything that increases or decreases the amount of incoming or outgoing energy will change global temperatures in response.\n\nHowever, Earth's energy balance and heat fluxes depend on many factors, such as atmospheric composition (mainly aerosols and greenhouse gases), the albedo (reflectivity) of surface properties, cloud cover and vegetation and land use patterns.\n\nChanges in surface temperature due to Earth's energy budget do not occur instantaneously, due to the inertia of the oceans and the cryosphere. The net heat flux is buffered primarily by becoming part of the ocean's heat content, until a new equilibrium state is established between radiative forcings and the climate response.\n\nIn spite of the enormous transfers of energy into and from the Earth, it maintains a relatively constant temperature because, as a whole, there is little net gain or loss: Earth emits via atmospheric and terrestrial radiation (shifted to longer electromagnetic wavelengths) to space about the same amount of energy as it receives via insolation (all forms of electromagnetic radiation).\n\nTo quantify Earth's \"heat budget\" or \"heat balance\", let the insolation received at the top of the atmosphere be 100 units (100 units = about 1,360 watts per square meter facing the sun), as shown in the accompanying illustration. Called the albedo of Earth, around 35 units are reflected back to space: 27 from the top of clouds, 2 from snow and ice-covered areas, and 6 by other parts of the atmosphere. The 65 remaining units are absorbed: 14 within the atmosphere and 51 by the Earth’s surface. These 51 units are radiated to space in the form of terrestrial radiation: 17 directly radiated to space and 34 absorbed by the atmosphere (19 through latent heat of condensation, 9 via convection and turbulence, and 6 directly absorbed). The 48 units absorbed by the atmosphere (34 units from terrestrial radiation and 14 from insolation) are finally radiated back to space. These 65 units (17 from the ground and 48 from the atmosphere) balance the 65 units absorbed from the sun in order to maintain zero net gain of energy by the Earth.\n\nThe total amount of energy received per second at the top of Earth's atmosphere (TOA) is measured in watts and is given by the solar constant times the cross-sectional area of the Earth. Because the surface area of a sphere is four times the cross-sectional surface area of a sphere (i.e. the area of a circle), the average TOA flux is one quarter of the solar constant and so is approximately 340 W/m². Since the absorption varies with location as well as with diurnal, seasonal and annual variations, the numbers quoted are long-term averages, typically averaged from multiple satellite measurements.\n\nOf the ~340 W/m² of solar radiation received by the Earth, an average of ~77 W/m² is reflected back to space by clouds and the atmosphere and ~23 W/m² is reflected by the surface albedo, leaving ~240 W/m² of solar energy input to the Earth's energy budget. This gives the earth a mean net albedo of 0.29.\n\nThe geothermal heat flux from the Earth's interior is estimated to be 47 terawatts. This comes to 0.087 watt/square metre, which represents only 0.027% of Earth's total energy budget at the surface, which is dominated by 173,000 terawatts of incoming solar radiation.\n\nHuman production of energy is even lower, at an estimated 18 TW.\n\nPhotosynthesis has a larger effect: photosynthetic efficiency turns up to 2% of incoming sunlight into biomass, for a total photosynthetic productivity of earth between ~1500–2250 TW (~1%+/-0.26% solar energy hitting the Earth's surface).\n\nOther minor sources of energy are usually ignored in these calculations, including accretion of interplanetary dust and solar wind, light from stars other than the Sun and the thermal radiation from space. Earlier, Joseph Fourier had claimed that deep space radiation was significant in a paper often cited as the first on the greenhouse effect.\n\nLongwave radiation is usually defined as outgoing infrared energy leaving the planet. However, the atmosphere absorbs parts initially, or cloud cover can reflect radiation. Generally, heat energy is transported between the planet's surface layers (land and ocean) to the atmosphere, transported via evapotranspiration and latent heat fluxes or conduction/convection processes. Ultimately, energy is radiated in the form of longwave infrared radiation back into space.\n\nRecent satellite observations indicate additional precipitation, which is sustained by increased energy leaving the surface through evaporation (the latent heat flux), offsetting increases in longwave flux to the surface.\n\nIf the incoming energy flux is not equal to the outgoing energy flux, net heat is added to or lost by the planet (if the incoming flux is larger or smaller than the outgoing respectively).\n\nAn imbalance must show in something on Earth warming or cooling (depending on the direction of the imbalance), and the ocean being the larger thermal reservoir on Earth, is a prime candidate for measurements.\n\nEarth's energy imbalance measurements provided by Argo floats have detected an accumulation of ocean heat content (OHC). The estimated imbalance was measured during a deep solar minimum of 2005–2010 to be 0.58 ± 0.15 W/m². This level of detail cannot be inferred directly from measurements of surface energy fluxes, which have combined uncertainties of the order of ± 17 W/m².\n\nSeveral satellites indirectly measure the energy absorbed and radiated by Earth and by inference the energy imbalance. The NASA Earth Radiation Budget Experiment (ERBE) project involves three such satellites: the Earth Radiation Budget Satellite (ERBS), launched October 1984; NOAA-9, launched December 1984; and NOAA-10, launched September 1986.\n\nToday NASA's satellite instruments, provided by CERES, part of the NASA's Earth Observing System (EOS), are designed to measure both solar-reflected and Earth-emitted radiation.\n\nThe major atmospheric gases (oxygen and nitrogen) are transparent to incoming sunlight but are also transparent to outgoing thermal (infrared) radiation. However, water vapor, carbon dioxide, methane and other trace gases are opaque to many wavelengths of thermal radiation. The Earth's surface radiates the net equivalent of 17 percent of the incoming solar energy in the form of thermal infrared. However, the amount that directly escapes to space is only about 12 percent of incoming solar energy. The remaining fraction, 5 to 6 percent, is absorbed by the atmosphere by greenhouse gas molecules.\nWhen greenhouse gas molecules absorb thermal infrared energy, their temperature rises. Those gases then radiate an increased amount of thermal infrared energy in all directions. Heat radiated upward continues to encounter greenhouse gas molecules; those molecules also absorb the heat, and their temperature rises and the amount of heat they radiate increases. The atmosphere thins with altitude, and at roughly 5–6 kilometres, the concentration of greenhouse gases in the overlying atmosphere is so thin that heat can escape to space.\n\nBecause greenhouse gas molecules radiate infrared energy in all directions, some of it spreads downward and ultimately returns to the Earth's surface, where it is absorbed. The Earth's surface temperature is thus higher than it would be if it were heated only by direct solar heating. This supplemental heating is the natural greenhouse effect. It is as if the Earth is covered by a blanket that allows high frequency radiation (sunlight) to enter, but slows the rate at which the low frequency infrared radiant energy emitted by the Earth leaves.\n\nA change in the incident radiated portion of the energy budget is referred to as a radiative forcing.\n\nClimate sensitivity is the steady state change in the equilibrium temperature as a result of changes in the energy budget.\n\nClimate forcings are changes that cause temperatures to rise or fall, disrupting the energy balance. Natural climate forcings include changes in the Sun's brightness, Milankovitch cycles (small variations in the shape of Earth's orbit and its axis of rotation that occur over thousands of years) and volcanic eruptions that inject light-reflecting particles as high as the stratosphere. Man-made forcings include particle pollution (aerosols) that absorb and reflect incoming sunlight; deforestation, which changes how the surface reflects and absorbs sunlight; and the rising concentration of atmospheric carbon dioxide and other greenhouse gases, which decreases the rate at which heat is radiated to space.\n\nA forcing can trigger feedbacks that intensify (positive feedback) or weaken (negative feedback) the original forcing. For example, loss of ice at the poles, which makes them less reflective, causes greater absorption of energy and so increases the rate at which the ice melts, is an example of a positive feedback.\n\nThe observed planetary energy imbalance during the recent solar minimum shows that solar forcing of climate, although natural and significant, is overwhelmed by anthropogenic climate forcing.\n\nIn 2012, NASA scientists reported that to stop global warming atmospheric CO content would have to be reduced to 350 ppm or less, assuming all other climate forcings were fixed. The impact of anthropogenic aerosols has not been quantified, but individual aerosol types are thought to have substantial heating and cooling effects.\n\n\n"}
{"id": "31450053", "url": "https://en.wikipedia.org/wiki?curid=31450053", "title": "Energy accidents", "text": "Energy accidents\n\nEnergy resources bring with them great social and economic promise, providing financial growth for communities and energy services for local economies. However, the infrastructure which delivers energy services can break down in an energy accident, sometimes causing much damage, and energy fatalities can occur, and with many systems often deaths will happen even when the systems are working as intended.\n\nHistorically, coal mining has been the most dangerous energy activity and the list of historical coal mining disasters is a long one. Underground mining hazards include suffocation, gas poisoning, roof collapse and gas explosions. Open cut mining hazards are principally mine wall failures and vehicle collisions. In the US alone, more than 100,000 coal miners have been killed in accidents over the past century, with more than 3,200 dying in 1907 alone.\n\nAccording to Benjamin K. Sovacool, 279 \"major\" energy accidents occurred from 1907 to 2007 and they caused 182,156 deaths with $41 billion in property damages, with these figures not including deaths from smaller accidents.\n\nHowever, by far the greatest energy fatalities that result from energy generation by humanity, is the creation of air pollution. The most lethal of which, particulate matter, which is primarily generated from the burning of fossil fuels and biomass is (counting outdoor air pollution effects only) estimated to cause 2.1 million deaths annually.\n\nAccording to Benjamin K. Sovacool, while responsible for less than 1 percent of the total number of energy accidents, hydroelectric facilities claimed 94 percent of reported immediate fatalities. Results on immediate fatalities are dominated by one disaster in which Typhoon Nina in 1975 washed out the Shimantan Dam (Henan Province, China) and 171,000 people perished. While the other major accident that involved greater than 1000 immediate deaths followed the rupture of the NNPC petroleum pipeline in 1998 and the resulting explosion. The other singular accident described by Sovacool is the \"predicted\" latent death toll of greater than 1000, as a result of the 1986 steam explosion at the Chernobyl nuclear reactor in the Ukraine. With approximately 4000 deaths in total, to eventually result in the decades ahead due to the radio-isotope pollution released.\n\nIn the oil and gas industry, the need for improved safety culture and training within companies is evidenced by the finding that workers new to a company are more likely to be involved in fatalities.\n\nCoal mining accidents resulted in 5,938 immediate deaths in 2005, and 4746 immediate deaths in 2006 in China alone according to the World Wildlife Fund. Coal mining is the most dangerous occupation in China, the death rate for every 100 tons of coal mined is 100 times that of the death rate in the US and 30 times that achieved in South Africa. Moreover, 600,000 Chinese coal miners, as of 2004, were suffering from Coalworker's pneumoconiosis (known as \"black lung\") a disease of the lungs caused by long-continued inhalation of coal dust. And the figure increases by 70,000 miners every year in China.\n\nHistorically, coal mining has been a very dangerous activity and the list of historical coal mining disasters is a long one. In the US alone, more than 100,000 coal miners were killed in accidents over the past century, with more than 3,200 dying in 1907 alone. In the decades following this peak, an annual death toll of 1,500 miner fatalities occurred every year in the US until approximately the 1970s. Coal mining fatalities in the US between 1990 and 2012 have continued to decline, with fewer than 100 each year. (See more Coal mining disasters in the United States)\n\nIn the United States, in the 2000s, after three decades of regulation on the Environmental impact of the coal industry, including regulations in the 1970s and 1990s from the Clean Air Act, an act created to cut down on pollution related deaths from fossil fuel usage, US coal fired power plants were estimated, in the 2000s, to continue to cause between 10,000 and 30,000 latent, or air pollution related deaths per year, due to the emissions of sulfur dioxide, nitrogen oxides and directly emitted particulate matter that result when coal is burnt.\n\nAccording to the World Health Organization in 2012, urban outdoor air pollution, from the burning of fossil fuels and biomass is estimated to cause 3 million deaths worldwide per year and indoor air pollution from biomass and fossil fuel burning is estimated to cause approximately 4.3 million premature deaths. In 2013 a team of researchers estimated the number of premature deaths caused by particulate matter in outdoor air pollution as 2.1 million, occurring annually.\n\nBenjamin Sovacool says that while hydroelectric plants were responsible for the most fatalities, nuclear power plants rank first in terms of their economic cost, accounting for 41 percent of all property damage. Oil and hydroelectric follow at around 25 percent each, followed by natural gas at 9 percent and coal at 2 percent. Excluding Chernobyl and the Shimantan Dam, the three other most expensive accidents involved the Exxon Valdez oil spill (Alaska), The Prestige oil spill (Spain), and the Three Mile Island nuclear accident (Pennsylvania). However analysis presented in the international Journal, \"Human and Ecological Risk Assessment\" found that coal, oil, Liquid petroleum gas and hydro accidents have cost more than nuclear power accidents.\n\nModern-day U.S. regulatory agencies frequently implement regulations on conventional pollution if one life or more is predicted saved per $6 million to $8 million of economic costs incurred.\n\n\n\n"}
{"id": "46348501", "url": "https://en.wikipedia.org/wiki?curid=46348501", "title": "Evolution of the cochlea", "text": "Evolution of the cochlea\n\nCochlea is Latin for “snail, shell or screw” and originates from the Greek word κοχλίας \"kohlias\". The modern definition, the auditory portion of the inner ear, originated in the late 17th century. Within the mammalian cochlea exists the organ of Corti, which contains hair cells that are responsible for translating the vibrations it receives from surrounding fluid-filled ducts into electrical impulses that are sent to the brain to process sound. This spiral-shaped cochlea is estimated to have originated during the early Cretaceous Period, around 120 million years ago. Further, the auditory innervation of the spiral-shaped cochlea also traces back to the Cretaceous period. The evolution of the human cochlea is a major area of scientific interest because of its favourable representation in the fossil record. During the last century, many scientists such as evolutionary biologists and paleontologists strove to develop new methods and techniques to overcome the many obstacles associated with working with ancient, delicate artifacts. In the past, scientists were limited in their ability to fully examine specimens without causing damage to them. In more recent times, technologies such as micro-CT scanning became available. These technologies allow for the visual differentiation between fossilized animal materials and other sedimentary remains. With the use of X-ray technologies, it is possible to ascertain some information about the auditory capabilities of extinct creatures, giving insight to human ancestors as well as their contemporary species.\n\nWhile the basic structure of the inner ear in lepidosaurs (lizards and snakes), archosaurs (birds and crocodilians) and mammals is similar, and the organs are considered to be homologous, each group has a unique type of auditory organ. The hearing organ arose within the lagenar duct of stem reptiles, lying between the saccular and lagenar epithelia. In lepidosaurs, the hearing organ, the basilar papilla, is generally small, with at most 2000 hair cells, whereas in archosaurs the basilar papilla can be much longer (>10mm in owls) and contain many more hair cells that show two typical size extremes, the short and the tall hair cells. In mammals, the structure is known as the organ of Corti and shows a unique arrangement of hair cells and supporting cells. All mammalian organs of Corti contain a supporting tunnel made up of pillar cells, on the inner side of which there are inner hair cells and outer hair cells on the outer side. The definitive mammalian middle ear and the elongated cochlea allows for better sensitivity for higher frequencies.\n\nAs in all lepidosaurs and archosaurs, the single-ossicle (columellar) middle ear transmits sound to the footplate of the columella, which sends a pressure wave through the inner ear. In snakes, the basilar papilla is roughly 1mm long and only responds to frequencies below about 1 kHz. In contrast, lizards tend to have two areas of hair cells, one responding below and the other above 1 kHz. The upper frequency limit in most lizards is roughly 5–8 kHz. The longest lizard papillae are about 2mm long and contain 2000 hair cells and their afferent innervating fibers can be very sharply tuned to frequency.\n\nIn birds and crocodilians, the similarity of the structure of the basilar papilla betrays their close evolutionary relationship. The basilar papilla is up to about 10mm long and contains up to 16500 hair cells. While most birds have an upper hearing limit of only about 6 kHz, the barn owl can hear up to 12 kHz and thus close to the human upper limit.\n\nEgg-laying mammals, the monotremes (spiny anteater and platypus), do not have a spiral cochlea, but one shaped more like a banana, up to about 7 mm long. Like in lepidosaurs and archosaurs, it contains a lagena, a vestibular sensory epithelium, at its tip. Only in therian mammals (marsupials and placentals) is the cochlea truly coiled 1.5 to 3.5 times. Whereas in monotremes there are many rows of both inner and outer hair cells in the organ of Corti, in therian (marsupial and placental) mammals the number of inner hair-cell rows is one, and there are generally only three rows of outer hair cells.\n\nAmphibians have unique inner ear structures. There are two sensory papillae involved in hearing, the basilar (higher frequency) and amphibian (lower frequency) papillae, but it is uncertain whether either is homologous to the hearing organs of lepidosaurs, archosaurs and mammals and we have no idea when they arose.\n\nFish have no dedicated auditory epithelium, but use various vestibular sensory organs that respond to sound. In most teleost fishes it is the saccular macula that responds to sound. In some, such as goldfishes, there is also a special bony connection to the gas bladder that increases sensitivity allowing hearing up to about 4 kHz.\n\nThe size of cochlea has been measured throughout its evolution based on the fossil record. In one study, the basal turn of the cochlea was measured, and it was hypothesized that cochlear size correlates with body mass. The size of the basal turn of the cochlea was not different in Neanderthals and Holocene humans, however it became larger in early modern humans and Upper Paleolithic humans. Furthermore, the position and orientation of the cochlea is similar between Neanderthals and Holocene humans, relative to plane of the lateral canal, whereas early modern and upper Paleolithic humans have a more superiorly placed cochlea than Holocene humans. When comparing hominins of the Middle Pleistocene and Neanderthals and Holocene humans, the apex of the cochlea faces more inferiorly in the hominins than the latter two groups. Finally, the cochlea of European middle Pleistocene hominins faces more inferiorly than Neanderthals, modern humans, and Homo erectus.\nHuman beings, along with Apes, are the only mammals that do not have high frequency (>32 kHz) hearing. Humans have long cochleae, but the space devoted to each frequency range is quite large (2.5mm per octave), resulting in a comparatively reduced upper frequency limit. The human cochlea has approximately 2.5 turns around the modiolus (the axis). Humans, like many mammals and birds, are able to perceive auditory signals that displace the eardrum by a mere picometre.\n\nBecause of its prominence and preserved state in the fossil record, until recently, the ear had been used to determine phylogeny. The ear itself contains different portions, including the outer ear, the middle ear, and the inner ear and all of these show evolutionary changes that are often unique to each lineage [14]. It was the independent evolution of a tympanic middle ear in the Triassic era that produced strong selection pressures towards improved hearing organs in the separate lineages of land vertebrates.\n\nThe cochlea is the tri-chambered auditory detection portion of the ear, consisting of the scala media, the scala tympani, and the scala vestibuli. Regarding mammals, placental and marsupial cochleae have similar cochlear responses to auditory stimulation as well as DC resting potentials. This leads to the investigation of the relationship between these therian mammals and researching their ancestral species to trace the origin of the cochlea.\n\nThis spiral-shaped cochlea that is in both marsupial and placental mammals is traced back to approximately 120 million years ago. The development of the most basic basilar papilla (the auditory organ that later evolved into the Organ of Corti in mammals) happened at the same time as the water-to-land transition of vertebrates, approximately 380 million years ago. The actual coiling or spiral nature of the cochlea occurred to save space inside the skull. The longer the cochlea, the higher is the potential resolution of sound frequencies given the same hearing range. The oldest of the truly coiled mammalian cochleae were approximately 4 mm in length.\n\nThe earliest evidence available for primates depicts a short cochlea with prominent laminae, suggesting that they had good high-frequency sensitivity as opposed to low-frequency sensitivity. After this, over a period of around 60 million years, evidence suggests that primates developed longer cochleae and less prominent laminae, which means that they had an improvement in low-frequency sensitivity and a decrease in high-frequency sensitivity. By the early Miocene period, the cycle of the elongation of the cochleae and the deterioration of the laminae was completed. Evidence shows that primates have had an increasing cochlear volume to body mass ratio over time. These changes in the cochlear labyrinth volume negatively affect the highest and lowest audible frequencies, causing a downward shift. Non-primates appear to have smaller cochlear labyrinth volumes overall when compared to primates. Some evidence also suggests that selective forces for the larger cochlear labyrinth may have started after the basal primate node.\nMammals are the subject of a substantial amount of research not only because of the potential knowledge to be gained regarding humans, but also because of their rich and abundant representation in the fossil record. The spiral shape of the cochlea evolved later on in the evolutionary pathway of mammals than previously believed, just before the therians split into the two lineages marsupials and placentals, about 120 million years ago.\n\nParallel to the evolution of the cochlea, prestins show an increased rate of evolution in therian mammals. Prestins are located in the outer hair cells of mammalian cochlea and are considered motor proteins. They are found in the hair cells of all vertebrates, including fish, but are thought to have initially been membrane transporter molecules. A high concentration of prestins are found only in the lateral membranes of therian outer hair cells (there is uncertainty with regard to concentrations in monotremes). This high concentration is not found in inner hair cells, and is also lacking in all hair cell types of non-mammals. Prestin also has a role in motility, which evolved a greater importance in the motor function in land vertebrates, but this developed vastly differently in different lineages. In certain birds and mammals, prestins function as both transporters and motors, but the strongest evolution to robust motor dynamics only evolved in therian mammals. It is hypothesized that this motor system is significant to the therian cochlea at high frequencies because of the distinctive cellular and bony composition of the organ of Corti that allows the prestins to intensify movements of the whole structure.\nModern ultra-sound echolocating species such as bats and toothed whales show highly evolved prestins, and these prestins show identical sequence alterations over time. Unusually, the sequences thus apparently evolved independent from each other during different time periods. Furthermore, the evolution of neurotransmitter receptor systems (acetylcholine) that regulate the motor feedback of the outer hair cells coincides with prestin evolution in therians. This suggests that there was a parallel evolution of a control system and a motor system in the inner ear of therian mammals.\n\nLand vertebrates evolved middle ears independently in each major lineage, and are this the result of parallel evolution. The configurations of the middle ears of monotreme and therian mammals can thus be interpreted as convergent evolution or homoplasy. Thus evidence from fossils demonstrate homoplasies for the detachment of the ear from the jaw. Furthermore, it is apparent that the land-based eardrum, or tympanic membrane, and connecting structures such as the Eustachian tube evolved convergently in multiple different settings as opposed to being a defining morphology.\n"}
{"id": "47971685", "url": "https://en.wikipedia.org/wiki?curid=47971685", "title": "Extended evolutionary synthesis", "text": "Extended evolutionary synthesis\n\nThe extended evolutionary synthesis consists of a set of theoretical concepts more comprehensive than the earlier modern synthesis of evolutionary biology that took place between 1918 and 1942. The extended evolutionary synthesis was called for in the 1950s by C. H. Waddington, argued for on the basis of punctuated equilibrium by Stephen Jay Gould and Niles Eldredge in the 1980s, and was reconceptualized in 2007 by Massimo Pigliucci and Gerd B. Müller.\n\nThe extended evolutionary synthesis revisits the relative importance of different factors at play, examining several assumptions of the earlier synthesis, and augmenting it with additional causative factors. It includes multilevel selection, transgenerational epigenetic inheritance, niche construction, evolvability, and several concepts from evo-devo.\n\nNot all biologists have agreed on the need for, or the scope of, an extended synthesis. Many have collaborated on another synthesis in evolutionary developmental biology, which concetrates on developmental molecular genetics and evolution to understand how natural selection operated on developmental processes and deep homologies between organisms at the level of highly conserved genes.\n\nThe modern synthesis was the widely accepted early-20th-century synthesis reconciling Charles Darwin's theory of evolution by natural selection and Gregor Mendel's theory of genetics in a joint mathematical framework. It established evolution as biology's central paradigm. The 19th-century ideas of natural selection by Darwin and Mendelian genetics were united by researchers who included Ronald Fisher, one of the three founders of population genetics, and J. B. S. Haldane and Sewall Wright, between 1918 and 1932. Julian Huxley introduced the phrase \"modern synthesis\" in his 1942 book, \"\".\n\nDuring the 1950s, the English biologist C. H. Waddington called for an extended synthesis based on his research on epigenetics and genetic assimilation. An extended synthesis was also proposed by the Austrian zoologist Rupert Riedl, with the study of evolvability. In 1978, Michael J. D. White wrote about an extension of the modern synthesis based on new research from speciation.\n\nIn the 1980s, the American palaeontologists Stephen Jay Gould and Niles Eldredge argued for an extended synthesis based on their idea of punctuated equilibrium, the role of species selection shaping large scale evolutionary patterns and natural selection working on multiple levels extending from genes to species.\nThe ethologist John Endler wrote a paper in 1988 discussing processes of evolution that he felt had been neglected.\n\nSome researchers in the field of evolutionary developmental biology proposed another synthesis. They argue that the modern and extended syntheses should mostly center on genes and suggest an integration of embryology with molecular genetics an evolution, aiming to understand how natural selection operates on gene regulation and deep homologies between organisms at the level of highly conserved genes, transcription factors and signalling pathways. By contrast, a different strand of evo-devo following an organismal approach contributes to the extended synthesis by emphasizing (amongst others) developmental bias (both through facilitation and constraint), evolvability, and inherency of form as primary factors in the evolution of complex structures and phenotypic novelties.\n\nThe idea of an extended synthesis was relaunched in 2007 by Massimo Pigliucci, and Gerd B. Müller with a book in 2010 titled \"Evolution: The Extended Synthesis\", which has served as a launching point for work on the extended synthesis. This includes:\n\n\nOther processes such as evolvability, phenotypic plasticity, reticulate evolution, sex evolution and symbiogenesis are said by proponents to have been excluded or missed from the modern synthesis. The goal of Piglucci's and Müller's extended synthesis is to take evolution beyond the gene-centered approach of population genetics to consider more organism- and ecology-centered approaches. Many of these causes are currently considered secondary in evolutionary causation, and proponents of the extended synthesis want them to be considered first-class evolutionary causes. The biologist Eugene Koonin wrote in 2009 that \"the new developments in evolutionary biology by no account should be viewed as refutation of Darwin. On the contrary, they are widening the trails that Darwin blazed 150 years ago and reveal the extraordinary fertility of his thinking.\"\n\nThe extended synthesis is characterized by its additional set of predictions that differ from the standard modern synthesis theory:\n\n\nThe extended evolutionary synthesis is currently being tested by a group of scientists from eight institutions in Britain, Sweden and the United States. The £7.7 million project is supported by a £5.7 million grant from the John Templeton Foundation.\n\nThe project is headed by Kevin N. Laland at the University of St Andrews and Tobias Uller at Lund University. According to Laland what the extended synthesis \"really boils down to is recognition that, in addition to selection, drift, mutation and other established evolutionary processes, other factors, particularly developmental influences, shape the evolutionary process in important ways.\"\n\nBiologists disagree on the need for an extended synthesis. Opponents contend that the modern synthesis is able to fully account for the newer observations, whereas others criticize that the Extended synthesis is not radical enough. Proponents think that the conceptions of evolution at the core of the modern synthesis are too narrow. Proponents argue that even when the modern synthesis allows for the ideas in the extended synthesis, using the modern synthesis affects the way that biologists think about evolution. For example, Denis Noble says that using terms and categories of the modern synthesis distort the picture of biology that modern experimentation has discovered. Proponents therefore claim that the extended synthesis is necessary to help expand the conceptions and framework of how evolution is considered throughout the biological disciplines.\n\nDefend the extended synthesis\n\nCriticism of the extended synthesis\n\n"}
{"id": "888727", "url": "https://en.wikipedia.org/wiki?curid=888727", "title": "Flag of Earth", "text": "Flag of Earth\n\nSome individuals and organizations have promoted designs for a flag representing the planet Earth, though none have been officially recognized as such by any governmental body. The most widely recognized flags associated with Earth are the flag of the United Nations and the Earth Day flag. Listed below are some of the unofficial contenders for a Flag of Earth:\n\nA flag designed by John McConnell in 1969 for the first Earth Day is a dark blue field charged with \"The Blue Marble\", a famous NASA photo of the Earth as seen from outer space. The first edition of McConnell's flag used screen-printing and used different colors: ocean and land were blue and the clouds were white. McConnell presented his flag to the United Nations as a symbol for consideration.\n\nBecause of the political views of its creator and its having become a symbol of Earth Day, the flag is associated with environmental awareness, and the celebration of the global community. It was offered for sale originally in the \"Whole Earth Catalog\", and is the only flag which was endorsed by McConnell. \n\n\"The Blue Marble\" image was placed in the public domain, and the public nature of this image was the basis of a legal battle that resulted in the invalidation of a trademark and copyright that was originally issued to the Earth Day flag through its original promotional entity, World Equity, Inc. This does not invalidate the official history of McConnell's flag, only the official documentation that was issued on it.\n\nThe One Flag in Space initiative is an offshoot of the Space Generation Congress (SGC), the Space Generation Advisory Council's yearly world meeting. It promotes usage of the \"Blue Marble\" flag for space exploration (it does not explicitly mention it being McConnell's design).\n\nAdopted in 1946, the flag of the United Nations has been used to indicate world unity, although it technically only represents the United Nations itself. It has a geographical representation of the planet, and its high visibility usage makes it a well-known contender for representing Earth. During the planning for NASA's moon landings of the 1960s, it was suggested that a UN flag be used in place of the flag of the US.\n\nJames William van Kirk, a minister from Youngstown, Ohio, designed in 1913 a peace flag with rainbow stripes, stars and a globe. With this flag, he twice made a peace tour through Europe. The Universal Peace Congress adopted this flag as its World Peace Flag.\n\nDesigned by Pierre de Coubertin in 1914 and adopted in Olympic games since 1920, the Olympic flag represents all mankind and has a white bar with 5 interlocking rings of 5 colors. The five rings represent the five continents. The six colors (including the white color of the background) represent all of the world's nations.\n\nThe World Citizen is a social movement for global citizenship under a proposed world government. In 1953, one of its activists, Garry Davis, founded the World Service Authority, which sells the World Passport (a fantasy travel document) with a proposed flag of the world.\n\nThe astrological (and astronomical) symbol of Earth is another candidate for Earth's flag, often depicted with a blue background, with the symbol at its center.\n\nAnother Earth flag was created around the same time in 1970 by a farmer from Homer, Illinois named James W. Cadle. Cadle's version of the Earth flag consists of a blue circle representing Earth in the center of the flag, a segment of a large yellow circle representing the sun and a small white circle for the moon, all on a black background. It is particularly popular amongst SETI researchers and is used by SETI worldwide. The flag flies at the Ohio State University Radio Observatory and was lowered to half mast when Carl Sagan died. Flag of Earth Co. International was also founded by Cadle which sold the flag. The Flag of Earth became public domain in 2003.\n\nThe World Flag is an international flag created in 1988 by Paul Carroll to act as a symbol to inspire \"positive global change while continuing to embrace and celebrate cultural diversity.\" The current 2008 version of the combined World Flag has a world map 216 flags; including the flags of every UN member state, the United Nations, and several territories of larger nations.\n\nThe World Flag has been flown at the UN Headquarters for the \"A Prayer for Peace\" event, The World Trade Center, Earth Day in Central Park, and at various other events around the world.\n\nIn 2015 a Swedish artist, Oskar Pernefeldt, proposed the \"International Flag of the Planet Earth\". It was conceived to be used in space expeditions and it has two main purposes: \n\nThe creators predict that it will be eventually used in Mars landing in 2025 or in a future colony on that planet. The flag is used by space research groups with intent to implanting a base on Mars. The design of the flag consists of seven rings intersecting each other and a deep-blue-sea in the background. The rings are centered on the flag forming a flower in the middle, representing life on Earth. The intersection of the rings represent that all things on Earth are linked directly or indirectly. The rings are organized in a Borromean rings–like fashion, representing not only the seven continents, but how no part of Earth can be removed without the whole structure collapsing. Finally, the deep-blue represents the ocean and the importance of water for life on Earth.\n\nThe One World Flag is a concept published in 2018 by the German activist and visual artist Thomas Mandl. The design of the flag consists of a blue circle on a translucent blank background. Through the usage of Sheer fabrics, the environment blends into the flag background and thus becomes an integral part of the flag design representing the ever-changing nature of culture, society, politics, and environment of the planet earth. \n\n\n"}
{"id": "248189", "url": "https://en.wikipedia.org/wiki?curid=248189", "title": "Gaia hypothesis", "text": "Gaia hypothesis\n\nThe Gaia hypothesis (, , ), also known as the Gaia theory or the Gaia principle, proposes that living organisms interact with their inorganic surroundings on Earth to form a synergistic and self-regulating, complex system that helps to maintain and perpetuate the conditions for life on the planet.\n\nThe hypothesis was formulated by the chemist James Lovelock and co-developed by the microbiologist Lynn Margulis in the 1970s. Lovelock named the idea after Gaia, the primordial goddess who personified the Earth in Greek mythology. In 2006, the Geological Society of London awarded Lovelock the Wollaston Medal in part for his work on the Gaia hypothesis.\n\nTopics related to the hypothesis include how the biosphere and the evolution of organisms affect the stability of global temperature, salinity of seawater, atmospheric oxygen levels, the maintenance of a hydrosphere of liquid water and other environmental variables that affect the habitability of Earth.\n\nThe Gaia hypothesis was initially criticized for being teleological and against the principles of natural selection, but later refinements aligned the Gaia hypothesis with ideas from fields such as Earth system science, biogeochemistry and systems ecology. Lovelock also once described the \"geophysiology\" of the Earth. Even so, the Gaia hypothesis continues to attract criticism, and today some scientists consider it to be only weakly supported by, or at odds with, the available evidence.\n\nGaian hypotheses suggest that organisms co-evolve with their environment: that is, they \"influence their abiotic environment, and that environment in turn influences the biota by Darwinian process\". Lovelock (1995) gave evidence of this in his second book, showing the evolution from the world of the early thermo-acido-philic and methanogenic bacteria towards the oxygen-enriched atmosphere today that supports more complex life.\n\nA reduced version of the hypothesis has been called \"influential Gaia\" in \"Directed Evolution of the Biosphere: Biogeochemical Selection or Gaia?\" by Andrei G. Lapenis, which states the biota influence certain aspects of the abiotic world, e.g. temperature and atmosphere. This is not the work of an individual but a collective of Russian scientific research that was combined into this peer reviewed publication. It states the coevolution of life and the environment through “micro-forces” and biogeochemical processes. An example is how the activity of photosynthetic bacteria during Precambrian times have completely modified the Earth atmosphere to turn it aerobic, and as such supporting evolution of life (in particular eukaryotic life).\n\nSince barriers existed throughout the Twentieth Century between Russia and the rest of the world, it is only relatively recently that the early Russian scientists who introduced concepts overlapping the Gaia hypothesis have become better known to the Western scientific community. These scientists include:\n\nBiologists and Earth scientists usually view the factors that stabilize the characteristics of a period as an undirected emergent property or entelechy of the system; as each individual species pursues its own self-interest, for example, their combined actions may have counterbalancing effects on environmental change. Opponents of this view sometimes reference examples of events that resulted in dramatic change rather than stable equilibrium, such as the conversion of the Earth's atmosphere from a reducing environment to an oxygen-rich one at the end of the Archaean and the beginning of the Proterozoic periods.\n\nLess accepted versions of the hypothesis claim that changes in the biosphere are brought about through the coordination of living organisms and maintain those conditions through homeostasis. In some versions of Gaia philosophy, all lifeforms are considered part of one single living planetary being called \"Gaia\". In this view, the atmosphere, the seas and the terrestrial crust would be results of interventions carried out by Gaia through the coevolving diversity of living organisms.\n\nThe Gaia hypothesis was an influence on the deep ecology movement.\n\nThe Gaia hypothesis posits that the Earth is a self-regulating complex system involving the biosphere, the atmosphere, the hydrospheres and the pedosphere, tightly coupled as an evolving system. The hypothesis contends that this system as a whole, called Gaia, seeks a physical and chemical environment optimal for contemporary life.\n\nGaia evolves through a cybernetic feedback system operated unconsciously by the biota, leading to broad stabilization of the conditions of habitability in a full homeostasis. Many processes in the Earth's surface essential for the conditions of life depend on the interaction of living forms, especially microorganisms, with inorganic elements. These processes establish a global control system that regulates Earth's surface temperature, atmosphere composition and ocean salinity, powered by the global thermodynamic disequilibrium state of the Earth system.\n\nThe existence of a planetary homeostasis influenced by living forms had been observed previously in the field of biogeochemistry, and it is being investigated also in other fields like Earth system science. The originality of the Gaia hypothesis relies on the assessment that such homeostatic balance is actively pursued with the goal of keeping the optimal conditions for life, even when terrestrial or external events menace them.\n\nSince life started on Earth, the energy provided by the Sun has increased by 25% to 30%; however, the surface temperature of the planet has remained within the levels of habitability, reaching quite regular low and high margins. Lovelock has also hypothesised that methanogens produced elevated levels of methane in the early atmosphere, giving a view similar to that found in petrochemical smog, similar in some respects to the atmosphere on Titan. This, he suggests tended to screen out ultraviolet until the formation of the ozone screen, maintaining a degree of homeostasis. However, the Snowball Earth research has suggested that \"oxygen shocks\" and reduced methane levels led, during the Huronian, Sturtian and Marinoan/Varanger Ice Ages, to a world that very nearly became a solid \"snowball\". These epochs are evidence against the ability of the pre Phanerozoic biosphere to fully self-regulate.\n\nProcessing of the greenhouse gas CO, explained below, plays a critical role in the maintenance of the Earth temperature within the limits of habitability.\n\nThe CLAW hypothesis, inspired by the Gaia hypothesis, proposes a feedback loop that operates between ocean ecosystems and the Earth's climate. The hypothesis specifically proposes that particular phytoplankton that produce dimethyl sulfide are responsive to variations in climate forcing, and that these responses lead to a negative feedback loop that acts to stabilise the temperature of the Earth's atmosphere.\n\nCurrently the increase in human population and the environmental impact of their activities, such as the multiplication of greenhouse gases may cause negative feedbacks in the environment to become positive feedback. Lovelock has stated that this could bring an extremely accelerated global warming, but he has since stated the effects will likely occur more slowly.\n\nJames Lovelock and Andrew Watson developed the mathematical model Daisyworld, in which temperature regulation arises from a simple ecosystem consisting of two species whose activity varies in response to the planet's environment. The model demonstrates that beneficial feedback mechanisms can emerge in this \"toy world\" containing only self-interested organisms rather than through classic group selection mechanisms.\n\nDaisyworld examines the energy budget of a planet populated by two different types of plants, black daisies and white daisies. The colour of the daisies influences the albedo of the planet such that black daisies absorb light and warm the planet, while white daisies reflect light and cool the planet. As the model runs the output of the \"sun\" increases, meaning that the surface temperature of an uninhabited \"gray\" planet will steadily rise. In contrast, on Daisyworld competition between the daisies (based on temperature-effects on growth rates) leads to a shifting balance of daisy populations that tends to favour a planetary temperature close to the optimum for daisy growth.\n\nIt has been suggested that the results were predictable because Lovelock and Watson selected examples that produced the responses they desired.\n\nOcean salinity has been constant at about 3.5% for a very long time. Salinity stability in oceanic environments is important as most cells require a rather constant salinity and do not generally tolerate values above 5%. The constant ocean salinity was a long-standing mystery, because no process counterbalancing the salt influx from rivers was known. Recently it was suggested that salinity may also be strongly influenced by seawater circulation through hot basaltic rocks, and emerging as hot water vents on mid-ocean ridges. However, the composition of seawater is far from equilibrium, and it is difficult to explain this fact without the influence of organic processes. One suggested explanation lies in the formation of salt plains throughout Earth's history. It is hypothesized that these are created by bacterial colonies that fix ions and heavy metals during their life processes.\n\nIn the biogeochemical processes of the earth, sources and sinks are the movement of elements. The composition of salt ions within our oceans and seas are: sodium (Na), chlorine (Cl), sulfate (SO), Magnesium (Mg), calcium (Ca) and potassium (K). The elements that comprise salinity do not readily change and are a conservative property of seawater. There are many mechanisms that change salinity from a particulate form to a dissolved form and back. The known sources of sodium i.e. salts is when weathering, erosion, and dissolution of rocks transport into rivers and deposit into the oceans.\n\nThe Mediterranean Sea as being Gaia's kidney is found (here) by Kenneth J. Hsue a correspondence author in 2001. The \"desiccation\" of the Mediterranean is the evidence of a functioning kidney. Earlier \"kidney functions\" were performed during the \"deposition of the Cretaceous (South Atlantic), Jurassic (Gulf of Mexico), Permo-Triassic (Europe), Devonian (Canada), Cambrian/Precambrian (Gondwana) saline giants.\"\n\nThe Gaia hypothesis states that the Earth's atmospheric composition is kept at a dynamically steady state by the presence of life. The atmospheric composition provides the conditions that contemporary life has adapted to. All the atmospheric gases other than noble gases present in the atmosphere are either made by organisms or processed by them.\n\nThe stability of the atmosphere in Earth is not a consequence of chemical equilibrium. Oxygen is a reactive compound, and should eventually combine with gases and minerals of the Earth's atmosphere and crust. Oxygen only began to persist in the atmosphere in small quantities about 50 million years before the start of the Great Oxygenation Event. Since the start of the Cambrian period, atmospheric oxygen concentrations have fluctuated between 15% and 35% of atmospheric volume. Traces of methane (at an amount of 100,000 tonnes produced per year) should not exist, as methane is combustible in an oxygen atmosphere.\n\nDry air in the atmosphere of Earth contains roughly (by volume) 78.09% nitrogen, 20.95% oxygen, 0.93% argon, 0.039% carbon dioxide, and small amounts of other gases including methane. Lovelock originally speculated that concentrations of oxygen above about 25% would increase the frequency of wildfires and conflagration of forests. Recent work on the findings of fire-caused charcoal in Carboniferous and Cretaceous coal measures, in geologic periods when O did exceed 25%, has supported Lovelock's contention. \n\nGaia scientists see the participation of living organisms in the carbon cycle as one of the complex processes that maintain conditions suitable for life. The only significant natural source of atmospheric carbon dioxide (CO) is volcanic activity, while the only significant removal is through the precipitation of carbonate rocks. Carbon precipitation, solution and fixation are influenced by the bacteria and plant roots in soils, where they improve gaseous circulation, or in coral reefs, where calcium carbonate is deposited as a solid on the sea floor. Calcium carbonate is used by living organisms to manufacture carbonaceous tests and shells. Once dead, the living organisms' shells fall to the bottom of the oceans where they generate deposits of chalk and limestone.\n\nOne of these organisms is \"Emiliania huxleyi\", an abundant coccolithophore algae which also has a role in the formation of clouds. CO excess is compensated by an increase of coccolithophoride life, increasing the amount of CO locked in the ocean floor. Coccolithophorides increase the cloud cover, hence control the surface temperature, help cool the whole planet and favor precipitations necessary for terrestrial plants. Lately the atmospheric CO concentration has increased and there is some evidence that concentrations of ocean algal blooms are also increasing.\n\nLichen and other organisms accelerate the weathering of rocks in the surface, while the decomposition of rocks also happens faster in the soil, thanks to the activity of roots, fungi, bacteria and subterranean animals. The flow of carbon dioxide from the atmosphere to the soil is therefore regulated with the help of living beings. When CO levels rise in the atmosphere the temperature increases and plants grow. This growth brings higher consumption of CO by the plants, who process it into the soil, removing it from the atmosphere.\n\nThe idea of the Earth as an integrated whole, a living being, has a long tradition. The mythical Gaia was the primal Greek goddess personifying the Earth, the Greek version of \"Mother Nature\" (from Ge = Earth, and Aia = \nPIE grandmother), or the Earth Mother. James Lovelock gave this name to his hypothesis after a suggestion from the novelist William Golding, who was living in the same village as Lovelock at the time (Bowerchalke, Wiltshire, UK). Golding's advice was based on Gea, an alternative spelling for the name of the Greek goddess, which is used as prefix in geology, geophysics and geochemistry. Golding later made reference to Gaia in his Nobel prize acceptance speech.\n\nIn the eighteenth century, as geology consolidated as a modern science, James Hutton maintained that geological and biological processes are interlinked. Later, the naturalist and explorer Alexander von Humboldt recognized the coevolution of living organisms, climate, and Earth's crust. In the twentieth century, Vladimir Vernadsky formulated a theory of Earth's development that is now one of the foundations of ecology. Vernadsky was a Ukrainian geochemist and was one of the first scientists to recognize that the oxygen, nitrogen, and carbon dioxide in the Earth's atmosphere result from biological processes. During the 1920s he published works arguing that living organisms could reshape the planet as surely as any physical force. Vernadsky was a pioneer of the scientific bases for the environmental sciences. His visionary pronouncements were not widely accepted in the West, and some decades later the Gaia hypothesis received the same type of initial resistance from the scientific community.\n\nAlso in the turn to the 20th century Aldo Leopold, pioneer in the development of modern environmental ethics and in the movement for wilderness conservation, suggested a living Earth in his biocentric or holistic ethics regarding land.\n\nAnother influence for the Gaia hypothesis and the environmental movement in general came as a side effect of the Space Race between the Soviet Union and the United States of America. During the 1960s, the first humans in space could see how the Earth looked as a whole. The photograph \"Earthrise\" taken by astronaut William Anders in 1968 during the Apollo 8 mission became, through the Overview Effect an early symbol for the global ecology movement.\n\nLovelock started defining the idea of a self-regulating Earth controlled by the community of living organisms in September 1965, while working at the Jet Propulsion Laboratory in California on methods of detecting life on Mars. The first paper to mention it was \"Planetary Atmospheres: Compositional and other Changes Associated with the Presence of Life\", co-authored with C.E. Giffin. A main concept was that life could be detected in a planetary scale by the chemical composition of the atmosphere. According to the data gathered by the Pic du Midi observatory, planets like Mars or Venus had atmospheres in chemical equilibrium. This difference with the Earth atmosphere was considered to be a proof that there was no life in these planets.\n\nLovelock formulated the \"Gaia Hypothesis\" in journal articles in 1972 and 1974, followed by a popularizing 1979 book \"Gaia: A new look at life on Earth\". An article in the \"New Scientist\" of February 6, 1975, and a popular book length version of the hypothesis, published in 1979 as \"The Quest for Gaia\", began to attract scientific and critical attention.\n\nLovelock called it first the Earth feedback hypothesis, and it was a way to explain the fact that combinations of chemicals including oxygen and methane persist in stable concentrations in the atmosphere of the Earth. Lovelock suggested detecting such combinations in other planets' atmospheres as a relatively reliable and cheap way to detect life.\n\nLater, other relationships such as sea creatures producing sulfur and iodine in approximately the same quantities as required by land creatures emerged and helped bolster the hypothesis.\n\nIn 1971 microbiologist Dr. Lynn Margulis joined Lovelock in the effort of fleshing out the initial hypothesis into scientifically proven concepts, contributing her knowledge about how microbes affect the atmosphere and the different layers in the surface of the planet. The American biologist had also awakened criticism from the scientific community with her theory on the origin of eukaryotic organelles and her contributions to the endosymbiotic theory, nowadays accepted. Margulis dedicated the last of eight chapters in her book, \"The Symbiotic Planet\", to Gaia. However, she objected to the widespread personification of Gaia and stressed that Gaia is \"not an organism\", but \"an emergent property of interaction among organisms\". She defined Gaia as \"the series of interacting ecosystems that compose a single huge ecosystem at the Earth's surface. Period\". The book's most memorable \"slogan\" was actually quipped by a student of Margulis': \"Gaia is just symbiosis as seen from space\".\n\nJames Lovelock called his first proposal the \"Gaia hypothesis\" but has also used the term \"Gaia theory\". Lovelock states that the initial formulation was based on observation, but still lacked a scientific explanation. The Gaia hypothesis has since been supported by a number of scientific experiments and provided a number of useful predictions. In fact, wider research proved the original hypothesis wrong, in the sense that it is not life alone but the whole Earth system that does the regulating.\n\nIn 1985, the first public symposium on the Gaia hypothesis, \"Is The Earth A Living Organism?\" was held at University of Massachusetts Amherst, August 1–6. The principal sponsor was the National Audubon Society. Speakers included James Lovelock, George Wald, Mary Catherine Bateson, Lewis Thomas, John Todd, Donald Michael, Christopher Bird, Thomas Berry, David Abram, Michael Cohen, and William Fields. Some 500 people attended.\n\nIn 1988, climatologist Stephen Schneider organised a conference of the American Geophysical Union. The first Chapman Conference on Gaia, was held in San Diego, California on March 7, 1988.\n\nDuring the \"philosophical foundations\" session of the conference, David Abram spoke on the influence of metaphor in science, and of the Gaia hypothesis as offering a new and potentially game-changing metaphorics, while James Kirchner criticised the Gaia hypothesis for its imprecision. Kirchner claimed that Lovelock and Margulis had not presented one Gaia hypothesis, but four -\n\n\nOf Homeostatic Gaia, Kirchner recognised two alternatives. \"Weak Gaia\" asserted that life tends to make the environment stable for the flourishing of all life. \"Strong Gaia\" according to Kirchner, asserted that life tends to make the environment stable, \"to enable\" the flourishing of all life. Strong Gaia, Kirchner claimed, was untestable and therefore not scientific.\n\nLovelock and other Gaia-supporting scientists, however, did attempt to disprove the claim that the hypothesis is not scientific because it is impossible to test it by controlled experiment. For example, against the charge that Gaia was teleological, Lovelock and Andrew Watson offered the Daisyworld Model (and its modifications, above) as evidence against most of these criticisms. Lovelock said that the Daisyworld model \"demonstrates that self-regulation of the global environment can emerge from competition amongst types of life altering their local environment in different ways\".\n\nLovelock was careful to present a version of the Gaia hypothesis that had no claim that Gaia intentionally or consciously maintained the complex balance in her environment that life needed to survive. It would appear that the claim that Gaia acts \"intentionally\" was a metaphoric statement in his popular initial book and was not meant to be taken literally. This new statement of the Gaia hypothesis was more acceptable to the scientific community. Most accusations of teleologism ceased, following this conference.\n\nBy the time of the 2nd Chapman Conference on the Gaia Hypothesis, held at Valencia, Spain, on 23 June 2000, the situation had changed significantly. Rather than a discussion of the Gaian teleological views, or \"types\" of Gaia hypotheses, the focus was upon the specific mechanisms by which basic short term homeostasis was maintained within a framework of significant evolutionary long term structural change.\n\nThe major questions were:\n\n\nIn 1997, Tyler Volk argued that a Gaian system is almost inevitably produced as a result of an evolution towards far-from-equilibrium homeostatic states that maximise entropy production, and Kleidon (2004) agreed stating: \"...homeostatic behavior can emerge from a state of MEP associated with the planetary albedo\"; \"...the resulting behavior of a biotic Earth at a state of MEP may well lead to near-homeostatic behavior of the Earth system on long time scales, as stated by the Gaia hypothesis\". Staley (2002) has similarly proposed \"...an alternative form of Gaia theory based on more traditional Darwinian principles... In [this] new approach, environmental regulation is a consequence of population dynamics, not Darwinian selection. The role of selection is to favor organisms that are best adapted to prevailing environmental conditions. However, the environment is not a static backdrop for evolution, but is heavily influenced by the presence of living organisms. The resulting co-evolving dynamical process eventually leads to the convergence of equilibrium and optimal conditions\".\n\nA fourth international conference on the Gaia hypothesis, sponsored by the Northern Virginia Regional Park Authority and others, was held in October 2006 at the Arlington, VA campus of George Mason University.\n\nMartin Ogle, Chief Naturalist, for NVRPA, and long-time Gaia hypothesis proponent, organized the event. Lynn Margulis, Distinguished University Professor in the Department of Geosciences, University of Massachusetts-Amherst, and long-time advocate of the Gaia hypothesis, was a keynote speaker. Among many other speakers: Tyler Volk, Co-director of the Program in Earth and Environmental Science at New York University; Dr. Donald Aitken, Principal of Donald Aitken Associates; Dr. Thomas Lovejoy, President of the Heinz Center for Science, Economics and the Environment; Robert Correll, Senior Fellow, Atmospheric Policy Program, American Meteorological Society and noted environmental ethicist, J. Baird Callicott.\n\nThis conference approached the Gaia hypothesis as both science and metaphor as a means of understanding how we might begin addressing 21st century issues such as climate change and ongoing environmental destruction.\n\nAfter initially being largely ignored by most scientists (from 1969 until 1977), thereafter for a period the initial Gaia hypothesis was criticized by a number of scientists, such as Ford Doolittle, Richard Dawkins and Stephen Jay Gould. Lovelock has said that because his hypothesis is named after a Greek goddess, and championed by many non-scientists, the Gaia hypothesis was interpreted as a neo-Pagan religion. Many scientists in particular also criticised the approach taken in his popular book \"Gaia, a New Look at Life on Earth\" for being teleological—a belief that things are purposeful and aimed towards a goal. Responding to this critique in 1990, Lovelock stated, \"Nowhere in our writings do we express the idea that planetary self-regulation is purposeful, or involves foresight or planning by the biota\".\n\nStephen Jay Gould criticised Gaia as being \"a metaphor, not a mechanism.\" He wanted to know the actual mechanisms by which self-regulating homeostasis was achieved. In his defense of Gaia, David Abram argues that Gould overlooked the fact that \"mechanism\", itself, is a metaphor — albeit an exceedingly common and often unrecognized metaphor — one which leads us to consider natural and living systems as though they were machines organized and built from outside (rather than as autopoietic or self-organizing phenomena). Mechanical metaphors, according to Abram, lead us to overlook the active or agential quality of living entities, while the organismic metaphorics of the Gaia hypothesis accentuate the active agency of both the biota and the biosphere as a whole. With regard to causality in Gaia, Lovelock argues that no single mechanism is responsible, that the connections between the various known mechanisms may never be known, that this is accepted in other fields of biology and ecology as a matter of course, and that specific hostility is reserved for his own hypothesis for other reasons.\n\nAside from clarifying his language and understanding of what is meant by a life form, Lovelock himself ascribes most of the criticism to a lack of understanding of non-linear mathematics by his critics, and a linearizing form of greedy reductionism in which all events have to be immediately ascribed to specific causes before the fact. He also states that most of his critics are biologists but that his hypothesis includes experiments in fields outside biology, and that some self-regulating phenomena may not be mathematically explainable.\n\nLovelock has suggested that global biological feedback mechanisms could evolve by natural selection, stating that organisms that improve their environment for their survival do better than those that damage their environment. However, in the early 1980s, W. Ford Doolittle and Richard Dawkins separately argued against Gaia. Doolittle argued that nothing in the genome of individual organisms could provide the feedback mechanisms proposed by Lovelock, and therefore the Gaia hypothesis proposed no plausible mechanism and was unscientific. Dawkins meanwhile stated that for organisms to act in concert would require foresight and planning, which is contrary to the current scientific understanding of evolution. Like Doolittle, he also rejected the possibility that feedback loops could stabilize the system.\n\nLynn Margulis, a microbiologist who collaborated with Lovelock in supporting the Gaia hypothesis, argued in 1999, that \"Darwin's grand vision was not wrong, only incomplete. In accentuating the direct competition between individuals for resources as the primary selection mechanism, Darwin (and especially his followers) created the impression that the environment was simply a static arena\". She wrote that the composition of the Earth's atmosphere, hydrosphere, and lithosphere are regulated around \"set points\" as in homeostasis, but those set points change with time.\n\nEvolutionary biologist W. D. Hamilton called the concept of Gaia Copernican, adding that it would take another Newton to explain how Gaian self-regulation takes place through Darwinian natural selection.\n\nThe Gaia hypothesis continues to be broadly skeptically received by the scientific community. For instance, arguments both for and against it were laid out in the journal \"Climatic Change\" in 2002 and 2003. A significant argument raised against it are the many examples where life has had a detrimental or destabilising effect on the environment rather than acting to regulate it. Several recent books have criticised the Gaia hypothesis, expressing views ranging from \"... the Gaia hypothesis lacks unambiguous observational support and has significant theoretical difficulties\" to \"Suspended uncomfortably between tainted metaphor, fact, and false science, I prefer to leave Gaia firmly in the background\" to \"The Gaia hypothesis is supported neither by evolutionary theory nor by the empirical evidence of the geological record\". The CLAW hypothesis, initially suggested as a potential example of direct Gaian feedback, has subsequently been found to be less credible as understanding of cloud condensation nuclei has improved. In 2009 the Medea hypothesis was proposed: that life has highly detrimental (biocidal) impacts on planetary conditions, in direct opposition to the Gaia hypothesis.\n\nIn a recent book-length evaluation of the Gaia hypothesis considering modern evidence from across the various relevant disciplines the author, Toby Tyrrell, concluded that: \"I believe Gaia is a dead end. Its study has, however, generated many new and thought provoking questions. While rejecting Gaia, we can at the same time appreciate Lovelock's originality and breadth of vision, and recognise that his audacious concept has helped to stimulate many new ideas about the Earth, and to champion a holistic approach to studying it\". Elsewhere he presents his conclusion \"The Gaia hypothesis is not an accurate picture of how our world works\". This statement needs to be understood as referring to the \"strong\" and \"moderate\" forms of Gaia—that the biota obeys a principle that works to make Earth optimal (strength 5) or favourable for life (strength 4) or that it works as a homeostatic mechanism (strength 3). The latter is the \"weakest\" form of Gaia that Lovelock has advocated. Tyrrell rejects it. However, he finds that the two weaker forms of Gaia—Coeveolutionary Gaia and Influential Gaia, which assert that there are close links between the evolution of life and the environment and that biology affects the physical and chemical environment—are both credible, but that it is not useful to use the term \"Gaia\" in this sense.\n\n\n"}
{"id": "11603215", "url": "https://en.wikipedia.org/wiki?curid=11603215", "title": "Geological history of Earth", "text": "Geological history of Earth\n\nThe geological history of Earth follows the major events in Earth's past based on the geological time scale, a system of chronological measurement based on the study of the planet's rock layers (stratigraphy). Earth formed about 4.54 billion years ago by accretion from the solar nebula, a disk-shaped mass of dust and gas left over from the formation of the Sun, which also created the rest of the Solar System.\n\nEarth was initially molten due to extreme volcanism and frequent collisions with other bodies. Eventually, the outer layer of the planet cooled to form a solid crust when water began accumulating in the atmosphere. The Moon formed soon afterwards, possibly as a result of the impact of a planetoid with the Earth. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, augmented by ice delivered from comets, produced the oceans.\n\nAs the surface continually reshaped itself over hundreds of millions of years, continents formed and broke apart. They migrated across the surface, occasionally combining to form a supercontinent. Roughly , the earliest-known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia, , then finally Pangaea, which broke apart .\n\nThe present pattern of ice ages began about , then intensified at the end of the Pliocene. The polar regions have since undergone repeated cycles of glaciation and thaw, repeating every 40,000–100,000 years. The last glacial period of the current ice age ended about 10,000 years ago.\n\nThe Precambrian includes approximately 90% of geologic time. It extends from 4.6 billion years ago to the beginning of the Cambrian Period (about 541 Ma). It includes three eons, the Hadean, Archean, and Proterozoic.\n\nMajor volcanic events altering the Earth's environment and causing extinctions may have occurred 10 times in the past 3 billion years.\n\nDuring Hadean time (4.6–4 Ga), the Solar System was forming, probably within a large cloud of gas and dust around the sun, called an accretion disc from which Earth formed .\nThe Hadean Eon is not formally recognized, but it essentially marks the era before we have adequate record of significant solid rocks. The oldest dated zircons date from about .\n\nEarth was initially molten due to extreme volcanism and frequent collisions with other bodies. Eventually, the outer layer of the planet cooled to form a solid crust when water began accumulating in the atmosphere. The Moon formed soon afterwards, possibly as a result of the impact of a large planetoid with the Earth. Some of this object's mass merged with the Earth, significantly altering its internal composition, and a portion was ejected into space. Some of the material survived to form an orbiting moon. More recent potassium isotopic studies suggest that the Moon was formed by a smaller, high-energy, high-angular-momentum giant impact cleaving off a significant portion of the Earth. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, augmented by ice delivered from comets, produced the oceans.\n\nDuring the Hadean the Late Heavy Bombardment occurred (approximately ) during which a large number of impact craters are believed to have formed on the Moon, and by inference on Earth, Mercury, Venus and Mars as well.\n\nThe Earth of the early Archean () may have had a different tectonic style. During this time, the Earth's crust cooled enough that rocks and continental plates began to form. Some scientists think because the Earth was hotter, that plate tectonic activity was more vigorous than it is today, resulting in a much greater rate of recycling of crustal material. This may have prevented cratonisation and continent formation until the mantle cooled and convection slowed down. Others argue that the subcontinental lithospheric mantle is too buoyant to subduct and that the lack of Archean rocks is a function of erosion and subsequent tectonic events.\n\nIn contrast to the Proterozoic, Archean rocks are often heavily metamorphized deep-water sediments, such as graywackes, mudstones, volcanic sediments and banded iron formations. Greenstone belts are typical Archean formations, consisting of alternating high- and low-grade metamorphic rocks. The high-grade rocks were derived from volcanic island arcs, while the low-grade metamorphic rocks represent deep-sea sediments eroded from the neighboring island rocks and deposited in a forearc basin. In short, greenstone belts represent sutured protocontinents.\n\nThe Earth's magnetic field was established 3.5 billion years ago. The solar wind flux was about 100 times the value of the modern Sun, so the presence of the magnetic field helped prevent the planet's atmosphere from being stripped away, which is what probably happened to the atmosphere of Mars. However, the field strength was lower than at present and the magnetosphere was about half the modern radius.\n\nThe geologic record of the Proterozoic () is more complete than that for the preceding Archean. In contrast to the deep-water deposits of the Archean, the Proterozoic features many strata that were laid down in extensive shallow epicontinental seas; furthermore, many of these rocks are less metamorphosed than Archean-age ones, and plenty are unaltered. Study of these rocks show that the eon featured massive, rapid continental accretion (unique to the Proterozoic), supercontinent cycles, and wholly modern orogenic activity. Roughly , the earliest-known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia, 600–540 Ma.\n\nThe first-known glaciations occurred during the Proterozoic, one began shortly after the beginning of the eon, while there were at least four during the Neoproterozoic, climaxing with the Snowball Earth of the Varangian glaciation.\n\nThe Phanerozoic Eon is the current eon in the geologic timescale. It covers roughly 541 million years. During this period continents drifted about, eventually collected into a single landmass known as Pangea and then split up into the current continental landmasses.\n\nThe Phanerozoic is divided into three eras – the Paleozoic, the Mesozoic and the Cenozoic.\n\nMost of biological evolution occurred during this time period.\n\nThe Paleozoic spanned from roughly (Ma) and is subdivided into six geologic periods; from oldest to youngest they are the Cambrian, Ordovician, Silurian, Devonian, Carboniferous and Permian. Geologically, the Paleozoic starts shortly after the breakup of a supercontinent called Pannotia and at the end of a global ice age. Throughout the early Paleozoic, the Earth's landmass was broken up into a substantial number of relatively small continents. Toward the end of the era the continents gathered together into a supercontinent called Pangaea, which included most of the Earth's land area.\n\nThe Cambrian is a major division of the geologic timescale that begins about 541.0 ± 1.0 Ma. Cambrian continents are thought to have resulted from the breakup of a Neoproterozoic supercontinent called Pannotia. The waters of the Cambrian period appear to have been widespread and shallow. Continental drift rates may have been anomalously high. Laurentia, Baltica and Siberia remained independent continents following the break-up of the supercontinent of Pannotia. Gondwana started to drift toward the South Pole. Panthalassa covered most of the southern hemisphere, and minor oceans included the Proto-Tethys Ocean, Iapetus Ocean and Khanty Ocean.\n\nThe Ordovician period started at a major extinction event called the Cambrian–Ordovician extinction event some time about 485.4 ± 1.9 Ma. During the Ordovician the southern continents were collected into a single continent called Gondwana. Gondwana started the period in the equatorial latitudes and, as the period progressed, drifted toward the South Pole. Early in the Ordovician the continents Laurentia, Siberia and Baltica were still independent continents (since the break-up of the supercontinent Pannotia earlier), but Baltica began to move toward Laurentia later in the period, causing the Iapetus Ocean to shrink between them. Also, Avalonia broke free from Gondwana and began to head north toward Laurentia. The Rheic Ocean was formed as a result of this. By the end of the period, Gondwana had neared or approached the pole and was largely glaciated.\n\nThe Ordovician came to a close in a series of extinction events that, taken together, comprise the second-largest of the five major extinction events in Earth's history in terms of percentage of genera that became extinct. The only larger one was the Permian-Triassic extinction event. The extinctions occurred approximately and mark the boundary between the Ordovician and the following Silurian Period.\n\nThe most-commonly accepted theory is that these events were triggered by the onset of an ice age, in the Hirnantian faunal stage that ended the long, stable greenhouse conditions typical of the Ordovician. The ice age was probably not as long-lasting as once thought; study of oxygen isotopes in fossil brachiopods shows that it was probably no longer than 0.5 to 1.5 million years. The event was preceded by a fall in atmospheric carbon dioxide (from 7000ppm to 4400ppm) which selectively affected the shallow seas where most organisms lived. As the southern supercontinent Gondwana drifted over the South Pole, ice caps formed on it. Evidence of these ice caps have been detected in Upper Ordovician rock strata of North Africa and then-adjacent northeastern South America, which were south-polar locations at the time.\n\nThe Silurian is a major division of the geologic timescale that started about 443.8 ± 1.5 Ma. During the Silurian, Gondwana continued a slow southward drift to high southern latitudes, but there is evidence that the Silurian ice caps were less extensive than those of the late Ordovician glaciation. The melting of ice caps and glaciers contributed to a rise in sea levels, recognizable from the fact that Silurian sediments overlie eroded Ordovician sediments, forming an unconformity. Other cratons and continent fragments drifted together near the equator, starting the formation of a second supercontinent known as Euramerica. The vast ocean of Panthalassa covered most of the northern hemisphere. Other minor oceans include Proto-Tethys, Paleo-Tethys, Rheic Ocean, a seaway of Iapetus Ocean (now in between Avalonia and Laurentia), and newly formed Ural Ocean.\n\nThe Devonian spanned roughly from 419 to 359 Ma. The period was a time of great tectonic activity, as Laurasia and Gondwana drew closer together. The continent Euramerica (or Laurussia) was created in the early Devonian by the collision of Laurentia and Baltica, which rotated into the natural dry zone along the Tropic of Capricorn. In these near-deserts, the Old Red Sandstone sedimentary beds formed, made red by the oxidized iron (hematite) characteristic of drought conditions. Near the equator Pangaea began to consolidate from the plates containing North America and Europe, further raising the northern Appalachian Mountains and forming the Caledonian Mountains in Great Britain and Scandinavia. The southern continents remained tied together in the supercontinent of Gondwana. The remainder of modern Eurasia lay in the Northern Hemisphere. Sea levels were high worldwide, and much of the land lay submerged under shallow seas. The deep, enormous Panthalassa (the \"universal ocean\") covered the rest of the planet. Other minor oceans were Paleo-Tethys, Proto-Tethys, Rheic Ocean and Ural Ocean (which was closed during the collision with Siberia and Baltica).\n\nThe Carboniferous extends from about 358.9 ± 0.4 to about 298.9 ± 0.15 Ma.\n\nA global drop in sea level at the end of the Devonian reversed early in the Carboniferous; this created the widespread epicontinental seas and carbonate deposition of the Mississippian. There was also a drop in south polar temperatures; southern Gondwana was glaciated throughout the period, though it is uncertain if the ice sheets were a holdover from the Devonian or not. These conditions apparently had little effect in the deep tropics, where lush coal swamps flourished within 30 degrees of the northernmost glaciers. A mid-Carboniferous drop in sea-level precipitated a major marine extinction, one that hit crinoids and ammonites especially hard. This sea-level drop and the associated unconformity in North America separate the Mississippian Period from the Pennsylvanian period.\n\nThe Carboniferous was a time of active mountain building, as the supercontinent Pangea came together. The southern continents remained tied together in the supercontinent Gondwana, which collided with North America-Europe (Laurussia) along the present line of eastern North America. This continental collision resulted in the Hercynian orogeny in Europe, and the Alleghenian orogeny in North America; it also extended the newly uplifted Appalachians southwestward as the Ouachita Mountains. In the same time frame, much of present eastern Eurasian plate welded itself to Europe along the line of the Ural mountains. There were two major oceans in the Carboniferous the Panthalassa and Paleo-Tethys. Other minor oceans were shrinking and eventually closed the Rheic Ocean (closed by the assembly of South and North America), the small, shallow Ural Ocean (which was closed by the collision of Baltica, and Siberia continents, creating the Ural Mountains) and Proto-Tethys Ocean.\n\nThe Permian extends from about 298.9 ± 0.15 to 252.17 ± 0.06 Ma.\n\nDuring the Permian all the Earth's major land masses, except portions of East Asia, were collected into a single supercontinent known as Pangaea. Pangaea straddled the equator and extended toward the poles, with a corresponding effect on ocean currents in the single great ocean (\"Panthalassa\", the \"universal sea\"), and the Paleo-Tethys Ocean, a large ocean that was between Asia and Gondwana. The Cimmeria continent rifted away from Gondwana and drifted north to Laurasia, causing the Paleo-Tethys to shrink. A new ocean was growing on its southern end, the Tethys Ocean, an ocean that would dominate much of the Mesozoic Era. Large continental landmasses create climates with extreme variations of heat and cold (\"continental climate\") and monsoon conditions with highly seasonal rainfall patterns. Deserts seem to have been widespread on Pangaea.\n\nThe Mesozoic extended roughly from .\n\nAfter the vigorous convergent plate mountain-building of the late Paleozoic, Mesozoic tectonic deformation was comparatively mild. Nevertheless, the era featured the dramatic rifting of the supercontinent Pangaea. Pangaea gradually split into a northern continent, Laurasia, and a southern continent, Gondwana. This created the passive continental margin that characterizes most of the Atlantic coastline (such as along the U.S. East Coast) today.\n\nThe Triassic Period extends from about 252.17 ± 0.06 to 201.3 ± 0.2 Ma. During the Triassic, almost all the Earth's land mass was concentrated into a single supercontinent centered more or less on the equator, called Pangaea (\"all the land\"). This took the form of a giant \"Pac-Man\" with an east-facing \"mouth\" constituting the Tethys sea, a vast gulf that opened farther westward in the mid-Triassic, at the expense of the shrinking Paleo-Tethys Ocean, an ocean that existed during the Paleozoic.\n\nThe remainder was the world-ocean known as Panthalassa (\"all the sea\"). All the deep-ocean sediments laid down during the Triassic have disappeared through subduction of oceanic plates; thus, very little is known of the Triassic open ocean. The supercontinent Pangaea was rifting during the Triassic—especially late in the period—but had not yet separated. The first nonmarine sediments in the rift that marks the initial break-up of Pangea—which separated New Jersey from Morocco—are of Late Triassic age; in the U.S., these thick sediments comprise the Newark Supergroup.\nBecause of the limited shoreline of one super-continental mass, Triassic marine deposits are globally relatively rare; despite their prominence in Western Europe, where the Triassic was first studied. In North America, for example, marine deposits are limited to a few exposures in the west. Thus Triassic stratigraphy is mostly based on organisms living in lagoons and hypersaline environments, such as \"Estheria\" crustaceans and terrestrial vertebrates.\n\nThe Jurassic Period extends from about 201.3 ± 0.2 to 145.0 Ma.\nDuring the early Jurassic, the supercontinent Pangaea broke up into the northern supercontinent Laurasia and the southern supercontinent Gondwana; the Gulf of Mexico opened in the new rift between North America and what is now Mexico's Yucatan Peninsula. The Jurassic North Atlantic Ocean was relatively narrow, while the South Atlantic did not open until the following Cretaceous Period, when Gondwana itself rifted apart.\nThe Tethys Sea closed, and the Neotethys basin appeared. Climates were warm, with no evidence of glaciation. As in the Triassic, there was apparently no land near either pole, and no extensive ice caps existed. The Jurassic geological record is good in western Europe, where extensive marine sequences indicate a time when much of the continent was submerged under shallow tropical seas; famous locales include the Jurassic Coast World Heritage Site and the renowned late Jurassic \"lagerstätten\" of Holzmaden and Solnhofen.\nIn contrast, the North American Jurassic record is the poorest of the Mesozoic, with few outcrops at the surface. Though the epicontinental Sundance Sea left marine deposits in parts of the northern plains of the United States and Canada during the late Jurassic, most exposed sediments from this period are continental, such as the alluvial deposits of the Morrison Formation. The first of several massive batholiths were emplaced in the northern Cordillera beginning in the mid-Jurassic, marking the Nevadan orogeny. Important Jurassic exposures are also found in Russia, India, South America, Japan, Australasia and the United Kingdom.\n\nThe Cretaceous Period extends from circa to .\n\nDuring the Cretaceous, the late Paleozoic-early Mesozoic supercontinent of Pangaea completed its breakup into present day continents, although their positions were substantially different at the time. As the Atlantic Ocean widened, the convergent-margin orogenies that had begun during the Jurassic continued in the North American Cordillera, as the Nevadan orogeny was followed by the Sevier and Laramide orogenies. Though Gondwana was still intact in the beginning of the Cretaceous, Gondwana itself broke up as South America, Antarctica and Australia rifted away from Africa (though India and Madagascar remained attached to each other); thus, the South Atlantic and Indian Oceans were newly formed. Such active rifting lifted great undersea mountain chains along the welts, raising eustatic sea levels worldwide.\n\nTo the north of Africa the Tethys Sea continued to narrow. Broad shallow seas advanced across central North America (the Western Interior Seaway) and Europe, then receded late in the period, leaving thick marine deposits sandwiched between coal beds. At the peak of the Cretaceous transgression, one-third of Earth's present land area was submerged. The Cretaceous is justly famous for its chalk; indeed, more chalk formed in the Cretaceous than in any other period in the Phanerozoic. Mid-ocean ridge activity—or rather, the circulation of seawater through the enlarged ridges—enriched the oceans in calcium; this made the oceans more saturated, as well as increased the bioavailability of the element for calcareous nanoplankton. These widespread carbonates and other sedimentary deposits make the Cretaceous rock record especially fine. Famous formations from North America include the rich marine fossils of Kansas's Smoky Hill Chalk Member and the terrestrial fauna of the late Cretaceous Hell Creek Formation. Other important Cretaceous exposures occur in Europe and China. In the area that is now India, massive lava beds called the Deccan Traps were laid down in the very late Cretaceous and early Paleocene.\n\nThe Cenozoic Era covers the  million years since the Cretaceous–Paleogene extinction event up to and including the present day. By the end of the Mesozoic era, the continents had rifted into nearly their present form. Laurasia became North America and Eurasia, while Gondwana split into South America, Africa, Australia, Antarctica and the Indian subcontinent, which collided with the Asian plate. This impact gave rise to the Himalayas. The Tethys Sea, which had separated the northern continents from Africa and India, began to close up, forming the Mediterranean sea.\n\nThe Paleogene (alternatively Palaeogene) Period is a unit of geologic time that began and ended 23.03 Ma and comprises the first part of the Cenozoic Era. This period consists of the Paleocene, Eocene and Oligocene Epochs.\n\nThe Paleocene, lasted from to .\n\nIn many ways, the Paleocene continued processes that had begun during the late Cretaceous Period. During the Paleocene, the continents continued to drift toward their present positions. Supercontinent Laurasia had not yet separated into three continents. Europe and Greenland were still connected. North America and Asia were still intermittently joined by a land bridge, while Greenland and North America were beginning to separate. The Laramide orogeny of the late Cretaceous continued to uplift the Rocky Mountains in the American west, which ended in the succeeding epoch. South and North America remained separated by equatorial seas (they joined during the Neogene); the components of the former southern supercontinent Gondwana continued to split apart, with Africa, South America, Antarctica and Australia pulling away from each other. Africa was heading north toward Europe, slowly closing the Tethys Ocean, and India began its migration to Asia that would lead to a tectonic collision and the formation of the Himalayas.\n\nDuring the Eocene ( - ), the continents continued to drift toward their present positions. At the beginning of the period, Australia and Antarctica remained connected, and warm equatorial currents mixed with colder Antarctic waters, distributing the heat around the world and keeping global temperatures high. But when Australia split from the southern continent around 45 Ma, the warm equatorial currents were deflected away from Antarctica, and an isolated cold water channel developed between the two continents. The Antarctic region cooled down, and the ocean surrounding Antarctica began to freeze, sending cold water and ice floes north, reinforcing the cooling. The present pattern of ice ages began about .\n\nThe northern supercontinent of Laurasia began to break up, as Europe, Greenland and North America drifted apart. In western North America, mountain building started in the Eocene, and huge lakes formed in the high flat basins among uplifts. In Europe, the Tethys Sea finally vanished, while the uplift of the Alps isolated its final remnant, the Mediterranean, and created another shallow sea with island archipelagos to the north. Though the North Atlantic was opening, a land connection appears to have remained between North America and Europe since the faunas of the two regions are very similar. India continued its journey away from Africa and began its collision with Asia, creating the Himalayan orogeny.\n\nThe Oligocene Epoch extends from about to . During the Oligocene the continents continued to drift toward their present positions.\n\nAntarctica continued to become more isolated and finally developed a permanent ice cap. Mountain building in western North America continued, and the Alps started to rise in Europe as the African plate continued to push north into the Eurasian plate, isolating the remnants of Tethys Sea. A brief marine incursion marks the early Oligocene in Europe. There appears to have been a land bridge in the early Oligocene between North America and Europe since the faunas of the two regions are very similar. During the Oligocene, South America was finally detached from Antarctica and drifted north toward North America. It also allowed the Antarctic Circumpolar Current to flow, rapidly cooling the continent.\n\nThe Neogene Period is a unit of geologic time starting 23.03 Ma. and ends at 2.588 Ma. The Neogene Period follows the Paleogene Period. The Neogene consists of the Miocene and Pliocene and is followed by the Quaternary Period.\n\nThe Miocene extends from about 23.03 to 5.333 Ma.\n\nDuring the Miocene continents continued to drift toward their present positions. Of the modern geologic features, only the land bridge between South America and North America was absent, the subduction zone along the Pacific Ocean margin of South America caused the rise of the Andes and the southward extension of the Meso-American peninsula. India continued to collide with Asia. The Tethys Seaway continued to shrink and then disappeared as Africa collided with Eurasia in the Turkish-Arabian region between 19 and 12 Ma (ICS 2004). Subsequent uplift of mountains in the western Mediterranean region and a global fall in sea levels combined to cause a temporary drying up of the Mediterranean Sea resulting in the Messinian salinity crisis near the end of the Miocene.\n\nThe Pliocene extends from to . During the Pliocene continents continued to drift toward their present positions, moving from positions possibly as far as from their present locations to positions only 70 km from their current locations.\n\nSouth America became linked to North America through the Isthmus of Panama during the Pliocene, bringing a nearly complete end to South America's distinctive marsupial faunas. The formation of the Isthmus had major consequences on global temperatures, since warm equatorial ocean currents were cut off and an Atlantic cooling cycle began, with cold Arctic and Antarctic waters dropping temperatures in the now-isolated Atlantic Ocean. Africa's collision with Europe formed the Mediterranean Sea, cutting off the remnants of the Tethys Ocean. Sea level changes exposed the land-bridge between Alaska and Asia. Near the end of the Pliocene, about (the start of the Quaternary Period), the current ice age began. The polar regions have since undergone repeated cycles of glaciation and thaw, repeating every 40,000–100,000 years.\n\nThe Pleistocene extends from to 11,700 years before present. The modern continents were essentially at their present positions during the Pleistocene, the plates upon which they sit probably having moved no more than relative to each other since the beginning of the period.\n\nThe Holocene Epoch began approximately 11,700 calendar years before present and continues to the present. During the Holocene, continental motions have been less than a kilometer.\n\nThe last glacial period of the current ice age ended about 10,000 years ago. Ice melt caused world sea levels to rise about in the early part of the Holocene. In addition, many areas above about 40 degrees north latitude had been depressed by the weight of the Pleistocene glaciers and rose as much as over the late Pleistocene and Holocene, and are still rising today. The sea level rise and temporary land depression allowed temporary marine incursions into areas that are now far from the sea. Holocene marine fossils are known from Vermont, Quebec, Ontario and Michigan. Other than higher latitude temporary marine incursions associated with glacial depression, Holocene fossils are found primarily in lakebed, floodplain and cave deposits. Holocene marine deposits along low-latitude coastlines are rare because the rise in sea levels during the period exceeds any likely upthrusting of non-glacial origin. Post-glacial rebound in Scandinavia resulted in the emergence of coastal areas around the Baltic Sea, including much of Finland. The region continues to rise, still causing weak earthquakes across Northern Europe. The equivalent event in North America was the rebound of Hudson Bay, as it shrank from its larger, immediate post-glacial Tyrrell Sea phase, to near its present boundaries.\n\n\n"}
{"id": "13844012", "url": "https://en.wikipedia.org/wiki?curid=13844012", "title": "Greenhouse and icehouse Earth", "text": "Greenhouse and icehouse Earth\n\nThroughout the Phanerozoic history of the Earth, the planet's climate has been fluctuating between two dominant climate states: the greenhouse Earth and the icehouse Earth. \nThese two climate states last for millions of years and should not be confused with glacial and interglacial periods, which occur only during an icehouse period and tend to last less than 1 million years. There are five known great glaciations in Earth's climate history; the main factors involved in changes of the paleoclimate are believed to be the concentration of atmospheric carbon dioxide, changes in the Earth's orbit, and oceanic and orogenic changes due to tectonic plate dynamics. Greenhouse and icehouse periods have profoundly shaped the evolution of life on Earth.\n\nA \"greenhouse Earth\" or \"hothouse Earth\" is a period in which there are no continental glaciers whatsoever on the planet, the levels of carbon dioxide and other greenhouse gases (such as water vapor and methane) are high, and sea surface temperatures (SSTs) range from 28 °C (82.4 °F) in the tropics to 0 °C (32 °F) in the polar regions. \n\nThis state should not be confused with a hypothetical \"hothouse earth\", which is an irreversible tipping point corresponding to the ongoing runaway greenhouse effect on Venus. The IPCC states that \"a 'runaway greenhouse effect'—analogous to [that of] Venus—appears to have virtually no chance of being induced by anthropogenic activities.\"\n\nThere are several theories as to how a greenhouse Earth can come about. The geological record shows CO and other greenhouse gases are abundant during this time. Tectonic movements were extremely active during the more well-known greenhouse ages (such as 368 million years ago in the Paleozoic Era). Because of continental rifting (continental plates moving away from each other) volcanic activity becomes more prominent, producing more CO and heating up the Earth's atmosphere. Earth is more commonly placed in a greenhouse state throughout the epochs, and the Earth has been in this state for approximately 80% of the past 500 million years, which makes understanding the direct causes somewhat difficult.\n\nAn \"icehouse Earth\" is the earth as it experiences an ice age. Unlike a greenhouse Earth, an icehouse Earth has ice sheets present, and these sheets wax and wane throughout times known as glacial periods and interglacial periods. During an icehouse Earth, greenhouse gases tend to be less abundant, and temperatures tend to be cooler globally. The Earth is currently in an icehouse stage, as ice sheets are present on both poles and glacial periods have occurred at regular intervals over the past million years.\n\nThe causes of an icehouse state are much debated, because not much is really known about the transition periods between greenhouse to icehouse climates and what could make the climate so different. One important aspect is clearly the decline of CO in the atmosphere, possibly due to low volcanic activity.\n\nOther important issues are the movement of the tectonic plates and the opening and closing of oceanic gateways. These seem to play a crucial part in icehouse Earths because they can bring forth cool waters from very deep water circulations that could assist in creating ice sheets or thermal isolation of areas. Examples of this occurring are the opening of the Tasmanian gateway 36.5 million years ago that separated Australia and Antarctica and which is believed to have set off the Cenozoic icehouse, and the creation of the Drake Passage 32.8 million years ago by the separation of South America and Antarctica, though it was believed by other scientists that this did not come into effect until around 23 million years ago. The closing of the Isthmus of Panama and the Indonesian seaway approximately 3 or 4 million years ago may have been a major cause for our current icehouse state. For the icehouse climate, tectonic activity also creates mountains, which are produced by one continental plate colliding with another one and continuing forward. The revealed fresh soils act as scrubbers of carbon dioxide, which can significantly affect the amount of this greenhouse gas in the atmosphere. An example of this is the collision between the Indian subcontinent and the Asian continent, which created the Himalayan Mountains about 50 million years ago.\n\nWithin icehouse states, there are \"glacial\" and \"interglacial\" periods that cause ice sheets to build up or retreat. The causes for these glacial and interglacial periods are mainly variations in the movement of the earth around the Sun. The astronomical components, discovered by the Serbian geophysicist Milutin Milanković and now known as Milankovitch cycles, include the axial tilt of the Earth, the orbital eccentricity (or shape of the orbit) and the precession (or wobble) of the Earth's spin. The tilt of the axis tends to fluctuate between 21.5° to 24.5° and back every 41,000 years on the vertical axis. This change actually affects the seasonality upon the earth, since more or less solar radiation hits certain areas of the planet more often on a higher tilt, while less of a tilt would create a more even set of seasons worldwide. These changes can be seen in ice cores, which also contains information that shows that during glacial times (at the maximum extension of the ice sheets), the atmosphere had lower levels of carbon dioxide. This may be caused by the increase or redistribution of the acid/base balance with bicarbonate and carbonate ions that deals with alkalinity. During an Icehouse, only 20% of the time is spent in interglacial, or warmer times.\n\nA \"snowball earth\" is the complete opposite of greenhouse Earth, in which the earth's surface is completely frozen over; however, a snowball earth technically does not have continental ice sheets like during the icehouse state. \"The Great Infra-Cambrian Ice Age\" has been claimed to be the host of such a world, and in 1964, the scientist W. Brian Harland brought forth his discovery of indications of glaciers in low latitudes (Harland and Rudwick). This became a problem for Harland because of the thought of the \"Runaway Snowball Paradox\" (a kind of Snowball effect) that, once the earth enters the route of becoming a snowball earth, it would never be able to leave that state. However, in 1992 brought up a solution to the paradox. It is believed that since the continents at this time were huddled at the low and mid-latitudes that there was a great cooling event by planetary albedo, or reflection of the earth’s surface. Kirschvink explained that the way to get out of the snowball could be connected to carbon dioxide, since volcanic activity would not halt, and that the buildup and lack of \"scrubbing\" of this carbon dioxide in the atmosphere, that the earth would return to a greenhouse state. Some scientists believe that the end of the snowball Earth caused an event known as the Cambrian Explosion, which produced the beginnings of multi-cellular life. However some biologists claim that a complete snowball Earth could not have happened since photosynthetic life would not have survived underneath many meters of ice without sunlight. However, it has been observed that, even under meters of thick ice around Antarctica, sunlight shows through. Most scientists today believe that a \"hard\" Snowball Earth, one completely covered by ice, is probably impossible. However, a \"slushball earth\", with points of openings near the equator, is possible.\n\nRecent studies may have again complicated the idea of a snowball earth. In October 2011, a team of French researchers announced that the carbon dioxide during the last speculated \"snowball earth\" may have been lower than originally stated, which provides a challenge in finding out how Earth was able to get out of its state and if it were a snowball or slushball.\n\nThe Eocene, which occurred between 53 and 49 million years ago, was the Earth's warmest temperature period for 100 million years. However, this \"super-greenhouse\" eventually became an icehouse by the late Eocene. It was believed that the decline of CO caused this change, though there are possible positive feedbacks, or added influence that contributes to the cooling.\n\nThe best record we have for a transition from an icehouse to a greenhouse period where plant life exists is during the Permian epoch that occurred around 300 million years ago. In 40 million years a major transition took place, causing the Earth to change from a moist, icy planet where rainforests covered the tropics, into a hot, dry, and windy location where little could survive. Professor Isabel P. Montañez of University of California, Davis, who has researched this time period, found the climate to be \"highly unstable\" and \"marked by dips and rises in carbon dioxide\".\n\nThe Eocene-Oligocene transition, the latest transition occurring approximately 34 million years ago, resulted in rapid global temperature decrease, the glaciation of Antarctica and a series of biotic extinction events. The most dramatic species turnover event associated with this time period is the Grande Coupure, a period which saw the replacement of European tree-dwelling and leaf-eating mammal species by migratory species from Asia.\n\nThe science of paleoclimatology attempts to understand the history of greenhouse and icehouse conditions over geological time. Through the study of ice cores, dendrochronology, ocean and lake sediments (varve), palynology (fossilized pollen) and isotope analysis (such as Radiometric dating and stable isotope analysis), scientists can create models of past climate. One study has shown that atmospheric carbon dioxide levels during the Permian age rocked back and forth between 250 parts per million (which is close to present-day levels) up to 2,000 parts per million. Studies on lake sediments suggest that the \"Hothouse\" or \"super-Greenhouse\" Eocene was in a \"permanent El Nino state\" after the 10 °C warming of the deep ocean and high latitude surface temperatures shut down the Pacific Ocean's El Nino-Southern Oscillation. A theory was suggested for the Paleocene–Eocene Thermal Maximum on the sudden decrease of carbon isotopic composition of global inorganic carbon pool by 2.5 parts per million. A hypothesis noted for this negative drop of isotopes could be the increase of methane hydrates, the trigger for which remains a mystery. This increase of methane in the atmosphere, which happens to be a potent, but short-lived greenhouse gas, increased the global temperatures by 6 °C with the assistance of the less potent carbon dioxide.\n\n\nCurrently, the Earth is in an icehouse climate state. About 34 million years ago, ice sheets began to form in Antarctica; the ice sheets in the Arctic did not start forming until 2 million years ago. Some processes that may have led to our current icehouse may be connected to the development of the Himalayan Mountains and the opening of the Drake Passage between South America and Antarctica. Scientists have been attempting to compare the past transitions between icehouse and greenhouse, and vice versa to understand where our planet is now heading.\n\nWithout the human influence on the greenhouse gas concentration, the Earth would be heading toward a glacial period. Predicted changes in orbital forcing suggest that in absence of human-made global warming the next glacial period would begin at least 50,000 years from now (see Milankovitch cycles).\n\nBut due to the ongoing anthropogenic greenhouse gas emissions, the Earth is instead heading toward a greenhouse Earth period. Permanent ice is actually a rare phenomenon in the history of the Earth, occurring only in coincidence with the icehouse effect, which has affected about 20% of Earth's history.\n\n"}
{"id": "48790532", "url": "https://en.wikipedia.org/wiki?curid=48790532", "title": "Hexamolybdenum", "text": "Hexamolybdenum\n\nHexamolybdenum, is a molybdenum dominant alloy discovered during a nanomineralogy investigation of the Allende meteorite. Hexamolybdenum was discovered in a small ultrarefractory inclusion within the Allende meteorite. This inclusion has been named ACM-1. Hexamolybdenum is hexagonal, with a calculated density of 11.90 g/cm. The new mineral was found along with allendeite. These minerals, are believed to demonstrate conditions during the early stages of the Solar System, as is the case with many CV3 carbonaceous chondrites such as the Allende meteorite. Hexamolybdenum lies on a continuum of high-temperature alloys that are found in meteorites and allows a link between osmium, ruthenium, and iron rich meteoritic alloys. The name hexamolybdenum refers to the crystal symmetry (primitive hexagonal) and the molybdenum rich composition. The Allende meteorite fell in 1969 near Pueblito de Allende, Chihuahua, Mexico.\n\nHexamolybdenum was found as nano-crystals in an ultrarefractory inclusion in the Allende meteorite. The Allende meteorite has shown to be full of new minerals, after nearly forty years it has produced one in ten of the now known minerals in meteorites. This CV3 carbonaceous chondrite was the largest ever recovered on earth and is referred to as the best-studied meteorite in history.\nThe inclusion has only been viewed via electron microscopy. The hexamolybdenum specimen was lost during an attempted ion probe analysis of a bordering grain. Other specimens can be found, however, in the Smithsonian Institution's National Museum of Natural History Allende section USNM 3509HC12 and in section USNM 7590 of NWA 1934, another VC3 chondrite.\n\nIt has also been reported from the NWA 1934 CV3 carbonaceous chondrite meteorite from the Erfoud region of Morocco and in the Danubian placer of Straubing, Bavaria.\n\nHexamolybdenum is an (molybdenum, ruthenium, iron, iridium, osmium) alloy.\n\nColor, streak, luster, hardness, tenacity, cleavage, fracture, density, and refractive index could not be observed because the grain size was too small and the section bearing the mineral was optically thick.\n\n"}
{"id": "14329", "url": "https://en.wikipedia.org/wiki?curid=14329", "title": "Historicism", "text": "Historicism\n\nHistoricism is the idea of attributing meaningful significance to space and time, such as historical period, geographical place, and local culture. Historicism tends to be hermeneutical because it values cautious, rigorous, and contextualized interpretation of information; or relativist, because it rejects notions of universal, fundamental and immutable interpretations. The approach varies from individualist theories of knowledge such as empiricism and rationalism, which neglect the role of traditions.\n\nThe term \"historicism\" (\"Historismus\") was coined by German philosopher Karl Wilhelm Friedrich Schlegel. Over time it has developed different and somewhat divergent meanings. Elements of historicism appear in the writings of French essayist Michel de Montaigne (1533–1592) and Italian philosopher G. B. Vico (1668–1744), and became more fully developed with the dialectic of Georg Hegel (1770–1831), influential in 19th-century Europe. The writings of Karl Marx, influenced by Hegel, also include historicism. The term is also associated with the empirical social sciences and with the work of Franz Boas.\n\nHistoricism may be contrasted with reductionist theories—which assumes that all developments can be explained by fundamental principles (such as in economic determinism)—or with theories that posit that historical changes occur as a result of random chance.\n\nThe Austrian-English philosopher Karl Popper condemned historicism along with the determinism and holism which he argued formed its basis. In his \"Poverty of Historicism\", he identified historicism with the opinion that there are \"inexorable laws of historical destiny\", which opinion he warned against. This contrasts with the contextually relative interpretation of historicism for which its proponents argue. Talcott Parsons criticized historicism as a case of idealistic fallacy in \"The Structure of Social Action\" (1937).\n\nPost-structuralism uses the term \"New Historicism\", which has some associations with both anthropology and Hegelianism.\n\nThe theological use of the word denotes the interpretation of biblical prophecy as being related to church history.\n\nHegel viewed the realization of human freedom as the ultimate purpose of history, which could only be achieved through the creation of the perfect state. And this progressive history would only occur through a dialectical process: namely, the tension between the purpose of humankind (freedom), the position that humankind currently finds itself, and mankind's attempt to bend the current world into accord with its nature. However, because humans are often not aware of the goal of both humanity and history, the process of achieving freedom is necessarily one of self-discovery. Hegel also saw the progress toward freedom being conducted by the \"spirit\" (Geist), a seemingly supernatural force that directed all human actions and interactions. Yet Hegel makes clear that the spirit is a mere abstraction, and only comes into existence \"through the activity of finite agents.\" Thus, Hegel's philosophy of history is not necessarily metaphysical, despite the fact that many of Hegel's opponents and interpreters have understood Hegel's philosophy of history as a metaphysical and determinist view of history. For example, Karl Popper in his book \"The Poverty of Historicism\" interpreted Hegel's philosophy of history as metaphysical and deterministic. Popper referred to this \"Hegelian\" philosophy of history as \"Historicism\".\n\nHegel's historicism also suggests that any human society and all human activities such as science, art, or philosophy, are defined by their history. Consequently, their essence can be sought only by understanding said history. The history of any such human endeavor, moreover, not only continues but also reacts against what has gone before; this is the source of Hegel's famous dialectic teaching usually summarized by the slogan \"thesis, antithesis, and synthesis\". (Hegel did not use these terms, although Johann Fichte did.) Hegel's famous aphorism, \"Philosophy is the history of philosophy,\" describes it bluntly.\n\nHegel's position is perhaps best illuminated when contrasted against the atomistic and reductionist opinion of human societies and social activities self-defining on an \"ad hoc\" basis through the sum of dozens of interactions. Yet another contrasting model is the persistent metaphor of a social contract. Hegel considers the relationship between individuals and societies as organic, not atomic: even their social discourse is mediated by language, and language is based on etymology and unique character. It thus preserves the culture of the past in thousands of half-forgotten metaphors. To understand why a person is the way he is, you must examine that person in his society: and to understand that society, you must understand its history, and the forces that influenced it. The \"Zeitgeist\", the \"Spirit of the Age,\" is the concrete embodiment of the most important factors that are acting in human history at any given time. This contrasts with teleological theories of activity, which suppose that the end is the determining factor of activity, as well as those who believe in a tabula rasa, or blank slate, opinion, such that individuals are defined by their interactions.\n\nThese ideas can be interpreted variously. The Right Hegelians, working from Hegel's opinions about the organicism and historically determined nature of human societies, interpreted Hegel's historicism as a justification of the unique destiny of national groups and the importance of stability and institutions. Hegel's conception of human societies as entities greater than the individuals who constitute them influenced nineteenth-century romantic nationalism and its twentieth-century excesses. The Young Hegelians, by contrast, interpreted Hegel's thoughts on societies influenced by social conflict for a doctrine of social progress, and attempted to manipulate these forces to cause various results. Karl Marx's doctrine of \"historical inevitabilities\" and historical materialism is one of the more influential reactions to this part of Hegel's thought. Significantly, Karl Marx's theory of alienation argues that capitalism disrupts traditional relationships between workers and their work.\n\nHegelian historicism is related to his ideas on the means by which human societies progress, specifically the dialectic and his conception of logic as representing the inner essential nature of reality. Hegel attributes the change to the \"modern\" need to interact with the world, whereas ancient philosophers were self-contained, and medieval philosophers were monks. In his History of Philosophy Hegel writes:\nIn modern times things are very different; now we no longer see philosophic individuals who constitute a class by themselves. With the present day all difference has disappeared; philosophers are not monks, for we find them generally in connection with the world, participating with others in some common work or calling. They live, not independently, but in the relation of citizens, or they occupy public offices and take part in the life of the state. Certainly they may be private persons, but if so, their position as such does not in any way isolate them from their other relationship. They are involved in present conditions, in the world and its work and progress. Thus their philosophy is only by the way, a sort of luxury and superfluity. This difference is really to be found in the manner in which outward conditions have taken shape after the building up of the inward world of religion. In modern times, namely, on account of the reconciliation of the worldly principle with itself, the external world is at rest, is brought into order — worldly relationships, conditions, modes of life, have become constituted and organized in a manner which is conformable to nature and rational. We see a universal, comprehensible connection, and with that individuality likewise attains another character and nature, for it is no longer the plastic individuality of the ancients. This connection is of such power that every individuality is under its dominion, and yet at the same time can construct for itself an inward world.\nThis opinion that entanglement in society creates an indissoluble bond with expression, would become an influential question in philosophy, namely, the requirements for individuality. It would be considered by Nietzsche, John Dewey and Michel Foucault directly, as well as in the work of numerous artists and authors. There have been various responses to Hegel's challenge. The Romantic period emphasized the ability of individual genius to transcend time and place, and use the materials from their heritage to fashion works which were beyond determination. The modern would advance versions of John Locke's infinite malleability of the human animal. Post-structuralism would argue that since history is not present, but only the image of history, that while an individual era or power structure might emphasize a particular history, that the contradictions within the story would hinder the very purposes that the history was constructed to advance.\n\nIn the context of anthropology and other sciences which study the past, historicism has a different meaning. Anthropological historicism is associated with the work of Franz Boas. His theory used the diffusionist concept that there were a few \"cradles of civilization\" which grew outwards, and merged it with the idea that societies would adapt to their circumstances, which is called historical particularism. The school of historicism grew in response to unilinear theories that social development represented adaptive fitness, and therefore existed on a continuum. While these theories were espoused by Charles Darwin and many of his students, their application as applied in social Darwinism and general evolution characterized in the theories of Herbert Spencer and Leslie White, historicism was neither anti-selection, nor anti-evolution, as Darwin never attempted nor offered an explanation for cultural evolution. However, it attacked the notion that there was one normative spectrum of development, instead emphasizing how local conditions would create adaptations to the local environment. Julian Steward refuted the viability of globally and universally applicable adaptive standards proposing that culture was honed adaptively in response to the idiosyncrasies of the local environment, the cultural ecology, by specific evolution. What was adaptive for one region might not be so for another. This conclusion has likewise been adopted by modern forms of biological evolutionary theory.\n\nThe primary method of historicism was empirical, namely that there were so many requisite inputs into a society or event, that only by emphasizing the data available could a theory of the source be determined. In this opinion, grand theories are unprovable, and instead intensive field work would determine the most likely explanation and history of a culture, and hence it is named \"historicism.\"\n\nThis opinion would produce a wide range of definition of what, exactly, constituted culture and history, but in each case the only means of explaining it was in terms of the historical particulars of the culture itself.\n\nSince the 1950s, when Jacques Lacan and Foucault argued that each epoch has its own knowledge system, within which individuals are inexorably entangled, many post-structuralists have used \"historicism\" to describe the opinion that all questions must be settled within the cultural and social context in which they are raised. Answers cannot be found by appeal to an external truth, but only within the confines of the norms and forms that phrase the question. This version of historicism holds that there are only the raw texts, markings and artifacts that exist in the present, and the conventions used to decode them. This school of thought is sometimes given the name of \"New Historicism\".\n\nThe same term, \"new historicism\" is also used for a school of literary scholarship which interprets a poem, drama, etc. as an expression of or reaction to the power-structures of its society. Stephen Greenblatt is an example of this school.\n\nWithin the context of 20th-century philosophy, debates continue as to whether ahistorical and immanent methods were sufficient to understand meaning—that is to say, \"what you see is what you get\" positivism—or whether context, background and culture are important beyond the mere need to decode words, phrases and references. While post-structural historicism is relativist in its orientation, that is, it sees each culture as its own frame of reference, a large number of thinkers have embraced the need for historical context, not because culture is self-referential, but because there is no more compressed means of conveying all of the relevant information except through history. This opinion is often seen as deriving from the work of Benedetto Croce. Recent historians using this tradition include Thomas Kuhn.\n\nIn Christianity, the term \"historicism\" refers to the confessional Protestant form of prophetical interpretation which holds that the fulfillment of biblical prophecy has occurred throughout history and continues to occur; as opposed to other methods which limit the time-frame of prophecy-fulfillment to the past or to the future.\n\nThere is also a particular opinion in ecclesiastical history and in the history of dogmas which has been described as historicist by Pope Pius XII in the encyclical \"Humani generis\". \"They add that the history of dogmas consists in the reporting of the various forms in which revealed truth has been clothed, forms that have succeeded one another in accordance with the different teachings and opinions that have arisen over the course of the centuries.\"\n\nThe social theory of Karl Marx, with respect to modern scholarship, has an ambiguous relation to historicism. Critics of Marx have charged his theory with historicism since its very genesis. However, the issue of historicism also finds itself important to many debates within Marxism itself; the charge of historicism has been made against various types of Marxism, typically disparaged by Marxists as \"vulgar\" Marxism.\n\nMarx himself expresses critical concerns with this historicist tendency in his Theses on Feuerbach:\n\nKarl Popper used the term \"historicism\" in his influential books \"The Poverty of Historicism\" and \"The Open Society and Its Enemies\", to mean: \"an approach to the social sciences which assumes that \"historical prediction\" is their primary aim, and which assumes that this aim is attainable by discovering the 'rhythms' or the 'patterns', the 'laws' or the 'trends' that underlie the evolution of history\". Karl Popper wrote with reference to Hegel's theory of history, which he criticized extensively. However, there is wide dispute whether Popper's description of \"historicism\" is an accurate description of Hegel, or more his characterisation of his own philosophical antagonists, including Marxist-Leninist thought, then widely held as posing a challenge to the philosophical basis of the West, as well as theories such as Spengler's which drew predictions about the future course of events from the past.\n\nIn \"The Open Society and Its Enemies\", Popper attacks \"historicism\" and its proponents, among whom (as well as Hegel) he identifies and singles out Plato and Marx—calling them all \"enemies of the open society\". The objection he makes is that historicist positions, by claiming that there is an inevitable and deterministic pattern to history, abrogate the democratic responsibility of each one of us to make our own free contributions to the evolution of society, and hence lead to totalitarianism.\n\nAnother of his targets is what he terms \"moral historicism\", the attempt to infer moral values from the course of history; in Hegel's words, that \"history is the world's court of justice\". This may take the form of conservatism (former might is right), positivism (might is right) or futurism (presumed coming might is right). As against these, Popper says that he does not believe \"that success proves anything or that history is our judge\". Futurism must be distinguished from prophecies that the right will prevail: these attempt to infer history from ethics, rather than ethics from history, and are therefore historicism in the normal sense rather than moral historicism.\n\nHe also attacks what he calls \"Historism\", which he regards as distinct from historicism. By historism, he means the tendency to regard every argument or idea as completely accounted for by its historical context, as opposed to assessing it by its merits. In Popperian terms, the \"New Historicism\" is an example of historism rather than of historicism proper.\n\nLeo Strauss used the term \"historicism\" and reportedly termed it the single greatest threat to intellectual freedom insofar as it denies any attempt to address injustice-pure-and-simple (such is the significance of historicism's rejection of \"natural right\" or \"right by nature\"). Strauss argued that historicism \"rejects political philosophy\" (insofar as this stands or falls by questions of permanent, trans-historical significance) and is based on the belief that \"all human thought, including scientific thought, rests on premises which cannot be validated by human reason and which came from historical epoch to historical epoch.\" Strauss further identified R. G. Collingwood as the most coherent advocate of historicism in the English language. Countering Collingwood's arguments, Strauss warned against historicist social scientists' failure to address real-life problems—most notably that of tyranny—to the extent that they relativize (or \"subjectivize\") all ethical problems by placing their significance strictly in function of particular or ever-changing socio-material conditions devoid of inherent or \"objective\" \"value.\" Similarly, Strauss criticized Eric Voegelin's abandonment of ancient political thought as guide or vehicle in interpreting modern political problems.\n\nIn his books, \"Natural Right and History\" and \"On Tyranny\", Strauss offers a complete critique of historicism as it emerges in the works of Hegel, Marx, and Heidegger. Many believe that Strauss also found historicism in Edmund Burke, Tocqueville, Augustine, and John Stuart Mill. Although it is largely disputed whether Strauss himself was a historicist, he often indicated that historicism grew out of and against Christianity and was a threat to civic participation, belief in human agency, religious pluralism, and, most controversially, an accurate understanding of the classical philosophers and religious prophets themselves. Throughout his work, he warns that historicism, and the understanding of progress that results from it, expose us to tyranny, totalitarianism, and democratic extremism. In his exchange with Alexandre Kojève in \"On Tyranny\", Strauss seems to blame historicism for Nazism and Communism. In a collection of his works by Kenneth Hart entitled \"Jewish Philosophy and the Crisis of Modernity\", he argues that Islam, traditional Judaism, and ancient Greece, share a concern for sacred law that makes them especially susceptible to historicism, and therefore to tyranny. Strauss makes use of Nietzsche's own critique of progress and historicism, although Strauss refers to Nietzsche himself (no less than to Heidegger) as a \"radical historicist\" who articulated a philosophical (if only untenable) justification for historicism.\n\n\n\n"}
{"id": "3592098", "url": "https://en.wikipedia.org/wiki?curid=3592098", "title": "Industrial melanism", "text": "Industrial melanism\n\nIndustrial melanism is an evolutionary effect prominent in several arthropods, where dark pigmentation (melanism) has evolved in an environment affected by industrial pollution, including sulphur dioxide gas and dark soot deposits. Sulphur dioxide kills lichens, leaving tree bark bare where in clean areas it is boldly patterned, while soot darkens bark and other surfaces. Darker pigmented individuals have a higher fitness in those areas as their camouflage matches the polluted background better; they are thus favoured by natural selection. This change, extensively studied by Bernard Kettlewell, is a popular teaching example in Darwinian evolution, providing evidence for natural selection. Kettlewell's results have been challenged by zoologists, creationists and the journalist Judith Hooper, but later researchers have upheld Kettlewell's findings.\nIndustrial melanism is widespread in the Lepidoptera (butterflies and moths), involving over 70 species such as \"Odontopera bidentata\" (scalloped hazel) and \"Lymantria monacha\" (dark arches), but the most studied is the evolution of the peppered moth, \"Biston betularia\". It is also seen in a beetle, \"Adalia bipunctata\" (two-spot ladybird), where camouflage is not involved as the insect has conspicuous warning coloration, and in the seasnake \"Emydocephalus annulatus\" where the melanism may help in excretion of trace elements through sloughing of the skin. The rapid decline of melanism that has accompanied the reduction of pollution, in effect a natural experiment, makes natural selection for camouflage \"the only credible explanation\".\n\nOther explanations for the observed correlation with industrial pollution have been proposed, including strengthening the immune system in a polluted environment, absorbing heat more rapidly when sunlight is reduced by air pollution, and the ability to excrete trace elements into melanic scales and feathers.\n\nIndustrial melanism was first noticed in 1900 by the geneticist William Bateson; he observed that the colour morphs were inherited, but did not suggest an explanation for the polymorphism.\n\nIn 1906, the geneticist Leonard Doncaster described the increase in frequency of the melanic forms of several moth species from about 1800 to 1850 in the heavily industrialised north-west region of England.\n\nIn 1924, the evolutionary biologist J. B. S. Haldane constructed a mathematical argument showing that the rapid growth in frequency of the \"carbonaria\" form of the peppered moth, \"Biston betularia\", implied selective pressure.\n\nFrom 1955 onwards, the geneticist Bernard Kettlewell conducted a series of experiments exploring the evolution of melanism in the peppered moth. He used a capture-mark-recapture technique to show that dark forms survived better than light ones.\n\nBy 1973, pollution in England had begun to decrease, and the dark \"carbonaria\" form had declined in frequency. This provided convincing evidence, gathered and analysed by Kettlewell and others such as the entomologist and geneticist Michael Majerus and the population geneticist Laurence M. Cook, that its rise and fall had been caused by natural selection in response to the changing pollution of the landscape.\n\nIndustrial melanism is known from over 70 species of moth that Kettlewell found in England, and many others from Europe and North America.\nAmong these, \"Apamea crenata\" (clouded border brindle moth) and \"Acronicta rumicis\" (knot grass moth) are always polymorphic, though the melanic forms are more common in cities and (like those of the peppered moth) are declining in frequency as those cities become less polluted.\n\nAmong other insects, industrial melanism has been observed in a beetle, \"Adalia bipunctata\", the two-spot ladybird.\n\nIn the vertebrates, industrial melanism is known from the turtle-headed seasnake \"Emydocephalus annulatus\", and may be present in urban feral pigeons.\n\nOriginally, peppered moths lived where light-colored lichens covered the trees. For camouflage from predators against that clean background, they had generally light coloration. During the Industrial Revolution in England, sulphur dioxide pollution in the atmosphere reduced the lichen cover, while soot blackened the bark of urban trees, making the light-colored moths more vulnerable to predation. This provided a selective advantage to the gene responsible for melanism, and the darker-colored moths increased in frequency. The melanic phenotype of \"Biston betularia\" has been calculated to give a fitness advantage as great as 30 per cent. By the end of the 19th century it almost completely replaced the original light-coloured type (var. \"typica\"), forming a peak of 98% of the population in 1895.\n\nMelanic \"B. betularia\" have been widely observed in North America. In 1959, 90% of \"B. betularia\" in Michigan and Pennsylvania were melanic. By 2001, melanism dropped to 6% of the population, following clean air legislation. The drop in melanism was correlated with an increase in species diversity of lichens, a decrease in the atmospheric pollutant sulphur dioxide, and an increase in the pale phenotype. The return of lichens is in turn directly correlated with the reduction in atmospheric sulphur dioxide.\n\nKettlewell's experiments were criticised by the zoologist Theodore David Sargent, who failed to reproduce Kettlewell's results between 1965 and 1969, and argued that Kettlewell had specially trained his birds to give the desired results. \nMichael Majerus however found that Kettlewell was basically correct in concluding that differential bird predation in a polluted environment was the primary cause of industrial melanism in the peppered moth. The story was in turn taken up in a 2002 book \"Of Moths and Men\", by the journalist Judith Hooper, asserting that Kettlewell's findings were fraudulent. The story was picked up by creationists who repeated the assertions of fraudulence. Zoologists including L. M. Cook, B. S. Grant, Majerus and David Rudge however all upheld Kettlewell's account, finding that each of Hooper's and the creationists' claims collapsed when the facts were examined. \n\nIt has been suggested that the demonstrated relationship between melanism and pollution can not be fully proven because the exact reason for increase in survivability can not be tracked and pin-pointed. However, as air quality has improved in industrial areas of America and Britain, through improved regulation, offering the conditions for a natural experiment, melanism has sharply declined in moths including \"B. betularia\" and \"Odontopera bidentata\". Cook and J. R. G. Turner have concluded that \"natural selection is the only credible explanation for the overall decline\", and other biologists working in the area concur with this judgement.\n\nIn 1921, the evolutionary biologist Richard Goldschmidt argued that the observed increase in the melanic form of the black arches moth, \"Lymantria monacha\", could not have been caused by mutation pressure alone, but required a selective advantage from an unknown cause: he did not consider camouflage as an explanation.\n\nNearly a century later, it was suggested that the moth's industrial melanism might, in addition (pleiotropy) to providing camouflage with \"the well-known protective dark coloration\", also confer better immunity to toxic chemicals from industrial pollution. The darker forms have a stronger immune response to foreign objects; these are encapsulated by haemocytes (insect blood cells), and the capsule so formed is then hardened with deposits of the dark pigment, melanin.\n\nA non-camouflage mechanism has been suggested for some vertebrates. In tropical ocean regions subject to industrial pollution the turtle-headed seasnake \"Emydocephalus annulatus\" is more likely to be melanic. These snakes shed their skin every two to six weeks. Sloughed skin contains toxic minerals, higher for dark skin, so industrial melanism could be selected for through improved excretion of trace elements. The same may apply in the case of urban feral pigeons, which have the ability to remove trace metals such as zinc to their feathers. However, toxic lead was not found to accumulate in feathers, so the putative mechanism is limited in its range.\n\nMelanic forms of the two-spot ladybird \"Adalia bipunctata\" are very frequent in and near cities, and rare in unpolluted countryside, so they appear to be industrial. Ladybirds are aposematic (with conspicuous warning coloration), so camouflage cannot explain the distribution. A proposed explanation is that the melanic forms have a thermal advantage directly linked to the pollution aspect of industrialization, since smoke and particulates in the air reduce the amount of sunlight that reaches the habitats of these species. Melanic phenotypes should then be favoured by natural selection, as the dark coloration absorbs the limited sunlight better. A possible explanation might be that in colder environments, the thermal advantages of industrial melanism might increase activity and the likelihood to mate. In the Netherlands, melanic \"A. bipunctata\" had a distinct mating advantage over the non-melanic form.\n\nHowever, thermal melanism failed to explain the distribution of the species near Helsinki where the city forms a relatively warm 'heat island', while near the Finnish coast there is more sunlight as well as more melanism, so the selective pressure driving melanism requires a different explanation. A study in Birmingham similarly found no evidence of thermal melanism but a strong correlation with smoke pollution; melanism declined from 1960 to 1978 as the city became cleaner. Further, the same study found that a related species, \"Adalia decempunctata\", experienced no change in frequency of melanism in the same places in that period.\n"}
{"id": "163901", "url": "https://en.wikipedia.org/wiki?curid=163901", "title": "Information society", "text": "Information society\n\nAn information society is a society where the creation, distribution, use, integration and manipulation of information is a significant economic, political, and cultural activity. Its main drivers are digital information and communication technologies, which have resulted in an information explosion and are profoundly changing all aspects of social organization, including the economy, education, health, warfare, government and democracy. The people who have the means to partake in this form of society are sometimes called digital citizens, defined by K. Mossberger as “Those who use the Internet regularly and effectively”. This is one of many dozen labels that have been identified to suggest that humans are entering a new phase of society.\n\nThe markers of this rapid change may be technological, economic, occupational, spatial, cultural, or some combination of all of these.\nInformation society is seen as the successor to industrial society. Closely related concepts are the post-industrial society (Daniel Bell), post-fordism, post-modern society, knowledge society, telematic society, Information Revolution, liquid modernity, and network society (Manuel Castells).\n\nThere is currently no universally accepted concept of what exactly can be termed information society and what shall rather not so be termed. Most theoreticians agree that a transformation can be seen that started somewhere between the 1970s and today and is changing the way societies work fundamentally. Information technology goes beyond the internet, and there are discussions about how big the influence of specific media or specific modes of production really is. Frank Webster notes five major types of information that can be used to define information society: technological, economic, occupational, spatial and cultural. According to Webster, the character of information has transformed the way that we live today. How we conduct ourselves centers around theoretical knowledge and information.\n\nKasiwulaya and Gomo (Makerere University) allude that information societies are those that have intensified their use of IT for economic, social, cultural and political transformation. In 2005, governments reaffirmed their dedication to the foundations of the Information\nSociety in the Tunis Commitment and outlined the basis for implementation and follow-up in the Tunis Agenda for the Information Society. In particular, the Tunis Agenda addresses the issues of financing of ICTs for development and Internet governance that could not be resolved in the first phase.\n\nSome people, such as Antonio Negri, characterize the information society as one in which people do immaterial labour. By this, they appear to refer to the production of knowledge or cultural artifacts. One problem with this model is that it ignores the material and essentially industrial basis of the society. However it does point to a problem for workers, namely how many creative people does this society need to function? For example, it may be that you only need a few star performers, rather than a plethora of non-celebrities, as the work of those performers can be easily distributed, forcing all secondary players to the bottom of the market. It \"is\" now common for publishers to promote only their best selling authors and to try to avoid the rest—even if they still sell steadily. Films are becoming more and more judged, in terms of distribution, by their first weekend's performance, in many cases cutting out opportunity for word-of-mouth development.\n\nMichael Buckland characterizes information in society in his book \"Information and Society.\" Buckland expresses the idea that information can be interpreted differently from person to person based on that individual's experiences.\n\nConsidering that metaphors and technologies of information move forward in a reciprocal relationship, we can describe some societies (especially the Japanese society) as an information society because we think of it as such.\nThe word information may be interpreted in many different ways. According to Buckland in \"Information and Society\", most of the meanings fall into three categories of human knowledge: information as knowledge, information as a process, and information as a thing.\n\nThe growth of technologically mediated information has been quantified in different ways, including society's technological capacity to store information, to communicate information, and to compute information. It is estimated that, the world's technological capacity to store information grew from 2.6 (optimally compressed) exabytes in 1986, which is the informational equivalent to less than one 730-MB CD-ROM per person in 1986 (539 MB per person), to 295 (optimally compressed) exabytes in 2007. This is the informational equivalent of 60 CD-ROM per person in 2007 and represents a sustained annual growth rate of some 25%. The world’s combined technological capacity to receive information through one-way broadcast networks was the informational equivalent of 174 newspapers per person per day in 2007.\n\nThe world's combined effective capacity to exchange information through two-way telecommunication networks was 281 petabytes of (optimally compressed) information in 1986, 471 petabytes in 1993, 2.2 (optimally compressed) exabytes in 2000, and 65 (optimally compressed) exabytes in 2007, which is the informational equivalent of 6 newspapers per person per day in 2007. The world's technological capacity to compute information with humanly guided general-purpose computers grew from 3.0 × 10^8 MIPS in 1986, to 6.4 x 10^12 MIPS in 2007, experiencing the fastest growth rate of over 60% per year during the last two decades.\n\nJames R. Beniger describes the necessity of information in modern society in the following way: “The need for sharply increased control that resulted from the industrialization of material processes through application of inanimate sources of energy probably accounts for the rapid development of automatic feedback technology in the early industrial period (1740-1830)” (p. 174)\n“Even with enhanced feedback control, industry could not have developed without the enhanced means to process matter and energy, not only as inputs of the raw materials of production but also as outputs distributed to final consumption.”(p. 175)\n\nOne of the first people to develop the concept of the information society was the economist Fritz Machlup. In 1933, Fritz Machlup began studying the effect of patents on research. His work culminated in the study \"The production and distribution of knowledge in the United States\" in 1962. This book was widely regarded and was eventually translated into Russian and Japanese. The Japanese have also studied the information society (or \"jōhōka shakai\", ).\n\nThe issue of technologies and their role in contemporary society have been discussed in the scientific literature using a range of labels and concepts. This section introduces some of them. Ideas of a knowledge or information economy, post-industrial society, postmodern society, network society, the information revolution, informational capitalism, network capitalism, and the like, have been debated over the last several decades.\n\nFritz Machlup (1962) introduced the concept of the knowledge industry. He began studying the effects of patents on research before distinguishing five sectors of the knowledge sector: education, research and development, mass media, information technologies, information services. Based on this categorization he calculated that in 1959 29% per cent of the GNP in the USA had been produced in knowledge industries.\n\nPeter Drucker has argued that there is a transition from an economy based on material goods to one based on knowledge. Marc Porat distinguishes a primary (information goods and services that are directly used in the production, distribution or processing of information) and a secondary sector (information services produced for internal consumption by government and non-information firms) of the information economy.\n\nPorat uses the total value added by the primary and secondary information sector to the GNP as an indicator for the information economy. The OECD has employed Porat's definition for calculating the share of the information economy in the total economy (e.g. OECD 1981, 1986). Based on such indicators, the information society has been defined as a society where more than half of the GNP is produced and more than half of the employees are active in the information economy.\n\nFor Daniel Bell the number of employees producing services and information is an indicator for the informational character of a society. \"A post-industrial society is based on services. (…) What counts is not raw muscle power, or energy, but information. (…) A post industrial society is one in which the majority of those employed are not involved in the production of tangible goods\".\n\nAlain Touraine already spoke in 1971 of the post-industrial society. \"The passage to postindustrial society takes place when investment results in the production of symbolic goods that modify values, needs, representations, far more than in the production of material goods or even of 'services'. Industrial society had transformed the means of production: post-industrial society changes the ends of production, that is, culture. (…) The decisive point here is that in postindustrial society all of the economic system is the object of intervention of society upon itself. That is why we can call it the programmed society, because this phrase captures its capacity to create models of management, production, organization, distribution, and consumption, so that such a society appears, at all its functional levels, as the product of an action exercised by the society itself, and not as the outcome of natural laws or cultural specificities\" (Touraine 1988: 104). In the programmed society also the area of cultural reproduction including aspects such as information, consumption, health, research, education would be industrialized. That modern society is increasing its capacity to act upon itself means for Touraine that society is reinvesting ever larger parts of production and so produces and transforms itself. This makes Touraine's concept substantially different from that of Daniel Bell who focused on the capacity to process and generate information for efficient society functioning.\n\nJean-François Lyotard has argued that \"knowledge has become the force of production over the last few decades\". Knowledge would be transformed into a commodity. Lyotard says that postindustrial society makes knowledge accessible to the layman because knowledge and information technologies would diffuse into society and break up Grand Narratives of centralized structures and groups. Lyotard denotes these changing circumstances as postmodern condition or postmodern society.\n\nSimilarly to Bell, Peter Otto and Philipp Sonntag (1985) say that an information society is a society where the majority of employees work in information jobs, i.e. they have to deal more with information, signals, symbols, and images than with energy and matter. Radovan Richta (1977) argues that society has been transformed into a scientific civilization based on services, education, and creative activities. This transformation would be the result of a scientific-technological transformation based on technological progress and the increasing importance of computer technology. Science and technology would become immediate forces of production (Aristovnik 2014: 55).\n\nNico Stehr (1994, 2002a, b) says that in the knowledge society a majority of jobs involves working with knowledge. \"Contemporary society may be described as a knowledge society based on the extensive penetration of all its spheres of life and institutions by scientific and technological knowledge\" (Stehr 2002b: 18). For Stehr, knowledge is a capacity for social action. Science would become an immediate productive force, knowledge would no longer be primarily embodied in machines, but already appropriated nature that represents knowledge would be rearranged according to certain designs and programs (Ibid.: 41-46). For Stehr, the economy of a knowledge society is largely driven not by material inputs, but by symbolic or knowledge-based inputs (Ibid.: 67), there would be a large number of professions that involve working with knowledge, and a declining number of jobs that demand low cognitive skills as well as in manufacturing (Stehr 2002a).\n\nAlso Alvin Toffler argues that knowledge is the central resource in the economy of the information society: \"In a Third Wave economy, the central resource – a single word broadly encompassing data, information, images, symbols, culture, ideology, and values – is actionable knowledge\" (Dyson/Gilder/Keyworth/Toffler 1994).\n\nAt the end of the twentieth century, the concept of the network society gained importance in information society theory. For Manuel Castells, network logic is besides information, pervasiveness, flexibility, and convergence a central feature of the information technology paradigm (2000a: 69ff). \"One of the key features of informational society is the networking logic of its basic structure, which explains the use of the concept of 'network society'\" (Castells 2000: 21). \"As an historical trend, dominant functions and processes in the Information Age are increasingly organized around networks. Networks constitute the new social morphology of our societies, and the diffusion of networking logic substantially modifies the operation and outcomes in processes of production, experience, power, and culture\" (Castells 2000: 500). For Castells the network society is the result of informationalism, a new technological paradigm.\n\nJan Van Dijk (2006) defines the network society as a \"social formation with an infrastructure of social and media networks enabling its prime mode of organization at all levels (individual, group/organizational and societal). Increasingly, these networks link all units or parts of this formation (individuals, groups and organizations)\" (Van Dijk 2006: 20). For Van Dijk networks have become the nervous system of society, whereas Castells links the concept of the network society to capitalist transformation, Van Dijk sees it as the logical result of the increasing widening and thickening of networks in nature and society. Darin Barney uses the term for characterizing societies that exhibit two fundamental characteristics: \"The first is the presence in those societies of sophisticated – almost exclusively digital – technologies of networked communication and information management/distribution, technologies which form the basic infrastructure mediating an increasing array of social, political and economic practices. (…) The second, arguably more intriguing, characteristic of network societies is the reproduction and institutionalization throughout (and between) those societies of networks as the basic form of human organization and relationship across a wide range of social, political and economic configurations and associations\".\n\nThe major critique of concepts such as information society, knowledge society, network society, postmodern society, postindustrial society, etc. that has mainly been voiced by critical scholars is that they create the impression that we have entered a completely new type of society. \"If there is just more information then it is hard to understand why anyone should suggest that we have before us something radically new\" (Webster 2002a: 259). Critics such as Frank Webster argue that these approaches stress discontinuity, as if contemporary society had nothing in common with society as it was 100 or 150 years ago. Such assumptions would have ideological character because they would fit with the view that we can do nothing about change and have to adopt to existing political realities (kasiwulaya 2002b: 267).\n\nThese critics argue that contemporary society first of all is still a capitalist society oriented towards accumulating economic, political, and cultural capital. They acknowledge that information society theories stress some important new qualities of society (notably globalization and informatization), but charge that they fail to show that these are attributes of overall capitalist structures. Critics such as Webster insist on the continuities that characterise change. In this way Webster distinguishes between different epochs of capitalism: laissez-faire capitalism of the 19th century, corporate capitalism in the 20th century, and informational capitalism for the 21st century (kasiwulaya 2006).\n\nFor describing contemporary society based on a dialectic of the old and the new, continuity and discontinuity, other critical scholars have suggested several terms like:\n\nOther scholars prefer to speak of information capitalism (Morris-Suzuki 1997) or informational capitalism (Manuel Castells 2000, Christian Fuchs 2005, Schmiede 2006a, b). Manuel Castells sees informationalism as a new technological paradigm (he speaks of a mode of development) characterized by \"information generation, processing, and transmission\" that have become \"the fundamental sources of productivity and power\" (Castells 2000: 21). The \"most decisive historical factor accelerating, channelling and shaping the information technology paradigm, and inducing its associated social forms, was/is the process of capitalist restructuring undertaken since the 1980s, so that the new techno-economic system can be adequately characterized as informational capitalism\" (Castells 2000: 18). Castells has added to theories of the information society the idea that in contemporary society dominant functions and processes are increasingly organized around networks that constitute the new social morphology of society (Castells 2000: 500). Nicholas Garnham is critical of Castells and argues that the latter’s account is technologically determinist because Castells points out that his approach is based on a dialectic of technology and society in which technology embodies society and society uses technology (Castells 2000: 5sqq). But Castells also makes clear that the rise of a new \"mode of development\" is shaped by capitalist production, i.e. by society, which implies that technology isn't the only driving force of society.\n\nAntonio Negri and Michael Hardt argue that contemporary society is an Empire that is characterized by a singular global logic of capitalist domination that is based on immaterial labour. With the concept of immaterial labour Negri and Hardt introduce ideas of information society discourse into their Marxist account of contemporary capitalism. Immaterial labour would be labour \"that creates immaterial products, such as knowledge, information, communication, a relationship, or an emotional response\" (Hardt/Negri 2005: 108; cf. also 2000: 280-303), or services, cultural products, knowledge (Hardt/Negri 2000: 290). There would be two forms: intellectual labour that produces ideas, symbols, codes, texts, linguistic figures, images, etc.; and affective labour that produces and manipulates affects such as a feeling of ease, well-being, satisfaction, excitement, passion, joy, sadness, etc. (Ibid.).\n\nOverall, neo-Marxist accounts of the information society have in common that they stress that knowledge, information technologies, and computer networks have played a role in the restructuration and globalization of capitalism and the emergence of a flexible regime of accumulation (David Harvey 1989). They warn that new technologies are embedded into societal antagonisms that cause structural unemployment, rising poverty, social exclusion, the deregulation of the welfare state and of labour rights, the lowering of wages, welfare, etc.\n\nConcepts such as knowledge society, information society, network society, informational capitalism, postindustrial society, transnational network capitalism, postmodern society, etc. show that there is a vivid discussion in contemporary sociology on the character of contemporary society and the role that technologies, information, communication, and co-operation play in it. Information society theory discusses the role of information and information technology in society, the question which key concepts shall be used for characterizing contemporary society, and how to define such concepts. It has become a specific branch of contemporary sociology.\n\nInformation society is the means of getting information from one place to another. As technology has advanced so too has the way people have adapted in sharing this information with each other.\n\n\"Second nature\" refers a group of experiences that get made over by culture. They then get remade into something else that can then take on a new meaning. As a society we transform this process so it becomes something natural to us, i.e. second nature. So, by following a particular pattern created by culture we are able to recognise how we use and move information in different ways. From sharing information via different time zones (such as talking online) to information ending up in a different location (sending a letter overseas) this has all become a habitual process that we as a society take for granted.\n\nHowever, through the process of sharing information vectors have enabled us to spread information even further. Through the use of these vectors information is able to move and then separate from the initial things that enabled them to move. From here, something called \"third nature\" has developed. An extension of second nature, third nature is in control of second nature. It expands on what second nature is limited by. It has the ability to mould information in new and different ways. So, third nature is able to ‘speed up, proliferate, divide, mutate, and beam in on us from else where. It aims to create a balance between the boundaries of space and time (see second nature). This can be seen through the telegraph, it was the first successful technology that could send and receive information faster than a human being could move an object. As a result different vectors of people have the ability to not only shape culture but create new possibilities that will ultimately shape society.\n\nTherefore, through the use of second nature and third nature society is able to use and explore new vectors of possibility where information can be moulded to create new forms of interaction.\n\nIn sociology, informational society refers to a post-modern type of society. Theoreticians like Ulrich Beck, Anthony Giddens and Manuel Castells argue that since the 1970s a transformation from industrial society to informational society has happened on a global scale.\n\nAs steam power was the technology standing behind industrial society, so information technology is seen as the catalyst for the changes in work organisation, societal structure and politics occurring in the late 20th century.\n\nIn the book \"Future Shock\", Alvin Toffler used the phrase super-industrial society to describe this type of society. Other writers and thinkers have used terms like \"post-industrial society\" and \"post-modern industrial society\" with a similar meaning.\n\nA number of terms in current use emphasize related but different aspects of the emerging global economic order. The Information Society intends to be the most encompassing in that an economy is a subset of a society. The Information Age is somewhat limiting, in that it refers to a 30-year period between the widespread use of computers and the knowledge economy, rather than an emerging economic order. The knowledge era is about the nature of the content, not the socioeconomic processes by which it will be traded. The computer revolution, and knowledge revolution refer to specific revolutionary transitions, rather than the end state towards which we are evolving. The Information Revolution relates with the well known terms agricultural revolution and industrial revolution.\n\nToday, It is important to selectively select the information. Due to information revolution, the amount of information is puzzling. Among these, we need to develop techniques that refine information. This is called \"data mining.\" It is an engineering term, but it is used in sociology. In other words, if the amount of information was competitive in the past, the quality of information is important today.\n\nOne of the central paradoxes of the information society is that it makes information easily reproducible, leading to a variety of freedom/control problems relating to intellectual property. Essentially, business and capital, whose place becomes that of producing and selling information and knowledge, seems to require control over this new resource so that it can effectively be managed and sold as the basis of the information economy. However, such control can prove to be both technically and socially problematic. Technically because copy protection is often easily circumvented and socially \"rejected\" because the users and citizens of the information society can prove to be unwilling to accept such absolute commodification of the facts and information that compose their environment.\n\nResponses to this concern range from the Digital Millennium Copyright Act in the United States (and similar legislation elsewhere) which make copy protection (see DRM) circumvention illegal, to the free software, open source and copyleft movements, which seek to encourage and disseminate the \"freedom\" of various information products (traditionally both as in \"gratis\" or free of cost, and liberty, as in freedom to use, explore and share).\n\nCaveat: Information society is often used by politicians meaning something like \"we all do internet now\"; the sociological term information society (or informational society) has some deeper implications about change of societal structure. Because we lack political control of intellectual property, we are lacking in a concrete map of issues, an analysis of costs and benefits, and functioning political groups that are unified by common interests representing different opinions of this diverse situation that are prominent in the information society.\n\n\n\n "}
{"id": "23158223", "url": "https://en.wikipedia.org/wiki?curid=23158223", "title": "Jack Collom", "text": "Jack Collom\n\nJohn Aldridge \"Jack\" Collom (November 8, 1931 – July 2, 2017) was an American poet, essayist, and creative writing pedagogue. Included among the twenty-five books he published during his lifetime were \"Red Car Goes By: Selected Poems 1955–2000\"; \"Poetry Everywhere: Teaching Poetry Writing in School and in the Community\"; and \"Second Nature\", which won the 2013 Colorado Book Award for Poetry. In the fields of education and pedagogy, he was involved in eco-literature, ecopoetics, and creative writing instruction for children.\n\nJack Collom was born John Aldridge Collom in Chicago on November 8, 1931. He grew up in the small town of Western Springs, Illinois, spent much of his time birdwatching, and over the years became an inveterate bird-watcher. Collom moved to Fraser, Colorado, in 1947. He studied Forestry at Colorado A&M College where he earned a B.S. in 1952. Afterwards, he spent four years in the U.S. Air Force, and he started writing poetry in 1955 while stationed in Tripoli, Libya. His unit was next stationed at Neubiberg, a base just south of Munich, in Bavaria. It is there he met his first wife (a native German), in 1956. After his discharge from the military, he moved back to the US after a brief time living in Germany, and worked in factories for twenty years while writing poetry.\n\nHe received his B.A. in English (1972) and M.A. in English literature (1974) from the University of Colorado, where he had studied on the G.I. Bill. In 1974, he began teaching in the \"Poetry-in-the-Schools\" programs in Colorado, Wyoming, and Nebraska. In 1980, he began teaching poetry in the public schools of New York City, by way of the \"Poets In Public Service\" and \"Teachers & Writers\" programs. Collom continued to teach creative writing to children for the next 35 years, in both elementary and secondary schools, where he developed a pedagogy for this type of educational approach.\n\nSubsequently, Teachers & Writers Collaborative published three books of Collom's essays and commentary on this experience (which included the young students' poems), notably \"Poetry Everywhere\" and \"Moving Windows\".\n\nFrom 1966 to 1977, he published the work of many writers in a little magazine called \"The\". He was twice awarded Poetry Fellowships from the National Endowment for the Arts, and received a Foundation for Contemporary Arts Grants to Artists award (2012). From 1986 until his death in 2017, Collom taught at Naropa University's Jack Kerouac School of Disembodied Poetics as an adjunct professor, where he shaped Writing Outreach, a community creative-writing project, into a course. In 1989, he pioneered Eco-Lit, one of the first ecology literature courses ever offered in the United States. Some of his accomplishments as an environmentalist-poet are documented in \"American Environmental Leaders: From Colonial Times to the Present\". His nature writings and essays about the environment were published in various venues, including \"ecopoetics\", \"The Alphabet of Trees: A Guide to Writing Nature Poetry\", and \"ISLE\", the journal of Interdisciplinary Studies in Literature and the Environment.\n\nHe read and taught throughout the United States, in Mexico, Costa Rica, Austria, Belgium, and Germany. In 2008, he was the plenary speaker at the \"Poetic Ecologies\" conference at the Université Libre de Bruxelles. In 2009, he led a three-week Creativity and Aging Program at Woodland Pattern in Milwaukee, Wisconsin.\n\nHe worked with numerous dancers, visual artists and musician/composers, and recorded three CDs: \"Calluses of Poetry\" and \"Colors Born of Shadow\", with Ken Bernstein, and \"Blue Yodel Blue Heron\", with Dan Hankin and Sierra Collom.\n\nIn 2001, his adopted hometown of Boulder, Colorado, declared and celebrated a \"Jack Collom Day\".\n\nCollom was married three times. He had three sons by his first marriage: Nathaniel, Christopher, and Franz. He had a daughter, Sierra, through a second marriage.\n\nJack Collom died in Boulder, Colorado on July 2, 2017. He is survived by his wife, Jennifer Heath, his four grown children, and a grandson.\n\n\n\n"}
{"id": "4205386", "url": "https://en.wikipedia.org/wiki?curid=4205386", "title": "Legal naturalism", "text": "Legal naturalism\n\nLegal naturalism is a term coined by Olufemi Taiwo to describe a current in the social philosophy of Karl Marx which can be interpreted as one of Natural Law. Taiwo considered it the manifestation of Natural Law in a dialectical materialist context.\n\n\n"}
{"id": "41782533", "url": "https://en.wikipedia.org/wiki?curid=41782533", "title": "List of Earth flybys", "text": "List of Earth flybys\n\nList of Earth flybys is a list of cases where spacecraft incidentally performed Earth flybys, typically for a gravity assist to another body.\n"}
{"id": "24694906", "url": "https://en.wikipedia.org/wiki?curid=24694906", "title": "List of invasive species in Asia", "text": "List of invasive species in Asia\n\nThis is a list of invasive species in Asia. A species is regarded as invasive if it has been introduced by human action to a location, area, or region where it did not previously occur naturally (i.e., is not a native species), becomes capable of establishing a breeding population in the new location without further intervention by humans, and becomes a \"pest\" in the new location, directly threatening agriculture and/or the local biodiversity.\n\nThe term invasive species refers to a subset of those species defined as introduced species. If a species has been introduced but remains local, and is not problematic to agriculture or to the local biodiversity, then it cannot be considered to be an invasive species and does not belong on this list.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24695021", "url": "https://en.wikipedia.org/wiki?curid=24695021", "title": "List of invasive species in North America", "text": "List of invasive species in North America\n\nThis is a list of invasive species in North America. A species is regarded as invasive if it has been introduced by human action to a location, area, or region where it did not previously occur naturally (i.e., is not a native species), becomes capable of establishing a breeding population in the new location without further intervention by humans, and becomes a pest in the new location, directly threatening human industry, such as agriculture, or the local biodiversity.\n\nThe term \"invasive species\" refers to a subset of those species defined as introduced species. If a species has been introduced, but remains local, and is not problematic for human industry or the local biodiversity, then it is not considered invasive, and does not belong on this list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "60773", "url": "https://en.wikipedia.org/wiki?curid=60773", "title": "List of woods", "text": "List of woods\n\nThis is a list of woods, in particular those most commonly used in the timber and lumber trade.\n\n\n\n\n\n"}
{"id": "636424", "url": "https://en.wikipedia.org/wiki?curid=636424", "title": "Manilkara bidentata", "text": "Manilkara bidentata\n\nManilkara bidentata is a species of \"Manilkara\" native to a large area of northern South America, Central America and the Caribbean. Common names include bulletwood, balatá, ausubo, massaranduba, and (ambiguously) \"cow-tree\".\n\nBalatá is a large tree, growing to tall. The leaves are alternate, elliptical, entire, and long. The flowers are white, and are produced at the beginning of the rainy season. The fruit is a yellow berry, in diameter, which is edible; it contains one (occasionally two) seed(s). Its latex is used industrially for products such as chicle.\n\nThe latex is extracted in the same manner in which sap is extracted from the rubber tree. It is then dried to form an inelastic rubber-like material. It is almost identical to gutta-percha (produced from a closely related southeast Asian tree), and is sometimes called \"gutta-balatá\".\n\nBalatá was often used in the production of high-quality golf balls, to use as the outer layer of the ball. Balatá-covered balls have a high spin rate, but do not travel as far as most balls with a Surlyn cover. Due to the nondurable nature of the material the golf club strikes, balatá-covered balls do not last long before needing to be replaced. While once favored by professional and low-handicap players, they are now obsolete, replaced by newer Surlyn and urethane technology.\n\nToday, Brazil is the largest producer of Massaranduba wood, where it is cut in the Amazon rainforest.\n\nThe tree is a hardwood with a red heart, which is used for furniture and as a construction material where it grows. Locals often refer to it as bulletwood for its extremely hard wood, which is so dense that it does not float in water. Drilling is necessary to drive nailed connections. In trade, it is occasionally (and incorrectly) called \"brazilwood\".\n\nThe fruit, like that of the related sapodilla (\"M. zapota\"), is edible.\n\nThough its heartwood may present in a shade of purple, \"Manilkara bidentata\" should not be confused with another tropical tree widely known as \"purpleheart\", \"Peltogyne pubescens\".\n\nThis timber is being used to produce outdoor furniture and is being marketed as \"Pacific Jarrah\" in Australia.\n"}
{"id": "3595285", "url": "https://en.wikipedia.org/wiki?curid=3595285", "title": "Maximum power principle", "text": "Maximum power principle\n\nThe maximum power principle or Lotka's principle has been proposed as the fourth principle of energetics in open system thermodynamics, where an example of an open system is a biological cell. According to Howard T. Odum, \"The maximum power principle can be stated: During self-organization, system designs develop and prevail that maximize power intake, energy transformation, and those uses that reinforce production and efficiency.\"\n\nChen (2006) has located the origin of the statement of maximum power as a formal principle in a tentative proposal by Alfred J. Lotka (1922a, b). Lotka's statement sought to explain the Darwinian notion of evolution with reference to a physical principle. Lotka's work was subsequently developed by the systems ecologist Howard T. Odum in collaboration with the Chemical Engineer Richard C. Pinkerton, and later advanced by the Engineer Myron Tribus.\n\nWhile Lotka's work may have been a first attempt to formalise evolutionary thought in mathematical terms, it followed similar observations made by Leibniz and Volterra and Ludwig Boltzmann, for example, throughout the sometimes controversial history of natural philosophy. In contemporary literature it is most commonly associated with the work of Howard T. Odum.\n\nThe significance of Odum's approach was given greater support during the 1970s, amid times of oil crisis, where, as Gilliland (1978, pp. 100) observed, there was an emerging need for a new method of analysing the importance and value of energy resources to economic and environmental production. A field known as energy analysis, itself associated with net energy and EROEI, arose to fulfill this analytic need. However, in energy analysis intractable theoretical and practical difficulties arose when using the energy unit to understand, a) the conversion among concentrated fuel types (or energy types), b) the contribution of labour, and c) the contribution of the environment.\n\nLotka said (1922b: 151): \nGilliland noted that these difficulties in analysis in turn required some new theory to adequately explain the interactions and transactions of these different energies (different concentrations of fuels, labour and environmental forces). Gilliland (Gilliland 1978, p. 101) suggested that Odum's statement of the maximum power principle (H.T.Odum 1978, pp. 54–87) was, perhaps, an adequate expression of the requisite theory:\nThis theory Odum called maximum power theory. In order to formulate maximum power theory Gilliland observed that Odum had added another law (the maximum power principle) to the already well established laws of thermodynamics. In 1978 Gilliland wrote that Odum's new law had not yet been validated (Gilliland 1978, p. 101). Gilliland stated that in maximum power theory the second law efficiency of thermodynamics required an additional physical concept: \"the concept of second law efficiency under maximum power\" (Gilliland 1978, p. 101):\nIn this way the concept of maximum power was being used as a principle to quantitatively describe the selective law of biological evolution. Perhaps H.T.Odum's most concise statement of this view was (1970, p. 62):\n\nThe Odum–Pinkerton approach to Lotka's proposal was to apply Ohm's law – and the associated maximum power theorem (a result in electrical power systems) – to ecological systems. Odum and Pinkerton defined \"power\" in electronic terms as the rate of work, where Work is understood as a \"useful energy transformation\". The concept of maximum power can therefore be defined as the \"maximum rate of useful energy transformation\". Hence the underlying philosophy aims to unify the theories and associated laws of electronic and thermodynamic systems with biological systems. This approach presupposed an analogical view which sees the world as an ecological-electronic-economic engine.\n\nOdum et al. viewed the maximum power theorem as a principle of power-efficiency reciprocity selection with wider application than just electronics. For example, Odum saw it in open systems operating on solar energy, like both photovoltaics and photosynthesis (1963, p. 438). Like the maximum power theorem, Odum's statement of the maximum power principle relies on the notion of 'matching', such that high-quality energy maximizes power by matching and amplifying energy (1994, pp. 262, 541): \"in surviving designs a matching of high-quality energy with larger amounts of low-quality energy is likely to occur\" (1994, p. 260). As with electronic circuits, the resultant rate of energy transformation will be at a maximum at an intermediate power efficiency. In 2006, T.T. Cai, C.L. Montague and J.S. Davis said that, \"The maximum power principle is a potential guide to understanding the patterns and processes of ecosystem development and sustainability. The principle predicts the selective persistence of ecosystem designs that capture a previously untapped energy source.\" (2006, p. 317). In several texts H.T. Odum gave the Atwood machine as a practical example of the 'principle' of maximum power.\n\nThe mathematical definition given by H.T. Odum is formally analogous to the definition provided on the maximum power theorem article. (For a brief explanation of Odum's approach to the relationship between ecology and electronics see Ecological Analog of Ohm's Law)\n\nWhether or not the principle of maximum power efficiency can be considered the fourth law of thermodynamics and the fourth principle of energetics is moot. Nevertheless, H.T. Odum also proposed a corollary of maximum power as the organisational principle of evolution, describing the evolution of microbiological systems, economic systems, planetary systems, and astrophysical systems. He called this corollary the maximum empower principle. This was suggested because, as S.E. Jorgensen, M.T. Brown, H.T. Odum (2004) note,\n\nC. Giannantoni may have confused matters when he wrote \"The \"Maximum Em-Power Principle\" (Lotka–Odum) is generally considered the \"Fourth Thermodynamic Principle\" (mainly) because of its practical validity for a very wide class of physical and biological systems\" (C. Giannantoni 2002, § 13, p. 155). Nevertheless, Giannantoni has proposed the Maximum Em-Power Principle as the fourth principle of thermodynamics (Giannantoni 2006).\n\nThe preceding discussion is incomplete. The \"maximum power\" was discovered several times independently, in physics and engineering, see: Novikov (1957), El-Wakil (1962), and Curzon and Ahlborn (1975). The incorrectness of this analysis and design evolution conclusions was demonstrated by Gyftopoulos (2002).\n\n\n"}
{"id": "36688650", "url": "https://en.wikipedia.org/wiki?curid=36688650", "title": "Moisture expansion", "text": "Moisture expansion\n\nMoisture expansion is the tendency of matter to change in volume in response to a change in moisture content. The macroscopic effect is similar to that of thermal expansion but the microscopic causes are very different. Moisture expansion is caused by hygroscopy.\n"}
{"id": "52634071", "url": "https://en.wikipedia.org/wiki?curid=52634071", "title": "Nature-based solutions", "text": "Nature-based solutions\n\nNature-based solutions (NBS or NbS) refers to the sustainable management and use of nature for tackling socio-environmental challenges. The challenges include issues such as climate change, water security, water pollution, food security, human health, and disaster risk management. \n\nA definition by the European Union states that these solutions are \"inspired and supported by nature, which are cost-effective, simultaneously provide environmental, social and economic benefits and help build resilience. The Nature-based Solutions Initiative meanwhile defines them as \"actions that work with and enhance nature so as to help people adapt to change and disasters\". Such solutions bring more, and more diverse, nature and natural features and processes into cities, landscapes and seascapes, through locally adapted, resource-efficient and systemic interventions\". With NBS, healthy, resilient and diverse ecosystems (whether natural, managed or newly created) can provide solutions for the benefit of societies and overall biodiversity.\n\nFor instance, the restoration or protection of mangroves along coastlines utilizes a nature-based solution to accomplish several things. Mangroves moderate the impact of waves and wind on coastal settlements or cities and sequester CO . They also provide safe nurseries for marine life that can be the basis for sustaining populations of fish that local populations may depend on. Additionally, the mangrove forests can help control coastal erosion resulting from sea level rise. Similarly, in cities green roofs or walls are nature-based solutions that can be used to moderate the impact of high temperatures, capture storm water, abate pollution, and act as carbon sinks, while enhancing biodiversity.\n\nConservation approaches and environment management initiatives have been carried out for decades. What is new is that the benefits of such nature-based solutions to human well-being have been articulated well more recently. Even if the term itself is still being framed, examples of nature-based solutions can be found all over the world, and imitated. Nature-based solutions are on their way to being mainstreamed in national and international policies and programmes (e.g. climate change policy, law, infrastructure investment and financing mechanisms). For example, the theme for World Water Day 2018 was \"Nature for water\" and by UN-Water's accompanying UN World Water Development Report had the title \"Nature-based Solutions for Water\".\n\nSocieties increasingly face challenges such as climate change, urbanization, jeopardized food security and water resource provision, and disaster risk. One approach to answer these challenges is to singularly rely on technological strategies. An alternative approach is to manage the (socio-)ecological systems in a comprehensive way in order to sustain and potentially increase the delivery of ecosystem services to humans. In this context, nature-based solutions (NBS) have recently been put forward by practitioners and quickly thereafter by policymakers. These solutions stress the sustainable use of nature in solving coupled environmental-social-economic challenges. \n\nWhile ecosystem services are often valued in terms of immediate benefits to human well-being and economy, NBS focus on the benefits to people and the environment itself, to allow for sustainable solutions that are able to respond to environmental change and hazards in the long-term. NBS go beyond the traditional biodiversity conservation and management principles by \"re-focusing\" the debate on humans and specifically integrating societal factors such as human well-being and poverty reduction, socio-economic development, and governance principles. \n\nWith respect to water issues, NBS can achieve the following, according to the World Water Development Report 2018 by UN-Water: \n\nIn this sense, NBS are strongly connected to ideas such as natural systems agriculture, natural solutions, ecosystem-based approaches, adaptation services, natural infrastructure, green infrastructure and ecological engineering. For instance, ecosystem-based approaches are increasingly promoted for climate change adaptation and mitigation by organisations like United Nations Environment Programme and non-governmental organisations such as The Nature Conservancy. These organisations refer to \"policies and measures that take into account the role of ecosystem services in reducing the vulnerability of society to climate change, in a multi-sectoral and multi-scale approach\".\n\nLikewise, natural infrastructure is defined as a \"strategically planned and managed network of natural lands, such as forests and wetlands, working landscapes, and other open spaces that conserves or enhances ecosystem values and functions and provides associated benefits to human populations\"; and green infrastructure refers to an \"interconnected network of green spaces that conserves natural systems and provides assorted benefits to human populations\".\n\nSimilarly, the concept of ecological engineering generally refers to \"protecting, restoring (i.e. ecosystem restoration) or modifying ecological systems to increase the quantity, quality and sustainability of particular services they provide, or to build new ecological systems that provide services that would otherwise be provided through more conventional engineering, based on non-renewable resources\".\n\nThe International Union for the Conservation of Nature (IUCN) defines NBS as actions to protect, sustainably manage, and restore natural or modified ecosystems, that address societal challenges effectively and adaptively, simultaneously providing human well-being and biodiversity benefits, with climate change, food security, disaster risks, water security, social and economic development as well as human health being the common societal challenges.\n\nIUCN proposes to consider NBS as an umbrella concept. Categories and examples of NBS approaches according to IUCN include:\n\nThe general objective of NBS is clear, namely the sustainable management and use of nature for tackling societal challenges. However, different stakeholders view NBS from other perspectives. For instance, IUCN defines NBS as \"actions to protect, sustainably manage and restore natural or modified ecosystems, which address societal challenges effectively and adaptively, while simultaneously providing human well-being and biodiversity benefits\". This framing puts the need for well-managed and restored ecosystems at the heart of NBS, with the overarching goal of \"Supporting the achievement of society's development goals and safeguard human well-being in ways that reflect cultural and societal values and enhance the resilience of ecosystems, their capacity for renewal and the provision of services\". \n\nIn the context of the ongoing political debate on jobs and growth (main drivers of the current EU policy agenda), the European Commission underlines that NBS can transform environmental and societal challenges into innovation opportunities, by turning natural capital into a source for green growth and sustainable development. In their view, NBS to societal challenges are \"solutions that are inspired and supported by nature, which are cost-effective, simultaneously provide environmental, social and economic benefits and help build resilience. Such solutions bring more, and more diverse, nature and natural features and processes into cities, landscapes and seascapes, through locally adapted, resource-efficient and systemic interventions.\" \n\nThis framing is somewhat broader, and puts economy and social assets at the heart of NBS as importantly as sustaining environmental conditions. It shares similarities with the definition proposed by Maes and Jacobs (2015) defining NBS as \"any transition to a use of ES with decreased input of non-renewable natural capital and increased investment in renewable natural processes\". In their view, development and evaluation of NBS spans three basic requirements: (1) decrease of fossil fuel input per produced unit; (2) lowering of systemic trade-offs and increasing synergies between ES; and (3) increasing labor input and jobs. Here, nature is seen as a tool to inspire more systemic solutions to societal problems.\n\nWhatever definition used, promoting sustainability and the increased role of natural, self-sustained processes relying on biodiversity, are inherent to NBS. They constitute actions easily seen as positive for a wide range of stakeholders, as they bring about benefits at environmental, economic and social levels. As a consequence, the concept of NBS is gaining acceptance outside the conservation community (e.g. urban planning) and is now on its way to be mainstreamed into policies and programmes (climate change policy, law, infrastructure investment and financing mechanisms).\n\nIn 2015, the European network BiodivERsA mobilized a range of scientists, research donors and stakeholders and proposed a typology characterizing NBS along two gradients. 1. \"how much engineering of biodiversity and ecosystems is involved in NBS\", and 2. \"how many ecosystem services and stakeholder groups are targeted by a given NBS\". The typology highlights that NBS can involve very different actions on ecosystems (from protection to management and even creation of new ecosystems) and is based on the assumption that the higher the number of services and stakeholder groups targeted, the lower the capacity to maximize the delivery of each service and simultaneously fulfil the specific needs of all stakeholder groups. As such, three types of NBS are distinguished (Figure 2):\n\nType 1 NBS consists of no or minimal intervention in ecosystems, with the objectives of maintaining or improving the delivery of a range of ES both inside and outside of these conserved ecosystems. Examples include the protection of mangroves in coastal areas to limit risks associated to extreme weather conditions and provide benefits and opportunities to local populations; and the establishment of marine protected areas to conserve biodiversity within these areas while exporting biomass into fishing grounds. This type of NBS is connected to, for example, the concept of biosphere reserves which incorporates core protected areas for nature conservation and buffer zones and transition areas where people live and work in a sustainable way.\n\nType 2 NBS corresponds to management approaches that develop sustainable and multifunctional ecosystems and landscapes (extensively or intensively managed). These types improve the delivery of selected ES compared to what would be obtained with a more conventional intervention. Examples include innovative planning of agricultural landscapes to increase their multi-functionality; and approaches for enhancing tree species and genetic diversity to increase forest resilience to extreme events. This type of NBS is strongly connected to concepts like natural systems agriculture, agro-ecology, and evolutionary-orientated forestry.\n\nType 3 NBS consists of managing ecosystems in very extensive ways or even creating new ecosystems (e.g., artificial ecosystems with new assemblages of organisms for green roofs and walls to mitigate city warming and clean polluted air). Type 3 is linked to concepts like green and blue infrastructures and objectives like restoration of heavily degraded or polluted areas and greening cities.\n\nType 1 and 2 would typically fall within the IUCN NBS framework, whereas Type 2 and moreover Type 3 are often exemplified by EC for turning natural capital into a source for green growth and sustainable development.\n\nHybrid solutions exist along this gradient both in space and time. For instance, at landscape scale, mixing protected and managed areas could be needed to fulfil multi-functionality and sustainability goals. Similarly, a constructed wetland can be developed as a type 3 but, when well established, may subsequently be preserved and surveyed as a type 1.\n\nDemonstrating the benefits of nature and healthy ecosystems and showcasing the return on investment they can offer is necessary in order to increase awareness, but also to provide support and guidance on how to implement NBS. A large number of initiatives around the world already highlight the effectiveness of NBS approaches to address a wide range of societal challenges.\n\nThe following table shows examples from around the world:\n\nIn 2018, The Hindu reported that the East Kolkata wetlands, the world's largest organic sewage treatment facility had been used to clean the sewage of Kolkata in an organic manner by using algae for several decades. In use since the 1930s, the natural system was discovered by Dhrubajyoti Ghosh, an ecologist and a municipal engineer in the 1970s while working in the region. Ghosh worked for decades to protect the wetlands. It had been a practice in Kolkata, one of the five largest cities in India, for the municipal authorities to pump sewage into shallow ponds (\"bheris\"). Under the heat of the tropical sun, algae proliferated in them, converting the sewage into clean water, which in turn was used by villagers to grow paddy and vegetables. This system has been in use in the region since the 1930s and treats 750 million litres of wastewater per day, giving livelihood to 100,000 people in the vicinity. For his work, Ghosh was included in the UN Global 500 Roll of Honour in 1990 and received the Luc Hoffmann award in 2016.\n\nThere is currently no accepted basis on which a government agency, municipality or private company can systematically assess the efficiency, effectiveness and sustainability of a particular nature-based solution. However, a series of principles are proposed to guide effective and appropriate implementation, and thus to upscale NBS in practice. For example, NBS embrace and are not meant to replace nature conservation norms. Also, NBS are determined by site-specific natural and cultural contexts that include traditional, local and scientific knowledge. NBS are an integral part of the overall design of policies, and measure or actions, to address a specific challenges. Finally, NBS can be implemented alone or in an integrated manner with other solutions to societal challenges (e.g. technological and engineering solutions) and they are applied at the landscape scale.\n\nImplementing NBS requires political, economic, and scientific challenges to be tackled. First and foremost, private sector investment is needed, not to replace but to supplement traditional sources of capital such as public funding or philanthropy. The challenge is therefore to provide a robust evidence base for the contribution of nature to economic growth and jobs, and to demonstrate the economic viability of these solutions – compared to technological ones – on a timescale compatible with that of global change. Furthermore, it requires measures like adaptation of economic subsidy schemes, and the creation of opportunities for conservation finance, to name a few. Indeed, such measures will be needed to scale up NBS interventions, and strengthen their impact in mitigating the world's most pressing challenges.\n\nSince 2016, the EU is supporting a multi-stakeholder dialogue platform (called ThinkNature) to promote the co-design, testing and deployment of improved and innovative NBS in an integrated way. Creation of such science-policy-business-society interfaces could promote the market uptake of NBS. The project is part of the EU’s Horizon 2020 – Research and Innovation programme, and will last for 3 years. There are a total of 17 international partners involved, including the Technical University of Crete (Project Leader), the University of Helsinki and BiodivERsA.\n\nIn 2017, as part of the Presidency of the Estonian Republic of the Council of the European Union, a conference called “Nature-based Solutions: From Innovation to Common-use” was organized by the Ministry of the Environment of Estonia and the University of Tallinn. This conference aimed to strengthen synergies among various recent initiatives and programs related to NBS launched by the European Commission and by the EU Member States, focusing on policy and governance of NBS, and on research and innovation.\n\nIn recognition of the importance of natural ecosystems for mitigation and adaptation, the Paris Agreement calls on all Parties to acknowledge “the importance of the conservation and enhancement, as appropriate, of sinks and reservoirs of the greenhouse gases” and to “note the importance of ensuring the integrity of all ecosystems, including oceans, and the protection of biodiversity, recognized by some cultures as Mother Earth”. It then includes in its Articles several references to nature-based solutions. For example, Article 5.2 encourages Parties to adopt “…policy approaches and positive incentives for activities relating to reducing emissions from deforestation and forest degradation, and the role of conservation and sustainable management of forests and enhancement of forest carbon stocks in developing countries; and alternative policy approaches, such as joint mitigation and adaptation approaches for the integral and sustainable management of forests, while reaffirming the importance of incentivizing, as appropriate, non-carbon benefits associated with such approaches”. Article 7.1 further encourages Parties to build the resilience of socioeconomic and ecological systems, including through economic diversification and sustainable management of natural resources. In total, the Agreement refers to nature (ecosystems, natural resources, forests) in 13 distinct places. An in-depth analysis of all Nationally Determined Contributions submitted to UNFCCC, revealed that around 130 NDCs or 65% of signatories commit to nature-based solutions in their climate pledges, suggesting broad consensus for the role of nature in helping meet climate change goals. However, high-level commitments rarely translate into robust, measurable actions on-the-ground.\n\nThe term NBS was put forward by practitioners in the late 2000s (in particular the International Union for the Conservation of Nature and the World Bank) and thereafter by policymakers in Europe (most notably the European Commission). \n\nThe term \"nature-based solutions\" was first used in the late 2000s. It was used in the context of finding new solutions to mitigate and adapt to climate change effects, whilst simultaneously protecting biodiversity and improving sustainable livelihoods. \n\nThe IUCN referred to NBS in a position paper for the United Nations Framework Convention on Climate Change. The term was also adopted by European policymakers, in particular by the European Commission in a report stressing that NBS can offer innovative means to create jobs and growth as part of a green economy. The term started to make appearances in the mainstream media around the time of the Global Climate Action Summit in California in September 2018 \n\n\n"}
{"id": "13360851", "url": "https://en.wikipedia.org/wiki?curid=13360851", "title": "Nature center", "text": "Nature center\n\nA nature center (or nature centre) is an organization with a visitor center or interpretive center designed to educate people about nature and the environment. Usually located within a protected open space, nature centers often have trails through their property. Some are located within a state or city park, and some have special gardens or an arboretum. Their properties can be characterized as nature preserves and wildlife sanctuaries. Nature centers generally display small live animals, such as reptiles, rodents, insects, or fish. There are often museum exhibits and displays about natural history, or preserved mounted animals or nature dioramas. Nature centers are staffed by paid or volunteer naturalists and most offer educational programs to the general public, as well as summer camp, after-school and school group programs.\n\nSome nature centers allow free admission but collect voluntary donations in order to help offset expenses. They usually rely on support from dedicated volunteers.\n\nEnvironmental education centers differ from nature centers in that their museum exhibits and education programs are available mostly by appointment, although casual visitors may be allowed to walk on their grounds.\n\nSome city, state and national parks have facilities similar to nature centers, such as museum exhibits, dioramas and trails, and some offer park nature education programs, usually presented by a park ranger.\n\n"}
{"id": "18400577", "url": "https://en.wikipedia.org/wiki?curid=18400577", "title": "Naturhistorieselskabet", "text": "Naturhistorieselskabet\n\nNaturhistorieselskabet - the Society for Natural History - was a private society that was the only institution to offer education in natural history in Denmark in the late 18th century. The spirit of the Age of Enlightenment and an escalating agricultural crisis, led the king and the Danish elite to call foreign experts on economy, including botany and silviculture, to the country. The autonomous University of Copenhagen, on the other hand, was reluctant to employ foreign experts in little-established disciplines. Naturhistorieselskabet was formed in 1788 in order to ensure education in botany, zoology and mineralogy based on private funds. For example, Martin Vahl lectured in botany. After the appointment in 1795 of a professor in geology and in 1797 one in botany, the society gradually lost its importance. It was soon abolished and its collections donated to the state (much later united with the university collections).\n\nWagner, P.H. 2001. Institutionaliseringen af botanik og geologi i Danmark-Norge i det 18. århundrede (colloquium). Institut for Videnskabshistorie.\n"}
{"id": "1841288", "url": "https://en.wikipedia.org/wiki?curid=1841288", "title": "Outline of energy", "text": "Outline of energy\n\nThe following outline is provided as an overview of and topical guide to energy:\n\nEnergy – in physics, this is an indirectly observed quantity often understood as the ability of a physical system to do work on other physical systems. Since work is defined as a force acting through a distance (a length of space), energy is always equivalent to the ability to exert force (a pull or a push) against an object that is moving along a definite path of certain length.\n\n\nUnits of energy\n\n\nEnergy industry\n\nSee especially and for a large number of conventional energy related topics.\n\n\nHistory of energy\n\n\n\n\n\n\nEnergy economics\n\n\n\n\n\n\n"}
{"id": "18106574", "url": "https://en.wikipedia.org/wiki?curid=18106574", "title": "Political naturalism", "text": "Political naturalism\n\nPolitical naturalism is a minor political ideology and legal system which believes that there is a natural law, just and obvious to all, that crosses ideologies, faiths and personal thinking, that naturally guaranties justice. It is inspired by sociological naturalism, and scientific naturalism's belief that the precision of natural sciences can be applied to social sciences, and hence to practical social activities like politics and law.\n\nIt may be seen as a natural law-based version of legalism/constitutionalism (especially of prescriptive constitutionalism, in the way it tries, idealistically, to make a constitution how it should justly be), and it bears relation with many constitutional monarchies (as in that system they too believe in rule of the law and in certain things who are naturally correct (like monarchy, monarchic institutions and traditions.\n\nThe roots of this legal political ideology may be found in positive visions of natural law (like John Locke's and Rousseau's, and even in the Founding Fathers of the United States. The Catholic German Centre Party politician and diplomat Karl Friedrich von Savigny also thought so.\n\nIts main modern thinker is Egyptian legal scholar and creator of the Egyptian Civil Code Al-Razzak Al-Sanhuri. Through the Egyptian Code, many other Arab constitutions (in monarchist and pre-dictatorships Iraq and Libya and modern Qatar) ended up including political naturalist laws, and Al-Sanhuri himself wrote the Syrian and Jordanian civil codes and the Kuwaiti commercial code.\n"}
{"id": "1413688", "url": "https://en.wikipedia.org/wiki?curid=1413688", "title": "Primary energy", "text": "Primary energy\n\nPrimary energy (PE) is an energy form found in nature that has not been subjected to any human engineered conversion process. It is energy contained in raw fuels, and other forms of energy received as input to a system. Primary energy can be non-renewable or renewable.\n\nWhere primary energy is used to describe fossil fuels, the embodied energy of the fuel is available as thermal energy and around 70% is typically lost in conversion to electrical or mechanical energy. There is a similar 60-80% conversion loss when solar and wind energy is converted to electricity, but today's UN conventions on energy statistics counts the electricity made from wind and solar as the primary energy itself for these sources. One consequence of this counting method is that the contribution of wind and solar energy is under reported compared to fossil energy sources, and there is hence an international debate on how to count primary energy from wind and solar. \n\nTotal primary energy supply (TPES) is the sum of production and imports subtracting exports and storage changes.\n\nThe concept of primary energy is used in energy statistics in the compilation of energy balances, as well as in the field of energetics. In energetics, a primary energy source (PES) refers to the energy forms required by the energy sector to generate the supply of energy carriers used by human society.\n\nSecondary energy is a carrier of energy, such as electricity. These are produced by conversion from a primary energy source.\n\nThe use of primary energy as a measure ignores conversion efficiency. Thus forms of energy with poor conversion efficiency, particularly the thermal sources, coal, gas and nuclear are overstated, whereas energy sources such as hydroelectricity which are converted efficiently, while a small fraction of primary energy are significantly more important than their total raw energy supply may seem to imply.\n\nPE and TPES are better defined in the context of worldwide energy supply.\n\nPrimary energy sources should not be confused with the energy system components (or conversion processes) through which they are converted into energy carriers.\n\nPrimary energy sources are transformed in energy conversion processes to more convenient forms of energy that can directly be used by society, such as electrical energy, refined fuels, or synthetic fuels such as hydrogen fuel. In the field of energetics, these forms are called energy carriers and correspond to the concept of \"secondary energy\" in energy statistics.\n\nEnergy carriers are energy forms which have been transformed from primary energy sources. Electricity is one of the most common energy carriers, being transformed from various primary energy sources such as coal, oil, natural gas, and wind. Electricity is particularly useful since it has low entropy (is highly ordered) and so can be converted into other forms of energy very efficiently. District heating is another example of secondary energy.\n\nAccording to the laws of thermodynamics, primary energy sources cannot be produced. They must be available to society to enable the production of energy carriers.\n\nConversion efficiency varies. For thermal energy, electricity and mechanical energy production is limited by Carnot's theorem, and generates a lot of waste heat. Other non-thermal conversions can be more efficient. For example, while wind turbines do not capture all of the wind's energy, they have a high conversion efficiency and generate very little waste heat since wind energy is low entropy. In principle solar photovoltaic conversions could be very efficient, but current conversion can only be done well for narrow ranges of wavelength, whereas solar thermal is also subject to Carnot efficiency limits. Hydroelectric power is also very ordered, and converted very efficiently. The amount of usable energy is the exergy of a system.\n\nSite energy is the term used in North America for the amount of end-use energy of all forms consumed at a specified location. This can be a mix of primary energy (such as natural gas burned at the site) and secondary energy (such as electricity). Site energy is measured at the campus, building, or sub-building level and is the basis for energy charges on utility bills.\n\nSource energy, in contrast, is the term used in North America for the amount of primary energy consumed in order to provide a facility’s site energy. It is always greater than the site energy, as it includes all site energy and adds to it the energy lost during transmission, delivery, and conversion. While source or primary energy provides a more complete picture of energy consumption, it cannot be measured directly and must be calculated using conversion factors from site energy measurements. For electricity, a typical value is three units of source energy for one unit of site energy. However, this can vary considerably depending on factors such as the primary energy source or fuel type, the type of power plant, and the transmission infrastructure. One full set of conversion factors is available as technical reference from Energy STAR.\n\nEither site or source energy can be an appropriate metric when comparing or analyzing energy use of different facilities. The U.S Energy Information Administration, for example, uses primary (source) energy for its energy overviews but site energy for its Commercial Building Energy Consumption Survey and Residential Building Energy Consumption Survey. The US Environmental Protection Agency's Energy STAR program recommends using source energy, and the US Department of Energy uses site energy in its definition of a zero net energy building.\n\nEnergy accidents are accidents that occur in systems that provide energy or power. These can result in fatalities, as can the normal running of many systems, for example those deaths due to pollution.\n\nGlobally, coal is responsible for 100,000 deaths per trillion kWh.\n\n\n\n"}
{"id": "5764764", "url": "https://en.wikipedia.org/wiki?curid=5764764", "title": "Radiation trapping", "text": "Radiation trapping\n\nRadiation trapping, imprisonment of resonance radiation, radiative transfer of spectral lines, line transfer or radiation diffusion is a phenomenon in physics whereby radiation may be \"trapped\" in a system as it is emitted by one atom and absorbed by another.\n"}
{"id": "1494813", "url": "https://en.wikipedia.org/wiki?curid=1494813", "title": "Ramsauer–Townsend effect", "text": "Ramsauer–Townsend effect\n\nThe Ramsauer–Townsend effect, also sometimes called the Ramsauer effect or the Townsend effect, is a physical phenomenon involving the scattering of low-energy electrons by atoms of a noble gas. Since its explanation requires the wave theory of quantum mechanics, it demonstrates the need for physical theories more sophisticated than those of Newtonian physics.\n\nWhen an electron moves through a gas, its interactions with the gas atoms cause scattering to occur. These interactions are classified as \"inelastic\" if they cause excitation or ionization of the atom to occur and \"elastic\" if they do not.\n\nThe \"probability of scattering\" in such a system is defined as the number of electrons scattered, per unit electron current, per unit path length, per unit pressure at 0 °C, per unit solid angle. The \"number of collisions\" equals the total number of electrons scattered elastically and inelastically in all angles, and the \"probability of collision\" is the total number of collisions, per unit electron current, per unit path length, per unit pressure at 0 °C.\n\nBecause noble gas atoms have a relatively high first ionization energy and the electrons do not carry enough energy to cause excited electronic states, ionization and excitation of the atom are unlikely, and the probability of elastic scattering over all angles is approximately equal to the probability of collision.\n\nThe effect is named for Carl Ramsauer (1879-1955) and John Sealy Townsend (1868-1957), who each independently studied the collisions between atoms and low-energy electrons in the early 1920s.\n\nIf one tries to predict the probability of collision with a classical model that treats the electron and atom as hard spheres, one finds that the probability of collision should be independent of the incident electron energy (see Kukolich). However, Ramsauer and Townsend observed that for slow-moving electrons in argon, krypton, or xenon, the probability of collision between the electrons and gas atoms obtains a minimum value for electrons with a certain amount of kinetic energy (about 1 electron volts for xenon gas). This is the Ramsauer–Townsend effect.\n\nNo good explanation for the phenomenon existed until the introduction of quantum mechanics, which explains that the effect results from the wave-like properties of the electron. A simple model of the collision that makes use of wave theory can predict the existence of the Ramsauer-Townsend minimum. Bohm presents one such model that considers the atom as a finite square potential well.\n\nPredicting from theory the kinetic energy that will produce a Ramsauer-Townsend minimum is quite complicated since the problem involves understanding the wave nature of particles. However, the problem has been extensively investigated both experimentally and theoretically and is well understood (see Johnson and Guet).\n\nIn 1970 Gryzinski has proposed classical explanation of Ramsauer effect using effective picture of atom as oscillating multipole of electric field (dipole, quadrupole, octupole), which was a consequence of his free-fall atomic model.\n\n"}
{"id": "44120129", "url": "https://en.wikipedia.org/wiki?curid=44120129", "title": "Revolving rivers", "text": "Revolving rivers\n\nRevolving rivers are a surprising, uncommon way of sand pile growth that can be found in a few sands around the world, but has been studied in detail only for one Cuban sand from a place called Santa Teresa (Pinar del Rio province).\n\nWhen pouring \"revolving\" sand on a flat surface from a fixed position, the growth of a conical pile does not occur by the common avalanche mechanism, where sand slides down the pile in a more or less random fashion. What happens in that a relatively thin \"river\" of flowing sand travels from the pouring point at the apex of the pile to its base, while the rest of the sand at the surface is static. In addition, the river \"revolves\" around the pile either in clockwise or counter-clockwise directions (looking from top) depending on the initial conditions of the experiment. Actually the river constitutes the \"cutting edge\" of a layer of sand that deposits as a helix on the conical pile, and makes it grow.\nFor small sandpiles, rivers are continuous, but they become intermittent\nfor larger piles.\n\nThe phenomenon was observed first by E. Altshuler at the University of Havana in 1995, but at the time he assumed that it was well known, and temporarily forgot about it. In 2000, being at the University of Houston, he told K. E. Bassler, who showed a vivid interest in the matter. Embarrassingly enough, Altshuler was unable to demonstrate it before Bassler using a random sand from Houston, so he had to send him a video from Cuba after his return to the island.\n\nOnce the existence of the strange phenomenon was confirmed for everyone, E. Altshuler and a number of collaborators performed a systematic study in Havana, which was then jointly published with Bassler.\nFurther work has been done to understand in more detail the\nphenomenon, and it has been found in other sands from different parts of the world. \nHowever, the connection between the physical, chemical (and possibly biological) properties of the grains in a specific sand, the nature of the inter-grain interactions, and the emergence of the revolving rivers is still an open question.\n\nSand from Santa Teresa is made of almost pure silicon dioxide grains with an average grain size of 0.2 mm approximately and no visible special features regarding grain shape. But in spite of its apparent simplicity, many puzzles still remain. For example, after many experiments one batch of sand may stop showing revolving rivers (just as singing sand eventually stops singing), which suggests that the decay is connected to certain properties of the surface of the grains that degrade by continued friction.\n\nVideos of the effect are available on YouTube.\n"}
{"id": "46324244", "url": "https://en.wikipedia.org/wiki?curid=46324244", "title": "Skeletal changes of organisms transitioning from water to land", "text": "Skeletal changes of organisms transitioning from water to land\n\nInnovations conventionally associated with terrestrially first appeared in aquatic elpistostegalians such as \"Panderichthys rhombolepis\", \"Elpistostege watsoni\", and \"Tiktaalik roseae\". Phylogenetic analyses distribute the features that developed along the tetrapod stem and display a stepwise process of character acquisition, rather than abrupt. The complete transition occurred over a period of 25 million years beginning with the tetrapodomorph diversification in the Middle Devonian (380 myr).\n\nBy the Upper Devonian period, the fin-limb transition as well as other skeletal changes such as gill arch reduction, opercular series loss, mid-line fin loss, and scale reduction were already completed in many aquatic organisms. As aquatic tetrapods began their transition to land, several skeletal changes are thought to have occurred to allow for movement and respiration on land. Some adaptations required to adjust to non-aquatic life include the movement and use of alternating limbs, the use of pelvic appendages as sturdy propulsors, and the use of a solid surface at the organism’s base to generate propulsive force required for walking.\n\nThe Osteolepiformes and Elpistostegalia are two crown groups of rhipidistians with respect to the tetrapods. The development of skull roof and cheekbone patterns in these organisms match those found in the first tetrapods. Palatal and nasal skeletal features like choanae are present in these groups and are also observed in modern amphibians. This indicates that incipient air breathing was developed, as well as modification of the hyoid arch towards stapes development. These characteristics account for why osteichthyans are accepted as the sister group of tetrapods.\n\nThe elpistostegalid fish are considered the most apomorphic of fish in comparison to tetrapods. From well-preserved fossils, it is observed that they share a paltybasic skull with eye ridges, and external nares situated on the margin of the mouth. Development of eye ridges and flatting of the skull are also observed in primitive fossil amphibians and reptiles. The most likely reason for the traits to be adaptive was for their use in aerial vision above the waterline. The traits enabled animals to check area on land for safe spots if being chased by a predator in water, as well as being useful for searching for prey items above the water. The water-based lateral line system was used substantially by these aquatic tetrapods to detect danger from predators. Within the Osteichthyan diversification, there were no changes related to respiration in the transition as can be seen by the nasal region and palatal morphology in elpistostegalid fishes. The primary change from basic ostelepiform ancestors to the first elpistostegalid in the middle Devonian was to the pre-existing roof skulls.\n\nIn \"Elginerpeton pancheni\", a prototetrapod from the late Frasnian, basic tetrapod characteristics in the lower jaw and the cranium are observed. The taxon is believed to fill the gap between elpistostegalid fishes and well-preserved Devonian tetrapods. The \"Elginerpeton\" is considered more derived than the elpistostegalid fishes due to presence of paired fangs on the parasymphysial toothplate, a slender shaped anterior coronoid, and in the loss of the intracranial joint and coronoid fossa. The loss of the intercranial joint was a direct functional necessity to strengthen the broad and long platybasic skull when the animal was out of the water. The tubular lower jaw of the \"Elginerpeton\", compared to the flat-lamina jaw shape of fishes gave it superior cross-sectional force, required when not supported in an aquatic setting – allowing for opening of the mouth outside of water. The adaptation may also be interpreted as a specialization for buccopharyngeal breathing. It is speculated to be the first step towards aerial respiration in the transition from fish to tetrapod.\n\nIn the tetrapod and higher clades from the lower-middle Famennian there are several defining changes on the basis of anatomy of \"Ichthyostega\", \"Tulerpeton\", and \"Acanthostega\". In the cranium, there is a stapes derived from the hyomandibular of fishes; a single bilateral pair of nasal bones, and a fenestra ovalis in the otic capsule of the braincase. The opening of the otic wall of the braincase can be considered a paedomorphic feature for tetrapods and is linked to the stapes functionally. The stapes was thought to be just a structural support between the palate and the stapedial plate of the braincase. In the \"Acanthostega\", it is likely that due to the otic capsule of the brain case being mesial to the stapedial plate, sound was picked up from the palate or the otic notch to allow for rudimentary hearing. It was able to perceive vibrations by opening its mouth by way of the palate. Other factors that caused aquatic tetrapods to spend more time on land caused the development of terrestrial hearing with the development of a tympanum within an otic notch and developed by convergent evolution at least three times.\nThere was also a change in the dermal bones of the skull in the aquatic tetrapods. It involved the enlargement of the jugal, ceasing the contact of the maxilla with the squamosal and the single bilateral pair of nasal bones. The feature allows for a stronger bite as well as increasing the strength of the skull.\n\nFeeding on land is a completely different task than feeding in water. Water is much more dense and viscous compared to air, causing hunting techniques adapted in water to be less successful when applied on land. The main technique used in water is suction feeding and is used by most aquatic vertebrates. This technique does not function in air so animals use methods of overtaking prey with jaws followed by biting down. Transitional forms prior to fully developed terrestrial tetrapods such as \"Acanthostega\", are thought to have captured prey in the water. Large coronoid fangs are present in the fishes \"Eusthenopteron\", \"Panderichthys\", and \"Tiktaalik\", and the early tetrapod, \"Ventasega\". In \"Acanthostega\", which is more derived, the large teeth are absent. In \"Eusthenopetron\" and \"Panderichthys\", an ossified operculum is exhibited unlike in the \"Tiktaalik\", \"Ventastega\", and \"Acanthostega\". These differences as well as reductions of the gill chamber and changes in the nature of the lower jaw are hypothesized to indicate a reduced reliance on suction feeding in early tetrapods in comparison to osteolepiform fish. This morphological data is not enough however to prove that suction feeding was less used as the morphological changes have been found in fish that use the suction feeding mechanism.\n\nCranial sutures are indicators of skull function and morphologies can be linked to specific feeding modes. Transitional feeding changes can be observed by examining cross sectional morphology of a suture in taxa of the fish-tetrapod transition. Comparing positionally comparable sutures in extant fish allows for the creation of a sutural morphospace. The main cause of sutural deformation is caused by strain during feeding activity, most prominent with feeding mechanisms involving sucking a prey into the mouth. There is a tension anteriorly, and compression posteriorly strain patterns are observed in \"Polypterus\", a prey-sucking predator. In terrestrial tetrapod \"Phonerpeton\", there is compression between the frontals and parietals and a complex loading between the post parietals. There is no evidence of tensile strain in any sutures. \"Acanthostega\" fossil records demonstrate that no strain pattern was exhibited that relate to prey capture by means of suction. The load compression is similar to extant tetrapods. It is most likely that the organism captured prey by biting in the water or near the edge of the water. This finding indicates that the terrestrial mode of feeding first emerged in an aquatic environment.\n\nThe cranial endoskeleton of \"T. roseae\" shares derived features with tetrapods. There was a loss of opercular and extrascapular elements, enhancing head mobility in \"T. roseae\" compared to other tetrapodomorph fish. The formation of the neck allowed for locomotion in shallow waters. This environment allows for less motility compared to the three-dimensional space that fish are able to orient themselves in. The body of the organism in these environments would be fixed in the shallow pools with appendages planted on a substrate.\n\nIn the \"Acanthostega\" and \"Ichthyostega\", which are considered to be more derived than other basal aquatic tetrapods, the pectoral girdle is decoupled from the skull. There is also a loss of the dorsal pectoral girdle bones, which permits a large degree of movement for the shoulder. This allowed for a greater degree of movement, and is a necessity for improving aquatic maneuveurs and terrestrial locomotion. This could have been driven by the need to lift the head to aid aerial respiration by using nostrils and choanae.\n\nLimbs in vertebrates are occasionally organized into stylopod (relating to the humerus and femur), zeugopod (relating to the radius and tibia, along with associated structures) and autopod (relating to digits) categories, although anatomically, the evolutionary differences between these groups in early tetrapods tends to be vague.\nThe transition from fins to limbs occurred once an endoskeleton entered the base of the fin, as seen in today's lungfish. This is thought to have originated in the group Sarcopterygians, including osteolipiforms like \"Eusthenopteron\", due to the homology of the tetrapod forelimb and the osteolepiform fin endoskeleton.\n\n\"Acanthostega\" is a partially aquatic tetrapod with developed limbs that shares features common with the earlier tetrapods, \"Panderichthys\" and \"Eusthenopteron\". Like \"Panderichthys\", the humerus of \"Acanthostega\" is flattened dorso-ventrally, the intermedium terminates level with the radius, and the endoskeleton can be divided into stylopodium, zeugopodium and autopodium segments. Similar to \"Eusthenopteron\", the radials do not articulate with the radius on the distal end. \"Acanthostega\" also has a 1:2 ratio of humerus to radius and ulna, a feature seen in all tetrapods higher than \"Acanthostega\" on the phylogeny.\n\nUnlike \"Panderichthys\", \"Acanthostega\" hind limbs are at least the size of its fore limbs, if not larger. This development of larger limbs is required to physically support the organism during emergence from an aquatic setting to land. The humerus and femur of \"Acanthostega\" also contain evidence of greater development of the appendicular muscles compared to more aquatic tetrapods, hinting at the presence of digits.\n\nSimilarly, \"Ossinodus\" has two hindlimbs located bilaterally and proximodistally aymmetrical. Due to the presence of a small femur during juvenile development, this Carboniferous- period tetrapod is thought to be aquatic during juvenile development; only emerging onto land once it reaches adulthood. \"Ossinodus\" also has a broad, flat tibia, akin to \"Acanthostega\", and is thought to be only partially terrestrial.\n\nThe development of the pelvic region was crucial for the adaptation from water to land, yet some features of tetrapod locomotion are thought to have arose before the origin of digited limbs or the transition from water to land. The fossil record of early tetrapods shows evidence of distinct pelvic development occurring in osteolepiforms, further supporting osteolepiform ancestry of terrestrial tetrapods.\n\n\"Acanthostega\" has a large pelvis, with the iliac region articulating with the axial skeleton and a broad ischial plate. It has a sacrum; a fundamental skeletal feature that allows the organism to transfer force produced in its hindlimbs to its axial skeleton, and move in a terrestrial environment. A pubo-ischiadic symphysis is also observed, uniting the two pelvic halves.\n\nIn contrast, \"Protopterus annectens\" (a member of lungfish, thought to be a sister group to tetrapods) has a small, anatomically simpler pelvis, a derived limb endoskeleton and a lack of digits. Yet, it shares the ability to lift itself using a solid surface as a base with its pelvic region with \"Acanthostega\" and is also observed to move with tetrapod-like locomotion in an aquatic environment. This illustrates that a fundamental innovation in tetrapods is also found in a lower, sister taxon, in which members lack a sacrum.\n\n\"Acanthostega\" is the earliest example of a digitized tetrapod. The humerus and femur of \"Acanthostega\" contain evidence of greater development of the appendicular muscles compared to more aquatic tetrapods. \"Acanthostega\" has a total lack of dermal fin rays and displays the presence of two or more spool-shaped bones or cartilages articulating individually in antero-posterial sets on the distal end of its limbs. This feature can now be distinguished as digits instead of the endoskeletal radials seen in earlier tetrapods.\n\n\"Pederpes\", a tetrapod from the Early Carboniferous period, also has hindlimbs containing 5 digits that are rotated to face anteriorly. Unlike previous tetrapods, who have been only partially adapted to land, \"Pederpes\" has the novel ability to bend its limbs and propel itself forwards in a terrestrial setting. This is attributed to the symmetry of the digits and limbs in \"Pederpes\", allowing it to rotate its hindlimbs to an anteriorly facing position and propel itself from the edge of the foot when moving forward. This morphological development of bendable wrists and ankles can distinguish \"Pederpes\" the first true terrestrial tetrapod.\n"}
{"id": "46594", "url": "https://en.wikipedia.org/wiki?curid=46594", "title": "Straw", "text": "Straw\n\nStraw is an agricultural byproduct consisting of the dry stalks of cereal plants after the grain and chaff have been removed. It makes up about half of the yield of cereal crops such as barley, oats, rice, rye and wheat. It has a number of different uses, including fuel, livestock bedding and fodder, thatching and basket making.\n\nStraw is usually gathered and stored in a straw bale, which is a bale, or bundle, of straw tightly bound with twine or wire. Straw bales may be square, rectangular, or round, and can be very large, depending on the type of baler used.\n\nCurrent and historic uses of straw include:\n\n\n\nDried straw presents a fire hazard that can ignite easily if exposed to sparks or an open flame. It can also trigger allergic rhinitis in people who are hypersensitive to airborne allergens such as straw dust.\n\nIn addition to its current and historic uses, straw is being investigated as a source of fine chemicals including alkaloids, flavonoids, lignins, phenols, and steroids.\n\n\n"}
{"id": "13680444", "url": "https://en.wikipedia.org/wiki?curid=13680444", "title": "Streaming vibration current", "text": "Streaming vibration current\n\nThe streaming vibration current (SVI) and the associated streaming vibration potential is an electric signal that arises when an acoustic wave propagates through a porous body in which the pores are filled with fluid.\n\nStreaming vibration current was experimentally observed in 1948 by M. Williams. A theoretical model was developed some 30 years later by Dukhin and coworkers. This effect opens another possibility for characterizing the electric properties of the surfaces in porous bodies.\n\n"}
{"id": "29529485", "url": "https://en.wikipedia.org/wiki?curid=29529485", "title": "The Windward Road", "text": "The Windward Road\n\nThe Windward Road: Adventures of a Naturalist on Remote Caribbean Shores, was written by Archie Carr and originally published in 1956. It is an account of Dr. Carr's travels around the Caribbean to study sea turtles and their migratory and behavior patterns, especially Kemp's ridley, a species about which little was known at the time. This book led to the formation of The Brotherhood of the Green Turtle, which later became the Caribbean Conservation Corporation, and is now known as the Sea Turtle Conservancy. It was awarded the 1957 John Burroughs Medal for nature writing, which is awarded annually by the American Museum of Natural History. The chapter entitled \"The Black Beach\", originally published in Mademoiselle, won a 1956 O. Henry Award.\n"}
{"id": "31880880", "url": "https://en.wikipedia.org/wiki?curid=31880880", "title": "Theoretical foundations of evolutionary psychology", "text": "Theoretical foundations of evolutionary psychology\n\nThe theoretical foundations of evolutionary psychology are the general and specific scientific theories that explain the ultimate origins of psychological traits in terms of evolution. These theories originated with Charles Darwin's work, including his speculations about the evolutionary origins of social instincts in humans. Modern evolutionary psychology, however, is possible only because of advances in evolutionary theory in the 20th century.\n\nEvolutionary psychologists say that natural selection has provided humans with many psychological adaptations, in much the same way that it generated humans' anatomical and physiological adaptations. As with adaptations in general, psychological adaptations are said to be specialized for the environment in which an organism evolved, the environment of evolutionary adaptedness, or EEA. Sexual selection provides organisms with adaptations related to mating. For male mammals, which have a relatively fast reproduction rate, sexual selection leads to adaptations that help them compete for females. For female mammals, with a relatively slow reproduction rate, sexual selection leads to choosiness, which helps females select higher quality mates. Charles Darwin described both natural selection and sexual selection, but he relied on group selection to explain the evolution of self-sacrificing behavior. Group selection is a weak explanation because in any group the less self-sacrificing animals will be more likely to survive and the group will become less self-sacrificing.\n\nIn 1964, William D. Hamilton proposed inclusive fitness theory, emphasizing a \"gene's-eye\" view of evolution. Hamilton noted that individuals can increase the replication of their genes into the next generation by helping close relatives with whom they share genes survive and reproduce. According to \"Hamilton's rule\", a self-sacrificing behavior can evolve if it helps close relatives so much that it more than compensates for the individual animal's sacrifice. Inclusive fitness theory resolved the issue of how \"altruism\" evolved. Other theories also help explain the evolution of altruistic behavior, including evolutionary game theory, tit-for-tat reciprocity, and generalized reciprocity. These theories not only help explain the development of altruistic behavior but also account for hostility toward cheaters (individuals that take advantage of others' altruism).\n\nSeveral mid-level evolutionary theories inform evolutionary psychology. The r/K selection theory proposes that some species prosper by having many offspring while others follow the strategy of having fewer offspring but investing much more in each one. Humans follow the second strategy. Parental investment theory explains how parents invest more or less in individual offspring based on how successful those offspring are likely to be, and thus how much they might improve the parents' inclusive fitness. According to the Trivers-Willard hypothesis, parents in good conditions tend to invest more in sons (who are best able to take advantage of good conditions), while parents in poor conditions tend to invest more in daughters (who are best able to have successful offspring even in poor conditions). According to life history theory, animals evolve life histories to match their environments, determining details such as age at first reproduction and number of offspring. Dual inheritance theory posits that genes and human culture have interacted, with genes affecting the development of culture and culture, in turn, affecting human evolution on a genetic level (see also the Baldwin effect).\n\nCritics of evolutionary psychology have sometimes challenged its theoretical underpinnings, saying that humans never developed powerful social instincts through natural selection and that the hypotheses of evolutionary psychologists are merely just-so-stories.\n\nEvolutionary psychology primarily uses the theories of natural selection, sexual selection, and inclusive fitness to explain the evolution of psychological adaptations.\n\nEvolutionary psychology is sometimes seen not simply as a subdiscipline of psychology but as a metatheoretical framework in which \"the entire field of psychology can be examined.\"\n\nEvolutionary psychologists consider Charles Darwin's theory of natural selection to be important to an understanding of psychology. Natural selection occurs because individual organisms who are genetically better suited to the current environment leave more descendants, and their genes spread through the population, thus explaining why organisms fit their environments so closely. This process is slow and cumulative, with new traits layered over older traits. The advantages created by natural selection are known as adaptations. Evolutionary psychologists say that animals, just as they evolve physical adaptations, evolve psychological adaptations.\n\nEvolutionary psychologists emphasize that natural selection mostly generates specialized adaptations, which are more efficient than general adaptations. They point out that natural selection operates slowly, and that adaptations are sometimes out of date when the environment changes rapidly. In the case of humans, evolutionary psychologists say that much of human nature was shaped during the stone age and may not match the contemporary environment.\n\nSexual selection favors traits that provide mating advantages, such as the peacock's tail, even if these same traits are usually hindrances. Evolutionary psychologists point out that, unlike natural selection, sexual selection typically leads to the evolution of sex differences. Sex differences typically make reproduction faster for one sex and slower for the other, in which case mates are relatively scarce for the faster sex. Sexual selection favors traits that increase the number of mates for the fast sex and the quality of mates for the slow sex. For mammals, the female has the slower reproduction rate. Males typically evolve either traits to help them fight other males or traits to impress females. Females typically evolve greater abilities to discern the qualities of males, such as choosiness in mating.\n\nInclusive fitness theory, proposed by William D. Hamilton, emphasized a \"gene's-eye\" view of evolution. Hamilton noted that what evolution ultimately selects are genes, not groups or species. From this perspective, individuals can increase the replication of their genes into the next generation not only directly via reproduction, by also indirectly helping close relatives with whom they share genes survive and reproduce. General evolutionary theory, in its modern form, \"is\" essentially inclusive fitness theory.\n\nInclusive fitness theory resolved the issue of how \"altruism\" evolved. The dominant, pre-Hamiltonian view was that altruism evolved via group selection: the notion that altruism evolved for the benefit of the group. The problem with this was that if one organism in a group incurred any fitness costs on itself for the benefit of others in the group, (i.e. acted \"altruistically\"), then that organism would reduce its own ability to survive and/or reproduce, therefore reducing its chances of passing on its altruistic traits.\n\nFurthermore, the organism that benefited from that altruistic act and only acted on behalf of its own fitness would increase its own chance of survival and/or reproduction, thus increasing its chances of passing on its \"selfish\" traits.\nInclusive fitness resolved \"the problem of altruism\" by demonstrating that altruism can evolve via kin selection as expressed in Hamilton's rule:\ncost < relatedness × benefit\nIn other words, altruism can evolve as long as the fitness \"cost\" of the altruistic act on the part of the actor is less than the \"degree of genetic relatedness\" of the recipient times the fitness \"benefit\" to that recipient.\nThis perspective reflects what is referred to as the gene-centered view of evolution and demonstrates that group selection is a very weak selective force.\n\nMiddle-level evolutionary theories are consistent with general evolutionary theory, but focus on certain domains of functioning (Buss, 2011) Specific evolutionary psychology hypotheses may be derivative from a mid-level theory (Buss, 2011). Three very important middle-level evolutionary theories were contributed by Robert Trivers as well as Robert MacArthur and E. O. Wilson\n\n"}
{"id": "1945275", "url": "https://en.wikipedia.org/wiki?curid=1945275", "title": "Wigner effect", "text": "Wigner effect\n\nThe Wigner effect (named for its discoverer, Eugene Wigner), also known as the discomposition effect or Wigner's Disease, is the dislocation of atoms in a solid caused by neutron radiation. \n\nAny solid can display the Wigner effect. The effect is of most concern in neutron moderators, such as graphite, intended to reduce the speed of fast neutrons, thereby turning them into thermal neutrons capable of sustaining a nuclear chain reaction involving uranium-235.\n\nTo create the Wigner effect, neutrons that collide with the atoms in a crystal structure must have enough energy to displace them from the lattice. This amount (threshold displacement energy) is approximately 25 eV. A neutron's energy can vary widely, but it is not uncommon to have energies up to and exceeding 10 MeV (10,000,000 eV) in the centre of a nuclear reactor. A neutron with a significant amount of energy will create a displacement cascade in a matrix via elastic collisions. For example, a 1 MeV neutron striking graphite will create 900 displacements; not all displacements will create defects, because some of the struck atoms will find and fill the vacancies that were either small pre-existing voids or vacancies newly formed by the other struck atoms.\n\nThe atoms that do not find a vacancy come to rest in non-ideal locations; that is, not along the symmetrical lines of the lattice. These atoms are referred to as interstitial atoms, or simply interstitials. An interstitial atom and its associated vacancy are known as a Frenkel defect. Because these atoms are not in the ideal location, they have an energy associated with them, much as a ball at the top of a hill has gravitational potential energy. This energy is referred to as Wigner energy. When a large number of interstitials have accumulated, they pose a risk of releasing all of their energy suddenly, creating a rapid, very great increase in temperature. Sudden, unplanned increases in temperature can present a large risk for certain types of nuclear reactors with low operating temperatures; one such was the indirect cause of the Windscale fire. Accumulation of energy in irradiated graphite has been recorded as high as 2.7 kJ/g, but is typically much lower than this. Graphite, having a heat capacity of 0.720 J/g°C, could see a sudden increase in temperature of about 3750 °C (6780 °F).\n\nDespite some reports, Wigner energy buildup had nothing to do with the cause of the Chernobyl disaster: this reactor, like all contemporary power reactors, operated at a high enough temperature to allow the displaced graphite structure to realign itself before any potential energy could be stored. Wigner energy may have played some part following the prompt critical neutron spike, when the accident entered the graphite fire phase of events.\n\nA buildup of Wigner energy can be relieved by heating the material. This process is known as annealing. In graphite this occurs at 250 °C.\n\nIn 2003, it was postulated that Wigner energy can be stored by the formation of metastable defect structures in graphite. Notably, the large energy release observed at 200–250 °C has been described in terms of a metastable interstitial-vacancy pair. The interstitial atom becomes trapped on the lip of the vacancy, and there is a barrier for it to recombine to give perfect graphite.\n\n"}
{"id": "49801999", "url": "https://en.wikipedia.org/wiki?curid=49801999", "title": "Wild Seasons (Kay Young)", "text": "Wild Seasons (Kay Young)\n\nWild Seasons: Gathering and Cooking Wild Plants of the Great Plains is a 1993 non-fiction book by author, illustrator, and ethno-botanist Kay Young. It features a variety of wild plants of the great plains area and how to prepare them in appetizing ways. The book includes a number of recipes as well as Young's enthusiasm and advocacy for eating wild crops. It was published by the University of Nebraska Press.\n\n"}
{"id": "18365403", "url": "https://en.wikipedia.org/wiki?curid=18365403", "title": "Wildland fire emission", "text": "Wildland fire emission\n\nWildland fire and wildland fire atmospheric emissions have been a part of the global biosphere for millennia. The major wildland fire emissions include greenhouse gasses and several criteria pollutants that impact human health and welfare.:\nCompared to the preindustrial era, wildland land fire in the conterminous U.S. has been reduced 90 percent with proportional reductions in wildland fire emissions. Land use changes (agriculture and urbanization) are responsible for roughly 50 percent of this decrease, and land management decisions (land fragmentation, suppression actions, etc.) are responsible for the remainder. Anthropogenic activities (e.g., industrial production, transportation, agriculture, etc.) today have more than replaced the lost preindustrial wildland fire atmospheric emissions.\n\nThe following charts compare preindustrial wildland fire emissions with contemporary emissions.\n"}
