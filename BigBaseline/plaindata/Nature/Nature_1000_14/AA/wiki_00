{"id": "1260420", "url": "https://en.wikipedia.org/wiki?curid=1260420", "title": "Anti-predator adaptation", "text": "Anti-predator adaptation\n\nAnti-predator adaptations are mechanisms developed through evolution that assist prey organisms in their constant struggle against predators. Throughout the animal kingdom, adaptations have evolved for every stage of this struggle, namely by avoiding detection, warding off attack, fighting back, or escaping when caught.\n\nThe first line of defence consists in avoiding detection, through mechanisms such as camouflage, masquerade, apostatic selection, living underground, or nocturnality. \n\nAlternatively, prey animals may ward off attack, whether by advertising the presence of strong defences in aposematism, by mimicking animals which do possess such defences, by startling the attacker, by signalling to the predator that pursuit is not worthwhile, by distraction, by using defensive structures such as spines, and by living in a group. Members of groups are at reduced risk of predation, despite the increased conspicuousness of a group, through improved vigilance, predator confusion, and the likelihood that the predator will attack some other individual.\n\nSome prey species are capable of fighting back against predators, whether with chemicals, through communal defence, or by ejecting noxious materials. Many animals can escape by fleeing rapidly, outrunning or outmanoeuvring their attacker. \n\nFinally, some species are able to escape even when caught by sacrificing certain body parts: crabs can shed a claw, while lizards can shed their tails, often distracting predators long enough to permit the prey to escape.\n\nAnimals may avoid becoming prey by living out of sight of predators, whether in caves, underground, or by being nocturnal. Nocturnality is an animal behavior characterized by activity during the night and sleeping during the day. This is a behavioral form of detection avoidance called crypsis used by animals to either avoid predation or to enhance prey hunting. Predation risk has long been recognized as critical in shaping behavioral decisions. For example, this predation risk is of prime importance in determining the time of evening emergence in echolocating bats. Although early access during brighter times permits easier foraging, it also leads to a higher predation risk from bat hawks and bat falcons. This results in an optimum evening emergence time that is a compromise between the conflicting demands.\nAnother nocturnal adaptation can be seen in kangaroo rats, which avoid moonlight. They forage in relatively open habitats and reduce their activity outside their nest burrows in response to moonlight. During a full moon, they shift their activity towards areas of relatively dense cover to compensate for the extra brightness.\n\nCamouflage uses any combination of materials, coloration, or illumination for concealment to make the organism hard to detect by sight. It is common in both terrestrial and marine animals. Camouflage can be achieved in many different ways, such as through resemblance to surroundings, disruptive coloration, shadow elimination by countershading or counter-illumination, self-decoration, cryptic behavior, or changeable skin patterns and colour. Animals such as the flat-tail horned lizard of North America have evolved to eliminate their shadow and blend in with the ground. The bodies of these lizards are flattened, and their sides thin towards the edge. This body form, along with the white scales fringed along their sides, allows the lizards to effectively hide their shadows. In addition, these lizards hide any remaining shadows by pressing their bodies to the ground.\n\nAnimals can hide in plain sight by masquerading as inedible objects. For example, the potoo, a South American bird, habitually perches on a tree, convincingly resembling a broken stump of a branch, while a butterfly, \"Kallima\", looks just like a dead leaf.\n\nAnother way to remain unattacked in plain sight is to look different from other members of the same species. Predators such as tits selectively hunt for abundant types of insect, ignoring less common types that were present, forming search images of the desired prey. This creates a mechanism for negative frequency-dependent selection, apostatic selection.\n\nMany species make use of behavioral strategies to deter predators.\n\nMany weakly-defended animals, including moths, butterflies, mantises, phasmids, and cephalopods such as octopuses, make use of patterns of threatening or startling behaviour, such as suddenly displaying conspicuous eyespots, so as to scare off or momentarily distract a predator, thus giving the prey animal an opportunity to escape. In the absence of toxins or other defences, this is essentially bluffing, in contrast to aposematism which involves honest signals.\n\nPursuit-deterrent signals are behavioral signals used by prey that convince predators not to pursue them. For example, gazelles stot, jumping high with stiff legs and an arched back. This is thought to signal to predators that they have a high level of fitness and can outrun the predator. As a result, predators may choose to pursue a different prey that is less likely to outrun them.\nWhite-tailed deer and other prey mammals flag with conspicuous (often black and white) tail markings when alarmed, informing the predator that it has been detected.\nWarning calls given by birds such as the Eurasian jay are similarly honest signals, benefiting both predator and prey: the predator is informed that it has been detected and might as well save time and energy by giving up the chase, while the prey is protected from attack.\n\nAnother pursuit-deterrent signal is thanatosis or playing dead. Thanatosis is a form of bluff in which an animal mimics its own dead body, feigning death to avoid being attacked by predators seeking live prey. Thanatosis can also be used by the predator in order to lure prey into approaching.\nAn example of this is seen in white-tailed deer fawns, which experience a drop in heart rate in response to approaching predators. This response, referred to as \"alarm bradycardia\", causes the fawn's heart rate to drop from 155 to 38 beats per minute within one beat of the heart. This drop in heart rate can last up to two minutes, causing the fawn to experience a depressed breathing rate and decrease in movement, called tonic immobility. Tonic immobility is a reflex response that causes the fawn to enter a low body position that simulates the position of a dead corpse. Upon discovery of the fawn, the predator loses interest in the \"dead\" prey. Other symptoms of alarm bradycardia, such as salivation, urination, and defecation, can also cause the predator to lose interest.\n\nMarine molluscs such as sea hares, cuttlefish, squid and octopuses give themselves a last chance to escape by distracting their attackers. To do this, they eject a mixture of chemicals, which may mimic food or otherwise confuse predators. In response to a predator, animals in these groups release ink, creating a cloud, and opaline, affecting the predator's feeding senses, causing it to attack the cloud.\n\nDistraction displays attract the attention of predators away from an object, typically the nest or young, that is being protected. Distraction displays are performed by some species of birds, which may feign a broken wing while hopping about on the ground, and by some species of fish.\n\nMimicry occurs when an organism (the mimic) simulates signal properties of another organism (the model) to confuse a third organism. This results in the mimic gaining protection, food, and mating advantages. There are two classical types of defensive mimicry: Batesian and Müllerian. Both involve aposematic coloration, or warning signals, to avoid being attacked by a predator.\n\nIn Batesian mimicry, a palatable, harmless prey species mimics the appearance of another species that is noxious to predators, thus reducing the mimic's risk of attack. This form of mimicry is seen in many insects. The idea behind Batesian mimicry is that predators that have tried to eat the unpalatable species learn to associate its colors and markings with an unpleasant taste. This results in the predator learning to avoid species displaying similar colours and markings, including Batesian mimics, which are in effect parasitic on the chemical or other defences of the unprofitable models. Some species of octopus can mimic a selection of other animals by changing their skin color, skin pattern and body motion. When a damselfish attacks an octopus, the octopus mimics a banded sea-snake. The model chosen varies with the octopus's predator and habitat. Most of these octopuses use Batesian mimicry, selecting an organism repulsive to predators as a model.\n\nIn Müllerian mimicry, two or more aposematic forms share the same warning signals, as in viceroy and monarch butterflies. Birds avoid eating both species because their wing patterns honestly signal their unpleasant taste.\n\nMany animals are protected against predators with armour in the form of hard shells (such as most molluscs), leathery or scaly skin (as in reptiles), or tough chitinous exoskeletons (as in arthropods).\n\nA spine is a sharp, needle-like structure used to inflict pain on predators. An example of this seen in nature is in the Sohal surgeonfish. These fish have a sharp scalpel-like spine on the front of each of their tail fins, able to inflict deep wounds. The area around the spines is often brightly colored to advertise the defensive capability; predators often avoid the Sohal surgeonfish. Defensive spines may be detachable, barbed or poisonous. Porcupine spines are long, stiff, break at the tip, and are barbed to stick into a would-be predator. In contrast, the hedgehog's short spines, which are modified hairs, readily bend, and are barbed into the body, so they are not easily lost; they may be jabbed at an attacker.\n\nMany species of slug caterpillar, Limacodidae, have numerous protuberances and stinging spines along their dorsal surfaces. Species that possess these stinging spines suffer less predation than larvae that lack them, and a predator, the paper wasp, chooses larvae without spines when given a choice.\n\nGroup living can decrease the risk of predation to the individual in a variety of ways, as described below.\n\nA dilution effect is seen when animals living in a group \"dilute\" their risk of attack, each individual being just one of many in the group. George C. Williams and W.D. Hamilton proposed that group living evolved because it provides benefits to the individual rather than to the group as a whole, which becomes more conspicuous as it becomes larger. One common example is the shoaling of fish. Experiments provide direct evidence for the decrease in individual attack rate seen with group living, for example in Camargue horses in Southern France. The horse-fly often attacks these horses, sucking blood and carrying diseases. When the flies are most numerous, the horses gather in large groups, and individuals are indeed attacked less frequently. Water striders are insects that live on the surface of fresh water, and are attacked from beneath by predatory fish. Experiments varying the group size of the water striders showed that the attack rate per individual water strider decreases as group size increases.\n\nThe selfish herd theory was proposed by W.D. Hamilton to explain why animals seek central positions in a group. The theory's central idea is to reduce the individual's domain of danger. A domain of danger is the area within the group in which the individual is more likely to be attacked by a predator. The center of the group has the lowest domain of danger, so animals are predicted to strive constantly to gain this position. Testing Hamilton's selfish herd effect, Alta De Vos and Justin O'Rainn (2010) studied brown fur seal predation from great white sharks. Using decoy seals, the researchers varied the distance between the decoys to produce different domains of danger. The seals with a greater domain of danger had an increased risk of shark attack.\n\nA radical strategy for avoiding predators which may otherwise kill a large majority of the emerging young of a population is to emerge very rarely, at irregular intervals. This strategy is seen in dramatic form in the periodical cicadas, which emerge at intervals of 13 or 17 years. Predators with a life-cycle of one or a few years are unable to reproduce rapidly enough in response to such an emergence, so predator satiation is a likely evolutionary explanation for the cicadas' unusual life-cycle, though not the only one. Predators may still feast on the emerging cicadas, but are unable to consume more than a fraction of the brief surfeit of prey.\n\nAnimals that live in groups often give alarm calls that give warning of an attack. For example, vervet monkeys give different calls depending on the nature of the attack: for an eagle, a disyllabic cough; for a leopard or other cat, a loud bark; for a python or other snake, a \"chutter\". The monkeys hearing these calls respond defensively, but differently in each case: to the eagle call, they look up and run into cover; to the leopard call, they run up into the trees; to the snake call, they stand on two legs and look around for snakes, and on seeing the snake, they sometimes mob it. Similar calls are found in other species of monkey, while birds also give different calls that elicit different responses.\n\nIn the improved vigilance effect, groups are able to detect predators sooner than solitary individuals. For many predators, success depends on surprise. If the prey is alerted early in an attack, they have an improved chance of escape. For example, wood pigeon flocks are preyed upon by goshawks. Goshawks are less successful when attacking larger flocks of wood pigeons than they are when attacking smaller flocks. This is because the larger the flock size, the more likely it is that one bird will notice the hawk sooner and fly away. Once one pigeon flies off in alarm, the rest of the pigeons follow. Wild ostriches in Tsavo National Park in Kenya feed either alone or in groups of up to four birds. They are subject to predation by lions. As the ostrich group size increases, the frequency at which each individual raises its head to look for predators decreases. Because ostriches are able to run at speeds that exceed those of lions for great distances, lions try to attack an ostrich when its head is down. By grouping, the ostriches present the lions with greater difficulty in determining how long the ostriches' heads stay down. Thus, although individual vigilance decreases, the overall vigilance of the group increases.\n\nIndividuals living in large groups may be safer from attack because the predator may be confused by the large group size. As the group moves, the predator has greater difficulty targeting an individual prey animal. The zebra has been suggested by the zoologist Martin Stevens and his colleagues as an example of this. When stationary, a single zebra stands out because of its large size. To reduce the risk of attack, zebras often travel in herds. The striped patterns of all the zebras in the herd may confuse the predator, making it harder for the predator to focus in on an individual zebra. Furthermore, when moving rapidly, the zebra stripes create a confusing, flickering motion dazzle effect in the eye of the predator.\n\nDefensive structures such as spines may be used both to ward off attack as already mentioned, and if need be to fight back against a predator. Methods of fighting back include chemical defences, mobbing, defensive regurgitation, and suicidal altruism.\n\nMany prey animals, and to defend against seed predation also seeds of plants, make use of poisonous chemicals for self-defence. These may be concentrated in surface structures such as spines or glands, giving an attacker a taste of the chemicals before it actually bites or swallows the prey animal: many toxins are bitter-tasting. A last-ditch defence is for the animal's flesh itself to be toxic, as in the puffer fish, danaid butterflies and burnet moths. Many insects acquire toxins from their food plants; \"Danaus\" caterpillars accumulate toxic cardenolides from milkweeds (Asclepiadaceae). \n\nSome prey animals are able to eject noxious materials to deter predators actively. The bombardier beetle has specialized glands on the tip of its abdomen that allows it to direct a toxic spray towards predators. The spray is generated explosively through oxidation of hydroquinones and is sprayed at a temperature of 100 °C. Armoured crickets similarly release blood at their joints when threatened (autohaemorrhaging). Several species of grasshopper including \"Poecilocerus pictus\", \"Parasanaa donovani\", \"Aularches miliaris\", and \"Tegra novaehollandiae\" secrete noxious liquids when threatened, sometimes ejecting these forcefully. Spitting cobras accurately squirt venom from their fangs at the eyes of potential predators, striking their target eight times out of ten, and causing severe pain. Termite soldiers in the Nasutitermitinae have a fontanellar gun, a gland on the front of their head which can secrete and shoot an accurate jet of resinous terpenes \"many centimeters\". The material is sticky and toxic to other insects. One of the terpenes in the secretion, pinene, functions as an alarm pheromone. Seeds deter predation with combinations of toxic non-protein amino acids, cyanogenic glycosides, protease and amylase inhibitors, and phytohemaglutinins.\n\nA few vertebrate species such as the Texas horned lizard are able to shoot squirts of blood from their eyes, by rapidly increasing the blood pressure within the eye sockets, if threatened. Because an individual may lose up to 53% of blood in a single squirt, this is only used against persistent predators like foxes, wolves and coyotes (Canidae), as a last defence. Canids often drop horned lizards after being squirted, and attempt to wipe or shake the blood out of their mouths, suggesting that the fluid has a foul taste; they choose other lizards if given the choice, suggesting a learned aversion towards horned lizards as prey.\n\nThe slime glands along the body of the hagfish secrete enormous amounts of mucus when it is provoked or stressed. The gelatinous slime has dramatic effects on the flow and viscosity of water, rapidly clogging the gills of any fish that attempt to capture hagfish; predators typically release the hagfish within seconds \"(pictured above)\". Common predators of hagfish include seabirds, pinnipeds and cetaceans, but few fish, suggesting that predatory fish avoid hagfish as prey.\n\nIn communal defence, prey groups actively defend themselves by grouping together, and sometimes by attacking or mobbing a predator, rather than allowing themselves to be passive victims of predation. Mobbing is the harassing of a predator by many prey animals. Mobbing is usually done to protect the young in social colonies. For example, red colobus monkeys exhibit mobbing when threatened by chimpanzees, a common predator. The male red colobus monkeys group together and place themselves between predators and the group's females and juveniles. The males jump together and actively bite the chimpanzees. Fieldfares are birds which may nest either solitarily or in colonies. Within colonies, fieldfares mob and defecate on approaching predators, shown experimentally to reduce predation levels.\n\nSome birds and insects use defensive regurgitation to ward off predators. The northern fulmar vomits a bright orange, oily substance called stomach oil when threatened. The stomach oil is made from their aquatic diets. It causes the predator's feathers to mat, leading to the loss of flying ability and the loss of water repellency. This is especially dangerous for aquatic birds because their water repellent feathers protect them from hypothermia when diving for food.\n\nEuropean roller chicks vomit a bright orange, foul smelling liquid when they sense danger. This repels prospective predators and may alert their parents to danger: they respond by delaying their return.\n\nNumerous insects utilize defensive regurgitation. The eastern tent caterpillar regurgitates a droplet of digestive fluid to repel attacking ants. Similarly, larvae of the noctuid moth regurgitate when disturbed by ants. The vomit of noctuid moths has repellent and irritant properties that help to deter predator attacks.\n\nAn unusual type of predator deterrence is observed in the Malaysian exploding ant. Social hymenoptera rely on altruism to protect the entire colony, so the self-destructive acts benefit all individuals in the colony. When a worker ant's leg is grasped, it suicidally expels the contents of its hypertrophied submandibular glands, expelling corrosive irritant compounds and adhesives onto the predator. These prevent predation and serve as a signal to other enemy ants to stop predation of the rest of the colony.\n\nThe normal reaction of a prey animal to an attacking predator is to flee by any available means, whether flying, gliding, falling, swimming, running, jumping, burrowing or rolling, according to the animal's capabilities. Escape paths are often erratic, making it difficult for the predator to predict which way the prey will go next: for example, birds such as snipe, ptarmigan and black-headed gulls evade fast raptors such as peregrine falcons with zigzagging or jinking flight. In the tropical rain forests of Southeast Asia in particular, many vertebrates escape predators by falling and gliding. Among the insects, many moths turn sharply, fall, or perform a powered dive in response to the sonar clicks of bats. Among fish, the stickleback follows a zigzagging path, often doubling back erratically, when chased by a fish-eating merganser duck.\n\nSome animals are capable of autotomy (self-amputation), shedding one of their own appendages in a last-ditch attempt to elude a predator's grasp or to distract the predator and thereby allow escape. The lost body part may be regenerated later. Certain sea slugs discard stinging papillae; arthropods such as crabs can sacrifice a claw, which can be regrown over several successive moults; among vertebrates, many geckos and other lizards shed their tails when attacked: the tail goes on writhing for a while, distracting the predator, and giving the lizard time to escape; a smaller tail slowly regrows.\n\nAristotle recorded observations (around 350 BC) of the antipredator behaviour of cephalopods in his \"History of Animals\", including the use of ink as a distraction, camouflage, and signalling.\n\nIn 1940, Hugh Cott wrote a compendious study of camouflage, mimicry, and aposematism, \"Adaptive Coloration in Animals\".\n\n\n\n"}
{"id": "19468941", "url": "https://en.wikipedia.org/wiki?curid=19468941", "title": "Balance of nature", "text": "Balance of nature\n\nThe balance of nature is a theory that proposes that ecological systems are usually in a stable equilibrium or homeostasis, which is to say that a small change in some particular parameter (the size of a particular population, for example) will be corrected by some negative feedback that will bring the parameter back to its original \"point of balance\" with the rest of the system. It may apply where populations depend on each other, for example in predator/prey systems, or relationships between herbivores and their food source. It is also sometimes applied to the relationship between the Earth's ecosystem, the composition of the atmosphere, and the world's weather.\n\nThe Gaia hypothesis is a balance of nature-based theory that suggests that the Earth and its ecology may act as co-ordinated systems in order to maintain the balance of nature.\n\nThe theory that nature is permanently in balance has been largely discredited by scientists working in ecology, as it has been found that chaotic changes in population levels are common, but nevertheless the idea continues to be popular in the general public. During the later half of the twentieth century the theory was superseded by catastrophe theory and chaos theory.\n\nThe concept that nature maintains its condition is of ancient provenance; Herodotus commented on the wonderful relationship between predator and prey species, which remained in a steady proportion to one another, with predators never excessively consuming their prey populations. The \"balance of nature\" concept once ruled ecological research, as well as once governing the management of natural resources. This led to a doctrine popular among some conservationists that nature was best left to its own devices, and that human intervention into it was by definition unacceptable. The validity of a \"balance of nature\" was already questioned in the early 1900s, but the general abandonment of the theory by scientists working in ecology only happened in the last quarter of that century when studies showed that it did not match what could be observed among plant and animal populations.\n\nPredator-prey populations tend to show chaotic behavior within limits, where the sizes of populations change in a way that may appear random, but is in fact obeying deterministic laws based only on the relationship between a population and its food source illustrated by the Lotka–Volterra equation. An experimental example of this was shown in an eight-year study on small Baltic Sea creatures such as plankton, which were isolated from the rest of the ocean. Each member of the food web was shown to take turns multiplying and declining, even though the scientists kept the outside conditions constant. An article in the journal \"Nature\" stated; \"Advanced mathematical techniques proved the indisputable presence of chaos in this food web ... short-term prediction is possible, but long-term prediction is not.\"\n\nAlthough some conservationist organizations argue that human activity is incompatible with a balanced ecosystem, there are numerous examples in history showing that several modern day habitats originate from human activity: some of Latin America's rain forests owe their existence to humans planting and transplanting them, while the abundance of grazing animals in the Serengeti plain of Africa is thought by some ecologists to be partly due to human-set fires that created savanna habitats.\n\nPossibly one of the best examples of an ecosystem fundamentally modified by human activity can be observed as a consequence of the Australian Aboriginal practice of \"Fire-stick farming\". The legacy of this practice over long periods has resulted in forests being converted to grasslands capable of sustaining larger populations of faunal prey, particularly in the northern and western regions of the continent. So thorough has been the effect of these deliberate regular burnings that many plant and tree species from affected regions have now completely adapted to the annual fire regime in that they require the passage of a fire before their seeds will even germinate. One school in Los Angeles states, \" “We have let our kids go to the forest area of the playground. However, five years later, we found that none of the flowers were growing, the natural damp soil had been hardened, and all of the beautiful grass had been plucked,”.\n\nDespite being discredited among ecologists, the theory is widely held to be true by the general public, with one authority calling it an \"enduring myth\". At least in Midwestern America, the \"balance of nature\" idea was shown to be widely held by both science majors and the general student population. In a study at the University of Patras, educational sciences students were asked to reason about the future of ecosystems which suffered human-driven disturbances. Subjects agreed that it was very likely for the ecosystems to fully recover their initial state, referring to either a 'recovery process' which restores the initial 'balance', or specific 'recovery mechanisms' as an ecosystem's inherent characteristic. In a 2017 study, Ampatzidis and Ergazaki discuss the learning objectives and design criteria that a learning environment for non-biology major students should meet to support them challenge the \"balance of nature\" idea.\n\n"}
{"id": "2377491", "url": "https://en.wikipedia.org/wiki?curid=2377491", "title": "Big History", "text": "Big History\n\nBig History is an academic discipline which examines history from the Big Bang to the present. Big History resists specialization, and searches for universal patterns or trends. It examines long time frames using a multidisciplinary approach based on combining numerous disciplines from science and the humanities, and explores human existence in the context of this bigger picture. It integrates studies of the cosmos, Earth, life, and humanity using empirical evidence to explore cause-and-effect relations, and is taught at universities and secondary schools often using web-based interactive presentations.\n\nAccording to historian David Christian, who has been credited with coining the term \"Big History\", the intellectual movement is made of an \"unusual coalition of scholars\". Some historians have expressed skepticism towards \"scientific history\" and argue that the claims of Big History are unoriginal. Others support the scientific merit but point out that cosmology and natural history have been studied since the Renaissance, and that the new term, Big History, continues such work.\n\nBig History examines the past using numerous time scales, from the Big Bang to modernity, unlike conventional history courses which typically begin with the introduction of farming and civilization, or with the beginning of written records. It explores common themes and patterns. Courses generally do not focus on humans until more than halfway through, and, unlike conventional history courses, there is not much focus on kingdoms or civilizations or wars or national borders. If conventional history focuses on human civilization with humankind at the center, Big History focuses on the universe and shows how humankind fits within this framework and places human history in the wider context of the universe's history.\n\nUnlike conventional history, Big History tends to go rapidly through detailed historical eras such as the Renaissance or Ancient Egypt. It draws on the latest findings from biology, astronomy, geology, climatology, prehistory, archaeology, anthropology, evolutionary biology, chemistry, psychology, hydrology, geography, paleontology, ancient history, physics, economics, cosmology, natural history, and population and environmental studies as well as standard history. One teacher explained:\n\nBig History arose from a desire to go beyond the specialized and self-contained fields that emerged in the 20th century. It tries to grasp history as a whole, looking for common themes across multiple time scales in history. Conventional history typically begins with the invention of writing, and is limited to past events relating directly to the human race. Big Historians point out that this limits study to the past 5,000 years and neglects the much longer time when humans existed on Earth. Henry Kannberg sees Big History as being a product of the Information Age, a stage in history itself following speech, writing, and printing. Big History covers the formation of the universe, stars, and galaxies, and includes the beginning of life as well as the period of several hundred thousand years when humans were hunter-gatherers. It sees the transition to civilization as a gradual one, with many causes and effects, rather than an abrupt transformation from uncivilized static cavemen to dynamic civilized farmers. An account in \"The Boston Globe\" describes what it polemically asserts to be the conventional \"history\" view:\n\nBig History, in contrast to conventional history, has more of an interdisciplinary basis. Advocates sometimes view conventional history as \"microhistory\" or \"shallow history\", and note that three-quarters of historians specialize in understanding the last 250 years while ignoring the \"long march of human existence.\" However, one historian disputed that the discipline of history has overlooked the big view, and described the \"grand narrative\" of Big History as a \"cliché that gets thrown around a lot.\" One account suggested that conventional history had the \"sense of grinding the nuts into an ever finer powder.\" It emphasizes long-term trends and processes rather than history-making individuals or events. Historian Dipesh Chakrabarty of the University of Chicago suggested that Big History was less politicized than contemporary history because it enables people to \"take a step back.\" It uses more kinds of evidence than the standard historical written records, such as fossils, tools, household items, pictures, structures, ecological changes and genetic variations.\n\nCritics of Big History, including sociologist Frank Furedi, have deemed the discipline an \"anti-humanist turn of history.\" The Big History narrative has also been challenged for failing to engage with the methodology of the conventional history discipline. According to historian and educator Sam Wineburg of Stanford University, Big History eschews the interpretation of texts in favor of a purely scientific approach, thus becoming \"less history and more of a kind of evolutionary biology or quantum physics.\"\n\nProfessor David Christian argued that the recent past is only understandable in terms of the \"whole 14-billion-year span of time itself.\" Big History seeks to retell the \"human story\" in light of scientific advances by such methods as radiocarbon dating and genetic analysis. In some instances, it uses mathematical modeling to explore interactions between long-term trends in sociological systems, and it has led to the coining of the term cliodynamics by Peter Turchin of the University of Connecticut to describe how mathematical models might explain events such as the growth of empires, social discontent, and the collapse of nations. It explores the mix of individual action and social and environmental forces, according to one view. While conventional history might see an invention such as sharper spear points as being deliberately created by some smart humans, and then copied by other humans, a Big History perspective might see sharper spear points as accidental, and then natural evolutionary processes enabled their users to be better hunters, even if they did not understand why this was the case. It seeks to discover repeating patterns during the 13.8 billion years since the Big Bang. For example, one pattern is that \"chaos catalyzes creativity\", such as the asteroid impact wiping out the dinosaurs.\n\nBig History makes comparisons based on different time scales, or what David Christian calls \"the play of scales\", and notes similarities and differences between the human, geological, and cosmological scales. Christian believes such \"radical shifts in perspective\" will yield \"new insights into familiar historical problems, from the nature/nurture debate to environmental history to the fundamental nature of change itself.\" It shows how human existence has been changed by both human-made and natural factors: for example, according to natural processes which happened more than four billion years ago, iron emerged from the remains of an exploding star and, as a result, humans could use this hard metal to forge weapons for hunting and war. The discipline addresses such questions as \"How did we get here?,\" \"How do we decide what to believe?,\" \"How did Earth form?,\" and \"What is life?\" It offers a \"grand tour of all the major scientific paradigms.\" According to one view, it helps students to become scientifically literate quickly.\n\nCosmic evolution, the scientific study of universal change, is closely related to Big History (as are the allied subjects of the epic of evolution and astrobiology); some researchers regard cosmic evolution as broader than Big History since the latter mainly (and rightfully) examines the specific historical trek from Big Bang → Milky Way → Sun → Earth → humanity. Cosmic evolution, while fully addressing all complex systems (and not merely those that led to humans), which is also sometimes called cosmological history or universal history, has been taught and researched for decades, mostly by astronomers and astrophysicists. This Big-Bang-to-humankind scenario well preceded the subject that some historians began calling Big History in the 1990s. Cosmic evolution is an intellectual framework that offers a grand synthesis of the many varied changes in the assembly and composition of radiation, matter, and life throughout the history of the universe. While engaging the time-honored queries of who we are and whence we came, this interdisciplinary subject attempts to unify the sciences within the entirety of natural history—a single, inclusive scientific narrative of the origin and evolution of all material things over ~14 billion years, from the origin of the universe to the present day on Earth.\n\nThe roots of the idea of cosmic evolution extend back millennia. Ancient Greek philosophers of the fifth century BCE, most notably Heraclitus, are celebrated for their reasoned claims that all things change. Early modern speculation about cosmic evolution began more than a century ago, including the broad insights of Robert Chambers, Herbert Spencer, and Lawrence Henderson. Only in the mid-20th century was the cosmic-evolutionary scenario articulated as a research paradigm to include empirical studies of galaxies, stars, planets, and life—in short, an expansive agenda that combines physical, biological, and cultural evolution. Harlow Shapley widely articulated the idea of cosmic evolution (often calling it \"cosmography\") in public venues at mid-century, and NASA embraced it in the late 20th century as part of its more limited astrobiology program. Carl Sagan, Eric Chaisson, Hubert Reeves, Erich Jantsch, and Preston Cloud, among others, extensively championed cosmic evolution at roughly the same time around 1980. This extremely broad subject now continues to be richly formulated as both a technical research program and a scientific worldview for the 21st century.\n\nCosmic evolution can elicit controversy for several reasons: evolution of any kind inherently attracts detractors, especially among religious fundamentalists; cosmic evolution addresses universal and human origins, which often elevate emotions; it challenges age-old ideas about life's sense of place in the cosmos; it embraces change, which many people dislike or distrust; it welcomes a broad interpretation of the concept of evolution, replacing the idea of evolution exclusive to life, which some biologists prefer; it proposes a sweeping, interdisciplinary worldview based on rationality and empiricism, which, despite its experimental tests, some find intellectually arrogant.\n\nOne popular collection of scholarly materials on cosmic evolution is based on teaching and research that has been underway at Harvard University since the mid-1970s\n\nCosmic evolution is a quantitative subject, whereas big history typically is not; this is because cosmic evolution is practiced mostly by natural scientists, while big history by social scholars. These two subjects, closely allied and overlapping, benefit from each other; cosmic evolutionists tend to treat universal history linearly, thus humankind enters their story only at the most very recent times, whereas big historians tend to stress humanity and its many cultural achievements, granting human beings a larger part of their story. One can compare and contrast these different emphases by watching two short movies portraying the Big-Bang-to-humankind narrative, one animating time linearly, and the other capturing time (actually look-back time) logarithmically; in the former, humans enter this 14-minute movie in the last second, while in the latter we appear much earlier—yet both are correct.\n\nThese different treatments of time over ~14 billion years, each with different emphases on historical content, are further clarified by noting that some cosmic evolutionists divide the whole narrative into three phases and seven epochs:\nThis contrasts with the approach used by some big historians who divide the narrative into many more thresholds, as noted in the discussion at the end of this section below. Yet another telling of the Big-Bang-to-humankind story is one that emphasizes the earlier universe, particularly the growth of particles, galaxies, and large-scale cosmic structure, such as in physical cosmology.\n\nNotable among quantitative efforts to describe cosmic evolution are Eric Chaisson's research efforts to describe the concept of energy flow through open, thermodynamic systems, including galaxies, stars, planets, life, and society. The observed increase of energy rate density (energy/time/mass) among a whole host of complex systems is one useful way to explain the rise of complexity in an expanding universe that still obeys the cherished second law of thermodynamics and thus continues to accumulate net entropy. As such, ordered material systems—from buzzing bees and redwood trees to shining stars and thinking beings—are viewed as temporary, local islands of order in a vast, global sea of disorder. A recent review article, which is especially directed toward big historians, summarizes much of this empirical effort over the past decade.\n\nOne striking finding of such complexity studies is the apparently ranked order among all known material systems in the universe. Although the \"absolute\" energy in astronomical systems greatly exceeds that of humans, and although the mass densities of stars, planets, bodies, and brains are all comparable, the energy rate \"density\" for humans and modern human society are approximately a million times greater than for stars and galaxies. For example, the Sun emits a vast luminosity, 4x10 erg/s (equivalent to nearly a billion billion billion watt light bulb), but it also has a huge mass, 2x10 g; thus each second an amount of energy equaling only 2 ergs passes through each gram of this star. In contrast to any star, more energy flows through each gram of a plant's leaf during photosynthesis, and much more (nearly a million times) rushes through each gram of a human brain while thinking (~20W/1350g).\n\nCosmic evolution is more than a subjective, qualitative assertion of \"one damn thing after another\". This inclusive scientific worldview constitutes an objective, quantitative approach toward deciphering much of what comprises organized, material Nature. Its uniform, consistent philosophy of approach toward all complex systems demonstrates that the basic differences, both within and among many varied systems, are of degree, not of kind. And, in particular, it suggests that optimal ranges of energy rate density grant opportunities for the evolution of complexity; those systems able to adjust, adapt, or otherwise take advantage of such energy flows survive and prosper, while other systems adversely affected by too much or too little energy are non-randomly eliminated.\n\nFred Spier is foremost among those big historians who have found the concept of energy flows useful, suggesting that Big History is the rise and demise of complexity on all scales, from sub-microscopic particles to vast galaxy clusters, and not least many biological and cultural systems in between.\n\nDavid Christian, in an 18-minute TED talk, described some of the basics of the Big History course. Christian describes each stage in the progression towards greater complexity as a \"threshold moment\" when things become more complex, but they also become more fragile and mobile. Some of Christian's threshold stages are:\n\nChristian elaborated that more complex systems are more fragile, and that while collective learning is a powerful force to advance humanity in general, it is not clear that humans are in charge of it, and it is possible in his view for humans to destroy the biosphere with the powerful weapons that have been invented.\n\nIn the 2008 lecture series through \"The Teaching Company's Great Courses\" entitled \"Big History: The Big Bang, Life on Earth, and the Rise of Humanity\", Christian explains Big History in terms of eight thresholds of increasing complexity:\n\n\nA theme in Big History is what has been termed Goldilocks conditions or the Goldilocks principle, which describes how \"circumstances must be right for any type of complexity to form or continue to exist,\" as emphasized by Spier in his recent book. For humans, bodily temperatures can neither be too hot nor too cold; for life to form on a planet, it can neither have too much nor too little energy from sunlight. Stars require sufficient quantities of hydrogen, sufficiently packed together under tremendous gravity, to cause nuclear fusion.\n\nChristian suggests that the universe creates complexity when these Goldilocks conditions are met, that is, when things are not too hot or cold, not too fast or slow. For example, life began not in solids (molecules are stuck together, preventing the right kinds of associations) or gases (molecules move too fast to enable favorable associations) but in liquids such as water which permitted the right kinds of interactions at the right speeds.\n\nSomewhat in contrast, Chaisson has maintained for well more than a decade that \"organizational complexity is mostly governed by the \"optimum\" use of energy—not too little as to starve a system, yet not too much as to destroy it\" (italics in the original published paper). Neither maximum energy principles nor minimum entropy states are likely relevant, and appeals to \"Goldilocks principles\" (or other such fairy tales) are unnecessary to appreciate the emergence of complexity in Nature writ large.\n\nAdvances in particular sciences such as archaeology, gene mapping, and evolutionary ecology have enabled historians to gain new insights into the early origins of humans, despite the lack of written sources. One account suggested that proponents of Big History were trying to \"upend\" the conventional practice in historiography of relying on written records.\n\nBig History proponents suggest that humans have been affecting climate change throughout history, by such methods as slash-and-burn agriculture, although past modifications have been on a lesser scale than in recent years during the Industrial Revolution.\n\nA book by Daniel Lord Smail in 2008 suggested that history was a continuing process of humans learning to self-modify our mental states by using stimulants such as coffee and tobacco, as well as other means such as religious rites or romance novels. His view is that culture and biology are highly intertwined, such that cultural practices may cause human brains to be wired differently from those in different societies.\n\nBig History is more likely than conventional history to be taught with interactive \"video-heavy\" websites without textbooks, according to one account. The discipline has benefited from having new ways of presenting themes and concepts in new formats, often supplemented by Internet and computer technology. For example, the ChronoZoom project is a way to explore the 14 billion year history of the universe in an interactive website format. It was described in one account:\n\nIn 2012, the History channel showed the film \"History of the World in Two Hours\". It showed how dinosaurs effectively dominated mammals for 160 million years until an asteroid impact wiped them out. One report suggested the History channel had won a sponsorship from StanChart to develop a Big History program entitled \"Mankind\". In 2013 the History channel's new H2 network debuted the 10-part series \"Big History\", narrated by Bryan Cranston and featuring David Christian and an assortment of historians, scientists and related experts. Each episode centered on a major Big History topic such as salt, mountains, cold, flight, water, meteors and megastructures.\n\nWhile the emerging field of Big History in its present state is generally seen as having emerged in the past two decades beginning around 1990, there have been numerous precedents going back almost 150 years. In the mid-19th century, Alexander von Humboldt's book \"Cosmos\", and Robert Chambers' 1844 book \"Vestiges of the Natural History of Creation\" were seen as early precursors to the field. In a sense, Darwin's theory of evolution was, in itself, an attempt to explain a biological phenomenon by examining longer term cause-and-effect processes. In the first half of the 20th century, secular biologist Julian Huxley originated the term \"evolutionary humanism\", while around the same time the French Jesuit paleontologist Pierre Teilhard de Chardin examined links between cosmic evolution and a tendency towards complexification (including human consciousness), while envisaging compatibility between cosmology, evolution, and theology. In the mid and later 20th century, \"The Ascent of Man\" by Jacob Bronowski examined history from a multidisciplinary perspective. Later, Eric Chaisson explored the subject of cosmic evolution quantitatively in terms of energy rate density, and the astronomer Carl Sagan wrote \"Cosmos\". Thomas Berry, a cultural historian, and the academic Brian Swimme explored meaning behind myths and encouraged academics to explore themes beyond organized religion.\n\nThe field continued to evolve from interdisciplinary studies during the mid-20th century, stimulated in part by the Cold War and the Space Race. Some early efforts were courses in \"Cosmic Evolution\" at Harvard University in the United States, and \"Universal History\" in the Soviet Union. One account suggested that the notable Earthrise photo, taken during a lunar orbit by the spacecraft Apollo 8, which showed Earth as a small blue and white ball behind a stark and desolate lunar landscape, not only stimulated the environmental movement but also caused an upsurge of interdisciplinary interest. The French historian Fernand Braudel examined daily life with investigations of \"large-scale historical forces like geology and climate\". Physiologist Jared Diamond in his book \"Guns, Germs, and Steel\" examined the interplay between geography and human evolution; for example, he argued that the horizontal shape of the Eurasian continent enabled human civilizations to advance more quickly than the vertical north-south shape of the American continent, because it enabled greater competition and information-sharing among peoples of the relatively same climate.\n\nIn the 1970s, scholars in the United States including geologist Preston Cloud of the University of Minnesota, astronomer G. Siegfried Kutter at Evergreen State College in Washington state, and Harvard University astrophysicists George B. Field and Eric Chaisson started synthesizing knowledge to form a \"science-based history of everything\", although each of these scholars emphasized somewhat their own particular specializations in their courses and books. In 1980, the Austrian philosopher Erich Jantsch wrote \"The Self-Organizing Universe\" which viewed history in terms of what he called \"process structures\". There was an experimental course taught by John Mears at Southern Methodist University in Dallas, Texas, and more formal courses at the university level began to appear.\n\nIn 1991 Clive Ponting wrote \"A Green History of the World: The Environment and the Collapse of Great Civilizations\". His analysis did not begin with the Big Bang, but his chapter \"Foundations of History\" explored the influences of large-scale geological and astronomical forces over a broad time period.\n\nSometimes the terms \"Deep History\" and \"Big History\" are interchangeable, but sometimes \"Deep History\" simply refers to history going back several hundred thousand years or more without the other senses of being a movement within history itself.\n\nOne exponent is David Christian of Macquarie University in Sydney, Australia. He read widely in diverse fields in science, and believed that much was missing from the general study of history. His first university-level course was offered in 1989. He developed a college course beginning with the Big Bang to the present in which he collaborated with numerous colleagues from diverse fields in science and the humanities and the social sciences. This course eventually became a Teaching Company course entitled \"Big History: The Big Bang, Life on Earth, and the Rise of Humanity\", with 24 hours of lectures, which appeared in 2008.\n\nSince the 1990s, other universities began to offer similar courses. In 1994 at the University of Amsterdam and the Eindhoven University of Technology, college courses were offered. In 1996, Fred Spier wrote \"The Structure of Big History\". Spier looked at structured processes which he termed \"regimes\":\n\nChristian's course caught the attention of philanthropist Bill Gates, who discussed with him how to turn Big History into a high school-level course. Gates said about David Christian:\n\nBy 2002, a dozen college courses on Big History had sprung up around the world. Cynthia Stokes Brown initiated Big History at the Dominican University of California, and she wrote \"Big History: From the Big Bang to the Present.\" In 2010, Dominican University of California launched the world's first Big History program to be required of all first-year students, as part of the school's general education track. This program, directed by Mojgan Behmand, includes a one-semester survey of Big History, and an interdisciplinary second-semester course exploring the Big History metanarrative through the lens of a particular discipline or subject. A course description reads:\n\nThe Dominican faculty's approach is to synthesize the disparate threads of Big History thought, in order to teach the content, develop critical thinking and writing skills, and prepare students to wrestle with the philosophical implications of the Big History metanarrative. In 2015, University of California Press published \"Teaching Big History\", a comprehensive pedagogical guide for teaching Big History, edited by Richard B. Simon, Mojgan Behmand, and Thomas Burke, and written by the Dominican faculty.\nBarry Rodrigue, at the University of Southern Maine, established the first general education course and the first online version, which has drawn students from around the world. The University of Queensland in Australia offers an undergraduate course entitled \"Global History\", required for all history majors, which \"surveys how powerful forces and factors at work on large time-scales have shaped human history\". By 2011, 50 professors around the world have offered courses. In 2012, one report suggested that Big History was being practiced as a \"coherent form of research and teaching\" by hundreds of academics from different disciplines.\nThere are efforts to bring Big History to younger students. In 2008, Christian and his colleagues began developing a course for secondary school students. In 2011, a pilot high school course was taught to 3,000 kids in 50 high schools worldwide. In 2012, there were 87 schools, with 50 in the United States, teaching Big History, with the pilot program set to double in 2013 for students in the ninth and tenth grades, and even in one middle school. The subject is a STEM course at one high school.\n\nThere are initiatives to make Big History a required standard course for university students throughout the world. An education project founded by philanthropist Bill Gates from his personal funds was launched in Australia and the United States, to offer a free online version of the course to high school students.\n\nCurrently, Big History is a consolidated academic field that is giving rise to new views and epistemological approaches, especially in Latin America and the Caribbean, whose decolonial vision of history, economics and science has opened new questions. In this sense, the transdisciplinary and biomimetics research of Javier Collado represents an ecology of knowledge between scientific knowledge and the ancestral wisdom of native peoples, such as Indigenous peoples in Ecuador. This transdisciplinary vision integrates and unifies diverse epistemes that are in, between, and beyond the scientific disciplines, that is, it includes ancestral wisdom, spirituality, art, emotions, mystical experiences and other dimensions forgotten in the history of science, specially by the positivist approach. In approaching the Big History from the complexity sciences, the transdisciplinary methodology seeks to understand the interconnections of the human race with the different levels of reality that co-exist in nature and in the cosmos, and this includes mystical and spiritual experiences, very present in the rituals of shamanism with ayahuasca and other sacred plants. The common denominator of all indigenous and aboriginal ancestral worldviews is the spiritual and ecological conception that structures their social organizations, which are in harmony and respect with the different forms of life that exist on our planet. In the same way that Fritjof Capra carried out an analysis of the parallels between modern physics and Eastern mysticism, the teaching of the Big History in universities of Brazil, Ecuador, Colombia, and Argentina is nourished by the worldview of their ancestor to analyze the parallels between the scientific discoveries and the original knowledge of the native and indigenous peoples.\n\nThe International Big History Association (IBHA) was founded at the Coldigioco Geological Observatory in Coldigioco, Marche, Italy, on 20 August 2010. Its headquarters is located at Grand Valley State University in Allendale, Michigan, United States. Its inaugural gathering in 2012 was described as \"big news\" in a report in \"The Huffington Post\".\n\nAcademics involved with the concept include:\n\n"}
{"id": "4290647", "url": "https://en.wikipedia.org/wiki?curid=4290647", "title": "Biological naturalism", "text": "Biological naturalism\n\nBiological naturalism is a theory about, among other things, the relationship between consciousness and body (i.e. brain), and hence an approach to the mind–body problem. It was first proposed by the philosopher John Searle in 1980 and is defined by two main theses: 1) all mental phenomena from pains, tickles, and itches to the most abstruse thoughts are caused by lower-level neurobiological processes in the brain; and 2) mental phenomena are higher-level features of the brain.\n\nThis entails that the brain has the right causal powers to produce intentionality. However, Searle's biological naturalism does not entail that brains and \"only\" brains can cause consciousness. Searle is careful to point out that while it appears to be the case that certain brain functions are sufficient for producing conscious states, our current state of neurobiological knowledge prevents us from concluding that they are necessary for producing consciousness. In his own words:\n\n\"The fact that brain processes cause consciousness does not imply that only brains can be conscious. The brain is a biological machine, and we might build an artificial machine that was conscious; just as the heart is a machine, and we have built artificial hearts. Because we do not know exactly how the brain does it we are not yet in a position to know how to do it artificially.\" (Biological Naturalism, 2004)\n\nSearle denies Cartesian dualism, the idea that the mind is a separate kind of substance to the body, as this contradicts our entire understanding of physics, and unlike Descartes, he does not bring God into the problem. Indeed, Searle denies any kind of dualism, the traditional alternative to monism, claiming the distinction is a mistake. He rejects the idea that because the mind is not objectively viewable, it does not fall under the rubric of physics.\n\nSearle believes that consciousness \"is a real part of the real world and it cannot be eliminated in favor of, or reduced to, something else\" whether that something else is a neurological state of the brain or a computer program. He contends, for example, that the software known as Deep Blue \"knows\" nothing about chess. He also believes that consciousness is both a cause of events in the body and a response to events in the body.\n\nOn the other hand, Searle doesn't treat consciousness as a ghost in the machine. He treats it, rather, as a state of the brain. The causal interaction of mind and brain can be described thus in naturalistic terms: Events at the micro-level (perhaps at that of individual neurons) cause consciousness. Changes at the macro-level (the whole brain) constitute consciousness. Micro-changes cause and then are impacted by holistic changes, in much the same way that individual football players cause a team (as a whole) to win games, causing the individuals to gain confidence from the knowledge that they are part of a winning team.\n\nHe articulates this distinction by pointing out that the common philosophical term 'reducible' is ambiguous. Searle contends that consciousness is \"causally reducible\" to brain processes without being \"ontologically reducible\". He hopes that making this distinction will allow him to escape the traditional dilemma between reductive materialism and substance dualism; he affirms the essentially physical nature of the universe by asserting that consciousness is completely caused by and realized in the brain, but also doesn't deny what he takes to be the obvious facts that humans really are conscious, and that conscious states have an essentially first-person nature.\n\nIt can be tempting to see the theory as a kind of property dualism, since, in Searle's view, a person's mental properties are categorically different from his or her micro-physical properties. The latter have \"third-person ontology\" whereas the former have \"first-person ontology.\" Micro-structure is accessible objectively by any number of people, as when several brain surgeons inspect a patient's cerebral hemispheres. But pain or desire or belief are accessible subjectively by the person who has the pain or desire or belief, and no one else has that mode of access. However, Searle holds mental properties to be a species of physical property—ones with first-person ontology. So this sets his view apart from a dualism of physical and non-physical properties. His mental properties are putatively physical.\n\nThere have been several criticisms of Searle's idea of biological naturalism.\n\nJerry Fodor suggests that Searle gives us no account at all of exactly \"why\" he believes that a biochemistry like, or similar to, that of the human brain is indispensable for intentionality. Fodor thinks that it seems much more plausible to suppose that it is the way in which an organism (or any other system for that matter) is connected to its environment that is indispensable in the explanation of intentionality. It is easier to see \"how the fact that my thought is causally connected to a tree might bear on its being a thought about a tree. But it's hard to imagine how the fact that (to put it crudely) my thought is made out of hydrocarbons could matter, except on the unlikely hypothesis that only hydrocarbons can be causally connected to trees in the way that brains are.\" \n\nJohn Haugeland takes on the central notion of some set of special \"right causal powers\" that Searle attributes to the biochemistry of the human brain. He asks us to imagine a concrete situation in which the \"right\" causal powers are those that our neurons have to reciprocally stimulate one another. In this case, silicon-based alien life forms can be intelligent just in case they have these \"right\" causal powers; i.e. they possess neurons with synaptics connections that have the power to reciprocally stimulate each other. Then we can take any speaker of the Chinese language and cover his neurons in some sort of wrapper which prevents them from being influenced by neurotransmitters and, hence, from having the right causal powers. At this point, \"Searle's demon\" (an English speaking nanobot, perhaps) sees what is happening and intervenes: he sees through the covering and determines which neurons would have been stimulated and which not and proceeds to stimulate the appropriate neurons and shut down the others himself. The experimental subject's behavior is unaffected. He continues to speak perfect Chinese as before the operation but now the causal powers of his neurotransmitters have been replaced by someone who does not understand the Chinese language. The point is generalizable: for any causal powers, it will always be possible to hypothetically replace them with some sort of Searlian demon which will carry out the operations mechanically. His conclusion is that Searle's is necessarily a dualistic view of the nature of causal powers, \"not intrinsically connected with the actual powers of physical objects.\" \n\nSearle himself actually does not rule out the possibility for alternate arrangements of matter bringing forth consciousness other than biological brains. He also disputes that Biological naturalism is dualistic in nature in a brief essay entitled \"Why I Am Not a Property Dualist\".\n\n\n\n"}
{"id": "7555", "url": "https://en.wikipedia.org/wiki?curid=7555", "title": "Casimir effect", "text": "Casimir effect\n\nIn quantum field theory, the Casimir effect and the Casimir–Polder force are physical forces arising from a quantized field. They are named after the Dutch physicist Hendrik Casimir who predicted them in 1948.\n\nThe Casimir effect can be understood by the idea that the presence of conducting metals and dielectrics alters the vacuum expectation value of the energy of the second quantized electromagnetic field. Since the value of this energy depends on the shapes and positions of the conductors and dielectrics, the Casimir effect manifests itself as a force between such objects.\n\nAny medium supporting oscillations has an analogue of the Casimir effect. For example, beads on a string as well as plates submerged in noisy water or gas illustrate the Casimir force.\n\nIn modern theoretical physics, the Casimir effect plays an important role in the chiral bag model of the nucleon; in applied physics, it is significant in some aspects of emerging microtechnologies and nanotechnologies.\n\nThe typical example is of the two uncharged conductive plates in a vacuum, placed a few nanometers apart. In a classical description, the lack of an external field means that there is no field between the plates, and no force would be measured between them. When this field is instead studied using the quantum electrodynamic vacuum, it is seen that the plates do affect the virtual photons which constitute the field, and generate a net force – either an attraction or a repulsion depending on the specific arrangement of the two plates. Although the Casimir effect can be expressed in terms of virtual particles interacting with the objects, it is best described and more easily calculated in terms of the zero-point energy of a quantized field in the intervening space between the objects. This force has been measured and is a striking example of an effect captured formally by second quantization.\n\nThe treatment of boundary conditions in these calculations has led to some controversy. In fact, \"Casimir's original goal was to compute the van der Waals force between polarizable molecules\" of the conductive plates. Thus it can be interpreted without any reference to the zero-point energy (vacuum energy) of quantum fields.\n\nBecause the strength of the force falls off rapidly with distance, it is measurable only when the distance between the objects is extremely small. On a submicron scale, this force becomes so strong that it becomes the dominant force between uncharged conductors. In fact, at separations of 10 nm – about 100 times the typical size of an atom – the Casimir effect produces the equivalent of about 1 atmosphere of pressure (the precise value depending on surface geometry and other factors).\n\nDutch physicists Hendrik Casimir and Dirk Polder at Philips Research Labs proposed the existence of a force between two polarizable atoms and between such an atom and a conducting plate in 1947; this special form is called the Casimir–Polder force. After a conversation with Niels Bohr, who suggested it had something to do with zero-point energy, Casimir alone formulated the theory predicting a force between neutral conducting plates in 1948 which is called the Casimir effect in the narrow sense.\n\nPredictions of the force were later extended to finite-conductivity metals and dielectrics, and recent calculations have considered more general geometries. Experiments before 1997 had observed the force qualitatively, and indirect validation of the predicted Casimir energy had been made by measuring the thickness of liquid helium films. However it was not until 1997 that a direct experiment by S. Lamoreaux quantitatively measured the force to within 5% of the value predicted by the theory. Subsequent experiments approach an accuracy of a few percent.\n\nThe causes of the Casimir effect are described by quantum field theory, which states that all of the various fundamental fields, such as the electromagnetic field, must be quantized at each and every point in space. In a simplified view, a \"field\" in physics may be envisioned as if space were filled with interconnected vibrating balls and springs, and the strength of the field can be visualized as the displacement of a ball from its rest position. Vibrations in this field propagate and are governed by the appropriate wave equation for the particular field in question. The second quantization of quantum field theory requires that each such ball-spring combination be quantized, that is, that the strength of the field be quantized at each point in space. At the most basic level, the field at each point in space is a simple harmonic oscillator, and its quantization places a quantum harmonic oscillator at each point. Excitations of the field correspond to the elementary particles of particle physics. However, even the vacuum has a vastly complex structure, so all calculations of quantum field theory must be made in relation to this model of the vacuum.\n\nThe vacuum has, implicitly, all of the properties that a particle may have: spin, or polarization in the case of light, energy, and so on. On average, most of these properties cancel out: the vacuum is, after all, \"empty\" in this sense. One important exception is the vacuum energy or the vacuum expectation value of the energy. The quantization of a simple harmonic oscillator states that the lowest possible energy or zero-point energy that such an oscillator may have is\n\nSumming over all possible oscillators at all points in space gives an infinite quantity. Since only \"differences\" in energy are physically measurable (with the notable exception of gravitation, which remains beyond the scope of quantum field theory), this infinity may be considered a feature of the mathematics rather than of the physics. This argument is the underpinning of the theory of renormalization. Dealing with infinite quantities in this way was a cause of widespread unease among quantum field theorists before the development in the 1970s of the renormalization group, a mathematical formalism for scale transformations that provides a natural basis for the process.\n\nWhen the scope of the physics is widened to include gravity, the interpretation of this formally infinite quantity remains problematic. There is currently no compelling explanation as to why it should not result in a cosmological constant that is many orders of magnitude larger than observed. However, since we do not yet have any fully coherent quantum theory of gravity, there is likewise no compelling reason as to why it should.\n\nThe Casimir effect for fermions can be understood as the spectral asymmetry of the fermion operator formula_2, where it is known as the Witten index.\n\nAlternatively, a 2005 paper by Robert Jaffe of MIT states that \"Casimir effects\ncan be formulated and Casimir forces can be computed without reference to zero-point energies. They are relativistic, quantum forces between charges and currents. The Casimir force (per unit\narea) between parallel plates vanishes as alpha, the fine structure constant, goes to zero, and the standard result, which appears to be independent of alpha, corresponds to the alpha approaching infinity limit,\" and that \"The Casimir force is simply the (relativistic, retarded) van der Waals force between the metal plates.\" Casimir and Polder's original paper used this method to derive the Casimir-Polder force. In 1978, Schwinger, DeRadd, and Milton published a similar derivation for the Casimir Effect between two parallel plates. In fact, the description in terms of van der Waals forces is the only correct description from the fundamental microscopic perspective, while other descriptions of Casimir force are merely effective macroscopic descriptions.\n\nCasimir's observation was that the second-quantized quantum electromagnetic field, in the presence of bulk bodies such as metals or dielectrics, must obey the same boundary conditions that the classical electromagnetic field must obey. In particular, this affects the calculation of the vacuum energy in the presence of a conductor or dielectric.\n\nConsider, for example, the calculation of the vacuum expectation value of the electromagnetic field inside a metal cavity, such as, for example, a radar cavity or a microwave waveguide. In this case, the correct way to find the zero-point energy of the field is to sum the energies of the standing waves of the cavity. To each and every possible standing wave corresponds an energy; say the energy of the \"n\"th standing wave is formula_3. The vacuum expectation value of the energy of the electromagnetic field in the cavity is then\n\nwith the sum running over all possible values of \"n\" enumerating the standing waves. The factor of 1/2 is present because the zero-point energy of the n'th mode is formula_5, where formula_3 is the energy increment for the n'th mode. (It is the same 1/2 as appears in the equation formula_7.) Written in this way, this sum is clearly divergent; however, it can be used to create finite expressions.\n\nIn particular, one may ask how the zero-point energy depends on the shape \"s\" of the cavity. Each energy level formula_3 depends on the shape, and so one should write formula_9 for the energy level, and formula_10 for the vacuum expectation value. At this point comes an important observation: the force at point \"p\" on the wall of the cavity is equal to the change in the vacuum energy if the shape \"s\" of the wall is perturbed a little bit, say by formula_11, at point \"p\". That is, one has\n\nThis value is finite in many practical calculations.\n\nAttraction between the plates can be easily understood by focusing on the one-dimensional situation. Suppose that a moveable conductive plate is positioned at a short distance \"a\" from one of two widely separated plates (distance \"L\" apart). With \"a\" « \"L\", the states within the slot of width \"a\" are highly constrained so that the energy \"E\" of any one mode is widely separated from that of the next. This is not the case in the large region \"L\", where there is a large number (numbering about \"L\" / \"a\") of states with energy evenly spaced between \"E\" and the next mode in the narrow slot – in other words, all slightly larger than \"E\". Now on shortening \"a\" by d\"a\" (< 0), the mode in the narrow slot shrinks in wavelength and therefore increases in energy proportional to −d\"a\"/\"a\", whereas all the \"L\" /\"a\" states that lie in the large region lengthen and correspondingly decrease their energy by an amount proportional to d\"a\"/\"L\" (note the denominator). The two effects nearly cancel, but the net change is slightly negative, because the energy of all the \"L\"/\"a\" modes in the large region are slightly larger than the single mode in the slot. Thus the force is attractive: it tends to make \"a\" slightly smaller, the plates attracting each other across the thin slot.\n\n\nIn the original calculation done by Casimir, he considered the space between a pair of conducting metal plates at distance formula_13 apart. In this case, the standing waves are particularly easy to calculate, because the transverse component of the electric field and the normal component of the magnetic field must vanish on the surface of a conductor. Assuming the plates lie parallel to the \"xy\"-plane, the standing waves are\n\nwhere formula_15 stands for the electric component of the electromagnetic field, and, for brevity, the polarization and the magnetic components are ignored here. Here, formula_16 and formula_17 are the wave numbers in directions parallel to the plates, and\n\nis the wave-number perpendicular to the plates. Here, \"n\" is an integer, resulting from the requirement that ψ vanish on the metal plates. The frequency of this wave is\n\nwhere \"c\" is the speed of light. The vacuum energy is then the sum over all possible excitation modes. Since the area of the plates is large, we may sum by integrating over two of the dimensions in \"k\"-space. The assumption of periodic boundary conditions yields,\n\nwhere \"A\" is the area of the metal plates, and a factor of 2 is introduced for the two possible polarizations of the wave. This expression is clearly infinite, and to proceed with the calculation, it is convenient to introduce a regulator (discussed in greater detail below). The regulator will serve to make the expression finite, and in the end will be removed. The zeta-regulated version of the energy per unit-area of the plate is\n\nIn the end, the limit formula_22 is to be taken. Here \"s\" is just a complex number, not to be confused with the shape discussed previously. This integral/sum is finite for \"s\" real and larger than 3. The sum has a pole at \"s\"=3, but may be analytically continued to \"s\"=0, where the expression is finite. The above expression simplifies to:\n\nwhere polar coordinates formula_24 were introduced to turn the double integral into a single integral. The formula_25 in front is the Jacobian, and the formula_26 comes from the angular integration. The integral converges if Re[\"s\"] > 3, resulting in\n\nThe sum diverges at \"s\" in the neighborhood of zero, but if the damping of large-frequency excitations corresponding to analytic continuation of the Riemann zeta function to \"s\"=0 is assumed to make sense physically in some way, then one has\n\nBut\n\nand so one obtains\n\nThe analytic continuation has evidently lost an additive positive infinity, somehow exactly accounting for the zero-point energy (not included above) outside the slot between the plates, but which changes upon plate movement within a closed system. The Casimir force per unit area formula_31 for idealized, perfectly conducting plates with vacuum between them is\n\nwhere\n\nThe force is negative, indicating that the force is attractive: by moving the two plates closer together, the energy is lowered. The presence of formula_33 shows that the Casimir force per unit area formula_31 is very small, and that furthermore, the force is inherently of quantum-mechanical origin.\n\nNOTE: In Casimir's original derivation , a moveable conductive plate is positioned at a short distance \"a\" from one of two widely separated plates (distance \"L\" apart). The 0-point energy on \"both\" sides of the plate is considered. Instead of the above \"ad hoc\" analytic continuation assumption, non-convergent sums and integrals are computed using Euler–Maclaurin summation with a regularizing function (e.g., exponential regularization) not so anomalous as formula_38 in the above.\n\nCasimir's analysis of idealized metal plates was generalized to arbitrary dielectric and realistic metal plates by Lifshitz and his students. Using this approach, complications of the bounding surfaces, such as the modifications to the Casimir force due to finite conductivity, can be calculated numerically using the tabulated complex dielectric functions of the bounding materials. Lifshitz's theory for two metal plates reduces to Casimir's idealized 1/\"a\" force law for large separations \"a\" much greater than the skin depth of the metal, and conversely reduces to the 1/\"a\" force law of the London dispersion force (with a coefficient called a Hamaker constant) for small \"a\", with a more complicated dependence on \"a\" for intermediate separations determined by the dispersion of the materials.\n\nLifshitz' result was subsequently generalized to arbitrary multilayer planar geometries as well as to anisotropic and magnetic materials, but for several decades the calculation of Casimir forces for non-planar geometries remained limited to a few idealized cases admitting analytical solutions. For example, the force in the experimental sphere–plate geometry was computed with an approximation (due to Derjaguin) that the sphere radius \"R\" is much larger than the separation \"a\", in which case the nearby surfaces are nearly parallel and the parallel-plate result can be adapted to obtain an approximate \"R\"/\"a\" force (neglecting both skin-depth and higher-order curvature effects). However, in the 2000s a number of authors developed and demonstrated a variety of numerical techniques, in many cases adapted from classical computational electromagnetics, that are capable of accurately calculating Casimir forces for arbitrary geometries and materials, from simple finite-size effects of finite plates to more complicated phenomena arising for patterned surfaces or objects of various shapes.\n\nOne of the first experimental tests was conducted by Marcus Sparnaay at Philips in Eindhoven (Netherlands), in 1958, in a delicate and difficult experiment with parallel plates, obtaining results not in contradiction with the Casimir theory, but with large experimental errors. Some of the experimental details as well as some background information on how Casimir, Polder and Sparnaay arrived at this point are highlighted in a 2007 interview with Marcus Sparnaay.\n\nThe Casimir effect was measured more accurately in 1997 by Steve K. Lamoreaux of Los Alamos National Laboratory, and by Umar Mohideen and Anushree Roy of the University of California, Riverside. In practice, rather than using two parallel plates, which would require phenomenally accurate alignment to ensure they were parallel, the experiments use one plate that is flat and another plate that is a part of a sphere with a large radius.\n\nIn 2001, a group (Giacomo Bressi, Gianni Carugno, Roberto Onofrio and Giuseppe Ruoso) at the University of Padua (Italy) finally succeeded in measuring the Casimir force between parallel plates using microresonators.\n\nIn order to be able to perform calculations in the general case, it is convenient to introduce a regulator in the summations. This is an artificial device, used to make the sums finite so that they can be more easily manipulated, followed by the taking of a limit so as to remove the regulator.\n\nThe heat kernel or exponentially regulated sum is\n\nwhere the limit formula_40 is taken in the end. The divergence of the sum is typically manifested as\n\nfor three-dimensional cavities. The infinite part of the sum is associated with the bulk constant \"C\" which \"does not\" depend on the shape of the cavity. The interesting part of the sum is the finite part, which is shape-dependent. The Gaussian regulator\n\nis better suited to numerical calculations because of its superior convergence properties, but is more difficult to use in theoretical calculations. Other, suitably smooth, regulators may be used as well. The zeta function regulator\n\nis completely unsuited for numerical calculations, but is quite useful in theoretical calculations. In particular, divergences show up as poles in the complex \"s\" plane, with the bulk divergence at \"s\"=4. This sum may be analytically continued past this pole, to obtain a finite part at \"s\"=0.\n\nNot every cavity configuration necessarily leads to a finite part (the lack of a pole at \"s\"=0) or shape-independent infinite parts. In this case, it should be understood that additional physics has to be taken into account. In particular, at extremely large frequencies (above the plasma frequency), metals become transparent to photons (such as X-rays), and dielectrics show a frequency-dependent cutoff as well. This frequency dependence acts as a natural regulator. There are a variety of bulk effects in solid state physics, mathematically very similar to the Casimir effect, where the cutoff frequency comes into explicit play to keep expressions finite. (These are discussed in greater detail in \"Landau and Lifshitz\", \"Theory of Continuous Media\".)\n\nThe Casimir effect can also be computed using the mathematical mechanisms of functional integrals of quantum field theory, although such calculations are considerably more abstract, and thus difficult to comprehend. In addition, they can be carried out only for the simplest of geometries. However, the formalism of quantum field theory makes it clear that the vacuum expectation value summations are in a certain sense summations over so-called \"virtual particles\".\n\nMore interesting is the understanding that the sums over the energies of standing waves should be formally understood as sums over the eigenvalues of a Hamiltonian. This allows atomic and molecular effects, such as the van der Waals force, to be understood as a variation on the theme of the Casimir effect. Thus one considers the Hamiltonian of a system as a function of the arrangement of objects, such as atoms, in configuration space. The change in the zero-point energy as a function of changes of the configuration can be understood to result in forces acting between the objects.\n\nIn the chiral bag model of the nucleon, the Casimir energy plays an important role in showing the mass of the nucleon is independent of the bag radius. In addition, the spectral asymmetry is interpreted as a non-zero vacuum expectation value of the baryon number, cancelling the topological winding number of the pion field surrounding the nucleon.\n\nThe dynamical Casimir effect is the production of particles and energy from an accelerated \"moving mirror\". This reaction was predicted by certain numerical solutions to quantum mechanics equations made in the 1970s. In May 2011 an announcement was made by researchers at the Chalmers University of Technology, in Gothenburg, Sweden, of the detection of the dynamical Casimir effect. In their experiment, microwave photons were generated out of the vacuum in a superconducting microwave resonator. These researchers used a modified SQUID to change the effective length of the resonator in time, mimicking a mirror moving at the required relativistic velocity. If confirmed this would be the first experimental verification of the dynamical Casimir effect.\nA similar analysis can be used to explain Hawking radiation that causes the slow \"evaporation\" of black holes (although this is generally visualized as the escape of one particle from a virtual particle-antiparticle pair, the other particle having been captured by the black hole).\n\nConstructed within the framework of quantum field theory in curved spacetime, the dynamical Casimir effect has been used to better understand acceleration radiation such as the Unruh effect.\n\nThere are few instances wherein the Casimir effect can give rise to repulsive forces between uncharged objects. Evgeny Lifshitz showed (theoretically) that in certain circumstances (most commonly involving liquids), repulsive forces can arise. This has sparked interest in applications of the Casimir effect toward the development of levitating devices. An experimental demonstration of the Casimir-based repulsion predicted by Lifshitz was carried out by Munday et al. Other scientists have also suggested the use of gain media to achieve a similar levitation effect, though this is controversial because these materials seem to violate fundamental causality constraints and the requirement of thermodynamic equilibrium (Kramers–Kronig relations). Casimir and Casimir-Polder repulsion can in fact occur for sufficiently anisotropic electrical bodies; for a review of the issues involved with repulsion see Milton et al.\n\nIt has been suggested that the Casimir forces have application in nanotechnology, in particular silicon integrated circuit technology based micro- and nanoelectromechanical systems, silicon array propulsion for space drives, and so-called Casimir oscillators.\n\nThe Casimir effect shows that quantum field theory allows the energy density in certain regions of space to be negative relative to the ordinary vacuum energy, and it has been shown theoretically that quantum field theory allows states where the energy can be \"arbitrarily\" negative at a given point. Many physicists such as Stephen Hawking, Kip Thorne, and others therefore argue that such effects might make it possible to stabilize a traversable wormhole.\n\nOn 4 June 2013 it was reported that a conglomerate of scientists from Hong Kong University of Science and Technology, University of Florida, Harvard University, Massachusetts Institute of Technology, and Oak Ridge National Laboratory have for the first time demonstrated a compact integrated silicon chip that can measure the Casimir force.\n\n\n\n\n\n"}
{"id": "7807", "url": "https://en.wikipedia.org/wiki?curid=7807", "title": "Cavitation", "text": "Cavitation\n\nCavitation is the formation of vapour cavities in a liquid, small liquid-free zones (\"bubbles\" or \"voids\"), that are the consequence of forces acting upon the liquid. It usually occurs when a liquid is subjected to rapid changes of pressure that cause the formation of cavities in the liquid where the pressure is relatively low. When subjected to higher pressure, the voids implode and can generate an intense shock wave.\n\nCavitation is a significant cause of wear in some engineering contexts. Collapsing voids that implode near to a metal surface cause cyclic stress through repeated implosion. This results in surface fatigue of the metal causing a type of wear also called \"cavitation\". The most common examples of this kind of wear are to pump impellers, and bends where a sudden change in the direction of liquid occurs. Cavitation is usually divided into two classes of behavior: inertial (or transient) cavitation and non-inertial cavitation.\n\nInertial cavitation is the process where a void or bubble in a liquid rapidly collapses, producing a shock wave. Inertial cavitation occurs in nature in the strikes of mantis shrimps and pistol shrimps, as well as in the vascular tissues of plants. In man-made objects, it can occur in control valves, pumps, propellers and impellers.\n\nNon-inertial cavitation is the process in which a bubble in a fluid is forced to oscillate in size or shape due to some form of energy input, such as an acoustic field. Such cavitation is often employed in ultrasonic cleaning baths and can also be observed in pumps, propellers, etc.\n\nSince the shock waves formed by collapse of the voids are strong enough to cause significant damage to moving parts, cavitation is usually an undesirable phenomenon. It is very often specifically avoided in the design of machines such as turbines or propellers, and eliminating cavitation is a major field in the study of fluid dynamics. However, it is sometimes useful and does not cause damage when the bubbles collapse away from machinery, such as in supercavitation.\n\nInertial cavitation was first observed in the late 19th century, considering the collapse of a spherical void within a liquid. When a volume of liquid is subjected to a sufficiently low pressure, it may rupture and form a cavity. This phenomenon is coined \"cavitation inception\" and may occur behind the blade of a rapidly rotating propeller or on any surface vibrating in the liquid with sufficient amplitude and acceleration. A fast-flowing river can cause cavitation on rock surfaces, particularly when there is a drop-off, such as on a waterfall.\n\nOther ways of generating cavitation voids involve the local deposition of energy, such as an intense focused laser pulse (optic cavitation) or with an electrical discharge through a spark. Vapor gases evaporate into the cavity from the surrounding medium; thus, the cavity is not a perfect vacuum, but has a relatively low gas pressure. Such a low-pressure bubble in a liquid begins to collapse due to the higher pressure of the surrounding medium. As the bubble collapses, the pressure and temperature of the vapor within increases. The bubble eventually collapses to a minute fraction of its original size, at which point the gas within dissipates into the surrounding liquid via a rather violent mechanism which releases a significant amount of energy in the form of an acoustic shock wave and as visible light. At the point of total collapse, the temperature of the vapor within the bubble may be several thousand kelvin, and the pressure several hundred atmospheres.\n\nInertial cavitation can also occur in the presence of an acoustic field. Microscopic gas bubbles that are generally present in a liquid will be forced to oscillate due to an applied acoustic field. If the acoustic intensity is sufficiently high, the bubbles will first grow in size and then rapidly collapse. Hence, inertial cavitation can occur even if the rarefaction in the liquid is insufficient for a Rayleigh-like void to occur. High-power ultrasonics usually utilize the inertial cavitation of microscopic vacuum bubbles for treatment of surfaces, liquids, and slurries.\n\nThe physical process of cavitation inception is similar to boiling. The major difference between the two is the thermodynamic paths that precede the formation of the vapor. Boiling occurs when the local temperature of the liquid reaches the saturation temperature, and further heat is supplied to allow the liquid to sufficiently phase change into a gas. Cavitation inception occurs when the local pressure falls sufficiently far below the saturated vapor pressure, a value given by the tensile strength of the liquid at a certain temperature.\n\nIn order for cavitation inception to occur, the cavitation \"bubbles\" generally need a surface on which they can nucleate. This surface can be provided by the sides of a container, by impurities in the liquid, or by small undissolved microbubbles within the liquid. It is generally accepted that hydrophobic surfaces stabilize small bubbles. These pre-existing bubbles start to grow unbounded when they are exposed to a pressure below the threshold pressure, termed Blake's threshold.\n\nThe vapor pressure here differs from the meteorological definition of vapor pressure, which describes the partial pressure of water in the atmosphere at some value less than 100% saturation. Vapor pressure as relating to cavitation refers to the vapor pressure in equilibrium conditions and can therefore be more accurately defined as the equilibrium (or saturated) vapor pressure.\n\nNon-inertial cavitation is the process in which small bubbles in a liquid are forced to oscillate in the presence of an acoustic field, when the intensity of the acoustic field is insufficient to cause total bubble collapse. This form of cavitation causes significantly less erosion than inertial cavitation, and is often used for the cleaning of delicate materials, such as silicon wafers.\n\nHydrodynamic cavitation describes the process of vaporisation, bubble generation and bubble implosion which occurs in a flowing liquid as a result of a decrease and subsequent increase in local pressure. Cavitation will only occur if the local pressure declines to some point below the saturated vapor pressure of the liquid and subsequent recovery above the vapor pressure. If the recovery pressure is not above the vapor pressure then flashing is said to have occurred. In pipe systems, cavitation typically occurs either as the result of an increase in the kinetic energy (through an area constriction) or an increase in the pipe elevation.\n\nHydrodynamic cavitation can be produced by passing a liquid through a constricted channel at a specific flow velocity or by mechanical rotation of an object through a liquid. In the case of the constricted channel and based on the specific (or unique) geometry of the system, the combination of pressure and kinetic energy can create the hydrodynamic cavitation cavern downstream of the local constriction generating high energy cavitation bubbles.\n\nThe process of bubble generation, and the subsequent growth and collapse of the cavitation bubbles, results in very high energy densities and in very high local temperatures and local pressures at the surface of the bubbles for a very short time. The overall liquid medium environment, therefore, remains at ambient conditions. When uncontrolled, cavitation is damaging; by controlling the flow of the cavitation, however, the power can be harnessed and non-destructive. Controlled cavitation can be used to enhance chemical reactions or propagate certain unexpected reactions because free radicals are generated in the process due to disassociation of vapors trapped in the cavitating bubbles.\n\nOrifices and venturi are reported to be widely used for generating cavitation. A venturi has an inherent advantage over an orifice because of its smooth converging and diverging sections, such that it can generate a higher flow velocity at the throat for a given pressure drop across it. On the other hand, an orifice has an advantage that it can accommodate a greater number of holes (larger perimeter of holes) in a given cross sectional area of the pipe.\n\nThe cavitation phenomenon can be controlled to enhance the performance of high-speed marine vessels and projectiles, as well as in material processing technologies, in medicine, etc. Controlling the cavitating flows in liquids can be achieved only by advancing the mathematical foundation of the cavitation processes. These processes are manifested in different ways, the most common ones and promising for control being bubble cavitation and supercavitation. The first exact classical solution should perhaps be credited to the well- known solution by H. Helmholtz in 1868. The earliest distinguished studies of academic type on the theory of a cavitating flow with free boundaries and supercavitation were published in the book \"Jets, wakes and cavities\" followed by \"Theory of jets of ideal fluid\". Widely used in these books was the well-developed theory of conformal mappings of functions of a complex variable, allowing one to derive a large number of exact solutions of plane problems. Another venue combining the existing exact solutions with approximated and heuristic models was explored in the work \"Hydrodynamics of Flows with Free Boundaries\" that refined the applied calculation techniques based on the principle of cavity expansion independence, theory of pulsations and stability of elongated axisymmetric cavities, etc. and in \"Dimensionality and similarity methods in the problems of the hydromechanics of vessels\".\n\nA natural continuation of these studies was recently presented in \"The Hydrodynamics of Cavitating Flows\" – an encyclopedic work encompassing all the best advances in this domain for the last three decades, and blending the classical methods of mathematical research with the modern capabilities of computer technologies. These include elaboration of nonlinear numerical methods of solving 3D cavitation problems, refinement of the known plane linear theories, development of asymptotic theories of axisymmetric and nearly axisymmetric flows, etc. As compared to the classical approaches, the new trend is characterized by expansion of the theory into the 3D flows. It also reflects a certain correlation with current works of an applied character on the hydrodynamics of supercavitating bodies.\n\nHydrodynamic cavitation can also improve some industrial processes. For instance, cavitated corn slurry shows higher yields in ethanol production compared to uncavitated corn slurry in dry milling facilities.\n\nThis is also used in the mineralization of bio-refractory compounds which otherwise would need extremely high temperature and pressure conditions since free radicals are generated in the process due to the dissociation of vapors trapped in the cavitating bubbles, which results in either the intensification of the chemical reaction or may even result in the propagation of certain reactions not possible under otherwise ambient conditions.\n\nIn industry, cavitation is often used to homogenize, or mix and break down, suspended particles in a colloidal liquid compound such as paint mixtures or milk. Many industrial mixing machines are based upon this design principle. It is usually achieved through impeller design or by forcing the mixture through an annular opening that has a narrow entrance orifice with a much larger exit orifice. In the latter case, the drastic decrease in pressure as the liquid accelerates into a larger volume induces cavitation. This method can be controlled with hydraulic devices that control inlet orifice size, allowing for dynamic adjustment during the process, or modification for different substances. The surface of this type of mixing valve, against which surface the cavitation bubbles are driven causing their implosion, undergoes tremendous mechanical and thermal localized stress; they are therefore often constructed of super-hard or tough materials such as stainless steel, Stellite, or even polycrystalline diamond (PCD).\n\nCavitating water purification devices have also been designed, in which the extreme conditions of cavitation can break down pollutants and organic molecules. Spectral analysis of light emitted in sonochemical reactions reveal chemical and plasma-based mechanisms of energy transfer. The light emitted from cavitation bubbles is termed sonoluminescence.\n\nUse of this technology has been tried successfully in alkali refining of vegetable oils.\n\nHydrophobic chemicals are attracted underwater by cavitation as the pressure difference between the bubbles and the liquid water forces them to join together. This effect may assist in protein folding.\n\nCavitation plays an important role for the destruction of kidney stones in shock wave lithotripsy. Currently, tests are being conducted as to whether cavitation can be used to transfer large molecules into biological cells (sonoporation). Nitrogen cavitation is a method used in research to lyse cell membranes while leaving organelles intact.\n\nCavitation plays a key role in non-thermal, non-invasive fractionation of tissue for treatment of a variety of diseases and can be used to open the blood-brain barrier to increase uptake of neurological drugs in the brain.\n\nCavitation also plays a role in HIFU, a thermal non-invasive treatment methodology for cancer. \n\nUltrasound sometimes is used to increase bone formation, for instance in post-surgical applications.\nUltrasound treatments or exposure can create cavitation that potentially may \"result in a syndrome involving manifestations of nausea, headache, tinnitus, pain, dizziness, and fatigue.\".\n\nIt has been suggested that the sound of \"cracking\" knuckles derives from the collapse of cavitation in the synovial fluid within the joint. Movements that cause cracking expand the joint space, thus reducing pressure to the point of cavitation. It remains controversial whether this is associated with clinically significant joint injury such as osteoarthritis. Some physicians say that osteoarthritis is caused by cracking knuckles regularly, as this causes wear and tear and may cause the bone to weaken. The implication being that, it is not the \"bubbles popping,\" but rather, the bones rubbing together, that causes osteoarthritis.\n\nIn industrial cleaning applications, cavitation has sufficient power to overcome the particle-to-substrate adhesion forces, loosening contaminants. The threshold pressure required to initiate cavitation is a strong function of the pulse width and the power input. This method works by generating controlled acoustic cavitation in the cleaning fluid, picking up and carrying contaminant particles away so that they do not reattach to the material being cleaned.\n\nCavitation has been applied to egg pasteurization. A hole-filled rotor produces cavitation bubbles, heating the liquid from within. Equipment surfaces stay cooler than the passing liquid, so eggs don't harden as they did on the hot surfaces of older equipment. The intensity of cavitation can be adjusted, making it possible to tune the process for minimum protein damage.\n\nCavitation is, in many cases, an undesirable occurrence. In devices such as propellers and pumps, cavitation causes a great deal of noise, damage to components, vibrations, and a loss of efficiency. Cavitation has also become a concern in the renewable energy sector as it may occur on the blade surface of tidal stream turbines.\n\nWhen the cavitation bubbles collapse, they force energetic liquid into very small volumes, thereby creating spots of high temperature and emitting shock waves, the latter of which are a source of noise. The noise created by cavitation is a particular problem for military submarines, as it increases the chances of being detected by passive sonar.\n\nAlthough the collapse of a small cavity is a relatively low-energy event, highly localized collapses can erode metals, such as steel, over time. The pitting caused by the collapse of cavities produces great wear on components and can dramatically shorten a propeller's or pump's lifetime.\n\nAfter a surface is initially affected by cavitation, it tends to erode at an accelerating pace. The cavitation pits increase the turbulence of the fluid flow and create crevices that act as nucleation sites for additional cavitation bubbles. The pits also increase the components' surface area and leave behind residual stresses. This makes the surface more prone to stress corrosion.\n\nMajor places where cavitation occurs are in pumps, on propellers, or at restrictions in a flowing liquid.\n\nAs an impeller's (in a pump) or propeller's (as in the case of a ship or submarine) blades move through a fluid, low-pressure areas are formed as the fluid accelerates around and moves past the blades. The faster the blade moves, the lower the pressure around it can become. As it reaches vapor pressure, the fluid vaporizes and forms small bubbles of gas. This is cavitation. When the bubbles collapse later, they typically cause very strong local shock waves in the fluid, which may be audible and may even damage the blades.\n\nCavitation in pumps may occur in two different forms:\n\nSuction cavitation occurs when the pump suction is under a low-pressure/high-vacuum condition where the liquid turns into a vapor at the eye of the pump impeller. This vapor is carried over to the discharge side of the pump, where it no longer sees vacuum and is compressed back into a liquid by the discharge pressure. This imploding action occurs violently and attacks the face of the impeller. An impeller that has been operating under a suction cavitation condition can have large chunks of material removed from its face or very small bits of material removed, causing the impeller to look spongelike. Both cases will cause premature failure of the pump, often due to bearing failure. Suction cavitation is often identified by a sound like gravel or marbles in the pump casing.\n\nCommon causes of suction cavitation can include clogged filters, pipe blockage on the suction side, poor piping design, pump running too far right on the pump curve, or conditions not meeting NPSH (net positive suction head) requirements.\n\nIn automotive applications, a clogged filter in a hydraulic system (power steering, power brakes) can cause suction cavitation making a noise that rises and falls in synch with engine RPM. It is fairly often a high pitched whine, like set of nylon gears not quite meshing correctly.\n\nDischarge cavitation occurs when the pump discharge pressure is extremely high, normally occurring in a pump that is running at less than 10% of its best efficiency point. The high discharge pressure causes the majority of the fluid to circulate inside the pump instead of being allowed to flow out the discharge. As the liquid flows around the impeller, it must pass through the small clearance between the impeller and the pump housing at extremely high flow velocity. This flow velocity causes a vacuum to develop at the housing wall (similar to what occurs in a venturi), which turns the liquid into a vapor. A pump that has been operating under these conditions shows premature wear of the impeller vane tips and the pump housing. In addition, due to the high pressure conditions, premature failure of the pump's mechanical seal and bearings can be expected. Under extreme conditions, this can break the impeller shaft.\n\nDischarge cavitation in joint fluid is thought to cause the popping sound produced by bone joint cracking, for example by deliberately cracking one's knuckles.\n\nSince all pumps require well-developed inlet flow to meet their potential, a pump may not perform or be as reliable as expected due to a faulty suction piping layout such as a close-coupled elbow on the inlet flange. When poorly developed flow enters the pump impeller, it strikes the vanes and is unable to follow the impeller passage. The liquid then separates from the vanes causing mechanical problems due to cavitation, vibration and performance problems due to turbulence and poor filling of the impeller. This results in premature seal, bearing and impeller failure, high maintenance costs, high power consumption, and less-than-specified head and/or flow.\n\nTo have a well-developed flow pattern, pump manufacturer's manuals recommend about (10 diameters?) of straight pipe run upstream of the pump inlet flange. Unfortunately, piping designers and plant personnel must contend with space and equipment layout constraints and usually cannot comply with this recommendation. Instead, it is common to use an elbow close-coupled to the pump suction which creates a poorly developed flow pattern at the pump suction.\n\nWith a double-suction pump tied to a close-coupled elbow, flow distribution to the impeller is poor and causes reliability and performance shortfalls. The elbow divides the flow unevenly with more channeled to the outside of the elbow. Consequently, one side of the double-suction impeller receives more flow at a higher flow velocity and pressure while the starved side receives a highly turbulent and potentially damaging flow. This degrades overall pump performance (delivered head, flow and power consumption) and causes axial imbalance which shortens seal, bearing and impeller life.\nTo overcome cavitation:\nIncrease suction pressure if possible.\nDecrease liquid temperature if possible.\nThrottle back on the discharge valve to decrease flow-rate.\nVent gases off the pump casing.\n\nCavitation can occur in control valves. If the actual pressure drop across the valve as defined by the upstream and downstream pressures in the system is greater than the sizing calculations allow, pressure drop flashing or cavitation may occur. The change from a liquid state to a vapor state results from the increase in flow velocity at or just downstream of the greatest flow restriction which is normally the valve port. To maintain a steady flow of liquid through a valve the flow velocity must be greatest at the vena contracta or the point where the cross sectional area is the smallest. This increase in flow velocity is accompanied by a substantial decrease in the fluid pressure which is partially recovered downstream as the area increases and flow velocity decreases. This pressure recovery is never completely to the level of the upstream pressure. If the pressure at the vena contracta drops below the vapor pressure of the fluid bubbles will form in the flow stream. If the pressure recovers after the valve to a pressure that is once again above the vapor pressure, then the vapor bubbles will collapse and cavitation will occur.\n\nWhen water flows over a dam spillway, the irregularities on the spillway surface will cause small areas of flow separation in a high-speed flow, and, in these regions, the pressure will be lowered. If the flow velocities are high enough the pressure may fall to below the local vapor pressure of the water and vapor bubbles will form. When these are carried downstream into a high pressure region the bubbles collapse giving rise to high pressures and possible cavitation damage.\n\nExperimental investigations show that the damage on concrete chute and tunnel spillways can start at clear water flow velocities of between 12 and 15 m/s, and, up to flow velocities of 20 m/s, it may be possible to protect the surface by streamlining the boundaries, improving the surface finishes or using resistant materials.\n\nWhen some air is present in the water the resulting mixture is compressible and this damps the high pressure caused\nby the bubble collapses. If the flow velocities near the spillway invert are sufficiently high, aerators (or aeration devices) must be introduced to prevent cavitation. Although these have been installed for some years, the mechanisms of air entrainment at the aerators and the slow movement of the air away from the spillway surface are still challenging.\n\nThe spillway aeration device design is based upon a small deflection of the spillway bed (or sidewall) such as a ramp and offset to deflect the high flow velocity flow away from the spillway surface. In the cavity formed below the nappe, a local subpressure beneath the nappe is produced by which air is sucked into the flow. The complete design includes the deflection device (ramp, offset) and the air supply system.\n\nSome larger diesel engines suffer from cavitation due to high compression and undersized cylinder walls. Vibrations of the cylinder wall induce alternating low and high pressure in the coolant against the cylinder wall. The result is pitting of the cylinder wall, which will eventually let cooling fluid leak into the cylinder and combustion gases to leak into the coolant.\n\nIt is possible to prevent this from happening with the use of chemical additives in the cooling fluid that form a protective layer on the cylinder wall. This layer will be exposed to the same cavitation, but rebuilds itself. Additionally a regulated overpressure in the cooling system (regulated and maintained by the coolant filler cap spring pressure) prevents the forming of cavitation.\n\nFrom about the 1980s, new designs of smaller gasoline engines also displayed cavitation phenomena. One answer to the need for smaller and lighter engines was a smaller coolant volume and a correspondingly higher coolant flow velocity. This gave rise to rapid changes in flow velocity and therefore rapid changes of static pressure in areas of high heat transfer. Where resulting vapor bubbles collapsed against a surface, they had the effect of first disrupting protective oxide layers (of cast aluminium materials) and then repeatedly damaging the newly formed surface, preventing the action of some types of corrosion inhibitor (such as silicate based inhibitors). A final problem was the effect that increased material temperature had on the relative electrochemical reactivity of the base metal and its alloying constituents. The result was deep pits that could form and penetrate the engine head in a matter of hours when the engine was running at high load and high speed. These effects could largely be avoided by the use of organic corrosion inhibitors or (preferably) by designing the engine head in such a way as to avoid certain cavitation inducing conditions.\n\nSome hypotheses relating to diamond formation posit a possible role for cavitation—namely cavitiation in the kimberlite pipes providing the extreme pressure needed to change pure carbon into the rare allotrope that is diamond.\n\nThe loudest three sounds ever recorded, during the 1883 eruption of Krakatoa, are now understood as the bursts of three huge cavitation bubbles, each larger than the last, formed in the volcano's throat. Rising magma, filled with dissolved gasses and under immense pressure, encountered a different magma that compressed easily, allowing bubbles to grow and combine. \n\nThere exist macroscopic white lamellae inside quartz and other minerals in the Bohemian Massif and even at another places in whole of the world like wavefronts generated by a meteorite impact according to the Rajlich's Hypothesis. The hypothetical wavefronts are composed of many microcavities. Their origin is seen in a physical phenomenon of ultrasonic cavitation, which is well known from the technical practice.\n\nCavitation occurs in the xylem of vascular plants when the tension of water within the xylem exceeds atmospheric pressure. The sap vaporizes locally so that either the vessel elements or tracheids are filled with water vapor. Plants are able to repair cavitated xylem in a number of ways. For plants less than 50 cm tall, root pressure can be sufficient to redissolve the vapor. Larger plants direct solutes into the xylem via \"ray cells\", or in tracheids, via osmosis through bordered pits. Solutes attract water, the pressure rises and vapor can redissolve. In some trees, the sound of the cavitation is audible, particularly in summer, when the rate of evapotranspiration is highest. Some deciduous trees have to shed leaves in the autumn partly because cavitation increases as temperatures decrease.\n\nJust as cavitation bubbles form on a fast-spinning boat propeller, they may also form on the tails and fins of aquatic animals. This primarily occurs near the surface of the ocean, where the ambient water pressure is low.\n\nCavitation may limit the maximum swimming speed of powerful swimming animals like dolphins and tuna. Dolphins may have to restrict their speed because collapsing cavitation bubbles on their tail are painful. Tuna have bony fins without nerve endings and do not feel pain from cavitation. They are slowed down when cavitation bubbles create a vapor film around their fins. Lesions have been found on tuna that are consistent with cavitation damage.\n\nSome sea animals have found ways to use cavitation to their advantage when hunting prey. The pistol shrimp snaps a specialized claw to create cavitation, which can kill small fish. The mantis shrimp (of the \"smasher\" variety) uses cavitation as well in order to stun, smash open, or kill the shellfish that it feasts upon.\n\nThresher sharks use 'tail slaps' to debilitate their small fish prey and cavitation bubbles have been seen rising from the apex of the tail arc.\n\nIn the last half-decade, coastal erosion in the form of inertial cavitation has been generally accepted. Bubbles in an incoming wave are forced into cracks in the cliff being eroded. Varying pressure decompresses some vapor pockets which subsequently implode. The resulting pressure peaks can blast apart fractions of the rock.\n\n\n\n"}
{"id": "1867894", "url": "https://en.wikipedia.org/wiki?curid=1867894", "title": "Certified wood", "text": "Certified wood\n\nCertified wood and paper products come from responsibly managed forests – as defined by a particular standard. With third-party forest certification, an independent organization develops standards of good forest management, and independent auditors issue certificates to forest operations that comply with those standards.\n\nForest certification programs typically require that forest management practices conform to existing laws. Other basic requirements or characteristics of forest certification programs include:\n\nBasic requirements of credible forest certification programs include:\n\nToday there are more than 50 certification programs worldwide addressing the many types of forests and tenures around the world. The two largest international forest certification programs are the Forest Stewardship Council (FSC) and the Programme for the Endorsement of Forest Certification (PEFC).\n\nThe PEFC is the largest certification framework in terms of forest area, with approximately two-thirds of the total certified area. The FSC program is the fastest growing.\n\nThird-party forest certification was pioneered in the early 1990s by the FSC, a collaboration between environmental NGOs, forest product companies and social interests. Competing systems quickly emerged throughout the world. Some commentators, including Jared Diamond, have suggested that many competing standards were set up by logging companies specifically aiming to confuse consumers with less rigorously enforced but similarly named competing standards.\n\nIn the United States and Canada, there are a number of forest certification programs. Three of these programs are endorsed by the PEFC. They are the American Tree Farm System (ATFS), the Canadian Standards Association’s Sustainable Forest Management Standard and the Sustainable Forestry Initiative (SFI) Program. ATFS is applicable only in the United States; the Canadian Standards Association SFM Standard is applicable only in Canada. SFI is applicable to both the United States and Canada. The FSC, program is applied throughout North America. SFI is the world’s largest regional forest certification program in terms of total certified forest area[1].\n\nThe National Association of State Foresters in the USA passed a resolution in 2008 that supports \"all\" of the forest certification systems used in the USA and recognized the value of their differences: “... the ATFS, FSC, and SFI systems include the fundamental elements of credibility and make positive contributions to forest sustainability... No certification program can credibly claim to be ‘best’, and no certification program that promotes itself as the only certification option can maintain credibility. Forest ecosystems are complex and a simplistic ‘one size fits all’ approach to certification cannot address all sustainability needs.”.\n\nThe Canadian Council of Forest Ministers issued a statement in 2008 on forest certification standards in Canada, which said: “In Canada, each jurisdiction’s forest laws, policies and administrative requirements comprise an framework that fully characterizes what sustainable forest management (SFM) means in that jurisdiction, and what actions may take place on public and/or private forest land. Governments in Canada support third-party forest certification as a tool to demonstrate the rigor of Canada’s forest management laws, and to document the country’s world-class sustainable forest management record. The forest management standards of the Canadian Standards Association (CSA), the FSC and the Sustainable Forestry Initiative (SFI) are all used in Canada. Governments in Canada accept that these standards demonstrate, and promote the sustainability of forest management practices in Canada.” \n\nChain of Custody certification tracks the certified material through the production process – from the forest to the consumer, including all successive stages of processing, transformation, manufacturing and distribution. It also provides evidence that certified material in a certified product originates from certified forests.\n\nThe United Nations reports that between January 2009 and May 2010, the total number of PEFC and FSC chain-of-custody certificates issued worldwide increased by 88% for a total of 23,717 certificates (this does not include SFI certificates).\n\nForest certification is a voluntary process. About 10% of the world’s forest under at least one certification program. Customers that choose to buy certified products are supporting land managers, land owners and forest product companies that have made a commitment to meeting the standards of forest certification.\n\nThird-party forest certification is a useful tool for those seeking to purchase paper and wood products that come from forests that are well-managed and use materials that are legally harvested. Incorporating third-party certification into forest product buying practices can be a centerpiece for responsible wood and paper purchasing policies that include factors such as the protection of sensitive forest values, thoughtful material selection and efficient use of products.\n\nThe 2009-2010 United Nations Market Review reported that companies that produced or traded in certified forest products often had a market advantage during the 2008-2009 recession because, in a buyers’ market, buyers could be more selective in choosing their sources of supply. The report cites four demand drivers for certification:\n\nThe World Resources Institute, in partnership with the Environmental Investigation Agency, released a fact sheet designed to answer some of the frequently asked questions about the Lacey Act, which was amended in 2008 to ban commerce in illegally sourced plants and their products—including timber, wood, and paper products. The fact sheet says forest certification is a very good approach for demonstrating due care by showing government and customers that a company has taken proactive steps to eliminate illegal wood or plant material from its supply chain. Certification does not relieve importers of the requirement to submit appropriate import declaration information to U.S. government agencies.\n\n"}
{"id": "32703814", "url": "https://en.wikipedia.org/wiki?curid=32703814", "title": "Chirality", "text": "Chirality\n\nChirality is a property of asymmetry important in several branches of science. The word \"chirality\" is derived from the Greek (\"kheir\"), \"hand,\" a familiar chiral object.\n\nAn object or a system is \"chiral\" if it is distinguishable from its mirror image; that is, it cannot be superposed onto it. Conversely, a mirror image of an \"achiral\" object, such as a sphere, cannot be distinguished from the object. A chiral object and its mirror image are called \"enantiomorphs\" (Greek, \"opposite forms\") or, when referring to molecules, \"enantiomers\". A non-chiral object is called \"achiral\" (sometimes also \"amphichiral\") and can be superposed on its mirror image. If the object is non-chiral and is imagined as being colored blue and its mirror image is imagined as colored yellow, then by a series of rotations and translations the two can be superposed, producing green, with none of the original colors remaining.\n\nThe term was first used by Lord Kelvin in 1893 in the second Robert Boyle Lecture at the Oxford University Junior Scientific Club which was published in 1894:\n\nHuman hands are perhaps the most universally recognized example of chirality. The left hand is a non-superimposable mirror image of the right hand; no matter how the two hands are oriented, it is impossible for all the major features of both hands to coincide across all axes. This difference in symmetry becomes obvious if someone attempts to shake the right hand of a person using their left hand, or if a left-handed glove is placed on a right hand. In mathematics, \"chirality\" is the property of a figure that is not identical to its mirror image.\n\nIn mathematics, a figure is chiral (and said to have chirality) if it cannot be mapped to its mirror image by rotations and translations alone. For example, a right shoe is different from a left shoe, and clockwise is different from anticlockwise. See for a full mathematical definition.\n\nA chiral object and its mirror image are said to be enantiomorphs. The word \"enantiomorph\" stems from the Greek (enantios) 'opposite' + (morphe) 'form'. A non-chiral figure is called achiral or amphichiral.\n\nThe helix (and by extension a spun string, a screw, a propeller, etc.) and Möbius strip are chiral two-dimensional objects in three-dimensional ambient space. The J, L, S and Z-shaped \"tetrominoes\" of the popular video game Tetris also exhibit chirality, but only in a two-dimensional space.\n\nMany other familiar objects exhibit the same chiral symmetry of the human body, such as gloves, glasses (where two lenses differ in prescription), and shoes. A similar notion of chirality is considered in knot theory, as explained below.\n\nSome chiral three-dimensional objects, such as the helix, can be assigned a right or left handedness, according to the right-hand rule.\n\nIn geometry a figure is achiral if and only if its symmetry group contains at least one \"orientation-reversing\" isometry.\nIn two dimensions, every figure that possesses an axis of symmetry is achiral, and it can be shown that every \"bounded\" achiral figure must have an axis of symmetry.\nIn three dimensions, every figure that possesses a plane of symmetry or a center of symmetry is achiral. There are, however, achiral figures lacking both plane and center of symmetry.\nIn terms of point groups, all chiral figures lack an improper axis of rotation (S). This means that they cannot contain a center of inversion (i) or a mirror plane (σ). Only figures with a point group designation of C, C, D, T, O, or I can be chiral.\n\nA knot is called achiral if it can be continuously deformed into its mirror image, otherwise it is called chiral. For example, the unknot and the figure-eight knot are achiral, whereas the trefoil knot is chiral.\n\nIn physics, chirality may be found in the spin of a particle, where the handedness of the object is determined by the direction in which the particle spins. Not to be confused with helicity, which is the projection of the spin along the linear momentum of a subatomic particle, chirality is a purely quantum mechanical phenomenon like spin. Although both can have left-handed or right-handed properties, only in the massless case do they have a simple relation. In particular for a massless particle the helicity is the same as the chirality while for an antiparticle they have opposite sign.\n\nThe \"handedness\" in both chirality and helicity relate to the rotation of a particle while it proceeds in linear motion with reference to the human hands. The thumb of the hand points towards the direction of linear motion whilst the fingers curl into the palm, representing the direction of rotation of the particle (i.e. clockwise and counterclockwise). Depending on the linear and rotational motion, the particle can either be defined by left-handedness (ex. translating leftwards and rotating counterclockwise) or right-handedness (ex. translating in the right direction and rotating clockwise). A symmetry transformation between the two is called parity. Invariance under parity by a Dirac fermion is called \"chiral symmetry\".\n\nElectromagnetic wave propagation as handedness is wave polarization and described in terms of helicity (occurs as a helix). Polarization of an electromagnetic wave is the property that describes the orientation, i.e., the time-varying, direction (vector), and amplitude of the electric field vector. For a depiction, see the adjacent image.\n\nA \"chiral molecule\" is a type of molecule that has a non-superposable mirror image. The feature that is most often the cause of chirality in molecules is the presence of an asymmetric carbon atom.\n\nThe term \"chiral\" in general is used to describe the object that is non-superposable on its mirror image.\n\nIn chemistry, chirality usually refers to molecules. Two mirror images of a chiral molecule are called enantiomers or optical isomers. Pairs of enantiomers are often designated as \"right-\", \"left-handed\" or if it has no bias achiral. As polarized light passes through a chiral molecule, the plane of polarization, when viewed along the axis toward the source, will be rotated in a clockwise (to the right) or anticlockwise (to the left). A right handed rotation is dextrorotary (d); that to the left is levorotary (l). The d- and l-isomers are the same compound but are called enantiomers. An equimolar mixture of the two optical isomers will produce no net rotation of polarized light as it passes through. Left handed molecules have l- prefixed to their names; d- is prefixed to right handed molecules.\n\nMolecular chirality is of interest because of its application to stereochemistry in inorganic chemistry, organic chemistry, physical chemistry, biochemistry, and supramolecular chemistry.\n\nMore recent developments in chiral chemistry include the development of chiral inorganic nanoparticles that may have the similar tetrahedral geometry as chiral centers associated with sp3 carbon atoms traditionally associated with chiral compounds, but at larger scale. Helical and other symmetries of chiral nanomaterials were also obtained.\n\nAll of the known life-forms show specific chiral properties in chemical structures as well as macroscopic anatomy, development and behavior. In any specific organism or evolutionarily related set thereof, individual compounds, organs, or behavior are found in the same single enantiomorphic form. Deviation (having the opposite form) could be found in a small number of chemical compounds, or certain organ or behavior but that variation strictly depends upon the genetic make up of the organism. From chemical level (molecular scale), biological systems show extreme stereospecificity in synthesis, uptake, sensing, metabolic processing. A living system usually deals with two enantiomers of same compound in a drastically different way.\n\nIn biology, homochirality is a common property of amino acids and carbohydrates. The chiral protein-making amino acids, which are translated through the ribosome from genetic coding, occur in the form. However, -amino acids are also found in nature. The monosaccharides (carbohydrate-units) are commonly found in -configuration. DNA double helix is chiral (as any kind of helix is chiral), and B-form of DNA shows a right-handed turn.\n\nSometimes, when two enantiomers of a compound found in organisms, they significantly differ in their taste, smell and other biological actions. For example, (+)-Limonene found in orange (causing its smell), and (–)-Limonene found in Lemons (causing its smell), show different smells due to different biochemical interactions at human nose. (+)-Carvone is responsible for the smell of Caraway seed oil whereas (–)-carvone is responsible for smell of Spearmint oil.\n\nAlso, for artificial compounds, including medicines, in case of chiral drugs, the two enantiomers sometimes show remarkable difference in effect of their biological actions. Darvon (Dextropropoxyphene) is a painkiller, whereas its enantiomer, Novrad (Levopropoxyphene) is an anti-cough agent. In case of Penicillamine, the S-isomer used in treatment of primary chronic arthritis, Whereas the R-isomer has no therapeutic effect as well as being highly toxic. In some cases the less therapeutically active enantiomer can cause side effects. For example, S-naproxen is an analgesic but the R-isomer cause renal problems. The naturally occurring plant form of alpha-tocopherol (vitamin E) is RRR-α-tocopherol whereas the synthetic form (all-racemic vitamin E, or dl-tocopherol) is equal parts of the stereoisomers RRR, RRS, RSS, SSS, RSR, SRS, SRR and SSR with progressively decreasing biological equivalency, so that 1.36 mg of dl-tocopherol is considered equivalent to 1.0 mg of d-tocopherol.\nMacroscopic example of Chirality is found in plant kingdom, animal kingdom and all other groups of organism. A simple example is the coiling direction of any climber plants. It may be one of two possible type of helix.\nIn anatomy, chirality is found in the imperfect mirror image symmetry of many kinds of animal bodies. Organisms such as gastropods exhibit chirality in their coiled shells, resulting in an asymmetrical appearance. Over 90% of gastropod species have \"dextral\" (right-handed) shells in their coiling, but a small minority of species and genera are virtually always \"sinistral\" (left-handed). A very few species (for example \"Amphidromus perversus\") show an equal mixture of dextral and sinistral individuals.\n\nIn humans, chirality (also referred to as \"handedness\" or \"laterality\") is an attribute of humans defined by their unequal distribution of fine motor skill between the left and right hands. An individual who is more dexterous with the right hand is called \"right-handed\", and one who is more skilled with the left is said to be \"left-handed\". Chirality is also seen in the study of facial asymmetry.\n\nIn flatfish, the Summer flounder or fluke are left-eyed, while halibut are right-eyed.\n\n\n"}
{"id": "34688121", "url": "https://en.wikipedia.org/wiki?curid=34688121", "title": "Coastal hazards", "text": "Coastal hazards\n\nCoastal Hazards are physical phenomena that expose a coastal area to risk of property damage, loss of life and environmental degradation. Rapid-onset hazards last over periods of minutes to several days and examples include major cyclones accompanied by high winds, waves and surges or tsunamis created by submarine earthquakes and landslides. Slow-onset hazards develop incrementally over longer time periods and examples include erosion and gradual inundation.\n\nSince early civilisation, coastal areas have been attractive settling grounds for human population as they provided abundant marine resources, fertile agricultural land and possibilities for trade and transport. This has led to high population densities and high levels of development in many coastal areas and this trend is continuing into the 21st century. At present, about 1,2 billion people live in coastal areas globally, and this number is predicted to increase to 1,8–5,2 billion by the 2080s due to a combination of population growth and coastal migration. Along with this increase follows major investments in infrastructure and the build environment.\n\nThe characteristics of coastal environments, however, pose some great challenges to human habitation. Coastlines are highly dynamic natural systems that interact with terrestrial, marine and atmospheric processes and undergo continuous change in response to these processes. Over the years, human society has often failed to recognize the hazards related to these dynamics and this has led to major disasters and societal disruption to various degrees. Even today, coastal development is often taking place with little regard to the hazards present in these environments, although climate change is likely to increase the general hazard levels. Societal activities in coastal areas can also pose a hazard to the natural balance of coastal systems, thereby disrupting e.g. sensitive ecosystems and subsequently human livelihood.\n\nCoastal hazard management has become an increasingly important aspect of coastal planning in order to improve the resilience of society to coastal hazards. Possible management options include hard engineering structures, soft protection measures, various accommodation approaches as well as a managed retreat from the coastline. For addressing coastal hazards, it is also important to have early warning systems and emergency management plans in place to be able to address sudden and potential disastrous hazards i.e. major flooding events. Events as the Hurricane Katrina affecting the southern USA in 2005 and the cyclone Nargis affecting Myanmar in 2008 provides clear examples of the importance of timely coastal hazard management.\n\nThere are many different types of environments along the coasts of the United States with very diverse features that affect, influence, and mold the near-shore processes that are involved. Understanding these ecosystems and environments can further advance the mitigating techniques and policy-making efforts against natural and man-made coastal hazards in these vulnerable areas. The five most common types of coastal zones range from the northern ice-pushing, mountainous coastline of Alaska and Maine, the barrier island coasts facing the Atlantic, the steep, cliff-back headlands along the pacific coast, the marginal-sea type coastline of the Gulf region, and the coral reef coasts bordering Southern Florida and Hawaii.\n\nIce-pushing/mountainous coastline\n\nThese coastal regions along the northernmost part of the nation were affected predominantly by, along with the rest of the Pacific Coast, continuous tectonic activity, forming a very long, irregular, ridged, steep and mostly mountainous coastline. These environments are heavily occupied with permafrost and glaciers, which are the two major conditions affecting Alaska's Coastal Development.\n\nBarrier island coastline\n\nBarrier islands are a land form system that consists of fairly narrow strips of sand running parallel to the mainland and play a significant role in mitigating storm surges and oceans swells as natural storm events occur. The morphology of the various types and sizes of barrier islands depend on the wave energy, tidal range, basement controls, and sea level trends. The islands create multiple unique environments of wetland systems including marshes, estuaries, and lagoons.\n\nSteep, cliff-backing abrasion coastline\n\nThe coastline along the western part of the nation consists of very steep, cliffed rock formations generally with vegetative slopes descending down and a fringing beach below. The various sedimentary, metamorphic, and volcanic rock formations assembled along a tectonically disturbed environment, all with altering resistances running perpendicular, cause the ridged, extensive stretch of uplifted cliffs that form the peninsulas, lagoons, and valleys.\n\nMarginal-sea type coastline\n\nThe southern banks of the United States border the Gulf of Mexico, intersecting numerous rivers, forming many inlets bays, and lagoons along its coast, consisting of vast areas of marsh and wetlands. This region of landform is prone to natural disasters yet highly and continuously developed, with man-made structures attaining to water flow and control.\n\nCoral reef coastline\n\nCoral reefs are located off the shores of the southern Florida and Hawaii consisting of rough and complex natural structures along the bottom of the ocean floor with extremely diverse ecosystems, absorbing up to ninety percent of the energy dissipated from wind-generated waves. This process is a significant buffer for the inner-lying coastlines, naturally protecting and minimizing the impact of storm surge and direct wave damage. Because of the highly diverse ecosystems, these coral reefs not only provide for the shoreline protection, but also deliver an abundant amount of services to fisheries and tourism, increasing its economic value.\n\nNatural VS Human disasters\n\nThe population that lives along or near our coastlines are an extremely vulnerable population. There are numerous issues facing our coastlines and there are two main categories that these hazards can be placed under, Natural disasters and Human disasters. Both of these issues cause great damage to our coastlines and discussion is still ongoing regarding what standards or responses need to be met to help both the individuals who want to continue living along the coastline, while keeping them safe and not eroding more coastline away. Natural disasters are disasters that are out of human control and are usually caused by the weather. Disasters that include but are not limited to; storms, tsunamis, typhoons, flooding, tides, waterspouts, nor'easters, and storm surge. Human disasters occur when humans are the main culprit behind why the disaster happened. Some human disasters are but are not limited to; pollution, trawling, and human development. Natural and human disasters continue to harm the coastlines severely and they need to be researched in order to prepare/stop the hazards if possible.\n\nThe populations that live near or along the coast experience many hazards and it affects millions of people. Around ten million people globally feel the effects of coastal problems yearly and most are due to certain natural hazards like coastal flooding with storm surges and typhoons. A major problem related to coastal regions deals with how the entire global environment is changing and in response, the coastal regions are easily affected.\n\nStorms, Flooding and Erosion\n\nStorms are one of the major hazards that are associated to coastal regions. Storms, flooding, and erosion are closely associated and can happen simultaneously. Tropical storms or Hurricanes especially can devastate coastal regions. For example, Florida during Hurricane Andrew occurred in 1992 that caused extreme damage. It was a category five hurricane that caused $26.5 billion in damages and even 23 individuals lost their lives from the storm. Hurricane Katrina also caused havoc along the coast to show the extreme force a hurricane can do in a certain region. The Chennai Floods of 2015, which affected many people, is an example of flooding due to cyclones. People across the whole state of Tamil Nadu felt its impact and even parts of Andhra Pradesh got affected. There was a loss of Rs.900 crore and 280 people died. Many cyclones like this happen across Asia but the media reports only minor hurricanes which hit the United States.\n\nAlmost all storms with high wind and water cause erosion along the coast. Erosion occurs when but not limited to; along shore currents, tides, sea level rise and fall, and high winds. Larger amounts of erosion cause the coastline to erode away at a faster rate and can leave people homeless and leave less land to develop or keep for environmental reasons. Coastal erosion has been increasing over the past few years and it is still on the rise which makes it a major coastline hazard. In the United States, 45 percent of its coast line is along the Atlantic or Gulf coast and the erosion rate per year along the Gulf coast is at six feet a year. The average rate of erosion along the Atlantic is around two to three feet a year. Even with these findings, erosion rates in specific locations vary because of various environmental factors such as major storms that can cause major erosion upwards to 100 feet or more in only one day.\n\nPollution, Trawling and Human Development\n\nPollution, trawling, and human development are major human disasters that affect coastal regions. There are two main categories related to pollution, point source pollution, and nonpoint source pollution. Point source pollution is when there is an exact location such as a pipeline or a body of water that leads into the rivers and oceans. Known dumping into the ocean is also another point source of pollution. Nonpoint source pollution would pertain more to fertilizer runoff, and industrial waste. Examples of pollution that affect the coastal regions are but are not limited to; fertilizer runoff, oil spills, and dumping of hazardous materials into the oceans. More human acts that hurt the coastline are as follows; waste discharge, fishing, dredging, mining, and drilling. Oil spills are one of the most hazardous dangers towards coastal communities. They are hard to contain, difficult to clean up, and devastate everything. The fish, animals such as birds, the water, and especially the coastline near the spill. The most recent oil spill that had everybody concerned with oil spill was the BP oil spill.\n\nTrawling hurts the normal ecosystems in the water around the coastline. It depletes all ecosystems on the ocean floor such as, flounder, shellfish, marsh etc.. It is simply a giant net that is drug across the ocean floor and destroys and catches anything in its path. Human development is one of the major problems when facing coastal hazards. The overall construction of buildings and houses on the coast line takes away the natural occurrences to handle the fluctuation in water and sea level rise. Building houses in pre-flood areas or high risk areas that are extremely vulnerable to flooding are major concerns towards human development in coastal regions. Having houses and buildings in areas that are known to have powerful storms that will create people to be in risk by living there. Also pertaining to barrier islands, where land is at risk for erosion but they still continue to build there anyway. More and more houses today are being taken by the ocean; look at picture above.\n\nCoastal hazards & climate change\n\nThe predicted climate change is adding an extra risk factor to human settlement in coastal areas. Whereas the natural dynamics that shape our coastlines have been relatively stable and predictable over the last centuries, much more rapid change is now expected in processes as sea level rise, ocean temperature and acidity, tropical storm intensity and precipitation/runoff patterns. The world's coastlines will respond to these changes in different ways and at different pace depending on their bio-geophysical characteristics, but generally society will have to recognize that past coastal trends cannot be directly projected into the future. Instead, it is necessary to consider how different coastal environments will respond to the predicted climate change and take the expected future hazards into account in the coastal planning processes.\n\nNational Flood Insurance Program\n\nThe National Flood Insurance Program or NFIP was instituted in 1968 and offers home owners in qualifying communities an opportunity to rebuild and recover after flooding events following the decision by insurance companies to discontinue providing flood insurance. This decision was made on behalf of the private insurers after continually high and widespread flood losses. The goals of this program are to not only better protect individuals from flood, but to reduce property losses, and reduce the total amount disbursed for flood loses by the government. Only communities which have adopted and implemented mitigation policies that are compliant with or exceed federal regulations. The regulatory policies reduce risk to life and property located within floodplains. The NFIP also comprehensively mapped domestic floodplains increasing public awareness of risk. The majority of structures were constructed after the mapping was completed and risk could be assessed. To reduce the cost to these owners, which constitute roughly 25% of the total policies the rates for insurance are subsidized.\n\nCoastal States Organization\n\nThe Coastal States Organization or COS was established in 1970 to represent 35 U.S. sub-federal governments on issues of coastal policies. CSO lobbies Congress on issues pertaining to Coastal Policy allowing states input on federal policy decisions. Funding, support, water quality, coastal hazards, and coastal zone management are the primary issues COS promotes. The strategic goals of COS are to provide information and assistance to members,evaluate and manage coastal needs, and secure long term funding for member states initiatives.\n\nCoastal Zone Management Act\n\nIn 1972 the Coastal Zone Management Act or CZMA works to streamline the policies which states create to a minimum federal standard for environmental protection. CZMA establishes the national policy for the development and implementation of regulatory programs for coastal land usage, which is supposed to be reflected in state legislation such as CAMA. CZMA also provides minimum building requirements to make the insurance provided through the NFIP less expensive for the government to operate by mitigating losses. Congress found that it was necessary to establish the minimum which programs should provide for. Each coastal state is required to have a program with 7 distinct parts: Identifying land uses,Identifying critical coastal areas, Management measures,Technical assistance, Public participation, Administrative coordination, State coastal zone boundary modification.\n\nThe Coastal Area Management Act\n\nThe Coastal Area Management Act or CAMA is policy that was implemented by the state of North Carolina in 1974 to work in-tandem with the CZMA. It creates a cooperative program between the state and local governments. The State government operates in an advisory capacity and reviews decisions made by local government planners. The goal of this legislation was to create a management system capable of preserving the coastal environment, insure the preservation of land and water resources, balance the use of coastal resources and establish guidelines and standards for conservations, economic development, tourism, transportation, and the protection of common law.\n\nDue to the increasing urbanization along the coastlines, planning and management are essential to protecting the ecosystems and environment from depleting. Coastal management is becoming implemented more because of the movement of people to the shore and the hazards that come with the territory. Some of the hazards include movement of barrier islands, sea level rise, hurricanes, nor'easters, earthquakes, flooding, erosion, pollution and human development along the coast. The Coastal Zone Management Act (CZMA) was created in 1972 because of the continued growth along the coast, this act introduced better management practices such as integrated coastal zone management, adaptive management and the use mitigation strategies when planning. According to the Coastal Zone Management Act, the objectives are to remain balanced to \"preserve, protect, develop, and where possible, to restore or enhance the resources of the nation's coastal zone\".\nThe development of the land can strongly affect the sea, for example the engineering of structures versus non-structures and the effects of erosion along the shore.\n\nIntegrated coastal zone management\n\nIntegrated coastal zone management means the integration of all aspects of the coastal zone; this includes environmentally, socially, culturally politically and economically to meet a sustainable balance all around. Sustainability is the goal to allow development yet protect the environment in which we develop. Coastal zones are fragile and do not do well with change so it is important to acquire sustainable development. The integration from all views will entitle a holistic view for the best implementation and management of that country, region and local scales. The five types of integration include integration among sectors, integration between land and water elements of the coastal zone, integration amount levels of government, integration between nations and integration among disciplines are all essential to meet the needs for implementation.\nManagement practices include\nThese four management practices should be based on a bottom-up approach, meaning the approach starts from a local level which is more intimate to the specific environment of that area. After assessment from the local level, the state and federal input can be implemented. The bottom-up approach is key for protecting the local environments because there is a diversity of environments that have specific needs all over the world.\n\nAdaptive management\n\nAdaptive management is another practice of development adaptation with the environment. Resources are the major factor when managing adaptively to a certain environment to accommodate all the needs of development and ecosystems. Strategies used must be flexible by either passive or active adaptive management include these key features:\nTo achieve adaptive management is testing the assumptions to achieve a desired outcome, such as trial and error, find the best known strategy then monitoring it to adapt to the environment, and learning the outcomes of success and failures of a project.\n\nMitigation\n\nThe purpose of mitigation is not only to minimize the loss of property damage, but minimize environmental damages due to development. To avoid impacts by not taking or limiting actions, to reduce or rectify impacts by rehabilitation or restoring the affected environments or instituting long-term maintenance operations and compensating for impacts by replacing or providing substitute environments for resources\nStructural mitigation is the current solution to eroding beaches and movement of sand is the use of engineered structures along the coast have been short lived and are only an illusion of safety to the public that result in long term damage of the coastline. Structural management deals with the use of the following: groins which are man-made solution to longshore current movements up and down the coast. The use of groins are efficient to some extent yet cause erosion and sand build up further down the beaches. Bulkheads are man-made structures that help protect the homes built along the coast and other bodies of water that actually induce erosion in the long run. Jetties are structures built to protect sand movement into the inlets where boats for fishing and recreation move through.\nThe use of nonstructural mitigation is the practice of using organic and soft structures for solutions to protect against coastal hazards. These include: artificial dunes, which are used to create dunes that have been either developed on or eroded. There needs to be at least two lines of dunes before any development can occur. Beach Nourishment is a major source of nonstructural mitigation to ensure that beaches are present for the communities and for the protection of the coastline. Vegetation is a key factor when protecting from erosion, specifically for to help stabilize dune erosion.\n\n\n"}
{"id": "7876225", "url": "https://en.wikipedia.org/wiki?curid=7876225", "title": "Compressed earth block", "text": "Compressed earth block\n\nA compressed earth block (CEB), also known as a \"pressed earth block\" or a \"compressed soil block\", is a building material made primarily from damp soil compressed at high pressure to form blocks. Compressed earth blocks use a mechanical press to form blocks out of an appropriate mix of fairly dry inorganic subsoil, non-expansive clay and aggregate. If the blocks are stabilized with a chemical binder such as Portland cement they are called \"compressed stabilized earth block\" (CSEB) or \"stabilized earth block\" (SEB). Typically, around is applied in compression, and the original soil volume is reduced by about half.\n\nCreating CEBs differs from rammed earth in that the latter uses a larger formwork into which earth is poured and manually tamped down, creating larger forms such as a whole wall or more at one time rather than building blocks. CEBs differ from mud bricks in that the latter are not compressed and solidify through chemical changes that take place as they air dry. The compression strength of properly made CEB can meet or exceed that of typical cement or mud brick. Building standards have been developed for CEB.\n\nCEBs are assembled onto walls using standard bricklaying and masonry techniques. The mortar may be a simple slurry made of the same soil/clay mix without aggregate, spread or brushed very thinly between the blocks for bonding, or cement mortar may also be used for high strength, or when construction during freeze-thaw cycles causes stability issues. Hydraform blocks are shaped to be interlocking.\n\nCEB technology has been developed for low-cost construction, as an alternative to adobe, and with some advantages. A commercial industry has been advanced by eco-friendly contractors, manufacturers of the mechanical presses, and by cultural acceptance of the method. In the United States, most general contractors building with CEB are in the Southwestern states: New Mexico, Colorado, Arizona, California, and to a lesser extent in Texas. The methods and presses have been used for many years in Mexico, and in developing countries.\n\nThe South African Department of Water Affairs and Forestry considers that CEB, locally called \"Dutch brick\" is an appropriate technology for a developing country, as are adobe, rammed earth and cob. All use natural building materials.\nIn 2002 the International Institute for Energy Conservation was one of the winners of a World Bank Development Marketplace Award for a project to make an energy-efficient Dutch brick-making machine for home construction in South Africa. By making cheaper bricks that use earth, the project would reduce housing costs while stimulating the building industry.\nThe machine would be mobile, allowing bricks to be made locally from earth.\n\nVarious types of CEB production machines exist, from manual to semi-automated and fully automated, with increasing capital-investment and production rates, and decreased labor. Automated machines are more common in the developed world, and manual machines in the developing world.\n\nThere are many advantages of the CEB system. On-site materials can be used, which reduces cost, minimizes shipping costs for materials, and increases efficiency and sustainability. The wait-time required to obtain materials is minimal, because after the blocks are pressed, materials are available very soon after a short drying period. The uniformity of the blocks simplifies construction, and minimizes or eliminates the need for mortar, thus reducing both the labor and materials costs. The blocks are strong, stable, water-resistant and long-lasting.\n\nCEB had very limited use prior to the 1980s. It was known in the 1950s in South America, where one of the most well-known presses, the Cinva Ram, was developed by Raul Ramirez in the Inter-American Housing Center (CINVA) in Bogota, Colombia. The Cinva Ram is a single-block, manual-press that uses a long, hand-operated lever to drive a cam, generating high pressure.\n\nIndustrial manufacturers produce much larger machines that run with diesel or gasoline engines and hydraulic presses that receive the soil/aggregate mixture through a hopper. This is fed into a chamber to create a block that is then ejected onto a conveyor.\n\nDuring the 1980s, soil-pressing technology became widespread. France, England, Germany, South Africa and Switzerland began to write standards. The Peace Corps, USAID, Habitat for Humanity and other programs began to implement it into housing projects.\n\nCompleted walls require either a reinforced bond beam or a ring beam on top or between floors and if the blocks are not stabilized, a plaster finish, usually stucco wire/stucco cement and/or lime plaster. Stabilized blocks can be left exposed with no outer plaster finish. In tropical environments, polycarbonate varnish is often used to provide an additional layer of wet-weather protection.\n\nStandards for foundations are similar to those for brick walls. A CEB wall is heavy. Footings must be at least 10 inches thick, with a minimum width that is 33 percent greater than the wall width. If a stem wall is used, it shall extend to an elevation not less than eight inches (203 mm) above the exterior finish grade. Rubble-filled foundation trench designs with a reinforced concrete grade beam above are allowed to support CEB construction.\n\nUsing the ASTM D1633-00 stabilization standard, a pressed and cured block must be submerged in water for four hours. It is then pulled from the water and immediately subjected to a compression test. The blocks must score at least a 300 pound-force per square inch (p.s.i) (2 MPa) minimum. This is a higher standard than for adobe, which must score an \"average\" of at least 300 p.s.i. (2 MPa)\n"}
{"id": "26834979", "url": "https://en.wikipedia.org/wiki?curid=26834979", "title": "Condensation cloud", "text": "Condensation cloud\n\nA transient condensation cloud, also called Wilson cloud, is observable at large explosions in humid air.\n\nWhen a nuclear weapon or a large amount of a conventional explosive is detonated in sufficiently humid air, the \"negative phase\" of the shock wave causes a rarefaction (reduction in density) of the air surrounding the explosion, but not contained within it. This rarefaction results in a temporary cooling of that air, which causes a condensation of some of the water vapor contained in it. When the pressure and the temperature return to normal, the Wilson cloud dissipates.\n\nSince heat does not leave the affected air mass, this change of pressure is adiabatic, with an associated change of temperature. In humid air, the drop in temperature in the most rarefied portion of the shock wave can bring the air temperature below its dew point, at which moisture condenses to form a visible cloud of microscopic water droplets. Since the pressure effect of the wave is reduced by its expansion (the same pressure effect is spread over a larger radius), the vapor effect also has a limited radius. Such vapor can also be seen in low pressure regions during high–g subsonic maneuvers of aircraft in humid conditions.\n\nScientists observing the Operation Crossroads nuclear tests in 1946 at Bikini Atoll named that transitory cloud a \"Wilson cloud\" because of its similarity to the appearance of the inside of a Wilson cloud chamber, an instrument they would have been familiar with. (The cloud chamber effect is caused by a temporary reduction in pressure in a closed system and marks the tracks of electrically-charged sub-atomic particles.) Analysts of later nuclear bomb tests used the more general term \"condensation cloud\".\n\nThe shape of the shock wave, influenced by different speed in different altitudes, and the temperature and humidity of different atmospheric layers determines the appearance of the Wilson clouds. During nuclear tests, condensation rings around or above the fireball are commonly observed. Rings around the fireball may become stable and form rings around the rising stem of the mushroom cloud.\n\nThe lifetime of the Wilson cloud during nuclear air bursts can be shortened by the thermal radiation from the fireball, which heats the cloud above the dew point and evaporates the droplets.\n\nThe same kind of condensation cloud is sometimes seen above the wings of aircraft in a moist atmosphere. The top of a wing has a reduction of air pressure as part of the process of generating lift. This reduction in air pressure causes a cooling, just as above, and the condensation of water vapor. Hence, the small, transient clouds that appear.\n\nThe vapor cone of a transonic aircraft is another example of a condensation cloud.\n"}
{"id": "14194971", "url": "https://en.wikipedia.org/wiki?curid=14194971", "title": "Crushed stone", "text": "Crushed stone\n\nCrushed stone or angular rock is a form of construction aggregate, typically produced by mining a suitable rock deposit and breaking the removed rock down to the desired size using crushers. It is distinct from gravel which is produced by natural processes of weathering and erosion, and typically has a more rounded shape.\n\nAngular crushed stone is the key material for macadam road construction which depends on the interlocking of the individual stones' angular faces for its strength. Crushed natural stone is also used similarly without a binder for riprap, railroad track ballast, and filter stone. It may be used with a binder in a composite material such as concrete, tarmac, or asphalt concrete.\n\nCrushed stone is one of the most accessible natural resources, and is a major basic raw material used by construction, agriculture, and other industries. Despite the low value of its basic products, the crushed stone industry is a major contributor to and an indicator of the economic well-being of a nation.\nThe demand for crushed stone is determined mostly by the level of construction activity, and, therefore, the demand for construction materials.\n\nStone resources of the world are very large. High-purity limestone and dolomite suitable for specialty uses are limited in many geographic areas. Crushed stone substitutes for roadbuilding include sand and gravel, and slag. Substitutes for crushed stone used as construction aggregates include sand and gravel, iron and steel slag, sintered or expanded clay or shale, and perlite or vermiculite.\n\nCrushed stone is a high-volume, low-value commodity. The industry is highly competitive and is characterized by many operations serving local or regional markets. Production costs are determined mainly by the cost of labor, equipment, energy, and water, in addition to the costs of compliance with environmental and safety regulations. These costs vary depending on geographic location, the nature of the deposit, and the number and type of products produced. Crushed stone has one of the lowest average by weight values of all mineral commodities. The average unit price increased from US$1.58 per metric ton, f.o.b. plant, in 1970 to US$4.39 in 1990. However, the unit price in constant 1982 dollars fluctuated between US$3.48 and US$3.91 per metric ton for the same period. Increased productivity achieved through increased use of automation and more efficient equipment was mainly responsible for maintaining the prices at this level.\n\nTransportation is a major factor in the delivered price of crushed stone. The cost of moving crushed stone from the plant to the market often equals or exceeds the sale price of the product at the plant. Because of the high cost of transportation and the large quantities of bulk material that have to be shipped, crushed stone is usually marketed locally. The high cost of transportation is responsible for the wide dispersion of quarries, usually located near highly populated areas. However, increasing land values combined with local environmental concerns are moving crushed stone quarries farther from the end-use locations, increasing the price of delivered material. Economies of scale, which might be realized if fewer, larger operations served larger marketing areas, would probably not offset the increased transportation costs.\n\nAccording to the United States Geological Survey, 1.72 billion tonnes of crushed stone worth $13.8 billion was sold or used in 2006, of which 1.44 billion tonnes was used as construction aggregate, 74.9 million tonnes used for cement manufacture, and 18.1 million tonnes used to make lime. Crushed marble sold or used totaled 11.8 million tonnes, the majority of which was ground very fine and used as calcium carbonate.\n\nIn 2006, 9.40 million tonnes of crushed stone (almost all limestone or dolomite) was used for soil treatment, primarily to reduce soil acidity. Soils tend to become acidic from heavy use of nitrogen-containing fertilizers, unless a soil conditioner is used. Using aglime or agricultural lime, a finely-ground limestone or dolomite, to change the soil from acidic to nearly neutral particularly benefits crops by maximizing availability of plant nutrients, and also by reducing aluminum or manganese toxicity, promoting soil microbe activity, and improving the soil structure.\n\nIn 2006, 5.29 million tonnes of crushed stone (mostly limestone or dolomite) was used as a flux in blast furnaces and in certain steel furnaces to react with gangue minerals (i.e. silica and silicate impurities) to produce liquid slag that floats and can be poured off from the much denser molten metal (i.e., iron). The slag cools to become a stone-like material that is commonly crushed and recycled as construction aggregate.\n\nIn addition, 4.53 million tonnes of crushed stone was used for fillers and extenders (including asphalt fillers or extenders), 2.71 million tonnes for sulfur oxide removal-mine dusting-acid water treatment, and 1.45 million tonnes sold or used for poultry grit or mineral food.\n\nCrushed stone is recycled primarily as construction aggregate or concrete.\nCrushed stone or 'road metal' is used in landscape design and gardening for gardens, parks, and municipal and private projects as a mulch, walkway, path, and driveway pavement, and cell infill for modular permeable paving units. As a mineral mulch its benefits include erosion control, water conservation, weed suppression, and aesthetic qualities. It is often seen used in rock gardens and cactus gardens.\n\n\n"}
{"id": "3079141", "url": "https://en.wikipedia.org/wiki?curid=3079141", "title": "Deistic evolution", "text": "Deistic evolution\n\nDeistic evolution is a position in the origins debate which involves accepting the scientific evidence for evolution and age of the universe whilst advocating the view that a deistic God created the universe but has not interfered since. The position is a counterpoint to theistic evolution and is endorsed by those who believe in both deism and the veracity of science.\n\nIn \"Christian Theology\", by Millard J. Erickson, 2013, is written:\n\nThe psychologist Steve Stewart-Williams in his book \"Darwin, God and the Meaning of Life\" (2010) states:\n\nStewart-Williams further writes that deistic evolution strips God of what most religious believers consider central. Any deistic God is not around for prayers, miracles or to intervene in people's lives and that because of this it is unpopular with monotheistic religions.\n\nDeistic Evolution adheres to the concept of some form of God, but denies any personal God. A recent defender of deistic evolution was Michael Anthony Corey, author of the book \"Back to Darwin: The Scientific Case for Deistic Evolution\" (1994).\n\nSome scholars have written that Charles Darwin was an advocate of deistic evolution.\n\nDeistic evolution is similarly the operative idea in Pandeism, which has been counted amongst the handful of spiritual beliefs which \"are compatible with modern science.\" and specifically wherein it is noted that \"\"pandeistic\" belief systems ... [present] the inclusion of God as the ever unfolding expression of a complex universe with an identifiable beginning but no teleological direction necessarily present.\"\n\nDeistic evolution is not the same as theistic evolution, yet they are sometimes confused. The difference rests on the difference between a theistic god that is interested in, if not actively involved in, the outcome of his creation and humanity specifically and a deistic god that is either disinterested in the outcome, and holds no special place for humanity, or will not intervene. Often, there is no discernible difference between the two positions—the choice of terminology has more to do with the believer and her or his need for a god, than fitting into a mostly arbitrary dictionary or academic definition.\n\nDeistic evolution has been criticised by Christian creationists as being incompatible with Christianity since it contradicts a literal reading of the Bible and more importantly, leaves no role for the \"Christian personal God\".\n\nM. J. Erickson wrote that deistic evolution is in conflict with the scriptural doctrine of providence according to which \"God is personally and intimately concerned with and involved in what is going on in the specific events within his entire creation.\"\n\nCharles P. Grannan wrote in 1894, \"Another baseless assumption of negative critics is that the general principles of Atheistic and Deistic evolution, admitted by many scientists to account for the origin of the various species of plants and animals, should also be applied to explain the origin of the Christian religion.\"\n\nCharles Wesley Rishell criticized the concept in 1899, comparing it to the notion (false, in his view), that gravity was a property of matter instead of a continued action of God:\n\nDeistic evolution does not oppose or contradict evolution or come into conflict with science as it says that a God started the process and then left it to natural processes. However deism is still a religious philosophy.\n\nStewart-Williams wrote regarding deistic evolution and science:\n\nThere is considerable room for this \"god of the gaps\" view, since scientific observation is entirely unable to shed any light on what happened during the Planck epoch, the earliest 10 seconds in the history of the universe. All development since this initial creative act merely follows laws and principles which He created:\n\n\nThe Roman Catholic Church disagrees with the doctrine of deistic evolution. In November 2005, Pope Benedict addressed a general audience of 25,000 in St. Peter's Square:\n\n"}
{"id": "4316600", "url": "https://en.wikipedia.org/wiki?curid=4316600", "title": "Eagle-bone whistle", "text": "Eagle-bone whistle\n\nThe eagle bone whistle is a highly sacred religious object, used by some members of Native American spiritual societies in particularly sacred ceremonies. They are made from bones of either the American bald eagle or the American golden eagle, and are considered extremely powerful spiritual objects.\n\nEagle bone whistles are only used in certain ceremonies in the Southwest and Plains cultures. The eagle bone whistle may be considered as a ceremonial or sacred object which may not be considered a musical instrument, if music is defined as entertainment: \"There is no time or need...to wallow in distinctions between a feather-and-bone raptor and a bone whistle avian mysticism; one would no doubt end in dichotomous Western readings thereof.\"\n\nThe whistle is used in some Peyote ceremonies of some sects of the Native American Church. Eagle bone whistles are used in a number of Sun Dance cultures, such as the Crow. The eagle-bone whistle is also used by the Lakota people in certain ceremonies, such as some Sun Dances.\n\nNavajo/Ute flutist R. Carlos Nakai claims to use an \"eagle-bone whistle\" (or possibly an imitation one) on multiple albums.\n\nBoth the bald and golden eagle are protected by federal law: the Migratory Bird Treaty Act of 1918 (MBTA) prohibits the taking, killing, possession, transportation, and importation of migratory birds, their eggs, parts, and nests except as authorized under a valid permit as outlined at 50 CFR 21.11 The MBTA authorizes and directs the Secretary of the Interior to determine if, and by what means, the hunting of migratory birds should be allowed, as well as to adopt and implement suitable regulations permitting and governing the hunting of any type of migratory bird (for example, hunting seasons for ducks and geese). The Eagle feather law is another name for the exemptions to this act that are sometimes granted to enrolled members of federally recognized Native American tribes. Penalties under the MBTA include a maximum of two years imprisonment and $250,000 fine for a felony conviction and six months imprisonment or $5,000 fine for a misdemeanor conviction. Fines double if the violator is an organization rather than an individual. These laws would apply to the collection and use of eagle bone whistles.\n\n"}
{"id": "38103098", "url": "https://en.wikipedia.org/wiki?curid=38103098", "title": "Earth pyramids of South Tyrol", "text": "Earth pyramids of South Tyrol\n\nThe earth pyramids in South Tyrol are a special natural phenomenon that comes about in particular terrain, usually after a landslide or an unhinging of the earth.\n\nThe main cause of the formation of earth pyramids is the continuous alternation of periods of torrential rain and\ndrought. These phenomena, in particularly friable terrain, over the years, increasingly erode the ground and form such earth pyramids. Usually the pyramids are formed in terrain very well sheltered from wind so that they cannot be damaged by it.\n\nMoreover, the life of the earth pyramids is strongly dependent on the climate which reigns during the time in which it is shaped by the rock that covers it.\n\nThere are several earth pyramids that can be safely visited. Among the most famous and admired the\nfollowing are the most outstanding:\n\nOther, less famous, earth pyramids are:\n\n"}
{"id": "190837", "url": "https://en.wikipedia.org/wiki?curid=190837", "title": "Evolutionary algorithm", "text": "Evolutionary algorithm\n\nIn artificial intelligence, an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.\n\nEvolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.\n\nStep One: Generate the initial population of individuals randomly. (First generation)\n\nStep Two: Evaluate the fitness of each individual in that population (time limit, sufficient fitness achieved, etc.)\n\nStep Three: Repeat the following regenerational steps until termination:\n\nSimilar techniques differ in genetic representation and other implementation details, and the nature of the particular applied problem.\n\nA possible limitation of many evolutionary algorithms is their lack of a clear genotype-phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype. This indirect encoding is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the evolvability of the organism. Such indirect (a.k.a. generative or developmental) encodings also enable evolution to exploit the regularity in the environment. Recent work in the field of artificial embryogeny, or artificial developmental systems, seeks to address these concerns. And gene expression programming successfully explores a genotype-phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes. \n\nSwarm algorithms include\n\n\nThe computer simulations \"Tierra\" and \"Avida\" attempt to model macroevolutionary dynamics.\n\n"}
{"id": "2132454", "url": "https://en.wikipedia.org/wiki?curid=2132454", "title": "Evolutionary graph theory", "text": "Evolutionary graph theory\n\nEvolutionary graph theory is an area of research lying at the intersection of graph theory, probability theory, and mathematical biology. Evolutionary graph theory is an approach to studying how topology affects evolution of a population. That the underlying topology can substantially affect the results of the evolutionary process is seen most clearly in a paper by Erez Lieberman, Christoph Hauert and Martin Nowak.\n\nIn evolutionary graph theory, individuals occupy vertices of a weighted directed graph and the weight w of an edge from vertex \"i\" to vertex \"j\" denotes the probability of \"i\" replacing \"j\". The weight corresponds to the biological notion of fitness where fitter types propagate more readily. \nOne property studied on graphs with two types of individuals is the \"fixation probability\", which is defined as the probability that a single, randomly placed mutant of type A will replace a population of type B. According to the \"isothermal theorem\", a graph has the same fixation probability as the corresponding Moran process if and only if it is isothermal, thus the sum of all weights that lead into a vertex is the same for all vertices. Thus, for example, a complete graph with equal weights describes a Moran process. The fixation probability is\nwhere \"r\" is the relative fitness of the invading type.\n\nGraphs can be classified into amplifiers of selection and suppressors of selection. If the fixation probability of a single advantageous mutation formula_2 is higher than the fixation probability of the corresponding Moran process formula_3 then the graph is an amplifier, otherwise a suppressor of selection. One example of the suppressor of selection is a linear process where only vertex \"i-1\" can replace vertex \"i\" (but not the other way around). In this case the fixation probability is formula_4 (where \"N\" is the number of vertices) since this is the probability that the mutation arises in the first vertex which will eventually replace all the other ones. Since formula_5 for all \"r\" greater than 1, this graph is by definition a suppressor of selection.\n\nEvolutionary graph theory may also be studied in a dual formulation, as a coalescing random walk, or as a stochastic process. We may consider the mutant population on a graph as a random walk between absorbing barriers representing mutant extinction and mutant fixation. For highly symmetric graphs, we can then use martingales to find the \"fixation probability\" as illustrated by Monk (2018).\n\nAlso evolutionary games can be studied on graphs where again an edge between \"i\" and \"j\" means that these two individuals will play a game against each other.\n\nClosely related stochastic processes include the voter model, which was introduced by Clifford and Sudbury (1973) and independently by Holley and Liggett (1975), and which has been studied extensively.\n\n\nA virtual laboratory for studying evolution on graphs:\n"}
{"id": "1827283", "url": "https://en.wikipedia.org/wiki?curid=1827283", "title": "Faith McNulty", "text": "Faith McNulty\n\nFaith McNulty (November 28, 1918 – April 10, 2005) was an American non-fiction author, probably best known for her 1980 literary journalism genre book \"The Burning Bed\". She is also known for her authorship of wildlife pieces and books, including children's books.\n\nFaith Trumbull Corrigan was born in New York City, the daughter of a judge. She attended Barnard College for one year, then attended Rhode Island State College. But she dropped out of college once she got a job as a copy girl at the \"New York Daily News\". She later went to work for \"Life\" magazine. She worked for the U.S. Office of War Information in London during World War II.\n\nMcNulty was a staff writer at \"The New Yorker\" magazine from 1953 to 1994. In 1980, a collection of her \"New Yorker\" work was published as \"The Wildlife Stories of Faith McNulty\". For many years, she edited the annual \"New Yorker\" compilation of the year's best children's books.\n\nShe also frequently wrote children's books on wildlife, including \"How to Dig a Hole to the Other Side of the World\" in 1979 and \"When I Lived With Bats\" in 1998. Her 1966 book \"The Whooping Crane: The Bird that Defies Distinction\" was written for adults.\n\nHer husband, John McNulty, was also a writer for \"The New Yorker\" and with Thomas Wolfe, Truman Capote, Gay Talese and James Baldwin, a major figure in the development of the literary genre of Creative nonfiction, which is also known as literary journalism or literature in fact. As earlier here noted, having herself been years exposed to Harold Ross' New Yorker magazine's rarefied environment, which was then so promoting of this evolving genre, Faith's own major nonfiction work, \"The Burning Bed\", is, itself, a quintessential and quality example of the genre of literary journalism or, as Thomas Wolfe once labeled it, the “New Journalism”. After her husband John died in 1956, Faith remarried, to Richard Martin, a set designer and an inventive designer of set props.\n\n\"The Burning Bed\" told the true story of Francine Hughes, who set fire to the bedroom in which her husband was sleeping. Hughes defended herself by saying that her husband had been abusing her for 13 years. The jury at her trial ruled that she had been temporarily insane, and she was found not guilty.\n\nFaith had fonder memories of life with kinder family, however. \"I can remember my father in his nightshirt, digging for worms for the baby robin in the bathroom. That's the kind of household it was; I had woodchucks in the bathroom, cats, squirrels, chipmunks\", McNulty once said.\n\nTowards the end of her life, she wrote a weekly column for \"The Providence Journal\" on a local animal shelter run by the Animal Welfare League. Her mother had founded the Animal Welfare League in southern Rhode Island. McNulty had long been known for taking in stray animals at her farm.\n\nShe suffered a stroke in 2004. She died at her farm in Wakefield, Rhode Island.\n\nMcNulty's last book was illustrated by Steven Kellogg and published by Scholastic Books in 2005, \"If You Decide to Go to the Moon\"—a picture book written in the second person. Next year (after McNulty's death) it won a major \"year's best\" children's literary award, the Boston Globe–Horn Book Award for Nonfiction.\n\n"}
{"id": "8782181", "url": "https://en.wikipedia.org/wiki?curid=8782181", "title": "Geographical zone", "text": "Geographical zone\n\nThe five main latitude regions of the Earth's surface comprise geographical zones, divided by the major circles of latitude. The differences between them relate to climate. They are as follows: \n\n\nOn the basis of latitudinal extent, the globe is divided into three broad heat zones.\n\nThe Torrid is also known as the Tropics. The zone is bounded on the north by the Tropic of Cancer and on the south by the Tropic of Capricorn; these latitudes mark the northern and southern extremes in which the sun seasonally passes directly overhead. This happens annually, but in the region between, the sun passes overhead twice a year.\n\nIn the Northern Hemisphere, in the sun's apparent northward migration after the March equinox, it passes overhead once, then after the June solstice, at which time it reaches the Tropic of Cancer, it passes over again on its apparent southward journey. After the September equinox the sun passes into the Southern Hemisphere. It then passes similarly over the southern tropical regions until it reaches the Tropic of Capricorn at the December solstice, and back again as it returns northwards to the Equator.\n\nIn the two Temperate Zones also known as tropical zone not, consisting of the tepid latitudes, the Sun is never directly overhead, and the climate is mild, generally ranging from warm to cool. The four annual seasons, spring, summer, autumn and winter, occur in these areas. The North Temperate Zone includes Europe, Northern Asia, and North and Central America. The South Temperate Zone includes Southern Australasia, southern South America, and Southern Africa.\n\nThe two Frigid Zones, or polar regions, experience the midnight sun and the polar night for part of the year - at the edge of the zone there is one day at the winter solstice when the Sun is invisible, and one day at the summer solstice when the sun remains above the horizon for 24 hours. In the center of the zone (the pole) the day is one year long with six months of daylight and six months of night. The Frigid Zones are the coldest regions of Earth and are generally covered in ice and snow.It receives slanting rays of the sun as this region lies farthest from the equator. Summer season in this region lies lasts for about 2 to 3 months and there is almost 24 hour sunlight during summer.\n\nThe concept of a geographical zone was first hypothesized by the ancient Greek scholar Parmenides and lastingly modified by Aristotle. Both philosophers theorized the Earth divided into three types of climatic zones based on their distance from the equator.\n\nLike Parmeneides, thinking that the area near the equator was too hot for habitation, Aristotle dubbed the region around the equator (from 23.5° N to 23.5° S) the \"Torrid Zone.\" Both philosophers reasoned the region from the Arctic Circle to the pole to be permanently frozen. This region, thought uninhabitable, was called the \"Frigid Zone.\" The only area believed to be habitable was the northern \"Temperate Zone\" (the southern one not having been discovered), lying between the \"Frigid Zones\" and the \"Torrid Zone\". However, humans have inhabited almost all climates on Earth, including inside the Arctic Circle.\n\nAs knowledge of the Earth's geography improved, a second \"Temperate Zone\" was discovered south of the equator, and a second \"Frigid Zone\" was discovered around the Antarctic. Although Aristotle's map was oversimplified, the general idea was correct. Today, the most commonly used climate map is the Köppen climate classification, developed by Russian climatologist of German descent and amateur botanist Wladimir Köppen (1846–1940), which divides the world into five major climate regions, based on average annual precipitation, average monthly precipitation, and average monthly temperature.\n\n"}
{"id": "13844012", "url": "https://en.wikipedia.org/wiki?curid=13844012", "title": "Greenhouse and icehouse Earth", "text": "Greenhouse and icehouse Earth\n\nThroughout the Phanerozoic history of the Earth, the planet's climate has been fluctuating between two dominant climate states: the greenhouse Earth and the icehouse Earth. \nThese two climate states last for millions of years and should not be confused with glacial and interglacial periods, which occur only during an icehouse period and tend to last less than 1 million years. There are five known great glaciations in Earth's climate history; the main factors involved in changes of the paleoclimate are believed to be the concentration of atmospheric carbon dioxide, changes in the Earth's orbit, and oceanic and orogenic changes due to tectonic plate dynamics. Greenhouse and icehouse periods have profoundly shaped the evolution of life on Earth.\n\nA \"greenhouse Earth\" or \"hothouse Earth\" is a period in which there are no continental glaciers whatsoever on the planet, the levels of carbon dioxide and other greenhouse gases (such as water vapor and methane) are high, and sea surface temperatures (SSTs) range from 28 °C (82.4 °F) in the tropics to 0 °C (32 °F) in the polar regions. \n\nThis state should not be confused with a hypothetical \"hothouse earth\", which is an irreversible tipping point corresponding to the ongoing runaway greenhouse effect on Venus. The IPCC states that \"a 'runaway greenhouse effect'—analogous to [that of] Venus—appears to have virtually no chance of being induced by anthropogenic activities.\"\n\nThere are several theories as to how a greenhouse Earth can come about. The geological record shows CO and other greenhouse gases are abundant during this time. Tectonic movements were extremely active during the more well-known greenhouse ages (such as 368 million years ago in the Paleozoic Era). Because of continental rifting (continental plates moving away from each other) volcanic activity becomes more prominent, producing more CO and heating up the Earth's atmosphere. Earth is more commonly placed in a greenhouse state throughout the epochs, and the Earth has been in this state for approximately 80% of the past 500 million years, which makes understanding the direct causes somewhat difficult.\n\nAn \"icehouse Earth\" is the earth as it experiences an ice age. Unlike a greenhouse Earth, an icehouse Earth has ice sheets present, and these sheets wax and wane throughout times known as glacial periods and interglacial periods. During an icehouse Earth, greenhouse gases tend to be less abundant, and temperatures tend to be cooler globally. The Earth is currently in an icehouse stage, as ice sheets are present on both poles and glacial periods have occurred at regular intervals over the past million years.\n\nThe causes of an icehouse state are much debated, because not much is really known about the transition periods between greenhouse to icehouse climates and what could make the climate so different. One important aspect is clearly the decline of CO in the atmosphere, possibly due to low volcanic activity.\n\nOther important issues are the movement of the tectonic plates and the opening and closing of oceanic gateways. These seem to play a crucial part in icehouse Earths because they can bring forth cool waters from very deep water circulations that could assist in creating ice sheets or thermal isolation of areas. Examples of this occurring are the opening of the Tasmanian gateway 36.5 million years ago that separated Australia and Antarctica and which is believed to have set off the Cenozoic icehouse, and the creation of the Drake Passage 32.8 million years ago by the separation of South America and Antarctica, though it was believed by other scientists that this did not come into effect until around 23 million years ago. The closing of the Isthmus of Panama and the Indonesian seaway approximately 3 or 4 million years ago may have been a major cause for our current icehouse state. For the icehouse climate, tectonic activity also creates mountains, which are produced by one continental plate colliding with another one and continuing forward. The revealed fresh soils act as scrubbers of carbon dioxide, which can significantly affect the amount of this greenhouse gas in the atmosphere. An example of this is the collision between the Indian subcontinent and the Asian continent, which created the Himalayan Mountains about 50 million years ago.\n\nWithin icehouse states, there are \"glacial\" and \"interglacial\" periods that cause ice sheets to build up or retreat. The causes for these glacial and interglacial periods are mainly variations in the movement of the earth around the Sun. The astronomical components, discovered by the Serbian geophysicist Milutin Milanković and now known as Milankovitch cycles, include the axial tilt of the Earth, the orbital eccentricity (or shape of the orbit) and the precession (or wobble) of the Earth's spin. The tilt of the axis tends to fluctuate between 21.5° to 24.5° and back every 41,000 years on the vertical axis. This change actually affects the seasonality upon the earth, since more or less solar radiation hits certain areas of the planet more often on a higher tilt, while less of a tilt would create a more even set of seasons worldwide. These changes can be seen in ice cores, which also contains information that shows that during glacial times (at the maximum extension of the ice sheets), the atmosphere had lower levels of carbon dioxide. This may be caused by the increase or redistribution of the acid/base balance with bicarbonate and carbonate ions that deals with alkalinity. During an Icehouse, only 20% of the time is spent in interglacial, or warmer times.\n\nA \"snowball earth\" is the complete opposite of greenhouse Earth, in which the earth's surface is completely frozen over; however, a snowball earth technically does not have continental ice sheets like during the icehouse state. \"The Great Infra-Cambrian Ice Age\" has been claimed to be the host of such a world, and in 1964, the scientist W. Brian Harland brought forth his discovery of indications of glaciers in low latitudes (Harland and Rudwick). This became a problem for Harland because of the thought of the \"Runaway Snowball Paradox\" (a kind of Snowball effect) that, once the earth enters the route of becoming a snowball earth, it would never be able to leave that state. However, in 1992 brought up a solution to the paradox. It is believed that since the continents at this time were huddled at the low and mid-latitudes that there was a great cooling event by planetary albedo, or reflection of the earth’s surface. Kirschvink explained that the way to get out of the snowball could be connected to carbon dioxide, since volcanic activity would not halt, and that the buildup and lack of \"scrubbing\" of this carbon dioxide in the atmosphere, that the earth would return to a greenhouse state. Some scientists believe that the end of the snowball Earth caused an event known as the Cambrian Explosion, which produced the beginnings of multi-cellular life. However some biologists claim that a complete snowball Earth could not have happened since photosynthetic life would not have survived underneath many meters of ice without sunlight. However, it has been observed that, even under meters of thick ice around Antarctica, sunlight shows through. Most scientists today believe that a \"hard\" Snowball Earth, one completely covered by ice, is probably impossible. However, a \"slushball earth\", with points of openings near the equator, is possible.\n\nRecent studies may have again complicated the idea of a snowball earth. In October 2011, a team of French researchers announced that the carbon dioxide during the last speculated \"snowball earth\" may have been lower than originally stated, which provides a challenge in finding out how Earth was able to get out of its state and if it were a snowball or slushball.\n\nThe Eocene, which occurred between 53 and 49 million years ago, was the Earth's warmest temperature period for 100 million years. However, this \"super-greenhouse\" eventually became an icehouse by the late Eocene. It was believed that the decline of CO caused this change, though there are possible positive feedbacks, or added influence that contributes to the cooling.\n\nThe best record we have for a transition from an icehouse to a greenhouse period where plant life exists is during the Permian epoch that occurred around 300 million years ago. In 40 million years a major transition took place, causing the Earth to change from a moist, icy planet where rainforests covered the tropics, into a hot, dry, and windy location where little could survive. Professor Isabel P. Montañez of University of California, Davis, who has researched this time period, found the climate to be \"highly unstable\" and \"marked by dips and rises in carbon dioxide\".\n\nThe Eocene-Oligocene transition, the latest transition occurring approximately 34 million years ago, resulted in rapid global temperature decrease, the glaciation of Antarctica and a series of biotic extinction events. The most dramatic species turnover event associated with this time period is the Grande Coupure, a period which saw the replacement of European tree-dwelling and leaf-eating mammal species by migratory species from Asia.\n\nThe science of paleoclimatology attempts to understand the history of greenhouse and icehouse conditions over geological time. Through the study of ice cores, dendrochronology, ocean and lake sediments (varve), palynology (fossilized pollen) and isotope analysis (such as Radiometric dating and stable isotope analysis), scientists can create models of past climate. One study has shown that atmospheric carbon dioxide levels during the Permian age rocked back and forth between 250 parts per million (which is close to present-day levels) up to 2,000 parts per million. Studies on lake sediments suggest that the \"Hothouse\" or \"super-Greenhouse\" Eocene was in a \"permanent El Nino state\" after the 10 °C warming of the deep ocean and high latitude surface temperatures shut down the Pacific Ocean's El Nino-Southern Oscillation. A theory was suggested for the Paleocene–Eocene Thermal Maximum on the sudden decrease of carbon isotopic composition of global inorganic carbon pool by 2.5 parts per million. A hypothesis noted for this negative drop of isotopes could be the increase of methane hydrates, the trigger for which remains a mystery. This increase of methane in the atmosphere, which happens to be a potent, but short-lived greenhouse gas, increased the global temperatures by 6 °C with the assistance of the less potent carbon dioxide.\n\n\nCurrently, the Earth is in an icehouse climate state. About 34 million years ago, ice sheets began to form in Antarctica; the ice sheets in the Arctic did not start forming until 2 million years ago. Some processes that may have led to our current icehouse may be connected to the development of the Himalayan Mountains and the opening of the Drake Passage between South America and Antarctica. Scientists have been attempting to compare the past transitions between icehouse and greenhouse, and vice versa to understand where our planet is now heading.\n\nWithout the human influence on the greenhouse gas concentration, the Earth would be heading toward a glacial period. Predicted changes in orbital forcing suggest that in absence of human-made global warming the next glacial period would begin at least 50,000 years from now (see Milankovitch cycles).\n\nBut due to the ongoing anthropogenic greenhouse gas emissions, the Earth is instead heading toward a greenhouse Earth period. Permanent ice is actually a rare phenomenon in the history of the Earth, occurring only in coincidence with the icehouse effect, which has affected about 20% of Earth's history.\n\n"}
{"id": "32236479", "url": "https://en.wikipedia.org/wiki?curid=32236479", "title": "Hanle effect", "text": "Hanle effect\n\nThe Hanle effect is a reduction in the polarization of light when the atoms emitting the light are subject to a magnetic field in a particular direction, and when they have themselves been excited by polarized light.\n\nIt is named after Wilhelm Hanle, who first described it in \"Zeitschrift für Physik\" in 1924. Attempts to understand the phenomenon were important in the subsequent development of quantum physics.\n\n"}
{"id": "7149688", "url": "https://en.wikipedia.org/wiki?curid=7149688", "title": "Hard inheritance", "text": "Hard inheritance\n\nHard inheritance was a model of heredity that explicitly excludes any acquired characteristics, such as of Lamarckism. It is the exact opposite of soft inheritance, coined by Ernst Mayr to contrast ideas about inheritance.\n\nHard inheritance states that characteristics of an organism's offspring (passed on through DNA) will not be affected by the actions that the parental organism performs during its lifetime. For example: a medieval blacksmith who uses only his right arm to forge steel will not sire a son with a stronger right arm than left because the blacksmith's actions do not alter his genetic code. Inheritance due to usage and non-usage is excluded. Inheritance works as described in the modern synthesis of evolutionary biology.\n\nThe existence of inherited epigenetic variants has led to renewed interest in soft inheritance.\n"}
{"id": "44817531", "url": "https://en.wikipedia.org/wiki?curid=44817531", "title": "Histoire Naturelle", "text": "Histoire Naturelle\n\nThe Histoire Naturelle, générale et particulière, avec la description du Cabinet du Roi (French for \"Natural History, General and Particular, with a Description of the King's Cabinet\") is an encyclopaedic collection of 36 large (quarto) volumes written between 1749–1804 by the Comte de Buffon, and continued in eight more volumes after his death by his colleagues, led by Bernard Germain de Lacépède. The books cover what was known of the \"natural sciences\" at the time, including what would now be called material science, physics, chemistry and technology as well as the natural history of animals.\n\nThe \"Histoire Naturelle, générale et particulière, avec la description du Cabinet du Roi\" is the work that the Comte de Buffon (1707–1788) is remembered for. He worked on it for some 50 years, initially at Montbard in his office in the Tour Saint-Louis, then in his library at Petit Fontenet. 36 volumes came out between 1749 and 1789, followed by 8 more after his death, thanks to Bernard Germain de Lacépède. It includes all the knowledge available in his time on the \"natural sciences\", a broad term that includes disciplines which today would be called material science, physics, chemistry and technology. Buffon notes the morphological similarities between men and apes, although he considered apes completely devoid of the ability to think, differentiating them sharply from human beings. Buffon's attention to internal anatomy made him an early comparative anatomist. \"L’intérieur, dans les êtres vivants, est le fond du dessin de la nature\", he wrote in his \"Quadrupèdes,\" \"the interior, in living things, is the foundation of nature's design.\"\n\nThe \"Histoire Naturelle\", which was meant to address the whole of natural history, actually covers only minerals, birds, and the quadrupeds among animals. It is accompanied by some discourses and a theory of the earth by way of introduction, and by supplements including an elegantly written account of the epochs of nature.\n\nThe \"Suppléments\" cover a wide range of topics; for example, in (Suppléments IV), there is a \"Discours sur le style\" (Discourse on Style) and an \"Essai d'arithmétique morale\" (essay on Moral Arithmetic).\n\nLouis Jean-Marie Daubenton assisted Buffon on the quadrupeds; Philippe Guéneau de Montbeillard worked on the birds. They were joined, from 1767, by Barthélemy Faujas de Saint-Fond, the abbot Gabriel Bexon and Charles-Nicolas-Sigisbert Sonnini de Manoncourt. The whole descriptive and anatomical part of \"l’Histoire des Quadrupèdes\" was the work of Daubenton and Jean-Claude Mertrud.\nBuffon attached much importance to the illustrations; Jacques de Sève illustrated the quadrupeds and François-Nicolas Martinet illustrated the birds. Nearly 2000 plates adorn the work, representing animals with care given both to aesthetics and anatomical accuracy, with dreamlike and mythological settings.\n\nOn minerals, Buffon collaborated with André Thouin. Barthélemy Faujas de Saint-Fond and Louis Bernard Guyton de Morveau provided sources for the mineral volumes.\n\nL’\"Histoire Naturelle\" met immense success, almost as great as \"Encyclopédie\" by Diderot, which came out in the same period. The first three volumes of \"L’Histoire Naturelle, générale et particulière, avec la description du cabinet du Roi\" were reprinted three times in six weeks.\n\nThe encyclopaedia appeared in 36 volumes :\n\n\"L’Histoire Naturelle\" was initially printed at the Imprimerie royale in 36 volumes (1749–1789). In 1764 Buffon bought back the rights to his work. It was continued by Bernard Germain de Lacépède, who described the egg-laying quadrupeds, snakes, fishes and cetaceans in 8 volumes (1788–1804).\n\nBuffon was assisted in the work by Jacques-François Artur (1708–1779), Gabriel Léopold Charles Amé Bexon (1748–1785), Louis Jean-Marie Daubenton (1716–1799), Edme-Louis Daubenton (1732–1786), Jacques de Sève (actif 1742–1788), Barthélemy Faujas de Saint-Fond (1741–1819), Philippe Guéneau de Montbeillard (1720–1785), Louis-Bernard Guyton-Morveau (1737–1816), Bernard Germain de Lacépède (1756–1825), François-Nicolas Martinet (1731–1800), the anatomist Jean-Claude Mertrud (1728–1802), Charles-Nicolas-Sigisbert Sonnini de Manoncourt (1751–1812), and André Thouin (1747–1823).\n\nEach group is introduced with a general essay. This is followed by an article, sometimes of many pages, on each animal (or other item). The article on the wolf begins with the claim that it is one of the animals with a specially strong appetite for flesh; it asserts that the animal is naturally coarse and cowardly (\"grossier et poltron\"), but becoming crafty at need, and hardy by necessity, driven by hunger. The language, as in this instance, is elegant and elaborate, even \"flowery and ornate\". Buffon was roundly criticised by his fellow academics for writing a \"purely popularizing work, empty and puffed up, with little real scientific value\".\n\nThe species is named in Greek, Latin, Italian, Spanish, German, English, Swedish, and Polish. The zoological descriptions of the species by Gessner, Ray, Linnaeus, Klein and Buffon himself (\"\"Canis ex griseo flavescens. Lupus vulgaris\". Buffon. \"Reg. animal. pag. 235\"\") are cited.\n\nThe text is written as a continuous essay, without the sections on identification, distribution and behaviour that might have been expected from other natural histories. Parts concern human responses rather than the animal itself, as for example that the wolf likes human flesh, and the strongest wolves sometimes eat nothing else. Measurements may be included; in the case of the wolf, 41 separate measurements are tabulated, in pre-revolutionary French feet and inches starting with the \"Length of the whole body measured in a straight line from the end of the muzzle to the anus...3 feet. 7 inches.\" (1.2 m); the \"Length of the largest claws\" is given as \"10 lines\" (2.2 cm).\n\nThe wolf is illustrated standing in farmland, and as a complete skeleton standing on a stone plinth in a landscape. The account of the species occupies 32 pages including illustrations.\n\nThe original edition of the \"Histoire Naturelle\" by Buffon comprised 36 volumes in quarto, divided into the following series: Histoire de la Terre et de l'Homme, Quadrupèdes, Oiseaux, Minéraux, Suppléments. Buffon edited 35 volumes in his lifetime. Soon after his death, the fifth and final volume of \"l’Histoire des minéraux\" appeared in 1788 at the \"Imprimerie des Bâtiments du Roi\". The seventh and final volume of \"Suppléments\" by Buffon was published posthumously in 1789 through Lacépède's hands. Lacépède continued the part of the \"Histoire Naturelle\" which dealt with animals. A few months before Buffon's death, en 1788, Lacépède published, as a continuation, the first volume of his \"Histoire des Reptiles\", on egg-laying quadrupeds. The next year, he wrote a second volume on snakes, published during the French Revolution. Between 1798 and 1803, he brought out the volume \"Histoire des Poissons\". Lacépède made use of the notes and collections left by Philibert Commerson (1727–1773). He wrote \"Histoire des Cétacés\" which was printed in 1804. At that point, the \"Histoire Naturelle\", by Buffon and Lacépède, thus contained 44 quarto volumes forming the definitive edition.\n\nAnother edition in quarto format was printed by the \"Imprimerie royale\" in 36 volumes (1774–1804). It consisted of 28 volumes par Buffon, and 8 volumes by Lacépède. The part containing anatomical articles by Louis Jean-Marie Daubenton was dropped. The supplements were merged into the relevant articles in the main volumes.\n\nThe \"Imprimerie royale\" also published two editions of the \"Histoire Naturelle\" in duodecimo format (1752–1805), occupying 90 or 71 volumes, depending on whether or not they included the part on anatomy. In this print format, the original work by Buffon occupied 73 volumes with the part on anatomy, or 54 volumes without the part on anatomy. The continuation by Lacépède took up 17 duodecimo volumes.\n\nA de luxe edition of \"Histoire Naturelle des Oiseaux\" (Birds) (1771–1786) was produced by the \"Imprimerie royale\" in 10 folio and quarto volumes, with 1008 engraved and hand-coloured plates, executed under Buffon's personal supervision by Edme-Louis Daubenton, cousin and brother-in-law of Buffon's principal collaborator.\n\nThe \"Histoire Naturelle\" was translated into languages including English, German, Swedish, Russian and Italian. Many translations, often partial (single volumes, or all volumes to a certain date), abridged, reprinted in the same translation by different printers, or with additional text (for example on insects) and new illustrations, were made at the end of the eighteenth century and the start of the nineteenth century, presenting a complicated publication history. Early translations were necessarily only of the earlier volumes. Given the complexity, all catalogue dates other than of single volumes should be taken as approximate.\n\nR. Griffith published an early translation of the volume on \"The Horse\" in London in 1762. T. Bell published a translation of the first six volumes in London between 1775 and 1776. William Creech published an edition in Edinburgh between 1780 and 1785. T. Cadell and W. Davies published another edition in London in 1812. An abridged edition was published by Wogan, Byrne et al. in Dublin in 1791; that same year R. Morison and Son of Perth, J. and J. Fairbairn of Edinburgh and T. Kay and C. Forster of London published their edition. W. Strahan and T. Cadell published a translation with notes by the encyclopaedist William Smellie in London around 1785. \"Barr's Buffon\" in ten volumes was published in London between 1797 and 1807. W. Davidson published an abridged version including the natural history of insects taken from Swammerdam, Brookes, Goldsmith et al., with \"elegant engravings on wood\"; its four volumes appeared in Alnwick in 1814.\n\nGerman translations include those published by Joseph Georg Trassler 1784–1785; by Pauli, 1772–1829; Grund and Holle, 1750–1775; and Johann Samuel Heinsius, 1756–1782.\n\nItalian translations include those published by Fratelle Bassaglia around 1788 and Boringherieri in 1959.\n\nPer Olof Gravander translated an 1802–1803 French abridgement into Swedish, publishing it in Örebro in 1806–1807.\n\nA Russian version (The General and Particular Natural History by Count Buffon; \"Всеобщая и частная естественная история графа Бюффона\") was brought out by The Imperial Academy of Sciences (Императорской Академией Наук) in St. Petersburg between 1789 and 1808.\n\nAn abridged edition for children was published by Frederick Warne in London and Scribner, Welford and Co. c. 1870.\n\nThe original edition was arranged as follows:\n\nNatural history, and description of the king's cabinet of curiosities\n\n\"Quadrupèdes\" (Quadrupeds)\n\n\"Histoire Naturelle des Oiseaux\" (Birds) (1770–1783)\n\n\n\"Histoire Naturelle des Minéraux\" (Minerals) (1783–1788)\n\n\"Suppléments à l’Histoire Naturelle, générale et particulière\" (Supplements) (1774–1789)\n\n\"Histoire Naturelle des Quadrupèdes ovipares et des Serpents\" (Egg-laying Quadrupeds and Snakes) (1788–1789)\n\n\n\"Histoire Naturelle des Poissons\" (Fish) (1798–1803)\n\n\"Histoire Naturelle des Cétacés\" (Cetaceans) (1804)\n\nThe \"Histoire Naturelle\" had a distinctly mixed reception in the eighteenth century. Wealthy homes in both England and France purchased copies, and the first edition was sold out within six weeks. But Buffon was criticised by some priests for suggesting (in the essay \"Les Epoques de Nature\", Volume XXXIV) that the earth was more than 6,000 years old and that mountains had arisen in geological time. Buffon cites as evidence that fossil sea-shells had been found at the tops of mountains; but the claim was seen as contradicting the biblical account in the Book of Genesis. Buffon also disagreed with Linnaeus's system of classifying plants as described in \"Systema Naturae\" (1735). In Buffon's view, expounded in the \"Premier Discours\" of the \"Histoire Naturelle\" (1749), the concept of species was entirely artificial, the only real entity in nature being the individual; as for a taxonomy based on the number of stamens or pistils in a flower, mere counting (despite Buffon's own training in mathematics) had no bearing on nature.\n\nThe Paris faculty of theology, acting as the official censor, wrote to Buffon with a list of statements in the \"Histoire Naturelle\" that were contradictory to Roman Catholic Church teaching. Hypocritically, Buffon replied that he believed firmly in the biblical account of creation, and was able to continue printing his book, and remain in position as the leader of the 'old school', complete with his job as director of the royal botanical garden. On Buffon's death, the 19-year-old Georges Cuvier celebrated with the words \"This time, the Comte de Buffon is dead and buried\". Soon afterwards, the French revolution went much further in sweeping away old attitudes to natural history, along with much else.\n\nThe Stanford Encyclopedia of Philosophy calls the \"Histoire Naturelle\" \"Buffon's major work\", observing that \"In addressing the history of the earth, Buffon also broke with the 'counter-factual' tradition of Descartes, and presented a secular and realist account of the origins of the earth and its life forms.\" In its view, the work created an \"age of Buffon\", defining what natural history itself was, while Buffon's \"Discourse on Method\" (unlike that of Descartes) at the start of the work argued that repeated observation could lead to a greater certainty of knowledge even than \"mathematical analysis of nature\". Buffon also led natural history away from the natural theology of British parson-naturalists such as John Ray. He thus offered both a new methodology and an empirical style of enquiry. Buffon's position on evolution is complex; he noted in Volume 4 from Daubenton's comparative anatomy of the horse and the donkey that species might \"transform\", but initially (1753) rejected the possibility. However, in doing so he changed the definition of a species from a fixed or universal class (which could not change, by definition) to \"the historical succession of ancestor and descendant linked by material connection through generation\", identified by the ability to mate and produce fertile offspring. Thus the horse and donkey, which produce only sterile hybrids, are seen empirically not to be the same species, even though they have similar anatomy. That empirical fact leaves open the possibility of evolution.\n\nThe botanist Sandra Knapp writes that \"Buffon's prose was so purple that the ideas themselves are almost hidden\", observing that this was also the contemporary academic opinion. She notes that some quite radical ideas are to be found in his work, but they are almost invisible, given the language they are cloaked in. She quotes Buffon's dramatic description of the lion, which along with the engraving in her view \"emphasized both the lion's regal bearing and personality not only in his text but also in the illustration... A reader was left in no doubt as to the importance and character of the animal.\" She concludes \"No wonder the cultured aristocratic public lapped it up – the text reads more like a romantic novel than a dry scientific treatise\".\n\nThe evolutionary biologist Ernst Mayr comments that \"In this monumental and fascinating \"Histoire naturelle\", Buffon dealt in a stimulating manner with almost all the problems that would subsequently be raised by evolutionists. Written in a brilliant style, this work was read in French or in one of the numerous translations by every educated person in Europe\". Mayr argued that \"virtually all the well-known writers of the Enlightenment\" were \"Buffonians\", and calls Buffon \"the father of all thought in natural history in the second half of the eighteenth century\".\n\nMayr notes that Buffon was not an \"evolutionist\", but was certainly responsible for creating the great amount of interest in natural history in France. He agrees that Buffon's thought is hard to classify and even self-contradictory, and that the theologians forced him to avoid writing some of his opinions openly. Mayr argues however that Buffon was \"fully aware of the possibility of 'common descent', and was perhaps the first author ever to articulate it clearly\", quoting Buffon at length, starting with \"Not only the ass and the horse, but also man, the apes, the quadrupeds, and all the animals might be regarded as constituting but a single family\", and later \"that man and ape have a common origin\", and that \"the power of nature...with sufficient time, she has been able from a single being to derive all the other organized beings\". Mayr notes, however, that Buffon immediately rejects the suggestion and offers three arguments against it, namely that no new species have arisen in historical times; that hybrid infertility firmly separates species; and that animals intermediate between, say, the horse and the donkey are not seen (in the fossil record).\n"}
{"id": "1420409", "url": "https://en.wikipedia.org/wiki?curid=1420409", "title": "International Early Warning Programme", "text": "International Early Warning Programme\n\nThe International Early Warning Program (IEWP), was first proposed at the Second International Early Warning Conference (EWCII) in 2003 in Bonn, Germany. It developed increasing importance in the wake of the 2004 Indian Ocean tsunami, which claimed over 200,000 lives and injured over half a million people. \n\nIn January 2005, the United Nations (UN) launched extensive plans to create a global warning system to lessen the impact of deadly natural disasters at the World Conference on Disaster Reduction, held in Kobe, Japan. The UN programme would help improve prevention and resilience to all types of natural disasters, including droughts, wildfires, floods, typhoons, hurricanes, landslides, volcanoes and tsunamis, by using a comprehensive set of methods including rapid information sharing and training communities at risk. It is believed that the loss of human life would have been dramatically reduced, if a tsunami warning system, like the one that exists for the volcano-and-earthquake prone Pacific Rim, had been operational in the Indian Ocean. Technology, such as tremor and tidal gauges, fast data transfer and alarm mechanisms, used in combination with training in the danger zones, would have given hundreds of thousands of people time to move to the safety of higher ground. \n\nEarly warning systems are now widely recognized as worthwhile and necessary investments to help save lives. In 2004, millions of people in the Americas and Asia were evacuated when tropical storms struck, which saved thousands of lives. According to Michel Jarraud, Secretary-General of the World Meteorological Organisation, about 90% of all natural disasters were caused by hazards related to weather and water. Speaking at the conference, he said: \"It is WMO's aim to halve the number of deaths due to natural disasters of meteorological, hydrological and climatic origin over the next 15 years, more specifically to reduce by half the associated ten-year average fatality from the period 1995-2004 to the period 2010-2019 for these disasters.\"\n\nThere was unanimous support among participants to the January 2005 conference, as an initial step towards an International Early Warning Programme, for UN-led efforts to establish an Indian Ocean Tsunami Warning System.\n\n"}
{"id": "12804696", "url": "https://en.wikipedia.org/wiki?curid=12804696", "title": "Jaramillo reversal", "text": "Jaramillo reversal\n\nThe Jaramillo reversal was a reversal and excursion of the Earth's magnetic field that occurred approximately one million years ago. In the geological time scale it was a \"short-term\" positive reversal in the then-dominant Matuyama reversed magnetic chronozone; its beginning is widely dated to 990,000 years before the present (BP), and its end to 950,000 BP (though an alternative date of 1.07 million years ago to 990,000 is also found in the scientific literature).\n\nThe causes and mechanisms of short-term reversals and excursions like the Jaramillo, as well as the major field reversals like the Brunhes–Matuyama reversal, are subjects of study and dispute among researchers. One theory associates the Jaramillo with the Bosumtwi impact event, as evidenced by a tektite strewnfield in the Ivory Coast, though this hypothesis has been claimed as \"highly speculative\" and \"refuted\".\n\n"}
{"id": "357190", "url": "https://en.wikipedia.org/wiki?curid=357190", "title": "Kondo effect", "text": "Kondo effect\n\nIn physics, the Kondo effect describes the scattering of conduction electrons in a metal due to magnetic impurities, resulting in a characteristic change in electrical resistivity with temperature.\nThe effect was first described by Jun Kondo, who applied third-order perturbation theory to the problem to account for s-d electron scattering. Kondo's model predicted that the scattering rate of conduction electrons of the magnetic impurity should diverge as the temperature approaches 0 K. Extended to a lattice of \"magnetic impurities\", the Kondo effect likely explains the formation of \"heavy fermions\" and \"Kondo insulators\" in intermetallic compounds, especially those involving rare earth elements like cerium, praseodymium, and ytterbium, and actinide elements like uranium. The Kondo effect has also been observed in quantum dot systems.\n\nThe temperature dependence of the resistivity including the Kondo effect is written as:\n\nformula_1\n\nwhere ρ is the residual resistance, aT shows the contribution from the Fermi liquid properties, and the term bT is from the lattice vibrations; a, b and c are constants. Jun Kondo derived the third term of the logarithmic dependence.\n\nKondo's model was derived using perturbation theory, but later methods used non-perturbative techniques to refine his result. These improvements produced a finite resistivity but retained the feature of a resistance minimum at a non-zero temperature. One defines the Kondo temperature as the energy scale limiting the validity of the Kondo results. The Anderson impurity model and accompanying Wilsonian renormalization theory were an important contribution to understanding the underlying physics of the problem. Based on the Schrieffer-Wolff transformation, it was shown that Kondo model lies in the strong coupling regime of the Anderson impurity model. The Schrieffer-Wolff transformation projects out the high energy charge excitations in Anderson impurity model, obtaining the Kondo model as an effective Hamiltonian.\n\nThe Kondo effect can be considered as an example of asymptotic freedom, i.e. a situation where the coupling becomes non-perturbatively strong at low temperatures and low energies. In the Kondo problem, the coupling refers to the interaction between the localized magnetic impurities and the itinerant electrons.\n\nExtended to a lattice of \"magnetic impurities\", the Kondo effect likely explains the formation of \"heavy fermions\" and \"Kondo insulators\" in intermetallic compounds, especially those involving rare earth elements like cerium, praseodymium, and ytterbium, and actinide elements like uranium. In heavy fermion materials, the nonperturbative growth of the interaction leads to quasi-electrons with masses up to thousands of times the free electron mass, i.e., the electrons are dramatically slowed by the interactions. In a number of instances they actually are superconductors. More recently, it is believed that a manifestation of the Kondo effect is necessary for understanding the unusual metallic delta-phase of plutonium.\n\nMore recently the Kondo effect has been observed in quantum dot systems. In such systems, a quantum dot with at least one unpaired electron behaves as a magnetic impurity, and when the dot is coupled to a metallic conduction band, the conduction electrons can scatter off the dot. This is completely analogous to the more traditional case of a magnetic impurity in a metal.\n\nIn 2012, Beri and Cooper proposed a topological Kondo effect could be found with Majorana fermions, while it has been shown that quantum simulations with ultracold atoms may also demonstrate the effect.\n\nIn 2017, teams from the Vienna University of Technology and Rice University conducted experiments into the development of new materials and theoretical work experimenting with structures made from the metals cerium, bismuth and palladium in specific combinations, respectively. The results of these experiments were published in December 2017, and contributed to theoretical work being undertaken by Dr. Hsin-Hua Lai and his team at Rice University, who realized the potential to create an entirely new material. They \"stumbled upon\" a model, and found that \"the mass had gone from like 1,000 times the mass of an electron to zero\". This is a characteristic of a Weyl semimetal. The team dubbed this new quantum material Weyl-Kondo semimetal.\n\n"}
{"id": "5318004", "url": "https://en.wikipedia.org/wiki?curid=5318004", "title": "Kunstformen der Natur", "text": "Kunstformen der Natur\n\nKunstformen der Natur (known in English as Art Forms in Nature) is a book of lithographic and halftone prints by German biologist Ernst Haeckel.\n\nOriginally published in sets of ten between 1899 and 1904 and collectively in two volumes in 1904, it consists of 100 prints of various organisms, many of which were first described by Haeckel himself. Over the course of his career, over 1000 engravings were produced based on Haeckel's sketches and watercolors; many of the best of these were chosen for \"Kunstformen der Natur\", translated from sketch to print by lithographer Adolf Giltsch.\n\nA second edition of \"Kunstformen\", containing only 30 prints, was produced in 1924.\n\nAccording to Haeckel scholar Olaf Breidbach, the work was \"not just a book of illustrations but also the summation of his view of the world.\" The over-riding themes of the \"Kunstformen\" plates are symmetry and level of organization. The subjects were selected to embody these to the full, from the scale patterns of boxfishes to the spirals of ammonites to the perfect symmetries of jellies and microorganisms, while images composing each plate are arranged for maximum visual impact.\n\nAmong the notable prints are numerous radiolarians, which Haeckel helped to popularize among amateur microscopists; at least one example is found in almost every set of 10. Cnidaria also feature prominently throughout the book, including sea anemones as well as Siphonophorae, Semaeostomeae, and other medusae. The first set included \"Desmonema annasethe\" (now \"Cyanea annasethe\"), a particularly striking jellyfish that Haeckel observed and described shortly after the death of his wife Anna Sethe.\n\n\"Kunstformen der Natur\" was influential in early 20th-century art, architecture, and design, bridging the gap between science and art. In particular, many artists associated with Art Nouveau were influenced by Haeckel's images, including René Binet, Karl Blossfeldt, Hans Christiansen, and Émile Gallé. One prominent example is the Amsterdam Commodities Exchange designed by Hendrik Petrus Berlage: it was in part inspired by \"Kunstformen\" illustrations.\n\nHaeckel's original classifications appear in \"italics\".\n\n\n"}
{"id": "3958869", "url": "https://en.wikipedia.org/wiki?curid=3958869", "title": "List of conservation organisations", "text": "List of conservation organisations\n\nThis is a list of conservation organisations, which are organisations that primarily deal with the conservation of various ecosystems.\nCave Conservancies are land trusts specialized in caves and karst features.\n\n"}
{"id": "660678", "url": "https://en.wikipedia.org/wiki?curid=660678", "title": "List of glaciers", "text": "List of glaciers\n\nA glacier ( ) or () is a persistent body of dense ice that is constantly moving under its own weight; it forms where the accumulation of snow exceeds its ablation (melting and sublimation) over many years, often centuries. Glaciers slowly deform and flow due to stresses induced by their weight, creating crevasses, seracs, and other distinguishing features. Because glacial mass is affected by long-term climate changes, e.g., precipitation, mean temperature, and cloud cover, glacial mass changes are considered among the most sensitive indicators of climate change.\n\nAfrica, specifically East Africa, has contained glacial regions, possibly as far back as the last glacier maximum 10 to 15 thousand years ago. Seasonal snow does exist on the highest peaks of East Africa as well as in the Drakensberg Range of South Africa, the Stormberg Mountains, and the Atlas Mountains in Morocco. Currently, the only remaining glaciers on the continent exist on Mount Kilimanjaro, Mount Kenya, and the Rwenzori.\n\nThere are many glaciers in the Antarctic. This set of lists does not include ice sheets, ice caps or ice fields, such as the Antarctic ice sheet, but includes glacial features that are defined by their flow, rather than general bodies of ice. The lists include outlet glaciers, valley glaciers, cirque glaciers, tidewater glaciers and ice streams. Ice streams are a type of glacier and many of them have \"glacier\" in their name, e.g. Pine Island Glacier. Ice shelves are listed separately in the List of Antarctic ice shelves. For the purposes of these lists, the Antarctic is defined as any latitude further south than 60° (the continental limit according to the Antarctic Treaty System).\n\nThere are also glaciers in the subantarctic. This includes one snow field (Murray Snowfield). Snow fields are not glaciers in the strict sense of the word, but they are commonly found at the accumulation zone or head of a glacier. For the purposes of this list, Antarctica is defined as any latitude further south than 60° (the continental limit according to the Antarctic Treaty).\n\nThe majority of Europe's glaciers are found in the Alps, Caucasus and the Scandinavian Mountains (mostly Norway) as well as in Iceland. Iceland has the largest glacier in Europe, Vatnajökull glacier, that covers between 8,100-8,300 km² in area and 3,100 km³ in volume. Norway alone has more than 2500 glaciers (including very small ones) covering an estimated 1% of mainland Norway's surface area. Several of mainland Europe's biggest glaciers are found here including; Jostedalsbreen(the largest in mainland Europe at 487 km), Vestre Svartisen(221 km), Søndre Folgefonna(168 km) and Østre Svartisen(148 km). The two Svartisen glaciers used to be one connected entity during the Little Ice Age but has since separated.\n\n\nThere are a number of glaciers existing in North America, currently or in recent centuries. In the United States, these glaciers are located in nine states, all in the Rocky Mountains or further west. The southernmost named glacier among them is the Lilliput Glacier in Tulare County, east of the Central Valley of California.\n\nMexico has about two dozen glaciers, all of which are located on Pico de Orizaba (Citlaltépetl), Popocatépetl and Iztaccíhuatl, the three tallest mountains in the country.\n\n\nGlaciers in South America develop exclusively on the Andes and are subject of the Andes various climatic regimes namely the Tropical Andes, Dry Andes and the Wet Andes. Apart from this there is a wide range of latitudes on which glaciers develop from 5000 m in the Altiplano mountains and volcanoes to reaching sealevel as tidewater glaciers from San Rafael Lagoon (45° S) and southwards. South America hosts two large ice fields, the Northern and Southern Patagonian Ice Fields, of which the second is the largest contiguous body of glaciers in extrapolar regions.\n\nThe glaciers of Chile cover 2.7% (20,188 km) of the land area of the country, excluding Antártica Chilena, and have a considerable impact on its landscape and water supply. By surface 80% of South America’s glaciers lie in Chile. Glaciers develop in the Andes of Chile from 27˚S southwards and in a very few places north of 18°30'S in the extreme north of the country: in between they are absent because of extreme aridity, though rock glaciers formed from permafrost are common. The largest glaciers of Chile are the Northern and Southern Patagonian Ice Fields. From a latitude of 47° S and south some glaciers reach sea level.\n\nApart from height and latitude, the settings of Chilean glaciers depend on precipitation patterns; in this sense two different regions exist: the Dry Andes and the Wet Andes.\n\nNo glaciers remain on the Australia mainland or Tasmania. A few, like the Heard Island glaciers are located in the territory of Heard Island and McDonald Islands in the southern Indian Ocean.\n\nNew Guinea has the Puncak Jaya glacier.\n\nNew Zealand contains many glaciers, mostly located near the Main Divide of the Southern Alps in the South Island. They are classed as mid-latitude mountain glaciers. There are eighteen small glaciers in the North Island on Mount Ruapehu.\n\nAn inventory of South Island glaciers compiled in the 1980s indicated there were about 3,155 glaciers with an area of at least one hectare (2.5 acres). Approximately one sixth of these glaciers covered more than 10 hectares. These include:\n\n\nThe following is the list of longest glaciers in the non-polar regions, generally regarded as between 60 degrees north and 60 degrees south latitude, though some definitions expand it slightly.\n\n"}
{"id": "1675601", "url": "https://en.wikipedia.org/wiki?curid=1675601", "title": "Manifold vacuum", "text": "Manifold vacuum\n\nManifold vacuum, or engine vacuum in an internal combustion engine is the difference in air pressure between the engine's intake manifold and Earth's atmosphere.\n\nManifold vacuum is an effect of a piston's movement on the induction stroke and the choked flow through a throttle in the intake manifold of an engine. It is a measure of the amount of restriction of airflow through the engine, and hence of the unused power capacity in the engine. In some engines, the manifold vacuum is also used as an auxiliary power source to drive engine accessories and for the crankcase ventilation system.\n\nManifold vacuum should not be confused with venturi vacuum, which is an effect exploited in carburetors to establish a pressure difference roughly proportional to mass airflow and to maintain a somewhat constant air/fuel ratio. It is also used in light airplanes to provide airflow for pneumatic gyroscopic instruments.\n\nThe rate of airflow through an internal combustion engine is an important factor determining the amount of power the engine generates. Most gasoline engines are controlled by limiting that flow with a throttle that restricts intake airflow, while a diesel engine is controlled by the amount of fuel supplied to the cylinder, and so has no \"throttle\" as such. Manifold vacuum is present in all naturally aspirated engines that use throttles (including carbureted and fuel injected gasoline engines using the Otto cycle or the two-stroke cycle; diesel engines do not have throttle plates).\n\nThe mass flow through the engine is the product of the rotation rate of the engine, the displacement of the engine, and the density of the intake stream in the intake manifold. In most applications the rotation rate is set by the application (engine speed in a vehicle or machinery speed in other applications). The displacement is dependent on the engine geometry, which is generally not adjustable while the engine is in use (although a handful of models do have this feature, see variable displacement). Restricting the input flow reduces the density (and hence pressure) in the intake manifold, reducing the amount of power produced. It is also a major source of engine drag (see engine braking), as the engine must pump material from the low-pressure intake manifold into the exhaust manifold (at ambient atmospheric pressure).\n\nWhen the throttle is opened (in a car, the accelerator pedal is depressed), ambient air is free to fill the intake manifold, increasing the pressure (filling the vacuum). A carburetor or fuel injection system adds fuel to the airflow in the correct proportion, providing energy to the engine. When the throttle is opened all the way, the engine's air induction system is exposed to full atmospheric pressure, and maximum airflow through the engine is achieved. In a naturally aspirated engine, output power is limited by the ambient barometric pressure. Superchargers and turbochargers boost manifold pressure above atmospheric pressure.\n\nModern engines use a manifold absolute pressure (abbreviated as \"MAP\") sensor to measure air pressure in the intake manifold. Manifold absolute pressure is one of a multitude of parameters used by the engine control unit (ECU) to optimize engine operation. It is important to differentiate between absolute and gauge pressure when dealing with certain applications, particularly those that experience changes in elevation during normal operation. \n\nMotivated by government regulations mandating reduction of fuel consumption (in the USA) or reduction of carbon dioxide emissions (in Europe), passenger cars and light trucks have been fitted with a variety of technologies (downsized engines; lockup, multi-ratio and overdrive transmissions; variable valve timing, forced induction, diesel engines, et al.) which render manifold vacuum inadequate or unavailable. Electric vacuum pumps are now commonly used for powering pneumatic accessories.\n\nManifold vacuum is caused by a different phenomenon than venturi vacuum, which is present inside carburetors. Venturi vacuum is caused by the venturi effect which, for fixed ambient conditions (air density and temperature), depends on the total mass flow through the carburetor. In engines that use carburetors, the venturi vacuum is approximately proportional to the total mass flow through the engine (and hence the total power output). As ambient pressure (altitude, weather) or temperature change, the carburetor may need to be adjusted to maintain this relationship.\n\nManifold pressure may also be \"ported\". Porting is selecting a location for the pressure tap within the throttle plate's range of motion. Depending on throttle position, a ported pressure tap may be either upstream or downstream of the throttle. As the throttle position changes, a \"ported\" pressure tap is selectively connected to either manifold pressure or ambient pressure. Antique (pre-OBD II) engines often used ported manifold pressure taps for ignition distributors and emission-control components.\n\nMost automobiles use four-stroke Otto cycle engines with multiple cylinders attached to a single inlet manifold. During the induction stroke, the piston descends in the cylinder and the intake valve is open. As the piston descends it effectively increases the volume in the cylinder above it, setting up low pressure. Atmospheric pressure pushes air through the manifold and carburetor or fuel injection system, where it is mixed with fuel. Because multiple cylinders operate at different times in the engine cycle, there is almost constant pressure difference through the inlet manifold from carburetor to engine.\n\nTo control the amount of fuel/air mix entering the engine, a simple butterfly valve (throttle plate) is generally fitted at the start of the intake manifold (just below the carburetor in carbureted engines). The butterfly valve is simply a circular disc fitted on a spindle, fitting inside the pipe work. It is connected to the accelerator pedal of the car, and is set to be fully open when the pedal is fully depressed and fully closed when the pedal is released. The butterfly valve often contains a small \"idle cutout\", a hole that allows small amounts of fuel/air mixture into the engine even when the valve is fully closed, or the carburetor has a separate air bypass with its own idle jet.\n\nIf the engine is operating under light or no load and low or closed throttle, there is high manifold vacuum. As the throttle is opened, the engine speed increases rapidly. The engine speed is limited only by the amount of fuel/air mixture that is available in the manifold. Under full throttle and light load, other effects (such as valve float, turbulence in the cylinders, or ignition timing) limit engine speed so that the manifold pressure can increase—but in practice, parasitic drag on the internal walls of the manifold, plus the restrictive nature of the venturi at the heart of the carburetor, means that a low pressure will always be set up as the engine's internal volume exceeds the amount of the air the manifold is capable of delivering.\n\nIf the engine is operating under heavy load at wide throttle openings (such as accelerating from a stop or pulling the car up a hill) then engine speed is limited by the load and minimal vacuum will be created. Engine speed is low but the butterfly valve is fully open. Since the pistons are descending more slowly than under no load, the pressure differences are less marked and parasitic drag in the induction system is negligible. The engine pulls air into the cylinders at the full ambient pressure.\n\nMore vacuum is created in some situations. On deceleration or when descending a hill, the throttle will be closed and a low gear selected to control speed. The engine will be rotating fast because the road wheels and transmission are moving quickly, but the butterfly valve will be fully closed. The flow of air through the engine is strongly restricted by the throttle, producing a strong vacuum on the engine side of the butterfly valve which will tend to limit the speed of the engine. This phenomenon, known as engine braking, is used to prevent acceleration or even to slow down with minimal or no brake usage (as when descending a long or steep hill). This vacuum braking should not be confused with compression braking (aka a \"Jake brake\"), or with exhaust braking, which are often used on large diesel trucks. Such devices are necessary for engine braking with a diesel as they lack a throttle to restrict the air flow enough to create sufficient vacuum to brake a vehicle.\n\nThis low (or negative) pressure can be put to uses. A pressure gauge measuring the manifold pressure can be fitted to give the driver an indication of how hard the engine is working and it can be used to achieve maximum momentary fuel efficiency by adjusting driving habits: minimizing manifold vacuum increases momentary efficiency. A weak manifold vacuum under closed-throttle conditions shows that the butterfly valve or internal components of the engine (valves or piston rings) are worn, preventing good pumping action by the engine and reducing overall efficiency.\n\nVacuum is often used to drive auxiliary systems on the vehicle. Vacuum-assist brake servos, for example, use atmospheric pressure pressing against the engine manifold vacuum to increase pressure on the brakes. Since braking is nearly always accompanied by the closing of the throttle and associated high manifold vacuum, this system is simple and almost foolproof. Vacuum tanks were installed on trailers to control their integrated braking systems.\n\nPrior to the introduction of Federal Motor Vehicle Safety Standards in the USA by the National Traffic and Motor Vehicle Safety Act of 1966, it was common to use manifold vacuum to drive windscreen wipers with a pneumatic motor. This system was cheap & simple but resulted in the comical yet unsafe effect of wipers which operate at full speed while the engine idles, operate around half speed while cruising, and stop altogether when the driver depresses the pedal fully. Vehicle HVAC systems also used manifold vacuum to drive actuators controlling airflow and temperature.\n\nAnother obsolete accessory is the \"Autovac\" fuel lifter which uses vacuum to raise fuel from the main tank to a small auxiliary tank, from which it flows by gravity to the carburetor. This eliminated the fuel pump which, in early cars, was an unreliable item.\n\nMany diesel engines do not have butterfly valve throttles. The manifold is connected directly to the air intake and the only suction created is that caused by the descending piston with no venturi to increase it, and the engine power is controlled by varying the amount of fuel that is injected into the cylinder by a fuel injection system. This assists in making diesels much more efficient than petrol engines.\n\nIf vacuum is required (vehicles that can be fitted with both petrol and diesel engines often have systems requiring it), a butterfly valve connected to the throttle can be fitted to the manifold. This reduces efficiency and is still not as effective as it is not connected to a venturi. Since low-pressure is only created on the overrun (such as when descending hills with a closed throttle), not over a wide range of situations as in a petrol engine, a vacuum tank is fitted.\n\nMost diesel engines now have a separate vacuum pump (\"exhauster\") fitted to provide vacuum at all times, at all engine speeds.\n\nMany new BMW petrol engines do not use a throttle in normal running, but instead use \"Valvetronic\" variable-lift intake valves to control the amount of air entering the engine. Like a diesel engine, manifold vacuum is practically non-existent in these engines and a different source must be utilised to power the brake servo.\n\n"}
{"id": "6748280", "url": "https://en.wikipedia.org/wiki?curid=6748280", "title": "Material", "text": "Material\n\nA material is a chemical substance or mixture of substances that constitute an object. Materials can be pure or impure, a singular composite or a complex mix, living or non-living matter, whether natural or man-made, either concrete or abstract. Materials can be classified based on different properties such as physical and chemical properties (see List of materials properties), geological, biological, choreographical, or philosophical properties. In the physical sense, materials are studied in the field of materials science.\n\nIn industry, materials are inputs to production or manufacturing processes. They may either be raw material, that is, unprocessed, or processed before being used in more advanced production processes, either by distillation or synthesis (synthetic materials).\n\nTypes of materials include:\n\nMaterials are classified according to many different criteria including their physical and chemical characteristics as well as their intended applications whether it is thermal, optical, electrical, magnetic, or combined. As their methods of usage dictate their physical appearance, they can be designed, tailored, and/or prepared in many forms such as powders, thin or thick films, and plates and could be introduced/studied in a single or multi layers. End products could be pure materials or doped ones with most useful compounds are those with controlled added impurities.The dopants could be added chemically or mixed and implanted physically. In case the impurities were added chemically, the dopants/co-dopants on substitutional/interstitial sites should be optimized and investigated thoroughly as well as any stresses instigated by their presence within the structure; whereas in the case of the physical mixing, the influence of the degree of heterogeneity of the prepared hybrid composites ought to be studied.The different physical and chemical preparation techniques can be used solely or combined including solid state synthesis, hydrothermal, sol-gel, precipitations and coprecipitations, spin coating, physical vapor deposition, and spray pyrolysis. Types of impurities along with their amounts are usually dictated by types of matrices to be added to, and their ability to maximize the desired products’ usefulness. Among the most commonly used characterization techniques are X-ray diffraction (XRD) either single crystal or powder, scanning electron microscopy (SEM), energy dispersive X-ray spectroscopy (EDS), X-ray fluorescence (XRF), differential scanning calorimetry (DSC), UV-Vis absorption Spectroscopy, Fourier transform infra-red (FTIR), and Photoluminescence spectrometry. In addition, it is usually considered of extreme importance to find theoretical models that can confirm and/or predict the experimental findings and assist in discussion, assignment, and the explanation of results and outcomes. Also, vision and room for future modification and development should always be pinpointed. Hence, one can classify the material as a smart one if its presence can serve multi purposes within the final product.\n\n"}
{"id": "14389994", "url": "https://en.wikipedia.org/wiki?curid=14389994", "title": "Natural landscape", "text": "Natural landscape\n\nA natural landscape is the original landscape that exists before it is acted upon by human culture. The natural landscape and the cultural landscape are separate parts of the landscape. However, in the twenty-first century landscapes that are totally untouched by human activity no longer exist, so that reference is sometimes now made to degrees of naturalness within a landscape.\n\nIn \"Silent Spring\" (1962) Rachel Carson describes a roadside verge as it used to look: \"Along the roads, laurel, viburnum and alder, great ferns and wildflowers delighted the traveler’s eye through much of the year\" and then how it looks now following the use of herbicides: \"The roadsides, once so attractive, were now lined with browned and withered vegetation as though swept by fire\". Even though the landscape before it is sprayed is biologically degraded, and may well contains alien species, the concept of what might constitute a natural landscape can still be deduced from the context.\n\nThe phrase \"natural landscape\" was first used in connection with landscape painting, and landscape gardening, to contrast a formal style with a more natural one, closer to nature. Alexander von Humboldt (1769 – 1859) was to further conceptualize this into the idea of a natural landscape \"separate\" from the cultural landscape. Then in 1908 geographer Otto Schlüter developed the terms original landscape (\"Urlandschaft\") and its opposite cultural landscape (\"Kulturlandschaft\") in an attempt to give the science of geography a subject matter that was different from the other sciences. An early use of the actual phrase \"natural landscape\" by a geographer can be found in Carl O. Sauer's paper \"The Morphology of Landscape\" (1925).\n\nThe concept of a natural landscape was first developed in connection with landscape painting, though the actual term itself was first used in relation to landscape gardening. In both cases it was used to contrast a formal style with a more natural one, that is closer to nature. Chunglin Kwa suggests, \"that a seventeenth-century or early-eighteenth-century person could experience natural scenery ‘just like on a painting,’ and so, with or without the use of the word itself, designate it as a landscape.\" With regard to landscape gardening John Aikin, commented in 1794: \"Whatever, therefore, there be of \"novelty\" in the singular scenery of an artificial garden, it is soon exhausted, whereas the infinite diversity of a natural landscape presents an inexhaustible flore of new forms\". Writing in 1844 the prominent American landscape gardener Andrew Jackson Downing comments: \"straight canals, round or oblong pieces of water, and all the regular forms of the geometric mode ... would evidently be in violent opposition to the whole character and expression of natural landscape\".\n\nIn his extensive travels in South America, Alexander von Humboldt became the first to conceptualize a natural landscape separate from the cultural landscape, though he does not actually use these terms. Andrew Jackson Downing was aware of, and sympathetic to, Humboldt's ideas, which therefore influenced American landscape gardening.\n\nSubsequently, the geographer Otto Schlüter, in 1908, argued that by defining geography as a \"Landschaftskunde\" (landscape science) would give geography a logical subject matter shared by no other discipline. He defined two forms of landscape: the \"Urlandschaft\" (original landscape) or landscape that existed before major human induced changes and the \"Kulturlandschaft\" (cultural landscape) a landscape created by human culture. Schlüter argued that the major task of geography was to trace the changes in these two landscapes.\n\nThe term natural landscape is sometimes used as a synonym for wilderness, but for geographers natural landscape is a scientific term which refers to the biological, geological, climatological and other aspects of a landscape, not the cultural values that are implied by the word wilderness.\n\nMatters are complicated by the fact that the words nature and natural have more than one meaning. On the one hand there is the main dictionary meaning for nature: \"The phenomena of the physical world collectively, including plants, animals, the landscape, and other features and products of the earth, as opposed to humans or human creations\". On the other hand, there is the growing awareness, especially since Charles Darwin, of humanities biological affinity with nature.\n\nThe dualism of the first definition has its roots is an \"ancient concept\", because early people viewed \"nature, or the nonhuman world […] as a divine \"Other\", godlike in its separation from humans\". In the West, Christianity's myth of the fall, that is the expulsion of humankind from the Garden of Eden, where all creation lived in harmony, into an imperfect world, has been the major influence. Cartesian dualism, from the seventeenth century on, further reinforced this dualistic thinking about nature. \nWith this dualism goes value judgement as to the superiority of the natural over the artificial. Modern science, however, is moving towards a holistic view of nature.\n\nWhat is meant by natural, within the American conservation movement, has been changing over the last century and a half.\n\nIn the mid-nineteenth century American began to realize that the land was becoming more and more domesticated and wildlife was disappearing. This led to the creation of American National Parks and other conservation sites. Initially it was believed that all that was needed to do was to separate what was seen as natural landscape and \"avoid disturbances such as logging, grazing, fire and insect outbreaks\". This, and subsequent environmental policy, until recently, was influenced by ideas of the wilderness. However, this policy was not consistently applied, and in Yellowstone Park, to take one example, the existing ecology was altered, firstly by the exclusion of Native Americans and later with the virtual extermination of the wolf population.\n\nA century later, in the mid-twentieth century, it began to be believed that the earlier policy of \"protection from disturbance was inadequate to preserve park values\", and that is that direct human intervention was necessary to restore the landscape of National Parks to its ‘’natural’’ condition. In 1963 the Leopold Report argued that \"A national park should represent a vignette of primitive America\". This policy change eventually led to the restoration of wolves in Yellowstone Park in the 1990s.\n\nHowever, recent research in various disciplines indicates that a pristine natural or \"primitive\" landscape is a myth, and it now realised that people have been changing the natural into a cultural landscape for a long while, and that there are few places untouched in some way from human influence. The earlier conservation policies were now seen as cultural interventions. The idea of what is natural and what artificial or cultural, and how to maintain the natural elements in a landscape, has been further complicated by the discovery of global warming and how it is changing natural landscapes.\n\nAlso important is a reaction recently amongst scholars against dualistic thinking about nature and culture. Maria Kaika comments: \"Nowadays, we are beginning to see nature and culture as intertwined once again – not ontologically separated anymore […].What I used to perceive as a compartmentalized world, consisting of neatly and tightly sealed, autonomous ‘space envelopes’ (the home, the city, and nature) was, in fact, a messy socio-spatial continuum”. And William Cronon argues against the idea of wilderness because it \"involves a dualistic vision in which the human is entirely outside the natural\" and affirms that \"wildness (as opposed to wilderness) can be found anywhere\" even \"in the cracks of a Manhattan sidewalk\". According to Cronon we have to \"abandon the dualism that sees the tree in the garden as artificial […] and the tree in the wilderness as natural […] Both in some ultimate sense are wild.\" Here he bends somewhat the regular dictionary meaning of wild, to emphasise that nothing natural, even in a garden, is fully under human control.\n\nThe landscape of Europe has considerably altered by people and even in an area, like the Cairngorm Mountains of Scotland, with a low population density, only \" the high summits of the Cairngorm Mountains, consist entirely of natural elements. These \"high summits\" are of course only part of the Cairngorms, and there are no longer wolves, bears, wild boar or lynx in Scotland's wilderness. The Scots pine in the form of the Caledonian forest also covered much more of the Scottish landscape than today.\n\nThe Swiss National Park, however, represent a more natural landscape. It was founded in 1914, and is one of the earliest national parks in Europe.\nVisitors are not allowed to leave the motor road, or paths through the park, make fire or camp. The only building within the park is Chamanna Cluozza, mountain hut. It is also forbidden to disturb the animals or the plants, or to take home anything found in the park. Dogs are not allowed. Due to these strict rules, the Swiss National Park is the only park in the Alps who has been categorized by the IUCN as a strict nature reserve, which is the highest protection level.\n\nNo place on the Earth is unaffected by people and their culture. People are part of biodiversity, but human activity affects biodiversity, and this alters the natural landscape. Mankind have altered landscape to such an extent that few places on earth remain pristine, but once free of human influences, the landscape can return to a natural or near natural state.\nEven the remote Yukon and Alaskan wilderness, the bi-national Kluane-Wrangell-St. Elias-Glacier Bay-Tatshenshini-Alsek park system comprising Kluane, Wrangell-St Elias, Glacier Bay and Tatshenshini-Alsek parks, a UNESCO World Heritage Site, is not free from human influence, because the Kluane National Park lies within the traditional territories of the Champagne and Aishihik First Nations and Kluane First Nation who have a long history of living in this region. Through their respective Final Agreements with the Canadian Government, they have made into law their rights to harvest in this region.\n\nCultural forces intentionally or unintentionally, have an influence upon the landscape. Cultural landscapes are places or artifacts created and maintained by people. Examples of cultural intrusions into a landscape are: fences, roads, parking lots, sand pits, buildings, hiking trails, management of plants, including the introduction of invasive species, extraction or removal of plants, management of animals, mining, hunting, natural landscaping, farming and forestry, pollution. Areas that might be confused with a natural landscape include public parks, farms, orchards, artificial lakes and reservoirs, managed forests, golf courses, nature center trails, gardens.\n\n"}
{"id": "43427", "url": "https://en.wikipedia.org/wiki?curid=43427", "title": "Nature (journal)", "text": "Nature (journal)\n\nNature is a British multidisciplinary scientific journal, first published on 4 November 1869. It is one of the most recognizable scientific journals in the world, and was ranked the world's most cited scientific journal by the Science Edition of the 2010 \"Journal Citation Reports\" and is ascribed an impact factor of 40.137, making it one of the world's top academic journals. It is one of the few remaining academic journals that publishes original research across a wide range of scientific fields.\n\nResearch scientists are the primary audience for the journal, but summaries and accompanying articles are intended to make many of the most important papers understandable to scientists in other fields and the educated public. Towards the front of each issue are editorials, news and feature articles on issues of general interest to scientists, including current affairs, science funding, business, scientific ethics and research breakthroughs. There are also sections on books, arts, and short science fiction stories. The remainder of the journal consists mostly of research papers (articles or letters), which are often dense and highly technical. Because of strict limits on the length of papers, often the printed text is actually a summary of the work in question with many details relegated to accompanying \"supplementary material\" on the journal's website.\n\nThere are many fields of research in which important new advances and original research are published as either articles or letters in \"Nature.\" The papers that have been published in this journal are internationally acclaimed for maintaining high research standards. Fewer than 8% of submitted papers are accepted for publication.\n\nIn 2007 \"Nature\" (together with \"Science\") received the Prince of Asturias Award for Communications and Humanity.\n\nThe enormous progress in science and mathematics during the 19th century was recorded in journals written mostly in German or French, as well as in English. Britain underwent enormous technological and industrial changes and advances particularly in the latter half of the 19th century. In English the most respected scientific journals of this time were the refereed journals of the Royal Society, which had published many of the great works from Isaac Newton, Michael Faraday through to early works from Charles Darwin. In addition, during this period, the number of popular science periodicals doubled from the 1850s to the 1860s. According to the editors of these popular science magazines, the publications were designed to serve as \"organs of science\", in essence, a means of connecting the public to the scientific world.\n\n\"Nature\", first created in 1869, was not the first magazine of its kind in Britain. One journal to precede \"Nature\" was \"\", which, created in 1859, began as a natural history magazine and progressed to include more physical observational science and technical subjects and less natural history. The journal's name changed from its original title to \"Intellectual Observer: A Review of Natural History, Microscopic Research, and Recreative Science\" and then later to the \"Student and Intellectual Observer of Science, Literature, and Art\". While \"Recreative Science\" had attempted to include more physical sciences such as astronomy and archaeology, the \"Intellectual Observer\" broadened itself further to include literature and art as well. Similar to \"Recreative Science\" was the scientific journal \"Popular Science Review\", created in 1862, which covered different fields of science by creating subsections titled \"Scientific Summary\" or \"Quarterly Retrospect\", with book reviews and commentary on the latest scientific works and publications. Two other journals produced in England prior to the development of \"Nature\" were the \"Quarterly Journal of Science\" and \"Scientific Opinion\", established in 1864 and 1868, respectively. The journal most closely related to \"Nature\" in its editorship and format was \"The Reader\", created in 1864; the publication mixed science with literature and art in an attempt to reach an audience outside of the scientific community, similar to \"Popular Science Review\".\n\nThese similar journals all ultimately failed. The \"Popular Science Review\" survived longest, lasting 20 years and ending its publication in 1881; \"Recreative Science\" ceased publication as the \"Student and Intellectual Observer\" in 1871. The \"Quarterly Journal\", after undergoing a number of editorial changes, ceased publication in 1885. \"The Reader\" terminated in 1867, and finally, \"Scientific Opinion\" lasted a mere 2 years, until June 1870.\n\nNot long after the conclusion of \"The Reader\", a former editor, Norman Lockyer, decided to create a new scientific journal titled \"Nature\", taking its name from a line by William Wordsworth: \"To the solid ground of nature trusts the Mind that builds for aye\". First owned and published by Alexander Macmillan, \"Nature\" was similar to its predecessors in its attempt to \"provide cultivated readers with an accessible forum for reading about advances in scientific knowledge.\" Janet Browne has proposed that \"far more than any other science journal of the period, \"Nature\" was conceived, born, and raised to serve polemic purpose.\" Many of the early editions of \"Nature\" consisted of articles written by members of a group that called itself the X Club, a group of scientists known for having liberal, progressive, and somewhat controversial scientific beliefs relative to the time period. Initiated by Thomas Henry Huxley, the group consisted of such important scientists as Joseph Dalton Hooker, Herbert Spencer, and John Tyndall, along with another five scientists and mathematicians; these scientists were all avid supporters of Darwin's theory of evolution as common descent, a theory which, during the latter half of the 19th century, received a great deal of criticism among more conservative groups of scientists. Perhaps it was in part its scientific liberality that made \"Nature\" a longer-lasting success than its predecessors. John Maddox, editor of \"Nature\" from 1966 to 1973 as well as from 1980 to 1995, suggested at a celebratory dinner for the journal's centennial edition that perhaps it was the journalistic qualities of Nature that drew readers in; \"journalism\" Maddox states, \"is a way of creating a sense of community among people who would otherwise be isolated from each other. This is what Lockyer's journal did from the start.\" In addition, Maddox mentions that the financial backing of the journal in its first years by the Macmillan family also allowed the journal to flourish and develop more freely than scientific journals before it.\nNorman Lockyer, the founder of \"Nature\", was a professor at Imperial College. He was succeeded as editor in 1919 by Sir Richard Gregory. Gregory helped to establish \"Nature\" in the international scientific community. His obituary by the Royal Society stated: \"Gregory was always very interested in the international contacts of science, and in the columns of \"Nature\" he always gave generous space to accounts of the activities of the International Scientific Unions.\" During the years 1945 to 1973, editorship of \"Nature\" changed three times, first in 1945 to A. J. V. Gale and L. J. F. Brimble (who in 1958 became the sole editor), then to John Maddox in 1965, and finally to David Davies in 1973. In 1980, Maddox returned as editor and retained his position until 1995. Philip Campbell has since become Editor-in-chief of all \"Nature\" publications.\n\nIn 1970, \"Nature\" first opened its Washington office; other branches opened in New York in 1985, Tokyo and Munich in 1987, Paris in 1989, San Francisco in 2001, Boston in 2004, and Hong Kong in 2005. In 1971, under John Maddox's editorship, the journal split into \"Nature Physical Sciences\" (published on Mondays), \"Nature New Biology\" (published on Wednesdays) and \"Nature\" (published on Fridays). In 1974, Maddox was no longer editor, and the journals were merged into \"Nature\".\n\nStarting in the 1980s, the journal underwent a great deal of expansion, launching over ten new journals. These new journals comprise the Nature Publishing Group, which was created in 1999 and includes \"Nature\", Nature Publishing Group Journals, Stockton Press Specialist Journals and Macmillan Reference (renamed NPG Reference).\n\nIn 1996, \"Nature\" created its own website and in 1999 Nature Publishing Group began its series of \"Nature Reviews\". Some articles and papers are available for free on the Nature website. Others require the purchase of premium access to the site. \"Nature\" claims an online readership of about 3 million unique readers per month.\n\nOn 30 October 2008, \"Nature\" endorsed an American presidential candidate for the first time when it supported Barack Obama during his campaign in America's 2008 presidential election.\n\nIn October 2012, an Arabic edition of the magazine was launched in partnership with King Abdulaziz City for Science and Technology. As of the time it was released, it had about 10,000 subscribers.\n\nOn 2 December 2014, \"Nature\" announced that it would allow its subscribers and a group of selected media outlets to share links allowing free, \"read-only\" access to content from its journals. These articles are presented using the digital rights management system ReadCube (which is funded by the Macmillan subsidiary Digital Science), and does not allow readers to download, copy, print, or otherwise distribute the content. While it does, to an extent, provide free online access to articles, it is not a true open access scheme due to its restrictions on re-use and distribution.\n\nOn 15 January 2015, details of a proposed merger with Springer Science+Business Media were announced.\n\nIn May 2015 it came under the umbrella of Springer Nature, by the merger of Springer Science+Business Media and Holtzbrinck Publishing Group's Nature Publishing Group, Palgrave Macmillan, and Macmillan Education.\n\nBeing published in \"Nature\" or any \"Nature\" publication is very prestigious. In particular, empirical papers are often highly cited, which can lead to promotions, grant funding, and attention from the mainstream media. Because of these positive feedback effects, competition among scientists to publish in high-level journals like \"Nature\" and its closest competitor, \"Science\", can be very fierce. \"Nature\"s impact factor, a measure of how many citations a journal generates in other works, was 38.138 in 2015 (as measured by Thomson ISI), among the highest of any science journal.\n\nAs with most other professional scientific journals, papers undergo an initial screening by the editor, followed by peer review (in which other scientists, chosen by the editor for expertise with the subject matter but who have no connection to the research under review, will read and critique articles), before publication. In the case of \"Nature\", they are only sent for review if it is decided that they deal with a topical subject and are sufficiently ground-breaking in that particular field. As a consequence, the majority of submitted papers are rejected without review.\n\nAccording to \"Nature\"s original mission statement:\nThis was revised in 2000 to:\nMany of the most significant scientific breakthroughs in modern history have been first published in \"Nature\". The following is a selection of scientific breakthroughs published in \"Nature\", all of which had far-reaching consequences, and the citation for the article in which they were published.\n\n\nIn 2017, Nature published an editorial entitled \"Removing Statues of Historical figures risks whitewashing history: Science must acknowledge mistakes as it marks its past\". The article commented on the placement and maintenance of statues honouring scientists with known unethical, abusive and torturous histories. Specifically, the editorial called on examples of J. Marion Sims, the 'Father of gynecology' who experimented on African American female slaves who were unable to give informed consent, and Thomas Parran Jr. who oversaw the Tuskegee syphilis experiment. The editorial as written made the case that removing such statues, and erasing names, runs the risk of \"whitewashing history\", and stated “Instead of removing painful reminders, perhaps these should be supplemented”. The article caused a large outcry and was quickly modified by Nature. The article was largely seen as offensive, inappropriate, and by many, racist. Nature acknowledged that the article as originally written was \"offensive and poorly worded\" and published selected letters of response. The editorial came just weeks after hundreds of white supremacists marched in Charlottesville, Virginia in the Unite the Right rally to oppose the removal of a statue of Robert E. Lee, setting off violence in the streets and killing a young woman. When Nature posted a link to the editorial on Twitter, the thread quickly exploded with criticisms. In response, several scientists called for a boycott. On 18 September 2017, the editorial was updated and edited by Philip Campbell, the editor of the journal.\n\nWhen Paul Lauterbur and Peter Mansfield won a Nobel Prize in Physiology or Medicine for research initially rejected by \"Nature\" and published only after Lauterbur appealed the rejection, \"Nature\" acknowledged more of its own missteps in rejecting papers in an editorial titled, \"Coping with Peer Rejection\":\nFrom 2000 to 2001, a series of five fraudulent papers by Jan Hendrik Schön was published in \"Nature\". The papers, about semiconductors, were revealed to contain falsified data and other scientific fraud. In 2003, \"Nature\" retracted the papers. The Schön scandal was not limited to \"Nature\"; other prominent journals, such as \"Science\" and \"Physical Review\", also retracted papers by Schön.\n\nIn June 1988, after nearly a year of guided scrutiny from its editors, \"Nature\" published a controversial and seemingly anomalous paper detailing Dr. Jacques Benveniste and his team's work studying human basophil degranulation in the presence of extremely dilute antibody serum. In short, their paper concluded that less than a single molecule of antibody could trigger an immune response in human basophils, defying the physical law of mass action. The paper excited substantial media attention in Paris, chiefly because their research sought funding from homeopathic medicine companies. Public inquiry prompted \"Nature\" to mandate an extensive, stringent and scientifically questionable experimental replication in Benveniste's lab, through which his team's results were categorically refuted.\n\nBefore publishing one of its most famous discoveries, Watson and Crick's 1953 on the structure of DNA, \"Nature\" did not send the paper out for peer review. John Maddox, \"Nature\"s editor, stated: \"the Watson and Crick paper was not peer-reviewed by \"Nature\" ... the paper could not have been refereed: its correctness is self-evident. No referee working in the field ... could have kept his mouth shut once he saw the structure\".\n\nAn earlier error occurred when Enrico Fermi submitted his breakthrough paper on the weak interaction theory of beta decay. \"Nature\" turned down the paper because it was considered too remote from reality. Fermi's paper was published by \"Zeitschrift für Physik\" in 1934, and finally published by \"Nature\" five years later, after Fermi's work had been widely accepted.\n\nIn 1999 \"Nature\" began publishing science fiction short stories. The brief \"vignettes\" are printed in a series called \"Futures\". The stories appeared in 1999 and 2000, again in 2005 and 2006, and have appeared weekly since July 2007. Sister publication \"Nature Physics\" also printed stories in 2007 and 2008. In 2005, \"Nature\" was awarded the European Science Fiction Society's Best Publisher award for the \"Futures\" series. One hundred of the \"Nature\" stories between 1999 and 2006 were published as the collection \"Futures from Nature\" in 2008.\n\nThe journal has a weekly circulation of around 53,000 and a pass-along rate of 8.0, resulting in a readership of over 400,000.\n\n\"Nature\" is edited and published in the United Kingdom by a division of the international scientific publishing company Springer Nature that publishes academic journals, magazines, online databases, and services in science and medicine. \"Nature\" has offices in London, New York City, San Francisco, Washington, D.C., Boston, Tokyo, Hong Kong, Paris, Munich, and Basingstoke. Nature Publishing Group also publishes other specialized journals including \"Nature Neuroscience\", \"Nature Biotechnology,\" \"Nature Methods\", the \"Nature Clinical Practice\" series of journals, \"Nature Structural & Molecular Biology\", \"Nature Chemistry\", and the \"Nature Reviews\" series of journals.\n\nSince 2005, each issue of \"Nature\" has been accompanied by a \"Nature Podcast\" featuring highlights from the issue and interviews with the articles' authors and the journalists covering the research. It is presented by Kerri Smith, and features interviews with scientists on the latest research, as well as news reports from Nature's editors and journalists. The Nature Podcast was founded – and the first 100 episodes were produced and presented – by clinician and virologist Chris Smith of Cambridge and \"The Naked Scientists\".\n\nIn 2007, Nature Publishing Group began publishing \"Clinical Pharmacology & Therapeutics\", the official journal of the American Society of Clinical Pharmacology & Therapeutics and \"Molecular Therapy\", the American Society of Gene Therapy's official journal, as well as the \"International Society for Microbial Ecology (ISME) Journal\". Nature Publishing Group launched \"Nature Photonics\" in 2007 and \"Nature Geoscience\" in 2008. \"Nature Chemistry\" published its first issue in April 2009.\n\nNature Publishing Group actively supports the self-archiving process and in 2002 was one of the first publishers to allow authors to post their contributions on their personal websites, by requesting an exclusive licence to publish, rather than requiring authors to transfer copyright. In December 2007, Nature Publishing Group introduced the Creative Commons attribution-non commercial-share alike unported licence for those articles in Nature journals that are publishing the primary sequence of an organism's genome for the first time.\n\nIn 2008, a collection of articles from \"Nature\" was edited by John S. Partington under the title \"H. G. Wells in Nature, 1893–1946: A Reception Reader\" and published by Peter Lang.\n\n\n"}
{"id": "12422403", "url": "https://en.wikipedia.org/wiki?curid=12422403", "title": "New7Wonders of Nature", "text": "New7Wonders of Nature\n\nNew7Wonders of Nature (2007–2011) was an initiative started in 2007 to create a list of seven natural wonders chosen by people through a global poll. It was led by Swiss-born Canadian Bernard Weber and organized by the New 7 Wonders Foundation, a Swiss-based foundation which Weber founded. The initiative followed an earlier New7Wonders of the World campaign, and attracted 100 million votes from around the world before voting finished on November 11, 2011.\n\nThe New7Wonders of Nature campaign started in 2007, immediately after the campaign to elect the man-made New7Wonders of the World, in which more than 100 million votes were cast. From over 440 participants representing over 220 countries and through a national qualification and race to become one of the Top 77, as well as the recommendations of the Panel of Experts led by Prof. Federico Mayor, the list of 28 \"Official Finalist Candidates\" was determined. Voting until November 2011, during which time the New7Wonders World Tour planned to visit each of the finalists to allow them to present themselves to the voters across the globe.\n\nIndonesia's Vice-Minister for Tourism said the company running the New7Wonders campaign used underhanded tactics, threatening to remove Indonesia's Komodo National Park from the list if Indonesia refused to host a declaration ceremony for $35m. Nothing in the New7Wonders voting procedure prohibited repetitive voting, making the results subject to government and tourism industry campaigns to vote often for local sites with the financial incentive of increased tourism.\nAlthough New7Wonders is a non-profit organization that under US law has absolutely no disclosure of accounts, many activities related to administering voting and other logistical duties are run by the for-profit organization New Open World Corporation.\n\nIn South Korea, over the past four years, millions of Koreans and non-Koreans at home and abroad were encouraged by the central and Jeju provincial governments to make phone calls to vote and help the island win the designation. Employees of Jeju Special Self-Governing Province of South Korea made hundreds of millions of international phone calls to ensure Jeju island was selected as one of the New7Wonders of Nature in a worldwide poll. The payments estimated to be about 20 billion won ($17 million).\n\n\n"}
{"id": "314610", "url": "https://en.wikipedia.org/wiki?curid=314610", "title": "Pebble", "text": "Pebble\n\nA pebble is a clast of rock with a particle size of 2 to 64 millimetres based on the Krumbein phi scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter). A rock made predominantly of pebbles is termed a conglomerate. Pebble tools are among the earliest known man-made artifacts, dating from the Palaeolithic period of human history.\n\nA beach composed chiefly of surface pebbles is commonly termed a shingle beach. This type of beach has armoring characteristics with respect to wave erosion, as well as ecological niches that provide habitat for animals and plants.\n\nInshore banks of shingle (large quantities of pebbles) exist in some locations, such as the entrance to the River Ore, where the moving banks of shingle give notable navigational challenges.\n\nPebbles come in various colors and textures and can have streaks, known as veins, of quartz or other minerals. Pebbles are mostly smooth but, dependent on how frequently they come in contact with the sea, they can have marks of contact with other rocks or other pebbles. Pebbles left above the high water mark may have growths of organisms such as lichen on them, signifying the lack of contact with seawater.\n\nPebbles are found in two locations – on the beaches of various oceans and seas, and inland where ancient seas used to cover the land. When then the seas retreated, the rocks became landlocked. They can also be found in lakes and ponds. Pebbles can also form in rivers, and travel into estuaries where the smoothing continues in the sea.\n\nBeach pebbles and river pebbles (also known as river rock) are distinct in their geological formation and appearance.\n\nBeach pebbles form gradually over time as the ocean water washes over loose rock particles. The result is a smooth, rounded appearance. The typical size range is from 2 mm to 50 mm. The colors range from translucent white to black, and include shades of yellow, brown, red and green. Some of the more plentiful pebble beaches are found along the coast of the Pacific Ocean, beginning in the United States and extending down to the tip of South America in Argentina. Other pebble beaches are found in northern Europe (particularly on the beaches of the Norwegian Sea), along the coast of the U.K. and Ireland, on the shores of Australia, and around the islands of Indonesia and Japan.\n\nInland pebbles (river pebbles of river rock) are usually found along the shores of large rivers and lakes. These pebbles form as the flowing water washes over rock particles on the bottom and along the shores of the river. The smoothness and color of river pebbles depends on several factors, such as the composition of the soil of the river banks, the chemical characteristics of the water, and the speed of the current. Because river current is gentler than the ocean waves, river pebbles are usually not as smooth as beach pebbles. The most common colors of river rock are black, grey, green, brown and white.\n\nBeach pebbles and river pebbles are used for a variety of purposes, both outdoors and indoors. They can be sorted by colour and size, and they can also be polished to improve the texture and colour. Outdoors, beach pebbles are often used for landscaping, construction and as decorative elements. Beach pebbles are often used to cover walkways and driveways, around pools, in and around plant containers, on patios and decks. Beach and river pebbles are also used to create water-smart gardens in areas where water is scarce. Small pebbles are also used to create living spaces and gardens on the rooftops of buildings. Indoors, pebbles can be used as bookends and paperweights. Large pebbles are also used to create \"pet rocks\" for children.\n\nOn Mars, slabs of pebbly conglomerate rock have been found and have been interpreted by scientists as having formed in an ancient streambed. The gravels, which were discovered by NASA's Mars rover Curiosity, range from the size of sand particles to the size of golf balls. Analysis has shown that the pebbles were deposited by a stream that flowed at walking pace and was ankle- to hip-deep.\n\n"}
{"id": "18106574", "url": "https://en.wikipedia.org/wiki?curid=18106574", "title": "Political naturalism", "text": "Political naturalism\n\nPolitical naturalism is a minor political ideology and legal system which believes that there is a natural law, just and obvious to all, that crosses ideologies, faiths and personal thinking, that naturally guaranties justice. It is inspired by sociological naturalism, and scientific naturalism's belief that the precision of natural sciences can be applied to social sciences, and hence to practical social activities like politics and law.\n\nIt may be seen as a natural law-based version of legalism/constitutionalism (especially of prescriptive constitutionalism, in the way it tries, idealistically, to make a constitution how it should justly be), and it bears relation with many constitutional monarchies (as in that system they too believe in rule of the law and in certain things who are naturally correct (like monarchy, monarchic institutions and traditions.\n\nThe roots of this legal political ideology may be found in positive visions of natural law (like John Locke's and Rousseau's, and even in the Founding Fathers of the United States. The Catholic German Centre Party politician and diplomat Karl Friedrich von Savigny also thought so.\n\nIts main modern thinker is Egyptian legal scholar and creator of the Egyptian Civil Code Al-Razzak Al-Sanhuri. Through the Egyptian Code, many other Arab constitutions (in monarchist and pre-dictatorships Iraq and Libya and modern Qatar) ended up including political naturalist laws, and Al-Sanhuri himself wrote the Syrian and Jordanian civil codes and the Kuwaiti commercial code.\n"}
{"id": "3090379", "url": "https://en.wikipedia.org/wiki?curid=3090379", "title": "Rubble", "text": "Rubble\n\nRubble is broken stone, of irregular size, shape and texture; undressed especially as a filling-in. Rubble naturally found in the soil is known also as 'brash' (compare cornbrash). Where present, it becomes more noticeable when the land is ploughed or worked.\n\n\"Rubble-work\" is a name applied to several types of masonry. One kind, where the stones are loosely thrown together in a wall between boards and grouted with mortar almost like concrete, is called in Italian \"muraglia di getto\" and in French \"bocage\". In Pakistan, walls made of rubble and concrete, cast in a formwork, are called 'situ', which probably derives from Sanskrit (similar to the Latin 'in situ' meaning 'made on the spot').\n\nWork executed with more or less large stones put together without any attempt at courses is called rubble walling. Where similar work is laid in courses, it is known as coursed rubble. Dry-stone walling is somewhat similar work done without the use of mortar. It is bound together by the fit of the stones and the regular placement of stones which extend through the thickness of the wall. A rubble wall built with mortar will be stronger if assembled in this way.\n\nRubble walls () are found all over the island of Malta. Similar walls are also frequently found in Sicily and the Arab countries. The various shapes and sizes of the stones used to build these walls look like stones that were found in the area lying on the ground or in the soil. It is most probable that the practice of building these walls around the field was inspired by the Arabs during their rule in Malta, as in Sicily who were also ruled by the Arabs around the same period. The Maltese farmer found that the technique of these walls was very useful especially during an era where resources were limited. Rubble walls are used to serve as borders between the property of one farm from the other. A great advantage that rubble walls offered is that when heavy rain falls, their structure would allow excessive water to pass through and therefore, excess water will not ruin the products. Soil erosion is minimised as the wall structure allows the water to pass through but it traps the soil and prevents it from being carried away from the field. One can see many rubble walls on the side of the hills and in valleys where the land slopes down and consequently the soil is in greater danger of being carried away.\n\n\n"}
{"id": "2071938", "url": "https://en.wikipedia.org/wiki?curid=2071938", "title": "Seven Natural Wonders", "text": "Seven Natural Wonders\n\nSeven Natural Wonders was a television series that was broadcast on BBC Two from 3 May to 20 June 2005. The programme took an area of England each week and, from votes by the people living in that area, showed the 'seven natural wonders' of that area in a programme.\n\nThe programmes were:\n\nThe series covered eight regions of England, having originated as a 'local' television project.\n\nThere was also a series, looking at a similar selection of 'man-made' wonders for each of eleven regions of England.\n\n"}
{"id": "4968799", "url": "https://en.wikipedia.org/wiki?curid=4968799", "title": "Sky brightness", "text": "Sky brightness\n\nSky brightness refers to the visual perception of the sky and how it scatters and diffuses light. The fact that the sky is not completely dark at night is easily visible. If light sources (e.g. the Moon and light pollution) were removed from the night sky, it would appear absolutely dark. Silhouettes of objects against the sky itself would not be visible.\n\nThe sky's brightness varies greatly over the day, and the primary cause differs as well. During daytime, when the Sun is above the horizon, the direct scattering of sunlight is the overwhelmingly dominant source of light. During twilight (the duration after sunset or before sunrise until or since, respectively, the full darkness of night), the situation is more complicated, and a further differentiation is required.\n\nTwilight (both dusk and dawn) is divided into three 6° segments that mark the Sun's position below the horizon. At civil twilight, the center of the Sun's disk appears to be between 1/4° and 6° below the horizon. At nautical twilight, the Sun's altitude is between –6° and –12°. At astronomical twilight, the Sun is between –12° and –18°. When the Sun's depth is more than 18°, the sky generally attains its maximum darkness.\n\nSources of the night sky's intrinsic brightness include airglow, indirect scattering of sunlight, scattering of starlight, and light pollution.\n\nWhen physicist Anders Ångström examined the spectrum of the aurora borealis, he discovered that even on nights when the aurora was absent, its characteristic green line was still present. It was not until the 1920s that scientists were beginning to identify and understand the emission lines in aurorae and of the sky itself, and what was causing them. The green line Angstrom observed is in fact an emission line with a wavelength of 557.7 nm, caused by the recombination of oxygen in the upper atmosphere.\n\nAirglow is the collective name of the various processes in the upper atmosphere that result in the emission of photons, with the driving force being primarily UV radiation from the Sun. Several emission lines are dominant: a green line from oxygen at 557.7 nm, a yellow doublet from sodium at 589.0 and 589.6 nm, and red lines from oxygen at 630.0 and 636.4 nm.\n\nThe sodium emissions come from a thin sodium layer approximately 10 km thick at an altitude of 90–100 km, above the mesopause and in the D-layer of the ionosphere. The red oxygen lines originate at altitudes of about 300 km, in the F-layer. The green oxygen emissions are more spatially distributed. How sodium gets to mesospheric heights is not yet well understood, but it is believed to be a combination of upward transport of sea salt and meteoritic dust.\n\nIn daytime, sodium and red oxygen emissions are dominant and roughly 1,000 times as bright as nighttime emissions because in daytime, the upper atmosphere is fully exposed to solar UV radiation. The effect is however not noticeable to the human eye, since the glare of directly scattered sunlight outshines and obscures it.\n\nIndirectly scattered sunlight comes from two directions. From the atmosphere itself, and from outer space. In the first case, the sun has just set but still illuminates the upper atmosphere directly. Because the amount of scattered sunlight is proportional to the number of scatterers (i.e. air molecules) in the line of sight, the intensity of this light decreases rapidly as the sun drops further below the horizon and illuminates less of the atmosphere.\n\nWhen the sun's altitude is < -6° 99% of the atmosphere in zenith is in the Earth's shadow and second order scattering takes over. At the horizon, however, 35% of the atmosphere along the line of sight is still directly illuminated, and continues to be until the sun reaches -12°. From -12° to -18° only the uppermost parts of the atmosphere along the horizon, directly above the spot where the sun is, is still illuminated. After that, all direct illumination ceases and astronomical darkness sets in.\n\nA second source sunlight is the zodiacal light, which is caused by reflection and scattering of sunlight on interplanetary dust. Zodiacal light varies quite a lot in intensity depending on the position of the earth, location of the observer, time of year, and composition and distribution of the reflecting dust.\n\nNot only sunlight is scattered by the molecules in the air. Starlight and the diffuse light of the milky way are also scattered by the air, and it is found that stars up to V magnitude 16 contribute to the diffuse scattered starlight.\n\nOther sources such as galaxies and nebulae don't contribute significantly.\n\nThe total brightness of all the stars was first measured by Burns in 1899, with a calculated result that the total brightness reaching earth was equivalent to that of 2,000 first-magnitude stars with subsequent measurements by others.\n\nLight pollution is an ever-increasing source of sky brightness in urbanized areas. In densely populated areas that do not have stringent light pollution control, the entire night sky is regularly 5 to 50 times brighter than it would be if all lights were switched off, and very often the influence of light pollution is far greater than natural sources (including moonlight). With urbanization and light pollution, one third of humanity, and the majority of those in developed countries, cannot see the Milky Way.\n\nWhen the sun has just set, the brightness of the sky decreases rapidly, thereby enabling us to see the airglow that is caused from such high altitudes that they are still fully sunlit until the sun drops more than about 12° below the horizon. During this time, yellow emissions from the sodium layer and red emissions from the 630 nm oxygen lines are dominant, and contribute to the purplish color sometimes seen during civil and nautical twilight.\n\nAfter the sun has also set for these altitudes at the end of nautical twilight, the intensity of light emanating from earlier mentioned lines decreases, until the oxygen-green remains as the dominant source.\n\nWhen astronomical darkness has set in, the green 557.7 nm oxygen line is dominant, and atmospheric scattering of starlight occurs.\n\nDifferential refraction causes different parts of the spectrum to dominate, producing a golden hour and a blue hour.\n\nThe following table gives the relative and absolute contributions to night sky brightness at zenith on a perfectly dark night at middle latitudes without moonlight and in the absence of any light pollution.\n\nThe total sky brightness in zenith is therefore ~220 S or 21.9 mag/arcsec² in the V-band. Note that the contributions from Airglow and Zodiacal light vary with the time of year, the solar cycle, and the observer's latitude roughly as follows:\n\nwhere \"S\" is the solar 10.7 cm flux in MJy, and various sinusoidally between 0.8 and 2.0 with the 11-year solar cycle, yielding an upper contribution of ~270 S at solar maximum.\n\nThe intensity of zodiacal light depends on the ecliptic latitude and longitude of the point in the sky being observed relative to that of the sun. At ecliptic longitudes differing from the sun's by > 90 degrees, the relation is \nwhere \"β\" is the ecliptic latitude and is smaller than 60°, when larger than 60 degrees the contribution is that given in the table. Along the ecliptic plane there are enhancements in the zodiacal light where it is much brighter near the sun and with a secondary maximum opposite the sun at 180 degrees longitude (the gegenschein).\n\nIn extreme cases natural zenith sky brightness can be as high as ~21.0 mag/arcsec², roughly twice as bright as nominal conditions.\n\n"}
{"id": "3469441", "url": "https://en.wikipedia.org/wiki?curid=3469441", "title": "Sorbent", "text": "Sorbent\n\nA sorbent is a material used to absorb or adsorb liquids or gases. Examples include:\n\n"}
{"id": "13581828", "url": "https://en.wikipedia.org/wiki?curid=13581828", "title": "Surface conductivity", "text": "Surface conductivity\n\nSurface conductivity is an additional conductivity of an electrolyte in the vicinity of charged surfaces. Close to charged surfaces a layer of counter ions of opposite polarity exists which is attracted by the surface charges. This layer of higher ionic concentration is a part of the interfacial double layer. The concentration of the ions in this layer is higher as compared to the volume conductivity far from the charged surface and leads to a higher conductivity of this layer.\n\nSmoluchowski was the first to recognize the importance of surface conductivity at the beginning of the 20th century.\n\nThere is a detailed description of surface conductivity by Lyklema in \"Fundamentals of Interface and Colloid Science\" \n\nThe Double Layer (DL) has two regions, according to the well established Gouy-Chapman-Stern model, Ref.2. The upper level, which is in contact with the bulk fluid is the diffuse layer. The inner layer that is in contact with interface is the Stern layer.\n\nIt is possible that the lateral motion of ions in both parts of the DL contributes to the surface conductivity.\n\nThe contribution of the Stern layer is less well described. It is often called \"additional surface conductivity\".\n\nThe theory of the surface conductivity of the diffuse part of the DL was developed by Bikerman. He derived a simple equation that links surface conductivity κ with the behaviour of ions at the interface. For symmetrical electrolyte and assuming identical ions diffusion coefficients D=D=D it is given in Ref.2:\n\nwhere\n\nThe parameter \"m\" characterizes the contribution of electro-osmosis to the motion of ions within the DL:\n\nThe Dukhin number is a dimensionless parameter that characterizes the contribution of the surface conductivity to a variety of electrokinetic phenomena, such as, electrophoresis and electroacoustic phenomena.\n\n\nSurface conductivity may refer to the electrical conduction across a solid surface measured by surface probes. Experiments may be done to test this material property as in the n-type surface conductivity of p-type . Additionally, surface conductivity is measured in coupled phenomena such as photoconductivity, for example, for the metal oxide semiconductor ZnO. Surface conductivity differs from bulk conductivity for analogous reasons to the electrolyte solution case, where the charge carriers of holes (+1) and electrons (-1) play the role of ions in solution.\n"}
{"id": "340304", "url": "https://en.wikipedia.org/wiki?curid=340304", "title": "Totem", "text": "Totem\n\nA totem (Ojibwe \"doodem\") is a spirit being, sacred object, or symbol that serves as an emblem of a group of people, such as a family, clan, lineage, or tribe.\n\nWhile the term \"totem\" is derived from the North American Ojibwe language, belief in tutelary spirits and deities is not limited to indigenous peoples of the Americas but common to a number of cultures worldwide. However, the traditional people of those cultures have words for their guardian spirits in their own languages, and do not call these spirits or symbols \"totems\".\n\nContemporary neoshamanic, New Age, and mythopoetic men's movements not otherwise involved in the practice of a tribal religion have been known to use \"totem\" terminology for the personal identification with a tutelary spirit or guide.\n\nTotem poles of the Pacific Northwest of North America are monumental poles of heraldry. They feature many different designs (bears, birds, frogs, people, and various supernatural beings and aquatic creatures) that function as crests of families or chiefs. They recount stories owned by those families or chiefs, or commemorate special occasions. These stories are known to be read from the bottom of the pole to the top.\n\nTotemism is a belief associated with animistic religions. The totem is usually an animal or other natural figure that spiritually represents a group of related people such as a clan.\n\nScottish ethnologist John Ferguson McLennan, following the vogue of 19th-century research, addressed totemism in a broad perspective in his study \"The Worship of Animals and Plants\" (1869, 1870). McLennan did not seek to explain the specific origin of the totemistic phenomenon but sought to indicate that all of the human race had, in ancient times, gone through a totemistic stage.\n\nAnother Scottish scholar, Andrew Lang, early in the 20th century, advocated a nominalistic explanation of totemism, namely, that local groups or clans, in selecting a totemistic name from the realm of nature, were reacting to a need to be differentiated. If the origin of the name was forgotten, Lang argued, there followed a mystical relationship between the object — from which the name was once derived — and the groups that bore these names. Through nature myths, animals and natural objects were considered as the relatives, patrons, or ancestors of the respective social units.\n\nBritish anthropologist Sir James George Frazer published \"Totemism and Exogamy\" in 1910, a four-volume work based largely on his research among Indigenous peoples of Australia and Melanesia, along with a compilation of the work of other writers in the field.\n\nIn 1910, Russian American ethnologist Alexander Goldenweiser, subjected totemistic phenomena to sharp criticism.\n\nThe founder of a French school of sociology, Émile Durkheim, examined totemism from a sociological and theological point of view, attempting to discover a pure religion in very ancient forms and claimed to see the origin of religion in totemism.\n\nThe leading representative of British social anthropology, A. R. Radcliffe-Brown, took a totally different view of totemism. Like Franz Boas, he was skeptical that totemism could be described in any unified way. In this he opposed the other pioneer of social anthropology in England, Bronisław Malinowski, who wanted to confirm the unity of totemism in some way and approached the matter more from a biological and psychological point of view than from an ethnological one. According to Malinowski, totemism was not a cultural phenomenon, but rather the result of trying to satisfy basic human needs within the natural world. As far as Radcliffe-Brown was concerned, totemism was composed of elements that were taken from different areas and institutions, and what they have in common is a general tendency to characterize segments of the community through a connection with a portion of nature. In opposition to Durkheim's theory of sacralization, Radcliffe-Brown took the point of view that nature is introduced into the social order rather than secondary to it. At first, he shared with Malinowski the opinion that an animal becomes totemistic when it is “good to eat.” He later came to oppose the usefulness of this viewpoint, since many totems—such as crocodiles and flies—are dangerous and unpleasant.\n\nAs a chief representative of modern structuralism, French ethnologist Claude Lévi-Strauss, and his, \"Le Totémisme aujourd'hui\" (\"Totemism Today\" [1958]) are often cited in the field.\n\nPoets, and to a lesser extent fiction writers, often use anthropological concepts, including the anthropological understanding of totemism. For this reason literary criticism often resorts to psychoanalytic, anthropological analyses.\n"}
{"id": "712222", "url": "https://en.wikipedia.org/wiki?curid=712222", "title": "Transit of Earth from Mars", "text": "Transit of Earth from Mars\n\nA transit of Earth across the Sun as seen from Mars takes place when the planet Earth passes directly between the Sun and Mars, obscuring a small part of the Sun's disc for an observer on Mars. During a transit, Earth would be visible from Mars as a small black disc moving across the face of the Sun. They occur every 26, 79 and 100 years, and every 1,000 years or so there is an extra 53rd-year transit.\n\nTransits of Earth from Mars usually occur in pairs, with one following the other after 79 years; rarely, there are three in the series. The transits also follow a 284-year cycle, occurring at intervals of 100.5, 79, 25.5, and 79 years; a transit falling on a particular date is usually followed by another transit 284 years later. Transits occurring when Mars is at its ascending node are in May, those at descending node happen in November. This cycle corresponds fairly closely to 151 Mars orbits, 284 Earth orbits, and 133 synodic periods, and is analogous to the cycle of transits of Venus from Earth, which follow a cycle of 243 years (121.5, 8, 105.5, 8). There are currently four such active series, containing from 8 to 25 transits. A new one is set to begin in 2394. The last series ending was in 1211.\n\nNo one has ever seen a transit of Earth from Mars, but the next transit will take place on November 10, 2084. The last such transit took place on May 11, 1984.\n\nDuring the event, the Moon could almost always also be seen in transit, although due to the distance between Earth and Moon, sometimes one body completes the transit before the other begins (this last occurred in the 1800 transit, and will happen again in 2394).\n\nA transit of Earth from Mars corresponds to Mars being perfectly uniformly illuminated at opposition from Earth, its phase being 180.0° without any defect of illumination. During the 1879 event, this permitted Charles Augustus Young to attempt a careful measurement of the oblateness (polar compression) of Mars. He obtained the value 1/219, or 0.0046. This is close to the modern value of 1/154 (many sources will cite somewhat different values, such as 1/193, because even a difference of only a couple of kilometers in the values of Mars' polar and equatorial radii gives a considerably different result).\n\nMuch more recently, better measurements of the oblateness of Mars have been made by using radar from the Earth. Also, better measurements have been made by using artificial satellites that have been put into orbit around Mars, including \"Mariner 9\", \"Viking 1\", \"Viking 2\", and Soviet orbiters, and the more recent orbiters that have been sent from the Earth to Mars.\n\nA science fiction short story published in 1971 by Arthur C. Clarke, called \"Transit of Earth\", depicts a doomed astronaut on Mars observing the transit in 1984. This short story was first published in the January 1971 issue of \"Playboy\" magazine.\n\nSometimes Earth only grazes the Sun during a transit. In this case it is possible that in some areas of Mars a full transit can be seen while in other regions there is only a partial transit (no second or third contact). The last transit of this type was on 30 April 1211, and the next such transit will occur on 27 November 4356. It is also possible that a transit of Earth can be seen in some parts of Mars as a partial transit, while in others Earth misses the Sun. Such a transit last occurred on 26 October 664, and the next transit of this type will occur on 14 December 5934.\n\nThe simultaneous occurrence of a transit of Venus and a transit of Earth is extremely rare, and will next occur in the year 571,471.\n\n\n\n"}
{"id": "32128", "url": "https://en.wikipedia.org/wiki?curid=32128", "title": "Uniformitarianism", "text": "Uniformitarianism\n\nUniformitarianism, also known as the Doctrine of Uniformity, refers to the invariance in the principles underpinning science, such as the constancy of causality, or causation, throughout time, but it has also been used to describe invariance of physical laws through time and space. Though an unprovable postulate that cannot be verified using the scientific method, uniformitarianism has been a key first principle of virtually all fields of science.\n\nIn geology, uniformitarianism has included the gradualistic concept that \"the present is the key to the past\" (that events occur at the same rate now as they have always done); many geologists now, however, no longer hold to a strict theory of gradualism. Coined by William Whewell, the word was proposed in contrast to catastrophism by British naturalists in the late 18th century, starting with the work of the geologist James Hutton in his many books including \"Theory of the Earth\". Hutton's work was later refined by scientist John Playfair and popularised by geologist Charles Lyell's \"Principles of Geology\" in 1830. Today, Earth's history is considered to have been a slow, gradual process, punctuated by occasional natural catastrophic events.\n\nThe earlier conceptions likely had little influence on 18th-century European geological explanations for the formation of Earth. Abraham Gottlob Werner (1749–1817) proposed Neptunism, where strata represented deposits from shrinking seas precipitated onto primordial rocks such as granite. In 1785 James Hutton proposed an opposing, self-maintaining infinite cycle based on natural history and not on the Biblical account.\nHutton then sought evidence to support his idea that there must have been repeated cycles, each involving deposition on the seabed, uplift with tilting and erosion, and then moving undersea again for further layers to be deposited. At Glen Tilt in the Cairngorm mountains he found granite penetrating metamorphic schists, in a way which indicated to him that the presumed primordial rock had been molten after the strata had formed. He had read about angular unconformities as interpreted by Neptunists, and found an unconformity at Jedburgh where layers of greywacke in the lower layers of the cliff face have been tilted almost vertically before being eroded to form a level plane, under horizontal layers of Old Red Sandstone. In the spring of 1788 he took a boat trip along the Berwickshire coast with John Playfair and the geologist Sir James Hall, and found a dramatic unconformity showing the same sequence at Siccar Point. Playfair later recalled that \"the mind seemed to grow giddy by looking so far into the abyss of time\", and Hutton concluded a 1788 paper he presented at the Royal Society of Edinburgh, later rewritten as a book, with the phrase \"we find no vestige of a beginning, no prospect of an end\".\n\nBoth Playfair and Hall wrote their own books on the theory, and for decades robust debate continued between Hutton's supporters and the Neptunists. Georges Cuvier's paleontological work in the 1790s, which established the reality of extinction, explained this by local catastrophes, after which other fixed species repopulated the affected areas. In Britain, geologists adapted this idea into \"diluvial theory\" which proposed repeated worldwide annihilation and creation of new fixed species adapted to a changed environment, initially identifying the most recent catastrophe as the biblical flood.\n\nFrom 1830 to 1833 Charles Lyell's multi-volume \"Principles of Geology\" was published. The work's subtitle was \"An attempt to explain the former changes of the Earth's surface by reference to causes now in operation\". He drew his explanations from field studies conducted directly before he went to work on the founding geology text, and developed Hutton's idea that the earth was shaped entirely by slow-moving forces still in operation today, acting over a very long period of time. The terms \"uniformitarianism\" for this idea, and \"catastrophism\" for the opposing viewpoint, were coined by William Whewell in a review of Lyell's book. \"Principles of Geology\" was the most influential geological work in the middle of the 19th century.\n\nGeoscientists support diverse systems of Earth history, the nature of which rest on a certain mixture of views about process, control, rate, and state which are preferred. Because geologists and geomorphologists tend to adopt opposite views over process, rate and state in the inorganic world, there are eight different systems of beliefs in the development of the terrestrial sphere. All geoscientists stand by the principle of uniformity of law. Most, but not all, are directed by the principle of simplicity. All make definite assertions about the quality of rate and state in the inorganic realm.\n\nAccording to Reijer Hooykaas (1963), Lyell's uniformitarianism is a family of four related propositions, not a single idea:\n\nNone of these connotations requires another, and they are not all equally inferred by uniformitarians.\n\nGould explained Lyell's propositions in \"Time's Arrow, Time's Cycle\" (1987), stating that Lyell conflated two different types of propositions: a pair of methodological assumptions with a pair of substantive hypotheses. The four together make up Lyell's uniformitarianism.\n\nThe two methodological assumptions below are accepted to be true by the majority of scientists and geologists. Gould claims that these philosophical propositions must be assumed before you can proceed as a scientist doing science. \"You cannot go to a rocky outcrop and observe either the constancy of nature's laws or the working of unknown processes. It works the other way around.\" You first assume these propositions and \"then you go to the outcrop.\"\n\nThe substantive hypotheses were controversial and, in some cases, accepted by few. These hypotheses are judged true or false on empirical grounds through scientific observation and repeated experimental data. This is in contrast with the previous two philosophical assumptions that come before one can do science and so cannot be tested or falsified by science.\n\nStephen Jay Gould's first scientific paper, \"Is uniformitarianism necessary?\" (1965), reduced these four assumptions to two. He dismissed the first principle, which asserted spatial and temporal invariance of natural laws, as no longer an issue of debate. He rejected the third (uniformity of rate) as an unjustified limitation on scientific inquiry, as it constrains past geologic rates and conditions to those of the present. So, Lyellian uniformitarianism was unnecessary.\n\nUniformitarianism was proposed in contrast to catastrophism, which states that the distant past \"consisted of epochs of paroxysmal and catastrophic action interposed between periods of comparative tranquility\" Especially in the late 19th and early 20th centuries, most geologists took this interpretation to mean that catastrophic events are not important in geologic time; one example of this is the debate of the formation of the Channeled Scablands due to the catastrophic Missoula glacial outburst floods. An important result of this debate and others was the re-clarification that, while the same principles operate in geologic time, catastrophic events that are infrequent on human time-scales can have important consequences in geologic history.\nDerek Ager has noted that \"geologists do not deny uniformitarianism in its true sense, that is to say, of interpreting the past by means of the processes that are seen going on at the present day, so long as we remember that the periodic catastrophe is one of those processes. Those periodic catastrophes make more showing in the stratigraphical record than we have hitherto assumed.\"\n\nEven Charles Lyell thought that ordinary geological processes would cause Niagara Falls to move upstream to Lake Erie within 10,000 years, leading to catastrophic flooding of a large part of North America.\n\nModern geologists do not apply uniformitarianism in the same way as Lyell. They question if rates of processes were uniform through time and only those values measured during the history of geology are to be accepted. The present may not be a long enough key to penetrate the deep lock of the past. Geologic processes may have been active at different rates in the past that humans have not observed. \"By force of popularity, uniformity of rate has persisted to our present day. For more than a century, Lyell's rhetoric conflating axiom with hypotheses has descended in unmodified form. Many geologists have been stifled by the belief that proper methodology includes an a priori commitment to gradual change, and by a preference for explaining large-scale phenomena as the concatenation of innumerable tiny changes.\"\n\nThe current consensus is that Earth's history is a slow, gradual process punctuated by occasional natural catastrophic events that have affected Earth and its inhabitants. In practice it is reduced from Lyell's conflation, or blending, to simply the two philosophical assumptions. This is also known as the principle of geological actualism, which states that all past geological action was like all present geological action. The principle of actualism is the cornerstone of paleoecology.\n\n\n\n"}
{"id": "926840", "url": "https://en.wikipedia.org/wiki?curid=926840", "title": "Uniformity of motive", "text": "Uniformity of motive\n\nIn astrobiology, the Uniformity of Motive theory suggests that any civilization in the universe would go through similar technological steps in their development. This theory supports the idea that at some point in their history, advanced alien civilizations would use the electromagnetic medium for communications, and thus would emit radio waves that could be detected by projects such as SETI.\n\nThe fact that no artificial EM band communications have ever been detected supports the Fermi Principle, which in conjunction with the Uniformity of Motive theory, and Occam's razor suggests that a civilization that uses this medium is a unique occurrence in Earth's region of the Milky Way Galaxy and perhaps the universe.\n"}
{"id": "17038488", "url": "https://en.wikipedia.org/wiki?curid=17038488", "title": "Weather risk management", "text": "Weather risk management\n\nWeather risk management is a type of risk management done by organizations to address potential financial losses caused by unusual weather.\n\nEnergy, agriculture, transportation, construction, municipalities, school districts, travel, food processors, retail sales and real estate are all examples of industries whose operations and profits can be significantly affected by the weather. For example, unusually mild winters diminish consumer demand for heating and erode the profit margins for utility companies. Unexpected weather events can cause significant financial losses. Weather information and forecasts utilized in risk management decision making is often referred to as meteorological intelligence and offered by companies such as Delphi Weather Analytics.\n\nThe weather risk market makes it possible to manage the financial impact of weather through risk transfer instruments based on a defined weather element, such as temperature, rain, snow, wind, etc. Weather risk management is a way for organizations to limit their financial exposure to disruptive weather events. By making a payment (a \"premium\") to a separate company that will assume the financial weather risk for them, an organization essentially is buying a type of insurance - the company assuming the risk will pay the buyer a pre-set amount of money which will correspond to the loss or cost increase caused by the disruptive weather.\n\nCatastrophic weather events such as hurricanes are typically managed through traditional insurance contracts that pay based on indemnity loss. Insurance is a heavily regulated industry with specific requirements and qualification criteria. Due to the indemnity nature of insurance, actual loss must be proven to an insurance carrier before the payment can be processed. In contrast, financial loss such as erosion of margin, portfolio loss or increased expenses usually do not qualify for insurance payouts. Financial instruments such as derivative transactions can provide more flexible and customized risk management opportunities than the typical insurance contracts as they are priced and settled on the parameters of measured weather rather than the associated financial loss.\n\nA wide range of capital providers make markets in weather risk. To date the weather risk management trading market is primarily made up of dedicated weather trading operations, such as Nephila Capital Ltd, Galileo Weather Risk Management Advisors LCC, Swiss Re, RenRe, and Coriolis Capital, who execute trade orders in weather or weather-contingent commodity trades, the trading desks of financial institutions and utilities, such as Susquahanna Energy and Aquila who hedge their own risk as well as speculative trades for a merchant portfolio, professional commodity traders, such as RJO and hedge and private equity funds such as Tudor Capital. Transactions can be effected over-the-counter (OTC) or on commodity exchanges such as The Chicago Mercantile Exchange (CME). Still other operations, such as Storm Exchange, Inc(Note: Storm Exchange is now defunct. and WeatherBill (WeatherBill is no longer serving markets outside of Agriculture), privately held eWeatherRisk now provide corporate and municipal clients with the necessary financial context to gauge the impact of the weather on profit and loss before executing trades either OTC or through the CME.\n\nIn the US, the Commodity Exchange Act, Section 5d establishes weather in a category of market exempt from Commission oversight.\n\nRule 36.2 defines those commodities that are eligible to trade on an exempt board of trade as commodities having:\nThe Commodity Futures Trading Commission determined that weather indices are eligible to be traded on EBOTs by order dated May 30, 2002.\n\nCompanies that are subject to public disclosure to regulators or their shareholders must demonstrate that the purchase or a sale of a derivatives is true and fair hedge, not speculation. SFAS 133 and IAS provide guidelines on the steps that are required. FAS 133 Accounting for Weather Derivatives: For U.S. accounting standards, Over-the-Counter (OTC) weather derivative transactions can generally get an exemption under derivatives & hedging disclosure rules of Financial Accounting Standard No. 133 section 10 for non-exchange contracts settled on climatic variables, although specific structures and applications have to be assessed for each company environment. All written non–exchanged–traded option–based weather derivatives contracts should be carried at fair value with subsequent changes in fair value reported in current earnings.\n\nWhen they are standardized and traded on exchanges, weather derivatives will fall within the scope of SFAS 133. EITF Issue No 99–2 \"Accounting for weather derivatives\" provides guidance on accounting for weather derivatives that are not exchange–traded. Entities that enter into speculatives or trading non–exchange derivatives contracts should apply the intrinsic method.\n\n\n\n"}
{"id": "56106", "url": "https://en.wikipedia.org/wiki?curid=56106", "title": "Wildfire", "text": "Wildfire\n\nA wildfire or wildland fire is a fire in an area of combustible vegetation occurring in rural areas. Depending on the type of vegetation present, a wildfire can also be classified more specifically as a brush fire, bushfire, desert fire, forest fire, grass fire, hill fire, peat fire, vegetation fire, and veld fire.\n\nFossil charcoal indicates that wildfires began soon after the appearance of terrestrial plants 420 million years ago. Wildfire's occurrence throughout the history of terrestrial life invites conjecture that fire must have had pronounced evolutionary effects on most ecosystems' flora and fauna. Earth is an intrinsically flammable planet owing to its cover of carbon-rich vegetation, seasonally dry climates, atmospheric oxygen, and widespread lightning and volcanic ignitions.\n\nWildfires can be characterized in terms of the cause of ignition, their physical properties, the combustible material present, and the effect of weather on the fire. Wildfires can cause damage to property and human life, though naturally occurring wildfires may have beneficial effects on native vegetation, animals, and ecosystems that have evolved with fire. High-severity wildfire creates complex early seral forest habitat (also called \"snag forest habitat\"), which often has higher species richness and diversity than unburned old forest. Many plant species depend on the effects of fire for growth and reproduction. Wildfires in ecosystems where wildfire is uncommon or where non-native vegetation has encroached may have strongly negative ecological effects. Wildfire behavior and severity result from the combination of factors such as available fuels, physical setting, and weather. Analyses of historical meteorological data and national fire records in western North America show the primacy of climate in driving large regional fires via wet periods that create substantial fuels or drought and warming that extend conducive fire weather.\n\nStrategies for wildfire prevention, detection, and suppression have varied over the years. One common and inexpensive technique is controlled burning, intentionally igniting smaller fires to minimize the amount of flammable material available for a potential wildfire. Vegetation may be burned periodically to maintain high species diversity and limit the accumulation of plants and other debris that may serve as fuel. Wildland fire use is the cheapest and most ecologically appropriate policy for many forests. Fuels may also be removed by logging, but fuels treatments and thinning have no effect on severe fire behavior when under extreme weather conditions. Wildfire itself is reportedly \"the most effective treatment for reducing a fire's rate of spread, fireline intensity, flame length, and heat per unit of area\" according to Jan Van Wagtendonk, a biologist at the Yellowstone Field Station. Building codes in fire-prone areas typically require that structures be built of flame-resistant materials and a defensible space be maintained by clearing flammable materials within a prescribed distance from the structure.\n\nThree major natural causes of wildfire ignitions exist: \n\nThe most common direct human causes of wildfire ignition include arson, discarded cigarettes, power-line arcs (as detected by arc mapping), and sparks from equipment. Ignition of wildland fires via contact with hot rifle-bullet fragments is also possible under the right conditions. Wildfires can also be started in communities experiencing shifting cultivation, where land is cleared quickly and farmed until the soil loses fertility, and slash and burn clearing. Forested areas cleared by logging encourage the dominance of flammable grasses, and abandoned logging roads overgrown by vegetation may act as fire corridors. Annual grassland fires in southern Vietnam stem in part from the destruction of forested areas by US military herbicides, explosives, and mechanical land-clearing and -burning operations during the Vietnam War.\n\nThe most common cause of wildfires varies throughout the world. In Canada and northwest China, lightning operates as the major source of ignition. In other parts of the world, human involvement is a major contributor. In Africa, Central America, Fiji, Mexico, New Zealand, South America, and Southeast Asia, wildfires can be attributed to human activities such as agriculture, animal husbandry, and land-conversion burning. In China and in the Mediterranean Basin, human carelessness is a major cause of wildfires. In the United States and Australia, the source of wildfires can be traced both to lightning strikes and to human activities (such as machinery sparks, cast-away cigarette butts, or arson). Coal seam fires burn in the thousands around the world, such as those in Burning Mountain, New South Wales; Centralia, Pennsylvania; and several coal-sustained fires in China. They can also flare up unexpectedly and ignite nearby flammable material.\n\nThe spread of wildfires varies based on the flammable material present, its vertical arrangement and moisture content, and weather conditions. Fuel arrangement and density is governed in part by topography, as land shape determines factors such as available sunlight and water for plant growth. Overall, fire types can be generally characterized by their fuels as follows: \n\nWildfires occur when all of the necessary elements of a fire triangle come together in a susceptible area: an ignition source is brought into contact with a combustible material such as vegetation, that is subjected to sufficient heat and has an adequate supply of oxygen from the ambient air. A high moisture content usually prevents ignition and slows propagation, because higher temperatures are required to evaporate any water within the material and heat the material to its fire point. Dense forests usually provide more shade, resulting in lower ambient temperatures and greater humidity, and are therefore less susceptible to wildfires. Less dense material such as grasses and leaves are easier to ignite because they contain less water than denser material such as branches and trunks. Plants continuously lose water by evapotranspiration, but water loss is usually balanced by water absorbed from the soil, humidity, or rain. When this balance is not maintained, plants dry out and are therefore more flammable, often a consequence of droughts.\n\nA wildfire \"front\" is the portion sustaining continuous flaming combustion, where unburned material meets active flames, or the smoldering transition between unburned and burned material. As the front approaches, the fire heats both the surrounding air and woody material through convection and thermal radiation. First, wood is dried as water is vaporized at a temperature of . Next, the pyrolysis of wood at releases flammable gases. Finally, wood can smoulder at or, when heated sufficiently, ignite at . Even before the flames of a wildfire arrive at a particular location, heat transfer from the wildfire front warms the air to , which pre-heats and dries flammable materials, causing materials to ignite faster and allowing the fire to spread faster. High-temperature and long-duration surface wildfires may encourage flashover or \"torching\": the drying of tree canopies and their subsequent ignition from below.\n\nWildfires have a rapid \"forward rate of spread\" (FROS) when burning through dense, uninterrupted fuels. They can move as fast as in forests and in grasslands. Wildfires can advance tangential to the main front to form a \"flanking\" front, or burn in the opposite direction of the main front by \"backing\". They may also spread by \"jumping\" or \"spotting\" as winds and vertical convection columns carry \"firebrands\" (hot wood embers) and other burning materials through the air over roads, rivers, and other barriers that may otherwise act as firebreaks. Torching and fires in tree canopies encourage spotting, and dry ground fuels that surround a wildfire are especially vulnerable to ignition from firebrands. Spotting can create \"spot fires\" as hot embers and firebrands ignite fuels downwind from the fire. In Australian bushfires, spot fires are known to occur as far as from the fire front.\n\nEspecially large wildfires may affect air currents in their immediate vicinities by the stack effect: air rises as it is heated, and large wildfires create powerful updrafts that will draw in new, cooler air from surrounding areas in thermal columns. Great vertical differences in temperature and humidity encourage pyrocumulus clouds, strong winds, and fire whirls with the force of tornadoes at speeds of more than . Rapid rates of spread, prolific crowning or spotting, the presence of fire whirls, and strong convection columns signify extreme conditions.\n\nThe thermal heat from wildfire can cause significant weathering of rocks and boulders, heat can rapidly expand a boulder and thermal shock can occur, which may cause an object's structure to fail.\n\nHeat waves, droughts, cyclical climate changes such as El Niño, and regional weather patterns such as high-pressure ridges can increase the risk and alter the behavior of wildfires dramatically. Years of precipitation followed by warm periods can encourage more widespread fires and longer fire seasons. Since the mid-1980s, earlier snowmelt and associated warming has also been associated with an increase in length and severity of the wildfire season in the Western United States. Global warming may increase the intensity and frequency of droughts in many areas, creating more intense and frequent wildfires. A 2015 study indicates that the increase in fire risk in California may be attributable to human-induced climate change. A study of alluvial sediment deposits going back over 8,000 years found warmer climate periods experienced severe droughts and stand-replacing fires and concluded climate was such a powerful influence on wildfire that trying to recreate presettlement forest structure is likely impossible in a warmer future.\n\nIntensity also increases during daytime hours. Burn rates of smoldering logs are up to five times greater during the day due to lower humidity, increased temperatures, and increased wind speeds. Sunlight warms the ground during the day which creates air currents that travel uphill. At night the land cools, creating air currents that travel downhill. Wildfires are fanned by these winds and often follow the air currents over hills and through valleys. Fires in Europe occur frequently during the hours of 12:00 p.m. and 2:00 p.m. Wildfire suppression operations in the United States revolve around a 24-hour \"fire day\" that begins at 10:00 a.m. due to the predictable increase in intensity resulting from the daytime warmth.\n\nWildfire's occurrence throughout the history of terrestrial life invites conjecture that fire must have had pronounced evolutionary effects on most ecosystems' flora and fauna. Wildfires are common in climates that are sufficiently moist to allow the growth of vegetation but feature extended dry, hot periods. Such places include the vegetated areas of Australia and Southeast Asia, the veld in southern Africa, the fynbos in the Western Cape of South Africa, the forested areas of the United States and Canada, and the Mediterranean Basin.\n\nHigh-severity wildfire creates complex early seral forest habitat (also called “snag forest habitat”), which often has higher species richness and diversity than unburned old forest. Plant and animal species in most types of North American forests evolved with fire, and many of these species depend on wildfires, and particularly high-severity fires, to reproduce and grow. Fire helps to return nutrients from plant matter back to soil, the heat from fire is necessary to the germination of certain types of seeds, and the snags (dead trees) and early successional forests created by high-severity fire create habitat conditions that are beneficial to wildlife. Early successional forests created by high-severity fire support some of the highest levels of native biodiversity found in temperate conifer forests. Post-fire logging has no ecological benefits and many negative impacts; the same is often true for post-fire seeding.\n\nAlthough some ecosystems rely on naturally occurring fires to regulate growth, some ecosystems suffer from too much fire, such as the chaparral in southern California and lower-elevation deserts in the American Southwest. The increased fire frequency in these ordinarily fire-dependent areas has upset natural cycles, damaged native plant communities, and encouraged the growth of non-native weeds. Invasive species, such as \"Lygodium microphyllum\" and \"Bromus tectorum\", can grow rapidly in areas that were damaged by fires. Because they are highly flammable, they can increase the future risk of fire, creating a positive feedback loop that increases fire frequency and further alters native vegetation communities.\n\nIn the Amazon Rainforest, drought, logging, cattle ranching practices, and slash-and-burn agriculture damage fire-resistant forests and promote the growth of flammable brush, creating a cycle that encourages more burning. Fires in the rainforest threaten its collection of diverse species and produce large amounts of CO. Also, fires in the rainforest, along with drought and human involvement, could damage or destroy more than half of the Amazon rainforest by the year 2030. Wildfires generate ash, reduce the availability of organic nutrients, and cause an increase in water runoff, eroding away other nutrients and creating flash flood conditions. A 2003 wildfire in the North Yorkshire Moors burned off of heather and the underlying peat layers. Afterwards, wind erosion stripped the ash and the exposed soil, revealing archaeological remains dating back to 10,000 BC. Wildfires can also have an effect on climate change, increasing the amount of carbon released into the atmosphere and inhibiting vegetation growth, which affects overall carbon uptake by plants.\n\nIn tundra there is a natural pattern of accumulation of fuel and wildfire which varies depending on the nature of vegetation and terrain. Research in Alaska has shown fire-event return intervals, (FRIs) that typically vary from 150 to 200 years with dryer lowland areas burning more frequently than wetter upland areas.\n\nPlants in wildfire-prone ecosystems often survive through adaptations to their local fire regime. Such adaptations include physical protection against heat, increased growth after a fire event, and flammable materials that encourage fire and may eliminate competition. For example, plants of the genus \"Eucalyptus\" contain flammable oils that encourage fire and hard sclerophyll leaves to resist heat and drought, ensuring their dominance over less fire-tolerant species. Dense bark, shedding lower branches, and high water content in external structures may also protect trees from rising temperatures. Fire-resistant seeds and reserve shoots that sprout after a fire encourage species preservation, as embodied by pioneer species. Smoke, charred wood, and heat can stimulate the germination of seeds in a process called \"serotiny\". Exposure to smoke from burning plants promotes germination in other types of plants by inducing the production of the orange butenolide.\n\nGrasslands in Western Sabah, Malaysian pine forests, and Indonesian \"Casuarina\" forests are believed to have resulted from previous periods of fire. Chamise deadwood litter is low in water content and flammable, and the shrub quickly sprouts after a fire. Cape lilies lie dormant until flames brush away the covering and then blossom almost overnight. Sequoia rely on periodic fires to reduce competition, release seeds from their cones, and clear the soil and canopy for new growth. Caribbean Pine in Bahamian pineyards have adapted to and rely on low-intensity, surface fires for survival and growth. An optimum fire frequency for growth is every 3 to 10 years. Too frequent fires favor herbaceous plants, and infrequent fires favor species typical of Bahamian dry forests.\n\nMost of the Earth's weather and air pollution resides in the troposphere, the part of the atmosphere that extends from the surface of the planet to a height of about . The vertical lift of a severe thunderstorm or pyrocumulonimbus can be enhanced in the area of a large wildfire, which can propel smoke, soot, and other particulate matter as high as the lower stratosphere. Previously, prevailing scientific theory held that most particles in the stratosphere came from volcanoes, but smoke and other wildfire emissions have been detected from the lower stratosphere. Pyrocumulus clouds can reach over wildfires. Satellite observation of smoke plumes from wildfires revealed that the plumes could be traced intact for distances exceeding . Computer-aided models such as CALPUFF may help predict the size and direction of wildfire-generated smoke plumes by using atmospheric dispersion modeling.\n\nWildfires can affect local atmospheric pollution, and release carbon in the form of carbon dioxide. Wildfire emissions contain fine particulate matter which can cause cardiovascular and respiratory problems. Increased fire byproducts in the troposphere can increase ozone concentration beyond safe levels. Forest fires in Indonesia in 1997 were estimated to have released between 0.81 and 2.57 gigatonnes (0.89 and 2.83 billion short tons) of CO into the atmosphere, which is between 13%–40% of the annual global carbon dioxide emissions from burning fossil fuels. Atmospheric models suggest that these concentrations of sooty particles could increase absorption of incoming solar radiation during winter months by as much as 15%.\n\nIn the Welsh Borders, the first evidence of wildfire is rhyniophytoid plant fossils preserved as charcoal, dating to the Silurian period (about ). Smoldering surface fires started to occur sometime before the Early Devonian period . Low atmospheric oxygen during the Middle and Late Devonian was accompanied by a decrease in charcoal abundance. Additional charcoal evidence suggests that fires continued through the Carboniferous period. Later, the overall increase of atmospheric oxygen from 13% in the Late Devonian to 30-31% by the Late Permian was accompanied by a more widespread distribution of wildfires. Later, a decrease in wildfire-related charcoal deposits from the late Permian to the Triassic periods is explained by a decrease in oxygen levels.\n\nWildfires during the Paleozoic and Mesozoic periods followed patterns similar to fires that occur in modern times. Surface fires driven by dry seasons are evident in Devonian and Carboniferous progymnosperm forests. Lepidodendron forests dating to the Carboniferous period have charred peaks, evidence of crown fires. In Jurassic gymnosperm forests, there is evidence of high frequency, light surface fires. The increase of fire activity in the late Tertiary is possibly due to the increase of C-type grasses. As these grasses shifted to more mesic habitats, their high flammability increased fire frequency, promoting grasslands over woodlands. However, fire-prone habitats may have contributed to the prominence of trees such as those of the genera \"Eucalyptus\", \"Pinus\" and \"Sequoia\", which have thick bark to withstand fires and employ serotiny.\n\nThe human use of fire for agricultural and hunting purposes during the Paleolithic and Mesolithic ages altered the preexisting landscapes and fire regimes. Woodlands were gradually replaced by smaller vegetation that facilitated travel, hunting, seed-gathering and planting. In recorded human history, minor allusions to wildfires were mentioned in the Bible and by classical writers such as Homer. However, while ancient Hebrew, Greek, and Roman writers were aware of fires, they were not very interested in the uncultivated lands where wildfires occurred. Wildfires were used in battles throughout human history as early thermal weapons. From the Middle ages, accounts were written of occupational burning as well as customs and laws that governed the use of fire. In Germany, regular burning was documented in 1290 in the Odenwald and in 1344 in the Black Forest. In the 14th century Sardinia, firebreaks were used for wildfire protection. In Spain during the 1550s, sheep husbandry was discouraged in certain provinces by Philip II due to the harmful effects of fires used in transhumance. As early as the 17th century, Native Americans were observed using fire for many purposes including cultivation, signaling, and warfare. Scottish botanist David Douglas noted the native use of fire for tobacco cultivation, to encourage deer into smaller areas for hunting purposes, and to improve foraging for honey and grasshoppers. Charcoal found in sedimentary deposits off the Pacific coast of Central America suggests that more burning occurred in the 50 years before the Spanish colonization of the Americas than after the colonization. In the post-World War II Baltic region, socio-economic changes led more stringent air quality standards and bans on fires that eliminated traditional burning practices. In the mid-19th century, explorers from observed Australian Aborigines using fire for ground clearing, hunting, and regeneration of plant food in a method later named fire-stick farming. Such careful use of fire has been employed for centuries in the lands protected by Kakadu National Park to encourage biodiversity.\n\nWildfires typically occurred during periods of increased temperature and drought. An increase in fire-related debris flow in alluvial fans of northeastern Yellowstone National Park was linked to the period between AD 1050 and 1200, coinciding with the Medieval Warm Period. However, human influence caused an increase in fire frequency. Dendrochronological fire scar data and charcoal layer data in Finland suggests that, while many fires occurred during severe drought conditions, an increase in the number of fires during 850 BC and 1660 AD can be attributed to human influence. Charcoal evidence from the Americas suggested a general decrease in wildfires between 1 AD and 1750 compared to previous years. However, a period of increased fire frequency between 1750 and 1870 was suggested by charcoal data from North America and Asia, attributed to human population growth and influences such as land clearing practices. This period was followed by an overall decrease in burning in the 20th century, linked to the expansion of agriculture, increased livestock grazing, and fire prevention efforts. A meta-analysis found that 17 times more land burned annually in California before 1800 compared to recent decades (1,800,000 hectares/year compared to 102,000 hectares/year).\n\nAccording to a paper published in Science, the number of natural and human-caused fires decreased by 24.3% between 1998 and 2015. Researchers explain this a transition from nomadism to settled lifestyle and intensification of agriculture that lead to a drop in the use of fire for land clearing.\n\nIncreases of certain native tree species (i.e. conifers) in favor of others (i.e. leaf trees) also increases wildfire risk, especially if these trees are also planted in monocultures\n\nSome invasive species, moved in by humans (i.e., for the pulp and paper industry) have in some cases also increased the intensity of wildfires. Examples include species such as Eucalyptus in California and gamba grass in Australia.\n\nWildfire prevention refers to the preemptive methods aimed at reducing the risk of fires as well as lessening its severity and spread. Prevention techniques aim to manage air quality, maintain ecological balances, protect resources, and to affect future fires. North American firefighting policies permit naturally caused fires to burn to maintain their ecological role, so long as the risks of escape into high-value areas are mitigated. However, prevention policies must consider the role that humans play in wildfires, since, for example, 95% of forest fires in Europe are related to human involvement. Sources of human-caused fire may include arson, accidental ignition, or the uncontrolled use of fire in land-clearing and agriculture such as the slash-and-burn farming in Southeast Asia.\n\nIn 1937, U.S. President Franklin D. Roosevelt initiated a nationwide fire prevention campaign, highlighting the role of human carelessness in forest fires. Later posters of the program featured Uncle Sam, characters from the Disney movie \"Bambi\", and the official mascot of the U.S. Forest Service, Smokey Bear. Reducing human-caused ignitions may be the most effective means of reducing unwanted wildfire. Alteration of fuels is commonly undertaken when attempting to affect future fire risk and behavior. Wildfire prevention programs around the world may employ techniques such as \"wildland fire use\" and \"prescribed or controlled burns\". \"Wildland fire use\" refers to any fire of natural causes that is monitored but allowed to burn. \"Controlled burns\" are fires ignited by government agencies under less dangerous weather conditions.\n\nVegetation may be burned periodically to maintain high species diversity and frequent burning of surface fuels limits fuel accumulation. Wildland fire use is the cheapest and most ecologically appropriate policy for many forests. Fuels may also be removed by logging, but fuels treatments and thinning have no effect on severe fire behavior Wildfire models are often used to predict and compare the benefits of different fuel treatments on future wildfire spread, but their accuracy is low.\n\nWildfire itself is reportedly \"the most effective treatment for reducing a fire's rate of spread, fireline intensity, flame length, and heat per unit of area\" according to Jan van Wagtendonk, a biologist at the Yellowstone Field Station.\n\nBuilding codes in fire-prone areas typically require that structures be built of flame-resistant materials and a defensible space be maintained by clearing flammable materials within a prescribed distance from the structure. Communities in the Philippines also maintain fire lines wide between the forest and their village, and patrol these lines during summer months or seasons of dry weather. Continued residential development in fire-prone areas and rebuilding structures destroyed by fires has been met with criticism. The ecological benefits of fire are often overridden by the economic and safety benefits of protecting structures and human life.\n\nFast and effective detection is a key factor in wildfire fighting. Early detection efforts were focused on early response, accurate results in both daytime and nighttime, and the ability to prioritize fire danger. Fire lookout towers were used in the United States in the early 20th century and fires were reported using telephones, carrier pigeons, and heliographs. Aerial and land photography using instant cameras were used in the 1950s until infrared scanning was developed for fire detection in the 1960s. However, information analysis and delivery was often delayed by limitations in communication technology. Early satellite-derived fire analyses were hand-drawn on maps at a remote site and sent via overnight mail to the fire manager. During the Yellowstone fires of 1988, a data station was established in West Yellowstone, permitting the delivery of satellite-based fire information in approximately four hours.\n\nCurrently, public hotlines, fire lookouts in towers, and ground and aerial patrols can be used as a means of early detection of forest fires. However, accurate human observation may be limited by operator fatigue, time of day, time of year, and geographic location. Electronic systems have gained popularity in recent years as a possible resolution to human operator error. A government report on a recent trial of three automated camera fire detection systems in Australia did, however, conclude \"...detection by the camera systems was slower and less reliable than by a trained human observer\". These systems may be semi- or fully automated and employ systems based on the risk area and degree of human presence, as suggested by GIS data analyses. An integrated approach of multiple systems can be used to merge satellite data, aerial imagery, and personnel position via Global Positioning System (GPS) into a collective whole for near-realtime use by wireless Incident Command Centers.\n\nA small, high risk area that features thick vegetation, a strong human presence, or is close to a critical urban area can be monitored using a local sensor network. Detection systems may include wireless sensor networks that act as automated weather systems: detecting temperature, humidity, and smoke. These may be battery-powered, solar-powered, or \"tree-rechargeable\": able to recharge their battery systems using the small electrical currents in plant material. Larger, medium-risk areas can be monitored by scanning towers that incorporate fixed cameras and sensors to detect smoke or additional factors such as the infrared signature of carbon dioxide produced by fires. Additional capabilities such as night vision, brightness detection, and color change detection may also be incorporated into sensor arrays.\n\nSatellite and aerial monitoring through the use of planes, helicopter, or UAVs can provide a wider view and may be sufficient to monitor very large, low risk areas. These more sophisticated systems employ GPS and aircraft-mounted infrared or high-resolution visible cameras to identify and target wildfires. Satellite-mounted sensors such as Envisat's Advanced Along Track Scanning Radiometer and European Remote-Sensing Satellite's Along-Track Scanning Radiometer can measure infrared radiation emitted by fires, identifying hot spots greater than . The National Oceanic and Atmospheric Administration's Hazard Mapping System combines remote-sensing data from satellite sources such as Geostationary Operational Environmental Satellite (GOES), Moderate-Resolution Imaging Spectroradiometer (MODIS), and Advanced Very High Resolution Radiometer (AVHRR) for detection of fire and smoke plume locations. However, satellite detection is prone to offset errors, anywhere from for MODIS and AVHRR data and up to for GOES data. Satellites in geostationary orbits may become disabled, and satellites in polar orbits are often limited by their short window of observation time. Cloud cover and image resolution and may also limit the effectiveness of satellite imagery.\n\nin 2015 a new fire detection tool is in operation at the U.S. Department of Agriculture (USDA) Forest Service (USFS) which uses data from the Suomi National Polar-orbiting Partnership (NPP) satellite to detect smaller fires in more detail than previous space-based products. The high-resolution data is used with a computer model to predict how a fire will change direction based on weather and land conditions. The active fire detection product using data from Suomi NPP's Visible Infrared Imaging Radiometer Suite (VIIRS) increases the resolution of fire observations to 1,230 feet (375 meters). Previous NASA satellite data products available since the early 2000s observed fires at 3,280 foot (1 kilometer) resolution. The data is one of the intelligence tools used by the USFS and Department of Interior agencies across the United States to guide resource allocation and strategic fire management decisions. The enhanced VIIRS fire product enables detection every 12 hours or less of much smaller fires and provides more detail and consistent tracking of fire lines during long duration wildfires – capabilities critical for early warning systems and support of routine mapping of fire progression. Active fire locations are available to users within minutes from the satellite overpass through data processing facilities at the USFS Remote Sensing Applications Center, which uses technologies developed by the NASA Goddard Space Flight Center Direct Readout Laboratory in Greenbelt, Maryland. The model uses data on weather conditions and the land surrounding an active fire to predict 12–18 hours in advance whether a blaze will shift direction. The state of Colorado decided to incorporate the weather-fire model in its firefighting efforts beginning with the 2016 fire season.\n\nIn 2014, an international campaign was organized in South Africa's Kruger National Park to validate fire detection products including the new VIIRS active fire data. In advance of that campaign, the Meraka Institute of the Council for Scientific and Industrial Research in Pretoria, South Africa, an early adopter of the VIIRS 375m fire product, put it to use during several large wildfires in Kruger.\n\nThe demand for timely, high-quality fire information has increased in recent years. Wildfires in the United States burn an average of 7 million acres of land each year. For the last 10 years, the USFS and Department of Interior have spent a combined average of about $2–4 billion annually on wildfire suppression.\n\nWildfire suppression depends on the technologies available in the area in which the wildfire occurs. In less developed nations the techniques used can be as simple as throwing sand or beating the fire with sticks or palm fronds. In more advanced nations, the suppression methods vary due to increased technological capacity. Silver iodide can be used to encourage snow fall, while fire retardants and water can be dropped onto fires by unmanned aerial vehicles, planes, and helicopters. Complete fire suppression is no longer an expectation, but the majority of wildfires are often extinguished before they grow out of control. While more than 99% of the 10,000 new wildfires each year are contained, escaped wildfires under extreme weather conditions are difficult to suppress without a change in the weather. Wildfires in Canada and the US burn an average of per year.\n\nAbove all, fighting wildfires can become deadly. A wildfire's burning front may also change direction unexpectedly and jump across fire breaks. Intense heat and smoke can lead to disorientation and loss of appreciation of the direction of the fire, which can make fires particularly dangerous. For example, during the 1949 Mann Gulch fire in Montana, USA, thirteen smokejumpers died when they lost their communication links, became disoriented, and were overtaken by the fire. In the Australian February 2009 Victorian bushfires, at least 173 people died and over 2,029 homes and 3,500 structures were lost when they became engulfed by wildfire.\n\nIn California, the U.S. Forest Service spends about $200 million per year to suppress 98% of wildfires and up to $1 billion to suppress the other 2% of fires that escape initial attack and become large.\n\nWildland fire fighters face several life-threatening hazards including heat stress, fatigue, smoke and dust, as well as the risk of other injuries such as burns, cuts and scrapes, animal bites, and even rhabdomyolysis. Between 2000–2016, more than 350 wildland firefighters died on-duty.\n\nEspecially in hot weather condition, fires present the risk of heat stress, which can entail feeling heat, fatigue, weakness, vertigo, headache, or nausea. Heat stress can progress into heat strain, which entails physiological changes such as increased heart rate and core body temperature. This can lead to heat-related illnesses, such as heat rash, cramps, exhaustion or heat stroke. Various factors can contribute to the risks posed by heat stress, including strenuous work, personal risk factors such as age and fitness, dehydration, sleep deprivation, and burdensome personal protective equipment. Rest, cool water, and occasional breaks are crucial to mitigating the effects of heat stress.\n\nSmoke, ash, and debris can also pose serious respiratory hazards to wildland fire fighters. The smoke and dust from wildfires can contain gases such as carbon monoxide, sulfur dioxide and formaldehyde, as well as particulates such as ash and silica. To reduce smoke exposure, wildfire fighting crews should, whenever possible, rotate firefighters through areas of heavy smoke, avoid downwind firefighting, use equipment rather than people in holding areas, and minimize mop-up. Camps and command posts should also be located upwind of wildfires. Protective clothing and equipment can also help minimize exposure to smoke and ash.\n\nFirefighters are also at risk of cardiac events including strokes and heart attacks. Fire fighters should maintain good physical fitness. Fitness programs, medical screening and examination programs which include stress tests can minimize the risks of firefighting cardiac problems. Other injury hazards wildland fire fighters face include slips, trips and falls, burns, scrapes and cuts from tools and equipment, being struck by trees, vehicles, or other objects, plant hazards such as thorns and poison ivy, snake and animal bites, vehicle crashes, electrocution from power lines or lightning storms, and unstable building structures.\n\nFire retardants are used to slow wildfires by inhibiting combustion. They are aqueous solutions of ammonium phosphates and ammonium sulfates, as well as thickening agents. The decision to apply retardant depends on the magnitude, location and intensity of the wildfire. In certain instances, fire retardant may also be applied as a precautionary fire defense measure.\n\nTypical fire retardants contain the same agents as fertilizers. Fire retardant may also affect water quality through leaching, eutrophication, or misapplication. Fire retardant's effects on drinking water remain inconclusive. Dilution factors, including water body size, rainfall, and water flow rates lessen the concentration and potency of fire retardant. Wildfire debris (ash and sediment) clog rivers and reservoirs increasing the risk for floods and erosion that ultimately slow and/or damage water treatment systems. There is continued concern of fire retardant effects on land, water, wildlife habitats, and watershed quality, additional research is needed. However, on the positive side, fire retardant (specifically its nitrogen and phosphorus components) has been shown to have a fertilizing effect on nutrient-deprived soils and thus creates a temporary increase in vegetation.\n\nCurrent USDA procedure maintains that the aerial application of fire retardant in the United States must clear waterways by a minimum of 300 feet in order to safeguard effects of retardant runoff. Aerial uses of fire retardant are required to avoid application near waterways and endangered species (plant and animal habitats). After any incident of fire retardant misapplication, the U.S. Forest Service requires reporting and assessment impacts be made in order to determine mitigation, remediation, and/or restrictions on future retardant uses in that area.\n\nWildfire modeling is concerned with numerical simulation of wildfires in order to comprehend and predict fire behavior. Wildfire modeling aims to aid wildfire suppression, increase the safety of firefighters and the public, and minimize damage. Using computational science, wildfire modeling involves the statistical analysis of past fire events to predict spotting risks and front behavior. Various wildfire propagation models have been proposed in the past, including simple ellipses and egg- and fan-shaped models. Early attempts to determine wildfire behavior assumed terrain and vegetation uniformity. However, the exact behavior of a wildfire's front is dependent on a variety of factors, including windspeed and slope steepness. Modern growth models utilize a combination of past ellipsoidal descriptions and Huygens' Principle to simulate fire growth as a continuously expanding polygon. Extreme value theory may also be used to predict the size of large wildfires. However, large fires that exceed suppression capabilities are often regarded as statistical outliers in standard analyses, even though fire policies are more influenced by large wildfires than by small fires.\n\nWildfire risk is the chance that a wildfire will start in or reach a particular area and the potential loss of human values if it does. Risk is dependent on variable factors such as human activities, weather patterns, availability of wildfire fuels, and the availability or lack of resources to suppress a fire. Wildfires have continually been a threat to human populations. However, human induced geographical and climatic changes are exposing populations more frequently to wildfires and increasing wildfire risk. It is speculated that the increase in wildfires arises from a century of wildfire suppression coupled with the rapid expansion of human developments into fire-prone wildlands. Wildfires are naturally occurring events that aid in promoting forest health. Global warming and climate changes are causing an increase in temperatures and more droughts nationwide which contributes to an increase in wildfire risk.\n\nThe most noticeable adverse effect of wildfires is the destruction of property. However, the release of hazardous chemicals from the burning of wildland fuels also significantly impacts health in humans.\n\nWildfire smoke is composed primarily of carbon dioxide and water vapor. Other common smoke components present in lower concentrations are carbon monoxide, formaldehyde, acrolein, polyaromatic hydrocarbons, and benzene. Small particulates suspended in air which come in solid form or in liquid droplets are also present in smoke. 80 -90% of wildfire smoke, by mass, is within the fine particle size class of 2.5 micrometers in diameter or smaller.\n\nDespite carbon dioxide's high concentration in smoke, it poses a low health risk due to its low toxicity. Rather, carbon monoxide and fine particulate matter, particularly 2.5 µm in diameter and smaller, have been identified as the major health threats. Other chemicals are considered to be significant hazards but are found in concentrations that are too low to cause detectable health effects.\n\nThe degree of wildfire smoke exposure to an individual is dependent on the length, severity, duration, and proximity of the fire. People are exposed directly to smoke via the respiratory tract though inhalation of air pollutants. Indirectly, communities are exposed to wildfire debris that can contaminate soil and water supplies.\n\nThe U.S. Environmental Protection Agency (EPA) developed the Air Quality Index (AQI), a public resource that provides national air quality standard concentrations for common air pollutants. The public can use this index as a tool to determine their exposure to hazardous air pollutants based on visibility range.\n\nAfter a wildfire, hazards remain. Residents returning to their homes may be at risk from falling fire-weakened trees. Humans and pets may also be harmed by falling into ash pits.\n\nFirefighters are at the greatest risk for acute and chronic health effects resulting from wildfire smoke exposure. Due to firefighters' occupational duties, they are frequently exposed to hazardous chemicals at a close proximity for longer periods of time. A case study on the exposure of wildfire smoke among wildland firefighters shows that firefighters are exposed to significant levels of carbon monoxide and respiratory irritants above OSHA-permissible exposure limits (PEL) and ACGIH threshold limit values (TLV). 5–10% are overexposed. The study obtained exposure concentrations for one wildland firefighter over a 10-hour shift spent holding down a fireline. The firefighter was exposed to a wide range of carbon monoxide and respiratory irritant (combination of particulate matter 3.5 µm and smaller, acrolein, and formaldehype) levels. Carbon monoxide levels reached up to 160ppm and the TLV irritant index value reached a high of 10. In contrast, the OSHA PEL for carbon monoxide is 30ppm and for the TLV respiratory irritant index, the calculated threshold limit value is 1; any value above 1 exceeds exposure limits.\n\nBetween 2001 and 2012, over 200 fatalities occurred among wildland firefighters. In addition to heat and chemical hazards, firefighters are also at risk for electrocution from power lines; injuries from equipment; slips, trips, and falls; injuries from vehicle rollovers; heat-related illness; insect bites and stings; stress; and rhabdomyolysis.\n\nResidents in communities surrounding wildfires are exposed to lower concentrations of chemicals, but they are at a greater risk for indirect exposure through water or soil contamination. Exposure to residents is greatly dependent on individual susceptibility. Vulnerable persons such as children (ages 0–4), the elderly (ages 65 and older), smokers, and pregnant women are at an increased risk due to their already compromised body systems, even when the exposures are present at low chemical concentrations and for relatively short exposure periods.\n\nAdditionally, there is evidence of an increase in maternal stress, as documented by researchers M.H. O'Donnell and A.M. Behie, thus affecting birth outcomes. In Australia, studies show that male infants born with drastically higher average birth weights were born in mostly severely fire-affected areas. This is attributed to the fact that maternal signals directly affect fetal growth patterns.\n\nAsthma is one of the most common chronic disease among children in the United States affecting estimated 6.2 million children. A recent area of research on asthma risk focuses specifically on the risk of air pollution during the gestational period. Several pathophysiology processes are involved are in this. In human's considerable airway development occurs during the 2 and 3 trimester and continue until 3 years of age. It is hypothesized that exposure to these toxins during this period could have consequential effects as the epithelium of the lungs during this time could have increased permeability to toxins. Exposure to air pollution during parental and pre-natal stage could induce epigenetic changes which are responsible for the development of asthma. Recent Meta-Analyses have found significant association between PM, NO and development of asthma during childhood despite heterogeneity among studies. Furthermore, maternal exposure to chronic stressor, which are most like to be present in distressed communities, which is also a relevant co relate of childhood asthma which may further help explain the early childhood exposure to air pollution, neighborhood poverty and childhood risk. Living in distressed neighborhood is not only linked to pollutant source location and exposure but can also be associated with degree of magnitude of chronic individual stress which can in turn alter the allostatic load of the maternal immune system leading to adverse outcomes in children, including increased susceptibility to air pollution and other hazards.\n\nWildfire smoke contains particulate matter that may have adverse effects upon the human respiratory system. Evidence of the health effects of wildfire smoke should be relayed to the public so that exposure may be limited. Evidence of health effects can also be used to influence policy to promote positive health outcomes.\n\nInhalation of smoke from a wildfire can be a health hazard. Wildfire smoke is composed of combustion products i.e. carbon dioxide, carbon monoxide, water vapor, particulate matter, organic chemicals, nitrogen oxides and other compounds. The principal health concern is the inhalation of particulate matter and carbon monoxide.\n\nParticulate matter (PM) is a type of air pollution made up of particles of dust and liquid droplets. They are characterized into three categories based on the diameter of the particle: coarse PM, fine PM, and ultrafine PM. Coarse particles are between 2.5 micrometers and 10 micrometers, fine particles measure 0.1 to 2.5 micrometers, and ultrafine particle are less than 0.1 micrometer.  Each size can enter the body through inhalation, but the PM impact on the body varies by size. Coarse particles are filtered by the upper airways and these particles can accumulate and cause pulmonary inflammation. This can result in eye and sinus irritation as well as sore throat and coughing. Coarse PM is often composed of materials that are heavier and more toxic that lead to short-term effects with stronger impact.\n\nSmaller particulate moves further into the respiratory system creating issues deep into the lungs and the bloodstream. In asthma patients, PM causes inflammation but also increases oxidative stress in the epithelial cells. These particulates also cause apoptosis and autophagy in lung epithelial cells. Both processes cause the cells to be damaged and impacts the cell function. This damage impacts those with respiratory conditions such as asthma where the lung tissues and function are already compromised. The third PM type is ultra-fine PM (UFP). UFP can enter the bloodstream like PM however studies show that it works into the blood much quicker. The inflammation and epithelial damage done by UFP has also shown to be much more severe. PM is of the largest concern in regards to wildfire. This is particularly hazardous to the very young, elderly and those with chronic conditions such as asthma, chronic obstructive pulmonary disease (COPD), cystic fibrosis and cardiovascular conditions. The illnesses most commonly with exposure to fine particle from wildfire smoke are bronchitis, exacerbation of asthma or COPD, and pneumonia. Symptoms of these complications include wheezing and shortness of breath and cardiovascular symptoms include chest pain, rapid heart rate and fatigue.\n\nSmoke from wildfires can cause health problems, especially for children and those who already have respiratory problems. Several epidemiological studies have demonstrated a close association between air pollution and respiratory allergic diseases such as bronchial asthma. \n\nAn observational study of smoke exposure related to the 2007 San Diego wildfires revealed an increase both in healthcare utilization and respiratory diagnoses, especially asthma among the group sampled. Projected climate scenarios of wildfire occurrences predict significant increases in respiratory conditions among young children. Particulate Matter (PM) triggers a series of biological processes including inflammatory immune response, oxidative stress, which are associated with harmful changes in allergic respiratory diseases.\n\nAlthough some studies demonstrated no significant acute changes in lung function among people with asthma related to PM from wildfires, a possible explanation for these counterintuitive findings is the increased use of quick-relief medications, such as inhalers, in response to elevated levels of smoke among those already diagnosed with asthma. In investigating the association of medication use for obstructive lung disease and wildfire exposure, researchers found increases both in the usage of inhalers and initiation of long-term control as in oral steroids. More specifically, some people with asthma reported higher use of quick-relief medications (inhalers). After two major wildfires in California, researchers found an increase in physician prescriptions for quick-relief medications in the years following the wildfires than compared to the year before each occurrence. \n\nThere is consistent evidence between wildfire smoke and the exacerbation of asthma.\n\nCarbon monoxide (CO) is a colorless, odorless gas that can be found at the highest concentration at close proximity to a smoldering fire. For this reason, carbon monoxide inhalation is a serious threat to the health of wildfire firefighters. CO in smoke can be inhaled into the lungs where it is absorbed into the bloodstream and reduces oxygen delivery to the body's vital organs. At high concentrations, it can cause headache, weakness, dizziness, confusion, nausea, disorientation, visual impairment, coma and even death. However, even at lower concentrations, such as those found at wildfires, individuals with cardiovascular disease may experience chest pain and cardiac arrhythmia. A recent study tracking the number and cause of wildfire firefighter deaths from 1990–2006 found that 21.9% of the deaths occurred from heart attacks.\n\nAnother important and somewhat less obvious health effect of wildfires is psychiatric diseases and disorders. Both adults and children from countries ranging from the United States and Canada to Greece and Australia who were directly and indirectly affected by wildfires were found by researchers to demonstrate several different mental conditions linked to their experience with the wildfires. These include post-traumatic stress disorder (PTSD), depression, anxiety, and phobias.\n\nIn a new twist to wildfire health effects, former uranium mining sites were burned over in the summer of 2012 near North Fork, Idaho. This prompted concern from area residents and Idaho State Department of Environmental Quality officials over the potential spread of radiation in the resultant smoke, since those sites had never been completely cleaned up from radioactive remains.\n\nThe western US has seen an increase in both frequency and intensity of wildfires over the last several decades. This increase has been attributed to the arid climate of the western US and the effects of global warming. An estimated 46 million people were exposed to wildfire smoke from 2004 to 2009 in the Western United States. Evidence has demonstrated that wildfire smoke can increase levels of particulate matter in the atmosphere.\n\nThe EPA has defined acceptable concentrations of particulate matter in the air, through the National Ambient Air Quality Standards and monitoring of ambient air quality has been mandated. Due to these monitoring programs and the incidence of several large wildfires near populated areas, epidemiological studies have been conducted and demonstrate an association between human health effects and an increase in fine particulate matter due to wildfire smoke.\n\nThe EPA has defined acceptable concentrations of particulate matter in the air. The National Ambient Air Quality Standards are part of the Clean Air Act and provide mandated guidelines for pollutant levels and the monitoring of ambient air quality. In addition to these monitoring programs, the increased incidence of wildfires near populated areas have precipitated several epidemiological studies. Such studies have demonstrated an association between negative human health effects and an increase in fine particulate matter due to wildfire smoke. The size of the particulate matter is significant as smaller particulate matter (fine) is easily inhaled into the human respiratory tract. Often, small particulate matter can be inhaled into deep lung tissue causing respiratory distress, illness, or disease. \n\nAn increase in PM smoke emitted from the Hayman fire in Colorado in June 2002, was associated with an increase in respiratory symptoms in patients with COPD. Looking at the wildfires in Southern California in October 2003 in a similar manner, investigators have shown an increase in hospital admissions due to asthma symptoms while being exposed to peak concentrations of PM in smoke. Another epidemiological study found a 7.2% (95% confidence interval: 0.25%, 15%) increase in risk of respiratory related hospital admissions during smoke wave days with high wildfire-specific particulate matter 2.5 compared to matched non-smoke-wave days.\n\nChildren participating in the Children's Health Study were also found to have an increase in eye and respiratory symptoms, medication use and physician visits. Recently, it was demonstrated that mothers who were pregnant during the fires gave birth to babies with a slightly reduced average birth weight compared to those who were not exposed to wildfire during birth. Suggesting that pregnant women may also be at greater risk to adverse effects from wildfire. Worldwide it is estimated that 339,000 people die due to the effects of wildfire smoke each year.\n\nWhile the size of particulate matter is an important consideration for health effects, the chemical composition of particulate matter (PM) from wildfire smoke should also be considered. Antecedent studies have demonstrated that the chemical composition of PM from wildfire smoke can yield different estimates of human health outcomes as compared to other sources of smoke. Health outcomes for people exposed to wildfire smoke may differ from those exposed to smoke from alternative sources such as solid fuels. \n\n"}
