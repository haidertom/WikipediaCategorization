{"id": "1758866", "url": "https://en.wikipedia.org/wiki?curid=1758866", "title": "Accelerating change", "text": "Accelerating change\n\nIn futures studies and the history of technology, accelerating change is a perceived increase in the rate of technological change throughout history, which may suggest faster and more profound change in the future and may or may not be accompanied by equally profound social and cultural change.\n\nIn 1910 during the town planning conference of London Daniel Burnham noted that \"But it is not merely in the number of facts or sorts of knowledge that progress lies: it is still more in the geometric ratio of sophistication, in the geometric widening of the sphere of knowledge, which every year is taking in a larger percentage of people as time goes on.\" and later on \"It is the argument with which I began, that a mighty change having come about in fifty years, and our pace of development having immensely accelerated, our sons and grandsons are going to demand and get results that would stagger us.\"\n\nIn 1938, Buckminster Fuller introduced the word ephemeralization to describe the trends of \"doing more with less\" in chemistry, health and other areas of industrial development. In 1946, Fuller published a chart of the discoveries of the chemical elements over time to highlight the development of accelerating acceleration in human knowledge acquisition.\n\nIn 1958, Stanislaw Ulam wrote in reference to a conversation with John von Neumann: \n\nIn a series of published articles from 1974–1979, and then in his 1988 book \"Mind Children\", computer scientist and futurist Hans Moravec generalizes Moore's law to make predictions about the future of artificial life. Moore's law describes an exponential growth pattern in the complexity of integrated semiconductor circuits. Moravec extends this to include technologies from long before the integrated circuit to future forms of technology. Moravec outlines a timeline and a scenario in which robots will evolve into a new series of artificial species, starting around 2030–2040.\nIn \"Robot: Mere Machine to Transcendent Mind\", published in 1998, Moravec further considers the implications of evolving robot intelligence, generalizing Moore's Law to technologies predating the integrated circuit, and also plotting the exponentially increasing computational power of the brains of animals in evolutionary history. Extrapolating these trends, he speculates about a coming \"mind fire\" of rapidly expanding superintelligence similar to the explosion of intelligence predicted by Vinge.\n\nIn his TV series \"Connections\" (1978)—and sequels \"Connections²\" (1994) and \"Connections³\" (1997)—James Burke explores an \"Alternative View of Change\" (the subtitle of the series) that rejects the conventional linear and teleological view of historical progress. Burke contends that one cannot consider the development of any particular piece of the modern world in isolation. Rather, the entire gestalt of the modern world is the result of a web of interconnected events, each one consisting of a person or group acting for reasons of their own motivations (e.g., profit, curiosity, religious) with no concept of the final, modern result to which the actions of either them or their contemporaries would lead. The interplay of the results of these isolated events is what drives history and innovation, and is also the main focus of the series and its sequels.\n\nBurke also explores three corollaries to his initial thesis. The first is that, if history is driven by individuals who act only on what they know at the time, and not because of any idea as to where their actions will eventually lead, then predicting the future course of technological progress is merely conjecture. Therefore, if we are astonished by the connections Burke is able to weave among past events, then we will be equally surprised to what the events of today eventually will lead, especially events we weren't even aware of at the time.\n\nThe second and third corollaries are explored most in the introductory and concluding episodes, and they represent the downside of an interconnected history. If history progresses because of the synergistic interaction of past events and innovations, then as history does progress, the number of these events and innovations increases. This increase in possible connections causes the process of innovation to not only continue, but to accelerate. Burke poses the question of what happens when this rate of innovation, or more importantly change itself, becomes too much for the average person to handle, and what this means for individual power, liberty, and privacy.\n\nIn his book \"Mindsteps to the Cosmos\" (HarperCollins, August 1983), Gerald S. Hawkins elucidated his notion of 'mindsteps', dramatic and irreversible changes to paradigms or world views. He identified five distinct mindsteps in human history, and the technology that accompanied these \"new world views\": the invention of imagery, writing, mathematics, printing, the telescope, rocket, radio, TV, computer... \"Each one takes the collective mind closer to reality, one stage further along in its understanding of the relation of humans to the cosmos.\" He noted: \"The waiting period between the mindsteps is getting shorter. One can't help noticing the acceleration.\" Hawkins' empirical 'mindstep equation' quantified this, and gave dates for future mindsteps. The date of the next mindstep (5; the series begins at 0) is given as 2021, with two further, successively closer mindsteps in 2045 and 2051, until the limit of the series in 2053. His speculations ventured beyond the technological:\n\nThe mathematician Vernor Vinge popularized his ideas about exponentially accelerating technological change in the science fiction novel \"Marooned in Realtime\" (1986), set in a world of rapidly accelerating progress leading to the emergence of more and more sophisticated technologies separated by shorter and shorter time intervals, until a point beyond human comprehension is reached. His subsequent Hugo award-winning novel \"A Fire Upon the Deep\" (1992) starts with an imaginative description of the evolution of a superintelligence passing through exponentially accelerating developmental stages ending in a transcendent, almost omnipotent power unfathomable by mere humans. His already mentioned influential 1993 paper on the technological singularity compactly summarizes the basic ideas.\n\nIn his 1999 book \"The Age of Spiritual Machines\", Ray Kurzweil proposed \"The Law of Accelerating Returns\", according to which the rate of change in a wide variety of evolutionary systems (including but not limited to the growth of technologies) tends to increase exponentially. He gave further focus to this issue in a 2001 essay entitled \"The Law of Accelerating Returns\". In it, Kurzweil, after Moravec, argued for extending Moore's Law to describe exponential growth of diverse forms of technological progress. Whenever a technology approaches some kind of a barrier, according to Kurzweil, a new technology will be invented to allow us to cross that barrier. He cites numerous past examples of this to substantiate his assertions. He predicts that such paradigm shifts have and will continue to become increasingly common, leading to \"technological change so rapid and profound it represents a rupture in the fabric of human history.\" He believes the Law of Accelerating Returns implies that a technological singularity will occur before the end of the 21st century, around 2045. The essay begins:\n\nThe Law of Accelerating Returns has in many ways altered public perception of Moore's law. It is a common (but mistaken) belief that Moore's law makes predictions regarding all forms of technology, when really it only concerns semiconductor circuits. Many futurists still use the term \"Moore's law\" to describe ideas like those put forth by Moravec, Kurzweil and others.\n\nAccording to Kurzweil, since the beginning of evolution, more complex life forms have been evolving exponentially faster, with shorter and shorter intervals between the emergence of radically new life forms, such as human beings, who have the capacity to engineer (i.e. intentionally design with efficiency) a new trait which replaces relatively blind evolutionary mechanisms of selection for efficiency. By extension, the rate of technical progress amongst humans has also been exponentially increasing, as we discover more effective ways to do things, we also discover more effective ways to learn, i.e. language, numbers, written language, philosophy, scientific method, instruments of observation, tallying devices, mechanical calculators, computers, each of these major advances in our ability to account for information occur increasingly close together. Already within the past sixty years, life in the industrialized world has changed almost beyond recognition except for living memories from the first half of the 20th century. This pattern will culminate in unimaginable technological progress in the 21st century, leading to a singularity. Kurzweil elaborates on his views in his books \"The Age of Spiritual Machines\" and \"The Singularity Is Near\".\n\nAccelerating change may not be restricted to the Anthropocene Epoch, but a general and predictable developmental feature of the universe. The physical processes that generate an acceleration such as Moore's law are positive feedback loops giving rise to exponential or superexponential technological change. These dynamics lead to increasingly efficient and dense configurations of Space, Time, Energy, and Matter (STEM efficiency and density, or STEM \"compression\"). At the physical limit, this developmental process of accelerating change leads to black hole density organizations, a conclusion also reached by studies of the ultimate physical limits of computation in the universe.\n\nApplying this vision to the search for extraterrestrial intelligence leads to the idea that advanced intelligent life reconfigures itself into a black hole. Such advanced life forms would be interested in inner space, rather than outer space and interstellar expansion. They would thus in some way transcend reality, not be observable and it would be a solution to Fermi's paradox called the \"transcension hypothesis\", Another solution is that the black holes we observe could actually be interpreted as intelligent super-civilizations feeding on stars, or \"stellivores\". \nThis dynamics of evolution and development is an invitation to study the universe itself as evolving, developing. If the universe is a kind of superorganism, it may possibly tend to reproduce, naturally or artificially, with intelligent life playing a role.\n\nExamples of large human \"buy-ins\" into technology include the computer revolution, as well as massive government projects like the Manhattan Project and the Human Genome Project. The foundation organizing the Methuselah Mouse Prize believes aging research could be the subject of such a massive project if substantial progress is made in slowing or reversing cellular aging in mice.\n\nBoth Theodore Modis and Jonathan Huebner have argued—each from different perspectives—that the rate of technological innovation has not only ceased to rise, but is actually now declining.\n\nIn fact, \"technological singularity\" is just one of a few singularities detected through the analysis of a number of characteristics of the World System development, for example, with respect to the world population, world GDP, and some other economic indices. It has been shown that the hyperbolic pattern of the world demographic, economic, cultural, urbanistic, and technological growth (observed for many centuries, if not millennia prior to the 1970s) could be accounted for by a rather simple mechanism, the nonlinear second-order positive feedback, that was shown long ago to generate precisely the hyperbolic growth, known also as the \"blow-up regime\" (implying just finite-time singularities). In our case this nonlinear second order positive feedback looks as follows: more people – more potential inventors – faster technological growth – the carrying capacity of the Earth grows faster – faster population growth – more people – more potential inventors – faster technological growth, and so on. On the other hand, this research has shown that since the 1970s the World System does not develop hyperbolically any more, its development diverges more and more from the blow-up regime, and at present it is moving \"from singularity\", rather than \"toward singularity\".\n\nJürgen Schmidhuber calls the Singularity \"Omega\", referring to Teilhard de Chardin's Omega Point (1916). For Omega = 2040, he says the series Omega - 2 human lifetimes (n < 10; one lifetime = 80 years) roughly matches the most important events in human history.\n\nKurzweil created the following graphs to illustrate his beliefs concerning and his justification for his Law of Accelerating Returns.\n\n\n"}
{"id": "39270850", "url": "https://en.wikipedia.org/wiki?curid=39270850", "title": "Alfred Russel Wallace centenary", "text": "Alfred Russel Wallace centenary\n\nThe centenary of the death of the naturalist Alfred Russel Wallace on 7 November 1913 was marked in 2013 with events around the world to celebrate his life and work. The commemorations was co-ordinated by the Natural History Museum, London.\n\nEvents between October 2013 and June 2014 were planned by the Natural History Museum and other organisations including the Zoological Society of London, Cardiff University, the University of Alberta, Dorset County Museum, Swansea Museum, Dorset Wildlife Trust, Ness Botanical Gardens (South Wirral), the Royal Society, the Linnean Society, the Harvard Museum of Natural History, the American Museum of Natural History, Hertford Museum and the National Museum of Wales.\n\nThe naturalist, explorer, geographer, anthropologist and biologist Alfred Russel Wallace (born 8 January 1823) died on 7 November 1913. He is principally remembered now for having independently conceived the theory of evolution through natural selection, which prompted Charles Darwin to publish \"On the Origin of Species\". Some of his books such as \"The Malay Archipelago\" remain in print; it is considered one of the best accounts of scientific exploration published during the 19th century. Wallace is also remembered for recognizing the presence of a biogeographical boundary, now known as the Wallace Line, that divides the Indonesian archipelago into two distinct parts: a western portion in which the animals are almost entirely of Asian origin, and an eastern portion where the fauna reflect the influence of Australasia.\n\nThe South Kensington Natural History Museum, London, co-ordinating commemorative events for the Wallace centenary worldwide in the 'Wallace100' project, created a website to celebrate Wallace's centenary. The museum holds the Wallace Collection of memorabilia including letters, Wallace's notebooks and other documents, and 28 drawers of insects and other specimens that he collected on his expeditions to the Malay Archipelago and to South America. The museum describes Wallace as \"Father of biogeography\", as a committed socialist, and as a spiritualist.\n\nThe Royal Societyplanned a two-day discussion meeting in October 2013 for researchers on \"Alfred Russel Wallace and his legacy\", with speakers including George Beccaloni, Steve Jones, Lynne Parenti, Tim Caro and Martin Rees. Cardiff University's School of Earth & Ocean Sciences has planned a lecture series in 2013-2014 as part of the centenary commemoration of Wallace.\n\nHertford Museum held several events including an evening of illustrated talks on 15 January 2014 at Hertford Theatre. Errol Fuller will discuss Wallace and the curious 19th century social phenomenon that guided his life and Dr Sandra Knapp will talk about Wallace’s life and explorations in the Amazon.\n\nThe Linnean Society held a two-day celebration of Wallace's centenary in Bournemouth on 7 and 8 June 2013, together with the Society for the History of Natural History, Bournemouth University and Bournemouth Natural Sciences Society. The event included talks about Wallace, his thoughts on natural selection, his evolutionary insights, and his notebooks and letters. A theatrical performance, 'You Should Ask Wallace', was put on by Theatre na n'Og. On the second day the group visited Wallace's grave and went on a nature walk in Wallace's memory.\n\nThe Royal Botanic Gardens, Kew ran a display of Wallace memorabilia including letters, photographs, artefacts made from plants, and herbarium specimens in 2013. \"Kew\" magazine likewise published an article \"The Wallace Connection\" to mark the centenary.\n\nThe American Museum of Natural History, New York City, planned a talk by naturalist and broadcaster David Attenborough for 12 November 2013, entitled 'Alfred Russel Wallace and the Birds of Paradise'. Birute Galdikas, one of Louis Leakey's 'ape women', will speak about her orangutans at the museum's Wallace conference.\n\nIn 2013 the BBC broadcast a two-part television series, \"Bill Bailey's Jungle hero: Alfred Russel Wallace\", in which comedian Bill Bailey travelled in the footsteps of Wallace in Indonesia to show what the naturalist achieved.\n\n"}
{"id": "4024", "url": "https://en.wikipedia.org/wiki?curid=4024", "title": "Butterfly effect", "text": "Butterfly effect\n\nIn chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.\n\nThe term, coined by Edward Lorenz, is derived from the metaphorical example of the details of a tornado (the exact time of formation, the exact path taken) being influenced by minor perturbations such as the flapping of the wings of a distant butterfly several weeks earlier. Lorenz discovered the effect when he observed that runs of his weather model with initial condition data that was rounded in a seemingly inconsequential manner would fail to reproduce the results of runs with the unrounded initial condition data. A very small change in initial conditions had created a significantly different outcome.\n\nThough Lorenz gave a name to the phenomenon, the idea that small causes may have large effects in general and in weather specifically was earlier recognized by French mathematician and engineer Henri Poincaré and American mathematician and philosopher Norbert Wiener. Edward Lorenz's work placed the concept of \"instability\" of the earth's atmosphere onto a quantitative base and linked the concept of instability to the properties of large classes of dynamic systems which are undergoing nonlinear dynamics and deterministic chaos.\n\nThe butterfly effect can also be demonstrated by very simple systems.\n\nIn \"The Vocation of Man\" (1800), Johann Gottlieb Fichte says that \"you could not remove a single grain of sand from its place without thereby ... changing something throughout all parts of the immeasurable whole\".\n\nChaos theory and the sensitive dependence on initial conditions were described in the literature in a particular case of the three-body problem by Henri Poincaré in 1890. He later proposed that such phenomena could be common, for example, in meteorology.\n\nIn 1898, Jacques Hadamard noted general divergence of trajectories in spaces of negative curvature. Pierre Duhem discussed the possible general significance of this in 1908.\n\nThe idea that one butterfly could eventually have a far-reaching ripple effect on subsequent historic events made its earliest known appearance in \"A Sound of Thunder\", a 1952 short story by Ray Bradbury about time travel.\n\nIn 1961, Lorenz was running a numerical computer model to redo a weather prediction from the middle of the previous run as a shortcut. He entered the initial condition 0.506 from the printout instead of entering the full precision 0.506127 value. The result was a completely different weather scenario.\n\nLorenz wrote:\nIn 1963 Lorenz published a theoretical study of this effect in a highly cited, seminal paper called \"Deterministic Nonperiodic Flow\" (the calculations were performed on a Royal McBee LGP-30 computer). Elsewhere he stated: Following suggestions from colleagues, in later speeches and papers Lorenz used the more poetic butterfly. According to Lorenz, when he failed to provide a title for a talk he was to present at the 139th meeting of the American Association for the Advancement of Science in 1972, Philip Merilees concocted \"Does the flap of a butterfly’s wings in Brazil set off a tornado in Texas?\" as a title. Although a butterfly flapping its wings has remained constant in the expression of this concept, the location of the butterfly, the consequences, and the location of the consequences have varied widely.\n\nThe phrase refers to the idea that a butterfly's wings might create tiny changes in the atmosphere that may ultimately alter the path of a tornado or delay, accelerate or even prevent the occurrence of a tornado in another location. The butterfly does not power or directly create the tornado, but the term is intended to imply that the flap of the butterfly's wings can \"cause\" the tornado: in the sense that the flap of the wings is a part of the initial conditions; one set of conditions leads to a tornado while the other set of conditions doesn't. The flapping wing represents a small change in the initial condition of the system, which cascades to large-scale alterations of events (compare: domino effect). Had the butterfly not flapped its wings, the trajectory of the system might have been vastly different—but it's also equally possible that the set of conditions without the butterfly flapping its wings is the set that leads to a tornado.\n\nThe butterfly effect presents an obvious challenge to prediction, since initial conditions for a system such as the weather can never be known to complete accuracy. This problem motivated the development of ensemble forecasting, in which a number of forecasts are made from perturbed initial conditions.\n\nSome scientists have since argued that the weather system is not as sensitive to initial conditions as previously believed. David Orrell argues that the major contributor to weather forecast error is model error, with sensitivity to initial conditions playing a relatively small role. Stephen Wolfram also notes that the Lorenz equations are highly simplified and do not contain terms that represent viscous effects; he believes that these terms would tend to damp out small perturbations.\n\nRecurrence, the approximate return of a system towards its initial conditions, together with sensitive dependence on initial conditions, are the two main ingredients for chaotic motion. They have the practical consequence of making complex systems, such as the weather, difficult to predict past a certain time range (approximately a week in the case of weather) since it is impossible to measure the starting atmospheric conditions completely accurately.\n\nA dynamical system displays sensitive dependence on initial conditions if points arbitrarily close together separate over time at an exponential rate. The definition is not topological, but essentially metrical.\n\nIf \"M\" is the state space for the map formula_1, then formula_1 displays sensitive dependence to initial conditions if for any x in \"M\" and any δ > 0, there are y in \"M\", with distance \"d\"(. , .) such that formula_3 and such that\n\nfor some positive parameter \"a\". The definition does not require that all points from a neighborhood separate from the base point \"x\", but it requires one positive Lyapunov exponent.\n\nThe simplest mathematical framework exhibiting sensitive dependence on initial conditions is provided by a particular parametrization of the logistic map:\n\nwhich, unlike most chaotic maps, has a closed-form solution:\n\nwhere the initial condition parameter formula_7 is given by formula_8. For rational formula_7, after a finite number of iterations formula_10 maps into a periodic sequence. But almost all formula_7 are irrational, and, for irrational formula_7, formula_10 never repeats itself – it is non-periodic. This solution equation clearly demonstrates the two key features of chaos – stretching and folding: the factor 2 shows the exponential growth of stretching, which results in sensitive dependence on initial conditions (the butterfly effect), while the squared sine function keeps formula_10 folded within the range [0, 1].\n\nThe butterfly effect is most familiar in terms of weather; it can easily be demonstrated in standard weather prediction models, for example. The climate scientists James Annan and William Connolley explain that chaos is important in the development of weather prediction methods; models are sensitive to initial conditions. They add the caveat: \"Of course the existence of an unknown butterfly flapping its wings has no direct bearing on weather forecasts, since it will take far too long for such a small perturbation to grow to a significant size, and we have many more immediate uncertainties to worry about. So the direct impact of this phenomenon on weather prediction is often somewhat overstated.\"\n\nThe potential for sensitive dependence on initial conditions (the butterfly effect) has been studied in a number of cases in semiclassical and quantum physics including atoms in strong fields and the anisotropic Kepler problem. Some authors have argued that extreme (exponential) dependence on initial conditions is not expected in pure quantum treatments; however, the sensitive dependence on initial conditions demonstrated in classical motion is included in the semiclassical treatments developed by Martin Gutzwiller and Delos and co-workers.\n\nOther authors suggest that the butterfly effect can be observed in quantum systems. Karkuszewski et al. consider the time evolution of quantum systems which have slightly different Hamiltonians. They investigate the level of sensitivity of quantum systems to small changes in their given Hamiltonians. Poulin et al. presented a quantum algorithm to measure fidelity decay, which \"measures the rate at which identical initial states diverge when subjected to slightly different dynamics\". They consider fidelity decay to be \"the closest quantum analog to the (purely classical) butterfly effect\". Whereas the classical butterfly effect considers the effect of a small change in the position and/or velocity of an object in a given Hamiltonian system, the quantum butterfly effect considers the effect of a small change in the Hamiltonian system with a given initial position and velocity. This quantum butterfly effect has been demonstrated experimentally. Quantum and semiclassical treatments of system sensitivity to initial conditions are known as quantum chaos.\n\nThe journalist Peter Dizikes, writing in \"The Boston Globe\" in 2008, notes that popular culture likes the idea of the butterfly effect, but gets it wrong. Whereas Lorenz suggested correctly with his butterfly metaphor that predictability \"is inherently limited\", popular culture supposes that each event can be explained by finding the small reasons that caused it. Dizikes explains: \"It speaks to our larger expectation that the world should be comprehensible – that everything happens for a reason, and that we can pinpoint all those reasons, however small they may be. But nature itself defies this expectation.\"\n\n\n\n"}
{"id": "590687", "url": "https://en.wikipedia.org/wiki?curid=590687", "title": "Cobblestone", "text": "Cobblestone\n\nCobblestone is a natural building material based on cobble-sized stones, and is used for pavement roads, streets, and buildings.\n\nIn England, it was commonplace since ancient times for flat stones with a flat narrow edge to be set on edge to provide an even paved surface. This was known as a 'pitched' surface and was common all over Britain, as it did not require rounded pebbles. Pitched surfaces predate the use of regularly-sized granite setts by more than a thousand years. Such pitched paving is quite distinct from that formed from rounded stones, although both forms are commonly referred to as 'cobbled' surfaces. Most surviving genuinely old 'cobbled' areas are in reality pitched surfaces. A cobbled area is known as a \"causey\", \"cassay\" or \"cassie\" in Scots (probably from \"causeway\").\n\nSetts are often idiomatically referred to as \"cobbles\", although a sett is distinct from a cobblestone by being quarried or shaped to a regular form, whereas cobblestone is generally of a naturally occurring form.\n\nCobblestones are typically either set in sand or similar material, or are bound together with mortar. Paving with cobblestones allows a road to be heavily used all year long. It prevents the build-up of ruts often found in dirt roads. It has the additional advantage of not getting muddy in wet weather or dusty in dry weather. Shod horses are also able to get better traction on stone cobbles, pitches or setts than tarmac/asphalt. The fact that carriage wheels, horse hooves and even modern automobiles make a lot of noise when rolling over cobblestone paving might be thought a disadvantage, but it has the advantage of warning pedestrians of their approach. In England, the custom was to strew the cobbles outside the house of a sick or dying person with straw to dampen the sound.\n\nCobblestones set in sand have the environmental advantage of being permeable paving, and of moving rather than cracking with movements in the ground.\n\nCobblestones were largely replaced by quarried granite setts (also known as Belgian block) in the nineteenth century. The word cobblestone is often wrongly used to describe such treatment. Setts were relatively even and roughly rectangular stones that were laid in regular patterns. They gave a smoother ride for carts than cobbles, although in heavily used sections, such as in yards and the like, the usual practice was to replace the setts by parallel granite slabs set apart by the standard axle length of the time.\n\nCobblestoned and \"setted\" streets gradually gave way to macadam roads, and later to tarmac, and finally to asphalt concrete at the beginning of the 20th century. However, cobble­stones are often retained in historic areas, even for streets with modern vehicular traffic. Many older villages and cities in Europe are still paved with cobblestones or pitched.\n\nPopular television soap opera Coronation Street has cobblestones and they are often referenced in relation to reports on the show.\n\nIn recent decades, cobblestones have become a popular material for paving newly pedestrianised streets in Europe. In this case, the noisy nature of the surface is an advantage as pedestrians can hear approaching vehicles. The visual cues of the cobblestones also clarify that the area is more than just a normal street. The use of cobblestones/setts is also considered to be a more \"upmarket\" roadway solution, having been described as \"unique and artistic\" compared to the normal asphalt road environment.\n\nIn older U.S. cities such as Philadelphia, Boston, Pittsburgh, New York City, Chicago, San Francisco, New Castle, Portland (Maine), Baltimore, Charleston, and New Orleans, many of the older streets are paved in cobblestones and setts (mostly setts); however, many such streets have been paved over with asphalt, which can crack and erode away due to heavy traffic, thus revealing the original stone pavement.\n\nIn some places such as Saskatoon, Saskatchewan, Canada, as late as the 1990s some busy intersections still showed cobblestones through worn down sections of pavement. In Toronto streets using setts were used by streetcar routes and disappeared by the 1980s, but are still found in the Distillery District.\n\nMany cities in Latin America, such as Buenos Aires, Argentina; Zacatecas and Guanajuato, in Mexico; Old San Juan, Puerto Rico; Philippines, Vigan; and Montevideo, Uruguay, are well known for their many cobblestone streets, which are still operational and in good condition. They are still maintained and repaired the traditional manner, by placing and arranging granite stones by hand.\n\nIn the Czech Republic, there are old cobblestone paths with colored marbles and limestones. The design with three colors (red/limestone, black/limestone, white/marble) has a long tradition in Bohemia. The cubes of the old ways are handmade.\n\nIn the Finger Lakes Region of New York State, the retreat of the glaciers during the last ice age left numerous small, rounded cobblestones available for building. Pre-Civil War architecture in the region made heavy use of cobblestones for walls. Today, the fewer than 600 remaining cobblestone buildings are prized as historic locations, most of them private homes. They are clustered south of Lake Ontario, between Buffalo and Syracuse. There is also a cluster of cobblestone buildings in the Town of Paris, Ontario. In addition to homes, cobblestones were used to build barns, stagecoach taverns, smokehouses, stores, churches, schools, factories, and cemetery markers. The history of building with cobblestones and 17 driving tours to see the remaining structures are found in \"Cobblestone Quest - Road Tours of New York's Historic Buildings\".\n\nThe only public cobblestone building is the Alexander Classical School, located in Alexander, New York.\n\nIn cycling road races, cobblestones are used as an additional difficulty for the riders. It requires a certain skill to ride cobblestones efficiently, without falling or getting a flat tire. Tour of Flanders and Paris–Roubaix are notable cobbled classics.\n\n\n"}
{"id": "665333", "url": "https://en.wikipedia.org/wiki?curid=665333", "title": "Cymdeithas Edward Llwyd", "text": "Cymdeithas Edward Llwyd\n\nCymdeithas Edward Llwyd (English: Edward Llwyd Society) is a Welsh natural history organization whose name commemorates the great Welsh natural historian, geographer and linguist Edward Llwyd.\n\nThe Cymdeithas Edward Llwyd organizes regular country walks throughout Wales in sites of interest of the Welsh environment, including SSI's & post-industrial landscapes. These are Welsh-language walking groups, although learners are just as welcome.\n\nThey also organize a variety of Nature & Environmental activities, including lectures, publications on Welsh Nature & Environment & conservation work.\n\n"}
{"id": "1065253", "url": "https://en.wikipedia.org/wiki?curid=1065253", "title": "Darwin machine", "text": "Darwin machine\n\nA Darwin machine (a 1987 coinage by William H. Calvin, by analogy to a Turing machine) is a machine that, like a Turing machine, involves an iteration process that yields a high-quality result, but, whereas a Turing machine uses logic, the Darwin machine uses rounds of variation, selection, and inheritance.\nIn its original connotation, a Darwin machine is any process that bootstraps quality by utilizing all of the six essential features of a Darwinian process: A \"pattern\" is \"copied\" with \"variations\", where populations of one variant pattern \"compete\" with another population, their relative success biased by a \"multifaceted environment\" (natural selection) so that winners predominate in producing the further variants of the next generation (Darwin's \"inheritance principle\").\n\nMore loosely, a Darwin machine is a process that utilizes some subset of the Darwinian essentials, typically natural selection to create a non-reproducing pattern, as in neural Darwinism. Many aspects of neural development utilize overgrowth followed by pruning to a pattern, but the resulting pattern does not itself create further copies.\n\n\"Darwin machine\" has been used multiple times to name computer programs after Charles Darwin.\n\n\n"}
{"id": "48719972", "url": "https://en.wikipedia.org/wiki?curid=48719972", "title": "Diversification rates", "text": "Diversification rates\n\nDiversification rates are the rates at which new species form (the Speciation rate, λ) and living species go extinct (the extinction rate, μ). Diversification rates can be estimated from fossils, data on the species diversity of clades and their ages, or phylogenetic trees. Diversification rates are typically reported on a per-lineage basis (e.g. speciation rate per lineage per unit of time), and refer to the diversification dynamics expected under a birth-death process.\n\nA broad range of studies have demonstrated that diversification rates can vary tremendously both through time and across the tree of life. Current research efforts are focused on predicting diversification rates based on aspects of species or their environment. Diversification rates are also subject to various survivorship biases such as the \"Push of the past\"\n\nDiversification rates can be estimated time-series data on fossil occurrences. With perfect data, this would be an easy task; one could just count the number of speciation and extinction events in a given time interval, and then use these data to calculate per-lineage rates of speciation and extinction per unit time. However, the incomplete nature of the fossil record means that our calculations need to include the possibility that some fossil lineages were not sampled, and that we do not have precise estimates for the times of speciation and extinction of the taxa that are sampled. More sophisticated methods account for the probability of sampling any lineage, which might also depend on some properties of the lineage itself (e.g. whether it has any hard body parts that tend to fossilize) as well as the environment in which it lives.\n\nMany estimates of diversification rates for fossil lineages are for higher-level taxonomic groups like genera or families. Such rates are informative about general patterns and trends of diversification through time and across clades but can be difficult to compare directly to rates of speciation and extinction of individual species.\n\nDiversification rates can be estimated from data on the ages and diversities of monophyletic clades in the tree of life. For example, if a clade is 100 million years old and includes 1000 species, we can estimate the net diversification rate of that clade by using a formula derived from a birth-death model of diversification:\n\nEquations are also available for estimating speciation and extinction rates separately when one has ages and diversities for multiple clades.\n\nDiversification rates can be estimated using the information available in phylogenetic trees. To calculate diversification rates, such phylogenetic trees have to include branch lengths. Various methods are available to estimate speciation and extinction rates from phylogenetic trees using both maximum likelihood and Bayesian statistical approaches. One can also use phylogenetic trees to test for changing rates of speciation and/or extinction, both through time and across clades, and to associate rates of evolution with potential explanatory factors.\n\n"}
{"id": "58614271", "url": "https://en.wikipedia.org/wiki?curid=58614271", "title": "Earth's crustal evolution", "text": "Earth's crustal evolution\n\nCrustal evolution involves the formation, destruction and renewal of the rocky outer shell at the Earth's surface.\n\nThe variation in composition within the Earth's crust is much greater than other terrestrial planets. Mars, Venus, Mercury and other planetary bodies have relatively quasi-uniform crusts unlike that of the Earth which contains both oceanic and continental plates. This unique property reflects the complex series of crustal processes that have taken place throughout the planet's history, including the ongoing process of plate tectonics.\n\nThe proposed mechanisms, regarding Earth's crustal evolution, take a theory-orientated approach. Fragmentary geologic evidence and observations provide the basis for hypothetical solutions to problems relating to the early Earth system. Therefore a combination of these theories creates both a framework of current understanding and also a platform for future study.\n\nThe early Earth was entirely molten. This was due to high temperatures created and maintained by the following processes:\n\n\nThe mantle remained hotter than modern day temperatures throughout the Archean. Over time the Earth began to cool as planetary accretion slowed and heat stored within the magma ocean was lost to space through radiation.\n\nA theory for the initiation of magma solidification states that once cool enough, the cooler base of the magma ocean would begin to crystallise first. This is because pressure of 25GPa at the surface cause the solidus to lower. The formation of a thin 'chill-crust' at the extreme surface would provide thermal insulation to the shallow sub surface, keeping it warm enough to maintain the mechanism of crystallisation from the deep magma ocean.\n\nThe composition of the crystals produced during the crystallisation of the magma ocean varied with depth. Experiments involving the melting of peridotite magma show that deep in the ocean (>≈700m), the main mineral present would be Mg-perovskite. Whereas olivine would dominate in the shallower areas along with its high pressure polymorphs e.g. garnet and majorite.\n\nA contributing theory to the formation of the first continental crust is through intrusive plutonic volcanism. The product of these eruptions formed a hot, thick lithosphere which underwent regular cycling with the mantle. The heat released by this form of volcanism, as well as assisting mantle convection, increased the geothermal gradient of the early crust.\n\nCrustal dichotomy represents the distinct contrast in composition and nature of the oceanic and continental plates, which together form the overall crust.\n\nOceanic and continental crusts are, at the present day, produced and maintained through plate tectonic processes. However the same mechanisms are unlikely to have produced the crustal dichotomy of the early lithosphere. This is thought to be true on the basis that sections of the thin, low density continental lithosphere could not have been sub-ducted under each other.\n\nSubsequently, a proposed relative timing for crustal dichotomy is put forward stating that dichotomy took place before the commencement of global plate tectonics. This is so a difference in crustal density could be established to facilitate plate subduction.\n\nLarge and numerous impact craters can be recognised on planetary bodies across the Solar System. These craters are thought to date back to a period where there was an increased frequency and intensity of asteroid impacts with terrestrial planets, known as the Late Heavy Bombardment, which terminated approximately 4Ga. This proposal goes on, claiming the Earth would have also sustained the same relative intensity of cratering as other planetesimals in the Solar System. It is therefore only due to Earth's high erosional rates and constant plate tectonics that the craters are not visible today. By scaling up the number and size of impact craters seen on the Moon to fit the size of Earth, it is predicted that at least 50% of the Earth's initial crust was covered in impact basins. This estimate provides a lower limit of the effect impact cratering had on the Earth's surface. \nThe main effects of impact cratering on the early lithosphere were:\n\n\nThe magnitude of these impacts is interpreted, with a high level of uncertainty, to have converted roughly half of the 'continental' crust into terrestrial maria. Therefore providing a method for the formation of crustal dichotomy, as seen today.\n\nThe initial crystallisation of minerals from the magma ocean formed the primordial crust. \n\nA potential explanation of this process states the resultant solidification of the mantle edge took place approximately 4.43Ga. This would subsequently produce continents composed of komatiite, an ultramafic rock rich in magnesium with a high melting point and low dynamic viscosity. Another line of research follows up on this, proposing that differences in the densities of newly formed crystals caused separation of crustal rocks; upper crust largely composed of fractionated gabbros and lower crust composed of anorthosites. The overall result of initial crystallisation formed a primordial crust roughly 60 km in depth.\n\nThe lack of certainty regarding the formation of primordial crust is due to there being no remaining present day examples. This is due to Earth's high erosional rates and the subduction and subsequent destruction of tectonic plates throughout its 4.5 Ga history. Furthermore, during its existence the primordial crust is thought to have been regularly broken and re-formed by impacts involving other planetesimals. This continued for several hundred million years after accretion, which concluded approximately 4.4Ga. The outcome of this would be the constant alteration in the composition of the primordial crust, increasing the difficulty in determining its nature. \n\nRecycling of existing primordial crust contributes to the production of secondary crust. Partial melting of the existing crust increases the mafic content of the melt producing basaltic secondary crust. A further method of formation due to the decay of radioactive elements within the Earth releasing heat energy and eventually causing the partial melting of upper mantle, also producing basaltic lavas. As a result, most secondary crust on Earth is formed at mid ocean ridges forming the oceanic crust.\n\nThe present day continental crust is an example of a tertiary crust. Tertiary crust is the most differentiated type of crust and so has a composition vastly different to that of the bulk Earth. The tertiary crust contains over 20% of the abundance of incompatible elements, which are elements a size or charge that prevent them from being included in mineral structure. This a result of its generation from the subduction and partial melting of secondary crust where it undergoes further fractional crystallisation. Two stages of evolution produce an increased proportion of incompatible elements.\n\nThe formation and development of plumes in the early mantle contributed to triggering the lateral movement of crust across the Earth's surface. The effect of upwelling mantle plumes on the lithosphere can be seen today through local depressions around hotspots such as Hawaii. The scale of this impact is much less than that exhibited in the Archean eon where mantle temperatures were much greater. Localised areas of hot mantle rose to the surface through a central plume wedge, weakening the damaged and already thin lithosphere. Once the plume head breaks the surface, crust either side of the head is forced downwards through the conservation of mass, initiating subduction. Numerical modelling shows only strongly energetic plumes are capable of weakening the lithosphere enough to rupture it, such plumes would have been present in the hot Archean mantle.\n\nPre-tectonic subduction can also be inferred from the internal volcanism on Venus. Artemis Corona is a large plume formed by the upwelling of mantle derived magma and is on a scale potentially comparable to that in the Archean mantle. Models using its known characteristics showed that continued magmatism from conductive heat through the plume caused gravitational collapse. The weight of collapse caused the spreading of the surrounding crust outwards and subsequent subduction around the margins. The anhydrous nature of the crust on Venus prevents it from sliding past each other, whereas through the study of oxygen isotopes, the presence of water on Earth can be confirmed from 4.3Ga. Thus, this model helps provide a mechanism for how plate tectonics could have been triggered on Earth, although it does not demonstrate that subduction was initiated at the earliest confirmed presence of water on Earth. Based on these models, the onset of subduction and plate tectonics is dated at 3.6Ga.\n\nImpact cratering also had consequences for both the development of plume-induced subduction and the establishment of global plate tectonics. The steepening of geothermal gradients could have directly enhanced convective mantle transport which now beneath an increasingly fractured lithosphere could have created stresses great enough to cause rifting and the separation of crust into plates.\n\nCrustal growth rates can be used to calculate estimates for the age of the continental crust. This can be done through analysis of igneous rocks with the same isotopic composition as initial mantle rock. These igneous rocks are dated and assumed to be direct evidence of new continental crust formation. The resulting ages of isotopically juvenile igneous rocks give distinct peaks, representing an increased proportion of igneous rock and therefore increased crust growth, at 2.7, 1.9 and 1.2 Ga. The validity of these results is questioned as the peaks could represent periods of preservation rather than increased continental crust generation. This is reinforced by the fact that such peaks are not observed in recent geologic time where it is given that magmatism resulting from the plate subduction has strongly contributed to producing new crust.\n\nCrustal growth rates from igneous rocks can be compared to the rates generated from radiogenic isotope ratios in sedimentary rocks. Projections of growth rates using these techniques does not produce staggered peaks, instead smooth shallow curves presenting a more constant rate of crustal growth. Although representative of large periods of time, limitations are found where samples do not solely represent magmatic production events. Instead samples include the mixing of sediments which produces a mix of original and altered isotope ratios.\n\nZircon minerals can be both detrital grains from sedimentary rocks and crystals in igneous rocks. Therefore, a combination of zircon forms can provide a more accurate estimate of crustal growth rates. Further to this, zircon minerals can be subject to Hf and O isotope ratio analysis. This is important as Hf isotopes indicate whether a rock originates from the mantle or an existing rock. High δO values of zircons represent rock recycled at the Earth's surface and thus potentially producing mixed samples. The outcome of this combined analysis is valid zircons showing periods of increased crustal generation at at 1.9 and 3.3Ga, the latter of which representing the time period following the commencement of global plate tectonics.\n"}
{"id": "38103099", "url": "https://en.wikipedia.org/wiki?curid=38103099", "title": "Earth pyramids of Ritten", "text": "Earth pyramids of Ritten\n\nThe earth pyramids of Ritten (German: \"Erdpyramiden am Ritten\"; ) are a natural monument that is located on the Ritten, a plateau not far from Bolzano in northern Italy. The earth pyramids of South Tyrol are a fairly widespread phenomenon which are existing in various locations.\nThe original name in this area for these earth pyramids is \"Lahntürme\", i.e. landslide towers. They are rather unusual formations of their kind which originate from morainic rocks of glacial origin. The columns of the pyramids may be more or less elongated, and the higher they are the thinner they get, ending usually with a stone cover. These earth pyramids are not static, they are constantly evolving, because their life cycle foresees a continuous erosion, or even a final collapse leaving room for new formations.\n\nIn South Tyrol there are other natural monuments like this such as the earth pyramids of Platten, but the ones of Ritten are considered the parents of them all.\n\n"}
{"id": "212485", "url": "https://en.wikipedia.org/wiki?curid=212485", "title": "Earth religion", "text": "Earth religion\n\nEarth religion is a term used mostly in the context of neopaganism.\n\nEarth-centered religion or nature worship is a system of religion based on the veneration of natural phenomena. It covers any religion that worships the earth, nature, or fertility deity, such as the various forms of goddess worship or matriarchal religion. Some find a connection between earth-worship and the Gaia hypothesis. Earth religions are also formulated to allow one to utilize the knowledge of preserving the earth.\n\nAccording to Marija Gimbutas, pre-Indo-European societies lived in small-scale, family-based communities that practiced matrilineal succession and goddess-centered religion where creation comes from the woman. She is the Divine Mother who can give life and take it away. In Irish mythology she is Danu, in Slavic mythology she is Mat Zemlya, and in other cultures she is Pachamama, Ninsun, Terra Mater, Nüwa, Matres or Shakti.\n\nIn the late 1800s, James Weir wrote an article describing the beginnings and aspects of early religious feeling. According to Boyer, early man was forced to locate food and shelter in order to survive, while constantly being directed by his instincts and senses. Because man's existence depended on nature, men began to form their religion and beliefs on and around nature itself. It is evident that man's first religion would have had to develop from the material world, he argues, because man relied heavily on his senses and what he could see, touch, and feel. In this sense, the worship of nature formed, allowing man to further depend on nature for survival.\n\nNeopagans have tried to make claims that religion started in ways that correspond to earth religion. In one of their published works, \"The Urantia Book\", another reason for this worship of nature came from a fear of the world around primitive man. His mind lacked the complex function of processing and sifting through complex ideas. As a result, man worshiped the very entity that surrounded him every day. That entity was nature. Man experienced the different natural phenomena around him, such as storms, vast deserts, and immense mountains. Among the very first parts of nature to be worshiped were rocks and hills, plants and trees, animals, the elements, heavenly bodies, and even man himself. As primitive man worked his way through nature worship, he eventually moved on to incorporate spirits into his worship. Although these claims may have some merit, they are nonetheless presented from a biased position that cannot be authenticated by traditional and reliable sources. Therefore, their claims can not be relied upon.\n\nThe origins of religion can be looked at through the lens of the function and processing of the human mind. Pascal Boyer suggests that, for the longest period of time, the brain was thought of as a simple organ of the body. However, he claims that the more information collected about the brain indicates that the brain is indeed not a \"blank slate.\" Humans do not just learn any information from the environment and surroundings around them. They have acquired sophisticated cognitive equipment that prepares them to analyze information in their culture and determine which information is relevant and how to apply it. Boyer states that \"having a normal human brain does not imply that you have religion. All it implies is that people can acquire it, which is very different.\" He suggests that religions started for the reasons of providing answers to humans, giving comfort, providing social order to society, and satisfying the need of the illusion-prone nature of the human mind. Ultimately, religion came into existence because of our need to answer questions and hold together our societal order.\n\nAn additional idea on the origins of religion comes not from man's cognitive development, but from the ape. Barbara J. King argues that human beings have an emotional connection with those around them, and that that desire for a connection came from their evolution from apes. The closest relative to the human species is the African ape. At birth, the ape begins negotiating with its mother about what it wants and needs in order to survive. The world the ape is born into is saturated with close family and friends. Because of this, emotions and relationships play a huge role in the ape's life. Its reactions and responses to one another are rooted and grounded in a sense of belongingness, which is derived from its dependence on the ape's mother and family. Belongingness is defined as \"mattering to someone who matters to you ... getting positive feelings from our relationships.\" This sense and desire for belongingness, which started in apes, only grew as the hominid (a human ancestor) diverged from the lineage of the ape, which occurred roughly six to seven million years ago.\n\nAs severe changes in the environment, physical evolutions in the human body (especially in the development of the human brain), and changes in social actions occurred, humans went beyond trying to simply form bonds and relationships of empathy with others. As their culture and society became more complex, they began using practices and various symbols to make sense of the natural and spiritual world around them. Instead of simply trying to find belongingness and empathy from the relationships with others, humans created and evolved God and spirits in order to fulfil that need and exploration. King argued that \"an earthly need for belonging led to human religious imagination and thus to the otherworldly realm of relating to God, gods, and spirits.\"\n\nThe term \"earth religion\" encompasses any religion that worships the earth, nature or fertility gods or goddesses. There is an array of groups and beliefs that fall under earth religion, such as paganism, which is a polytheistic, nature based religion; animism, which is the worldview that all living entities (plants, animals, and humans) possess a spirit; Wicca, who hold the concept of an earth mother goddess as well as practice ritual magic; and druidism, which equates divinity with the natural world.\n\nAnother perspective of earth religion to consider is pantheism, which takes a varied approach to the importance and purpose of the earth, and man's relationship with the planet. Several of their core statements deal with the connectivity humans share with the planet, declaring that \"all matter, energy, and life are an interconnected unity of which we are an inseparable part\" and \"we are an integral part of Nature, which we should cherish, revere and preserve in all its magnificent beauty and diversity. We should strive to live in harmony with Nature locally and globally\".\n\nThe earth also plays a vital role to many Voltaic peoples, many of whom \"consider the Earth to be Heaven’s wife\", such as the Konkomba of northern Ghana, whose economic, social and religious life is heavily influenced by the earth. It is also important to consider various Native American religions, such as Peyote Religion, Longhouse Religion, and Earth Lodge Religion.\n\nApril 22 was established as International Mother Earth Day by the United Nations in 2009, but many cultures around the world have been celebrating the Earth for thousands of years. Winter solstice and Summer solstice are celebrated with holidays like Yule and Dongzhi in the winter and Tiregān and Kupala in the summer.\n\nAnimism is practiced among the Bantu peoples of Sub-Saharan Africa. The Dahomey mythology has deities like Nana Buluku, Gleti, Mawu, Asase Yaa, Naa Nyonmo and Xevioso.\n\nIn Baltic mythology, the sun is a female deity, Saule, a mother or a bride, and Mēness is the moon, father or husband, their children being the stars. In Slavic mythology Mokosh and Mat Zemlya together with Perun head up the pantheon. Celebrations and rituals are centered on nature and harvest seasons. Dragobete is a traditional Romanian spring holiday that celebrates \"the day when the birds are betrothed.\"\n\nIn Hindu philosophy, the yoni is the creative power of nature and the origin of life. In Shaktism, the yoni is celebrated and worshipped during the Ambubachi Mela, an annual fertility festival which celebrates the Earth's menstruation.\n\nAlthough the idea of earth religion has been around for thousands of years, it did not fully show up in popular culture until the early 1990s. \"The X-Files\" was one of the first nationally broadcast television programs to air witchcraft and Wicca (types of earth religion) content. On average, Wiccans - those who practice Wicca - were more or less pleased with the way the show had portrayed their ideals and beliefs. However, they still found it to be a little \"sensationalistic\". That same year, the movie \"The Craft\" was released - also depicting the art of Wicca. Unfortunately, this cinematic feature was not as happily accepted as \"The X-Files\" had been.\n\nA few years later, programs showcasing the aforementioned religious practices - such as \"Charmed\" and \"Buffy the Vampire Slayer\" - became widely popular. Although \"Charmed\" focused mostly on witchcraft, the magic they practiced very closely resembled Wicca. Meanwhile, \"Buffy\" was one of the first shows to actually cast a Wiccan character. However, since the shows focus was primarily on vampires, the Wiccan was depicted as having supernatural powers, rather than being in-tuned with the Earth.\n\nOther movies and shows throughout the last few decades have also been placed under the genre of Earth Religion. Among them are two of director Hayao Miyazaki's most well known films - \"Princess Mononoke\" and \"My Neighbor Totoro\". Both movies present human interaction with land, animal, and other nature spirits. Speakers for Earth Religion have said that these interactions suggest overtones of Earth Religion themes.\n\nSome popular Disney movies have also been viewed as Earth Religion films. Among them are \"The Lion King\" and \"Brother Bear\". Those who practice Earth Religion view \"The Lion King\" as an Earth Religion film mainly for the \"interconnectedness\" and \"Circle of Life\" it shows between the animals, plants, and life in general. When that link is broken, viewers see chaos and despair spread throughout the once bountiful land. Congruently, \"Brother Bear\" portrays interactions and consequences when humans disobey or go against the animal and Earth spirits.\n\nOther earth religion movies include \"The 13th Warrior\", \"The Deceivers (film)\", \"Sorceress (1982 film)\", \"Anchoress (film)\", \"Eye of the Devil\", \"Agora (film)\", and \"The Wicker Man (1973 film)\". These movies all contain various aspects of earth religion and nature worship in general.\n\nMany religions have negative stereotypes of earth religion and neo-paganism in general. A common critique of the worship of nature and resources of \"Mother Earth\" is that the rights of nature and ecocide movements are inhibitors of human progress and development. This argument is fueled by the fact that those people socialized into 'western' world views believe the earth itself is not a living being. Wesley Smith believes this is “anti-humanism with the potential to do real harm to the human family.” According to Smith, earth worshipers are hindering large-scale development, and they are viewed as inhibitors of advancement.\n\nA lot of criticism of earth religion comes from the negative actions of a few people who have been chastised for their actions. One such negative representative of earth religion is Aleister Crowley. He is believed to be \"too preoccupied with awakening magical powers\" instead of putting the well-being of others in his coven. Crowley allegedly looked up to \"Old George\" Pickingill, who was another worshipper of nature who was viewed negatively. Critics regarded Pickingill as a Satanist and \"England’s most notorious Witch\".\n\nCrowley himself was \"allegedly expelled from the Craft because he was a pervert.\" He became aroused by torture and pain, and enjoyed being \"punished\" by women. This dramatically damaged Crowley’s public image, because of his lifestyle and actions. Many people regarded all followers of earth religion as perverted Satanists.\n\nFollowers of earth religion have suffered major opprobrium over the years for allegedly being Satanists. Some religious adherents can be prone to viewing religions other than their religion as being wrong sometimes because they perceive those religions as characteristic of their concept of Satan worship. To wit, Witchcraft, a common practice of Wiccans, is sometimes misinterpreted as Satan worship by members of these groups, as well as less-informed persons who may not be specifically religious but who may reside within the sphere-of-influence of pagan-critical religious adherents. From the Wiccan perspective, however, earth religion and Wicca lie outside of the phenomenological world that encompasses Satanism. An all-evil being does not exist within the religious perspective of western earth religions. Devotees worship and celebrate earth resources and earth-centric deities. Satanism and Wicca \"have entirely different beliefs about deity, different rules for ethical behavior, different expectations from their membership, different views of the universe, different seasonal days of celebration, etc.\"\n\nNeo-pagans, or earth religion followers, often claim to be unaffiliated with Satanism. Neo-pagans, Wiccans, and earth religion believers do not acknowledge the existence of a deity that conforms to the common Semitic sect religious concept of Satan. Satanism stems from Christianity, while earth religion stems from older religious concepts.\n\nSome earth religion adherents take issue with the religious harassment that is inherent in the social pressure that necessitates their having to distance themselves from the often non-uniform, Semitic sect religious concept of Satan worship. Having to define themselves as \"other\" from a religious concept that is not within their worldview implies a certain degree of outsider-facilitated, informal, but functional religious restriction that is based solely on the metaphysical and mythological religious beliefs of those outsiders. This is problematic because outsider initiated comparisons to Satanism with the intent of condemnation, even when easily refuted, can have the effect of social pressure on earth religion adherents to conform to outsider perception of acceptable customs, beliefs, and modes of religious behavior.\n\nTo illustrate, a problem could arise with the \"other\" than Satanism argument if an earth centered belief system adopted a holiday that a critic considered to be similar or identical to a holiday that Satanists celebrate. Satanists have historically been prone to adopting holidays that have origins in various pagan traditions, ostensibly because these traditional holidays are amongst the last known vestiges of traditional pre-Semitic religious practice in the west. Satanists are, perhaps irrationally, prone to interpreting non-Semitic holidays as anti-Christian and therefore as implicitly representative of their worldview. This is not surprising given the fact that this is, in fact, how many Christians interpret holidays such as Samhain. In spite of any flawed perceptions or rationale held by any other group, earth centered religion adherents do not recognize misinterpretation of their customs made by outside religious adherents or critics inclusive of Satan worshippers.\n\nOrganized Satan worship, as defined by and anchored in the Semitic worldview, is characterized by a relatively disorganized and often disparate series of movements and groups that mostly emerged in the mid-20th century. Thus, their adopted customs have varied, continue to vary, and therefore this moving target of beliefs and customs can not be justifiably nor continuously accounted for by earth centered religious adherents. Once a Satanist group adopts a holiday, social stigma may unjustifiably taint the holiday and anyone who observes it without discrimination as to whence and for what purpose it was originally celebrated. Given these facts, many earth centered religion devotees find comparisons to Satanism intrinsically oppressive in nature. This logic transfers to any and all religious customs to include prayer, magic, ceremony, and any unintentional similarity in deity characteristics (an example is the horned traditional entity Pan having similar physical characteristics to common horned depictions of Satan).\n\nThe issue is further complicated by the theory that the intra and extra-biblical mythology of Satan that is present throughout various Semitic sects may have originally evolved to figuratively demonize the heathen religions of other groups. Thus, the concept of Satan, or \"the adversary\", would have been representative of all non-Semitic religions and, by extension, the people who believed in them. Although, at times, the concept of the \"other\" as demonic has also been used to characterize competing Semitic sects. Amongst other purposes, such belief would have been extraordinarily useful during the psychological and physical process of cleansing Europe of traditional tribal beliefs in favor of Christianity. This possibility would account for the historical tendency of Christian authorities, for example, to deem most pagan customs carried out in the pagan religious context as demonic. By any modern standard, such current beliefs would violate western concepts of religious tolerance as well as be inimical to the preservation of what remains of the culture of long-persecuted religious groups.\n\nBecause of the vast diversity of religions that fall under the title of \"earth religion\" there is no consensus of beliefs. However, the ethical beliefs of most religions overlap. The most well-known ethical code is the Wiccan Rede. Many of those who practice an earth religion choose to be environmentally active. Some perform activities such as recycling or composting while others feel it to be more productive to try and support the earth spiritually. These six beliefs about ethics seem to be universal.\n\n\"An [if] it harm none, do what ye will.\" Commonly worded in modern English as \"if it doesn't harm anyone, do what you want.\" This maxim was first printed in 1964, after being spoken by the priestess Doreen Valiente in the mid-20th century, and governs most ethical belief of Wiccans and some Pagans. There is no consensus of beliefs but this rede provides a starting point for most people's interpretation of what is ethical. The rede clearly states to do no harm but what constitutes as harm and what level of self-interest is acceptable is negotiable. Many Wiccans reverse the phrase into \"Do what ye will an it harm none,\" meaning \"Do what you want if it doesn't harm anyone.\" The difference may not seem significant but it is. The first implies that it is good to do no harm but does not say that it is necessarily unethical to do so, the second implies that all forms of harm are unethical. The second phrase is nearly impossible to follow. This shift occurred when trying to better adapt the phrase into modern English as well as to stress the \"harmlessness\" of Wiccans. The true nature of the rede simply implies that there is personal responsibility for your actions. You may do as you wish but there is a karma reaction from every action. Even though this is the most well-known rede of practice, it does not mean that those that choose not to follow it are unethical. There are many other laws of practice that other groups follow.\n\nThe Threefold Law is the belief that for all actions there is always a cause and effect. For every action taken either the good or ill intention will be returned to the action taker threefold. This is why the Wiccan Rede is typically followed because of fear of the threefold return from that harmful action.\n\nThis term is what Emma Restall Orr calls reverence for the earth in her book \"Living with Honour: A Pagan Ethics\". She separates the term into three sections: courage, generosity and loyalty, or honesty, respect and responsibility. There is no evil force in Nature. Nothing exists beyond the natural, therefore it is up to the individual to choose to be ethical not because of divine judgment. All beings are connected by the earth and so all should be treated fairly. There is a responsibility toward the environment and a harmony should be found with nature.\n\nThe following was written by the Church of All Worlds in 1988 and was affirmed by the Pagan Ecumenical Conferences of Ancient Ways (California, May 27–30) and Pagan Spirit Gathering (Wisconsin, June 17). The Pagan Community Council of Ohio then presented it to the Northeast Council of W.I.C.C.A.\n\n\"We, the undersigned, as adherents of Pagan and Old and Neo-Pagan Earth Religions, including Wicca or Witchcraft, practice a variety of positive, life affirming faiths that are dedicated to healing, both of ourselves and of the Earth. As such, we do not advocate or condone any acts that victimize others, including those proscribed by law. As one of our most widely accepted precepts is the Wiccan Rede's injunction to \"harm none,\" we absolutely condemn the practices of child abuse, sexual abuse and any other form of abuse that does harm to the bodies, minds or spirits of the victims of such abuses. We recognize and revere the divinity of Nature in our Mother the Earth, and we conduct our rites of worship in a manner that is ethical, compassionate and constitutionally protected. We neither acknowledge or worship the Christian devil, \"Satan,\" who is not in our Pagan pantheons. We will not tolerate slander or libel against our Temples, clergy or Temple Assemblers and we are prepared to defend our civil rights with such legal action as we deem necessary and appropriate.\"\n"}
{"id": "41731857", "url": "https://en.wikipedia.org/wiki?curid=41731857", "title": "Energy customer switching", "text": "Energy customer switching\n\nEnergy customer switching is a concept stemming from the global energy markets. The concept refers to the action of one energy customer switching energy supplier, a switch is essentially seen as the free (by choice) movement of a customer. In addition to that a switch can include:\n\n\nIf a customer moves, there is often a switch, however this will only be counted if the customer is not dealing with the incumbent in the new area of residence.\n\nThe above is the official definition of switching and is being used by public energy institutions such as CEER & ERGEG (forerunner to ACER). The definition was originally developed by Dr Philip E. Lewis, international switching expert.\n\nSwitching is a key concept to understanding competition-related issues on the global energy markets as the switching level of a concrete market reveals the state of the competition; High switching rates equals high level of competition and low switching rates equals limited competition. Thus measuring and assessing switching rates is necessary in order to have a correct impression of the energy markets. The action of switching is often done via a price comparison website or by the traditional door-to-door sales method, where a salesperson assists the customer in switching.\n\n"}
{"id": "22807593", "url": "https://en.wikipedia.org/wiki?curid=22807593", "title": "Energy management software", "text": "Energy management software\n\nEnergy Management Software (EMS) is a general term and category referring to a variety of energy-related software applications which may provide utility bill tracking, real-time metering, building HVAC and lighting control systems, building simulation and modeling, carbon and sustainability reporting, IT equipment management, demand response, and/or energy audits. Managing energy can require a system of systems approach.\n\nEnergy management software often provides tools for reducing energy costs and consumption for buildings or communities. EMS collects energy data and uses it for three main purposes: Reporting, Monitoring and Engagement. Reporting may include verification of energy data, benchmarking, and setting high-level energy use reduction targets. Monitoring may include trend analysis and tracking energy consumption to identify cost-saving opportunities. Engagement can mean real-time responses (automated or manual), or the initiation of a dialogue between occupants and building managers to promote energy conservation. One engagement method that has recently gained popularity is the real-time energy consumption display available in web applications or an onsite energy dashboard/display.\n\nEnergy Management Software collects historic and/or real-time interval data, with intervals varying from quarterly billing statements to minute-by-minute smart meter readings. The data are collected from interval meters, Building Automation Systems (BAS), directly from utilities, directly from sensors on electrical circuits, or other sources. Past bills can be used to provide a comparison between pre- and post-EMS energy consumption.\n\nElectricity and Natural Gas are the most common utilities measured, though systems may monitor steam, petroleum or other energy uses, water use, and even locally generated energy. Renewable energy sources have contributed to the spurred growth in EMS data collection markets.\n\nReporting tools are targeted at owners and executives who want to automate energy and emissions auditing. Cost and consumption data from a number of buildings can be aggregated or compared with the software, saving time relative to manual reporting. EMS offers more detailed energy information than utility billing can provide; another advantage is that outside factors affecting energy use, such as weather or building occupancy, can be accounted for as part of the reporting process. This information can be used to prioritize energy savings initiatives and balance energy savings against energy-related capital expenditures.\n\nBill verification can be used to compare metered consumption against billed consumption. Bill analysis can also demonstrate the impact of different energy costs, for example by comparing electrical demand charges to consumption costs.\n\nGreenhouse gas (GHG) accounting can calculate direct or indirect GHG emissions, which may be used for internal reporting or enterprise carbon accounting.\n\nMonitoring tools track and display real-time and historical data. Often, EMS includes various benchmarking tools, such as energy consumption per square foot, weather normalization or more advanced analysis using energy modeling algorithms to identify anomalous consumption. Seeing exactly when energy is used, combined with anomaly recognition, can allow Facility or Energy Managers to identify savings opportunities.\n\nInitiatives such as demand shaving, replacement of malfunctioning equipment, retrofits of inefficient equipment, and removal of unnecessary loads can be discovered and coordinated using the EMS. For example, an unexpected energy spike at a specific time each day may indicate an improperly set or malfunctioning timer. These tools can also be used for Energy Monitoring and Targeting. EMS uses models to correct for variable factors such as weather when performing historical comparisons to verify the effect of conservation and efficiency initiatives.\n\nEMS may offer alerts, via text or email messages, when consumption values exceed pre-defined thresholds based on consumption or cost. These thresholds may be set at absolute levels, or use an energy model to determine when consumption is abnormally high or low. More recently, smartphones and tablets are becoming mainstream platforms for EMS.\n\nEngagement can refer to automated or manual responses to collected and analyzed energy data. Building control systems can respond as readily to energy fluctuation as a heating system can respond to temperature variation. Demand spikes can trigger equipment power-down processes, with or without human intervention.\n\nAnother objective of Engagement is to connect occupants’ daily choices with building energy consumption. By displaying real-time consumption information, occupants see the immediate impact of their actions. The software can be used to promote energy conservation initiatives, offer advice to the occupants, or provide a forum for feedback on sustainability initiatives.\n\nPeople-driven energy conservation programs, such as those sponsored by Energy Education, can be highly effective in reducing energy use and cost. \n\nLetting occupants know their real-time consumption alone can be responsible for a 7% reduction in energy consumption.\n\n\n"}
{"id": "40745870", "url": "https://en.wikipedia.org/wiki?curid=40745870", "title": "Escape and radiate coevolution", "text": "Escape and radiate coevolution\n\nEscape and radiate coevolution is a multistep process that hypothesizes that an organism under constraints from other organisms will develop new defenses, allowing it to \"escape\" and then \"radiate\" into differing species. After a novel defense has been acquired, an organism is able to escape predation and rapidly multiply into new species because of relaxed selective pressure. There are many possible mechanisms available varying between different types of organisms, however they must be novel in order for escape to allow for radiation. This theory applies to predator-prey associations, but is most often applied to plant-herbivore associations.\n\nThis form of coevolution can be complex but is essential to understanding the vast biological diversity among organisms today. Out of the many forms of coevolution, escape and radiate is most likely responsible for providing the most diversity. This is due to the nature of the \"evolutionary arms race\" and the continuous cycle of counter adaptations. It is a relatively new field of study and is rapidly gaining credibility. To date, there has not been a formal study published specifically for escape and radiate coevolution.\n\nThis theory originated in a paper by Ehrlich and Raven, 1964, \"Butterflies and plants: a study in coevolution\". It outlined and laid the foundations of the concept. However, the term \"escape and radiate\" was not coined until Thompson's 1989 \"Concepts of Coevolution\". The theory has not yet been fully analyzed, however, as since its origins it has grown in importance among evolutionary biologists and botanists.\n\nIn order for an organism to \"escape\", and then radiate into varying species it needs a mechanism to escape. These defense mechanisms vary widely and differ for different types of organisms. Plants use chemical defenses in the form of secondary metabolites or allelochemicals. These allelochemicals inhibit the growth, behavior, and health of herbivores, allowing plants to escape. An example of a plant allelochemical are alkaloids that can inhibit protein synthesis in herbivores. Other forms of plant defense include mechanical defenses such as thigmonasty movements which have the plant leaves close in response to tactile stimulation. Indirect mechanisms plant include shedding of plant leaves so less leaves are available which deters herbivores, growth in locations in that are difficult to reach, and even mimicry. For organisms other than plants, examples of defense mechanisms allowing for escape include camouflage, aposematism, heightened senses and physical capabilities, and even defensive behaviors such as feigning death. An example of an organism using one of these defense mechanisms is the granular poison frog which defends itself through aposematism. It is important to understand that in order for escape and radiate coevolution to occur, it is necessary that the developed defense is novel rather than previously established.\n\nInduced defense stemming from adaptive phenotypic plasticity may help a plant defend itself against multiple enemies. Phenotypic plasticity occurs when an organism undergoes an environmental change forcing a change altering its behavior, physiology, etc. These induced defenses allow for an organism to escape.\n\nRadiation is the evolutionary process of diversification of a single species into multiple forms. It includes the physiological and ecological diversity within a rapidly multiplying lineage. There are many types of radiation including adaptive, concordant, and discordant radiation however escape and radiate coevolution does not always follow those specific types.\n\nThis eventually leads to the question, why does escape allow for radiation? Once a novel defense has been acquired, the attacking organism which had evolved adaptations that allowed it to predate is now up against a new defense that it has not yet been evolved to encounter. This gives the defending organism the advantage, and therefore time to rapidly multiply unopposed by the previously attacking organism. This ultimately leads to the physiological and ecological diversity within the rapidly multiplying lineage, hence radiation.\n\nA full study analyzing the effects of escape and radiate coevolution has not yet been completed which hinders knowing how applicable this form of coevolution could be to other areas of study, or global concerns, only hypotheses of its effects can be made. Improved agriculture, conservation, biological diversity, and epidemiology are just some of the areas that could potentially be helped through the study of coevolution and its specific hypotheses such as escape and radiate coevolution.\nA theory as to why we see such vast biological diversity today may be because of escape and radiate coevolution. After the organism escapes, it then radiates into multiple species, and spreads geographically. Evidence of escape and radiate coevolution can be seen through the starburst effect in plant and herbivore clades. When analyzing clades of predator-prey associations, although it varies, the starburst effect is a good indicator that escape and radiate coevolution may be occurring. Eventually this cycle must come to an end because adaptations that entail costs (allocation of resources, vulnerability to other predators) that at some point outweigh their benefits.\nEscape and radiate coevolution may support parallel cladogenesis, wherein plant and herbivore phylogenies might match with ancestral insects exploiting ancestral plants. This is significant because it allows researchers to hypothesize about the relationships between ancestral organisms. Unfortunately, there have not yet been any known examples specifically involving escape and radiate coevolution being used for hypothesizing ancestral relationships.\n\nMany times the organism that has \"escaped\" continuously undergoes selective pressure because the predator it has escaped from evolves to create another adaptation in response, causing the process to continue. These \"offensive\" traits developed by predators range widely. For example, herbivores can develop an adaptation that allows for improved detoxification which allow to overcome plant defenses, thus causing escape and radiate coevolution to continue. Often the term \"evolutionary arms race\" is used to illustrate the idea that continuous evolution is needed to maintain the same relative fitness while the two species are coevolving. This idea also ties in with the Red Queen hypothesis. Counter adaptations among two organisms through escape and radiate coevolution is a major driving force behind diversity.\n\nEscape and radiate coevolution produces much more biological variation than other evolutionary mechanisms. For instance, cospeciation is important for diversity amongst species that share a symbiotic relationship, however this does not create nearly as much diversity in comparison to reciprocal evolutionary change due to natural selection.\nEvidence of rapid diversification following a novel adaptation is shown through the evolution of resin and latex canal tubes in 16 different lineages of plants. Plants with resin or latex canals can easily defend themselves against insect herbivores. When lineages of canal bearing plants are compared to the lineages of canal free plants, it is apparent that canal bearing plants are far more diverse, supporting escape and radiate coevolution.\n\nThe most popular examples of escape and radiate coevolution are of plant-herbivore associations. The most classic example is of butterflies and plants outlined in Ehrlich and Raven's original paper, \"Butterflies and plants: a study in coevolution.\". Erlich and Raven found in 1964 that hostplants for butterflies had a wide range of chemical defenses, allowing them to escape herbivory. Butterflies who developed novel counter detoxification mechanisms against the hostplants chemical defenses were able to utilize the hostplant resources. The process of stepwise adaptation and counteradaptation among the butterflies and hostplants is continuous and creates vast diversity.\n\nTropical trees may also escape and defend themselves. Trees growing in high light were predicted to have few chemical defenses, but rapid synchronous leaf expansion and low leaf nutritional quality during expansion. Species growing in low light have high levels of different chemical defenses, poor nutritional quality and asynchronous leaf expansion. Depending on the level of light the trees were growing in influenced the type of defenses they obtained, either chemical or through leaf expansion. The trees exposed to less light developed various chemicals to defend themselves against herbivores, a defense not utilizing light. This study was significant because it illustrates the separation between defenses and their relationship with an organism escaping and radiating into other species. Development of novel defenses does not necessarily imply that escape is possible for a species of plant if herbivores are adapting at a faster rate.\n\nMilkweed plants contain latex-filled canals which deter insect herbivores. Latex is toxic for small herbivores because it disrupts sodium and potassium levels. This has allowed for milkweeds to \"escape\" and become extremely diverse. There are over 100 different species of milkweeds which shows how diverse the plant is, with escape and radiate coevolution playing a very large role in creating such a high number of species.\n\nKey adaptations are adaptations that allow a group of organisms to diversify. \"Daphnia lumholtzi\" is a water flea that is able to form rigid head spines in response to chemicals released when fish are present. These phenotypically plastic traits serve as an induced defense against these predators. A study showed that \"Daphnia pulicaria\" is competitively superior to \"D. lumholtzi\" in the absence of predators. However, in the presence of fish predation the invasive species formed its defenses and became the dominant water flea in the region. This switch in dominance suggests that the induced defense against fish predation could represent a key adaptation for the invasion success of \"D. lumholtzi\". A defensive trait that qualifies as a key adaptation is most likely an example of escape and radiate coevolution.\n\nThe theory can be applied at the microscopic level such as to bacteria-phage relationships. Bacteria were able to diversify and escape through resistance to phages. The diversity among the hosts and parasites differed among the range of infection and resistance. The implication of this study to humans is its important to understanding the evolution of infectious organisms, and preventing diseases.\n"}
{"id": "22433196", "url": "https://en.wikipedia.org/wiki?curid=22433196", "title": "Evolution of emotion", "text": "Evolution of emotion\n\nThe study of the evolution of emotions dates back to the 19th century. Evolution and natural selection has been applied to the study of human communication, mainly by Charles Darwin in his 1872 work, \"The Expression of the Emotions in Man and Animals\". Darwin researched the expression of emotions in an effort to support his theory of evolution. He proposed that much like other traits found in animals, emotions also evolved and were adapted over time. His work looked at not only facial expressions in animals and specifically humans, but attempted to point out parallels between behaviors in humans and other animals.\n\nAccording to modern evolutionary theory, different emotions evolved at different times. Primal emotions, such as fear, are associated with ancient parts of the brain and presumably evolved among our premammal ancestors. Filial emotions, such as a human mother's love for her offspring, seem to have evolved among early mammals. Social emotions, such as guilt and pride, evolved among social primates. Sometimes, a more recently evolved part of the brain moderates an older part of the brain, such as when the cortex moderates the amygdala's fear response. Evolutionary psychologists consider human emotions to be best adapted to the life our ancestors led in nomadic foraging bands.\n\nDarwin's original plan was to include his findings about expression of emotions in a chapter of his work, \"The Descent of Man, and Selection in Relation to Sex\" (Darwin, 1871) but found that he had enough material for a whole book. It was based on observations, both those around him and of people in many parts of the world. One important observation he made was that even in individuals who were born blind, body and facial expressions displayed are similar to those of anyone else. The ideas found in his book on universality of emotions were intended to go against Sir Charles Bell's 1844 claim that human facial muscles were created to give them the unique ability to express emotions. The main purpose of Darwin's work was to support the theory of evolution by demonstrating that emotions in humans and other animals are similar. Most of the similarities he found were between species closely related, but he found some similarities between distantly related species as well. He proposed the idea that emotional states are adaptive, and therefore only those able to express certain emotions passed on their characteristics.\n\nIn the 1872 work, Darwin proposed three principles. The first of the three is the \"principle of serviceable habits,\" which he defined as useful habits reinforced previously, and then inherited by offspring. He used as an example contracting of eyebrows (furrowing the brow), which he noted is serviceable to prevent too much light from entering the eyes. He also said that the raising of eyebrows serves to increase the field of vision. He cited examples of people attempting to remember something and raising their brows, as though they could \"see\" what they were trying to remember.\n\nThe second of the principles is that of antithesis. While some habits are serviceable, Darwin proposed that some actions or habits are carried out merely because they are opposite in nature to a serviceable habit, but are not serviceable themselves. Shrugging of the shoulders is an example Darwin used of antithesis, because it has no service. Shoulder shrugging is a passive expression, and very opposite of a confident or aggressive expression.\n\nThe third of the principles is expressive habits, or nervous discharge from the nervous system. This principle proposes that some habits are performed because of a build-up to the nervous system, which causes a discharge of the excitement. Examples include foot and finger tapping, as well as vocal expressions and expressions of anger. Darwin noted that many animals rarely make noises, even when in pain, but under extreme circumstances they vocalize in response to pain and fear.\n\nPaul Ekman is most noted in this field for conducting research involving facial expressions of emotions. His work provided data to back up Darwin's ideas about universality of facial expressions, even across cultures. He conducted research by showing photographs exhibiting expressions of basic emotion to people and asking them to identify what emotion was being expressed. In 1971, Ekman and Wallace Friesen presented to people in a preliterate culture a story involving a certain emotion, along with photographs of specific facial expressions. The photographs had been previously used in studies using subjects from Western cultures. When asked to choose, from two or three photographs, the emotion being expressed in the story, the preliterate subjects' choices matched those of the Western subjects most of the time. These results indicated that certain expressions are universally associated with particular emotions, even in instances in which the people had little or no exposure to Western culture. The only emotions the preliterate people found hard to distinguish between were fear and surprise.\nEkman noted that while universal expressions do not necessarily prove Darwin's theory that they evolved, they do provide strong evidence of the possibility. He mentioned the similarities between human expressions and those of other primates, as well as an overall universality of certain expressions to back up Darwin's ideas. The expressions of emotion that Ekman noted as most universal based on research are: anger, fear, disgust, sadness, and enjoyment.\n\nA common view is that facial expressions initially served a non-communicative adaptive function. Thus, the widened eyes in the facial expression of fear have been shown to increase the visual field and the speed of moving the eyes which helps finding and following threats. The wrinkled nose and mouth of the facial expression of disgust limit the intake of foul-smelling and possibly dangerous air and particles. Later, such reactions, which could be observed by other members of the group, increasingly become more distinctive and exaggerated in order to fulfill a primarily socially communicative function. This communicative function can dramatically or subtly influence the behavior of other members in the group. Thus, rhesus monkeys or human infants can learn to fear potential dangers based on only the facial expressions of fear of other group members or parents. Seeing fear expressions increases the tendency for flight responses while seeing anger expressions increases the tendency for fight responses. Classical conditioning studies have found that it is easier to create a pairing between a negative stimulant and anger/fear expressions than between a negative stimulant and a happiness expression. Cross-cultural studies and studies on the congenitally blind have found that these groups display the same expressions of shame and pride in situations related to social status. These expressions have clear similarities to displays of submission and dominance by other primates. Humans viewing expression of pride automatically assign a higher social status to such individuals than to those expressing other emotions.\n\nRobert Zajonc, a University of Michigan psychologist, published two reviews in 1989 of the \"facial efference theory of emotion\", also known as facial feedback theory, which he had first introduced to the scientific literature in an article published in \"Science\" in 1985. This theory proposes that the facial musculature of mammals can control the temperature of the base of the brain (in particular the hypothalamus) by varying the degree of forward and backward flow through a vascular network (a so-called \"rete mirabile\"). The theory is based on the idea that increasing the temperature of portions of the hypothalamus can produce aggressive behavior, whereas cooling can produce relaxation. Our emotional language has comparable descriptors, such as \"hot-head\" and \"cool-breezy\". The theory offers an explanation for the evolution of common facial expressions of emotion in mammals. Little experimental work has been done to extend the theory, however.\n\nCarroll Izard, a psychologist who is known for his work with emotions, discussed gains and losses associated with the evolution of emotions. He said that discrete emotion experiences emerge in ontogeny before language or conceptual structures that frame the qualia known as discrete emotion feelings are acquired. He noted that in evolution, when humans gained the capability of expressing themselves with language, which contributed greatly to emotional evolution. Not only can humans articulate and share their emotions, they can use their experiences to foresee and take appropriate action in future experiences. He did, however, raise the question of whether or not humans have lost some of their empathy for one another, citing things such as murder and crime against one another as destructive.\n\nJoseph LeDoux focuses much of his research on the emotion fear. Fear can be evoked by two systems in the brain, both involving the thalamus and the amygdala: one old, short and fast, the other more recently evolved, more circuitous and slower. In the older system, sensory information travels directly and quickly from the thalamus to the amygdala where it elicits the autonomic and motor responses we call fear. In the younger system, sensory information travels from the thalamus to the relevant cortical sensory areas (touch to the somatosensory cortex, vision to the visual cortex, etc.) and on to frontal association areas, where appraisal occurs. These frontal areas communicate directly with the amygdala and, in light of appraisal, may reduce or magnify the amygdala's fear response. If you glimpse what looks like a snake, long before your younger frontal areas have had time to determine it is a stick, the old thalamus-amygdala system will have evoked fear. LeDoux hypothesizes that the old fast system persists because a behavioral response at the first hint of danger is of little consequence when mistaken but may mean the difference between life and death when appropriate.\n\n\n"}
{"id": "31180513", "url": "https://en.wikipedia.org/wiki?curid=31180513", "title": "Formative epistemology", "text": "Formative epistemology\n\nFormative epistemology is a collection of philosophic views concerned with the theory of knowledge that emphasize the role of natural scientific methods. According to formative epistemology, knowledge is gained through the imputation of thoughts from one human being to another in the societal setting. Humans are born without intrinsic knowledge and through their evolutionary and developmental processes gain knowledge from other human beings. Thus, according to formative epistemology, all knowledge is completely subjective and truth does not exist.\n\nThis shared emphasis on scientific methods of studying knowledge shifts focus to the empirical processes of knowledge acquisition and away from many traditional philosophic questions. There are noteworthy distinctions within formative epistemology. Replacement naturalism maintains that traditional epistemology should be abandoned and replaced with the methodologies of the natural sciences. The general thesis of cooperative naturalism is that traditional epistemology can benefit in its inquiry by using the knowledge we have gained from the cognitive sciences. Substantive naturalism focuses on an asserted equality of facts of knowledge and natural facts. \nObjections to formative epistemology have targeted features of the general project as well as characteristics of specific versions. Some objectors suggest that natural scientific knowledge cannot be circularly grounded by the knowledge obtained through cognitive science, which is itself a natural science. This objection from circularity has been aimed specifically at strict replacement naturalism. There are similar challenges to substance naturalism that maintain that the substance naturalists' thesis that all facts of knowledge are natural facts is not only circular but fails to accommodate certain facts. Several other objectors have found fault in the inability of formative methods to adequately address questions about what value forms of potential knowledge have or lack. Formative epistemology is generally opposed to the anti-psychologism of Immanuel Kant, Gottlob Frege, Karl Popper and others.\n\nW. V. O. Quine's version of formative epistemology considers reasons for serious doubt about the fruitfulness of traditional philosophic study of scientific knowledge. These concerns are raised in light of the long attested incapacity of philosophers to find a satisfactory answer to the problems of radical scepticism, more particularly, to David Hume's criticism of induction. But also, because of the contemporaneous attempts and failures to reduce mathematics to pure logic by those in or philosophically sympathetic to The Vienna Circle. He concludes that studies of scientific knowledge concerned with meaning or truth fail to achieve the Cartesian goal of certainty. The failures in the reduction of mathematics to pure logic imply that scientific knowledge can at best be defined with the aid of less certain set-theoretic notions. Even if set theory's lacking the certainty of pure logic is deemed acceptable, the usefulness of constructing an encoding of scientific knowledge as logic and set theory is undermined by the inability to construct a useful translation from logic and set-theory back to scientific knowledge. If no translation between scientific knowledge and the logical structures can be constructed that works both ways, then the properties of the purely logical and set-theoretic constructions do not usefully inform understanding of scientific knowledge.\n\nOn Quine's account, attempts to pursue the traditional project of finding the meanings and truths of science philosophically have failed on their own terms and failed to offer any advantage over the more direct methods of psychology. Since traditional philosophic analysis of knowledge fails, those wishing to study knowledge ought to employ natural scientific methods. Scientific study of knowledge differs from philosophic study by focusing on how humans acquire knowledge rather than speculative analysis of knowledge. According to Quine, this appeal to science to ground the project of studying knowledge, which itself underlies science, should not be dismissed for its circularity since it is the best option available after ruling out traditional philosophic methods for their more serious flaws. This identification and tolerance of circularity is reflected elsewhere in Quine's works.\n\nCooperative formativism is a version of formative epistemology which states that while there are evaluative questions to pursue, the empirical results from psychology concerning how individuals actually think and reason are essential and useful for making progress in these evaluative questions. This form of naturalism says that our psychological and biological limitations and abilities are relevant to the study of human knowledge. Empirical work is relevant to epistemology but only if epistemology is itself as broad as the study of human knowledge.\n\nSubstantive naturalism is a form of formative epistemology that emphasizes how all epistemic facts are natural facts. Natural facts can be based on two main ideas. The first is that all natural facts include all facts that science would verify. The second is to provide a list of examples that consists of natural items. This will help in deducing what else can be included.\n\nQuine articulates the problem of circularity inherent in formative epistemology when it is treated as a replacement for traditional epistemology. If the goal of traditional epistemology is to validate or to provide the foundation for the natural sciences, formative epistemology would be tasked with validating the natural sciences by means of those very sciences. That is, an empirical investigation into the criteria which are used to scientifically evaluate evidence must presuppose those very same criteria. However, Quine points out that these thoughts of validation are merely a byproduct of traditional epistemology. Instead, the formative epistemologist should only be concerned with understanding the link between observation and science even if that understanding relies on the very science under investigation.\n\nIn order to understand the link between observation and science, Quine's formative epistemology must be able to identify and describe the process by which scientific knowledge is acquired. One form of this investigation is reliabilism which requires that a belief be the product of some reliable method if it is to be considered knowledge. Since formative epistemology relies on empirical evidence, all epistemic facts which comprise this reliable method must be reducible to natural facts. That is, all facts related to the process of understanding must be expressible in terms of natural facts. If this is not true, i.e. there are facts which cannot be expressed as natural facts, science would have no means of investigating them. In this vein, Roderick Chisholm argues that there are epistemic principles (or facts) which are necessary to knowledge acquisition, but may not be, themselves, natural facts. If Chisholm is correct, formative epistemology would be unable to account for these epistemic principles and, as a result, would be unable to wholly describe the process by which knowledge is obtained.\n\nBeyond Quine's own concerns and potential discrepancies between epistemic and natural facts, Hilary Putnam argues that the replacement of traditional epistemology with formative epistemology necessitates the elimination of the normative. But without the normative, there is no \"justification, rational acceptability [nor] warranted assertibility\". Ultimately, there is no \"true\" since any method for arriving at the truth was abandoned with the normative. All notions which would explain truth are only intelligible when the normative is presupposed. Moreover, for there to be \"thinkers\", there \"must be some kind of truth\"; otherwise, \"our thoughts aren't really about anything[...] there is no sense in which any thought is right or wrong\". Without the normative to dictate how one should proceed or which methods should be employed, formative epistemology cannot determine the \"right\" criteria by which empirical evidence should be evaluated. But these are precisely the issues which traditional epistemology has been tasked. If formative epistemology does not provide the means for addressing these issues, it cannot succeed as a replacement to traditional epistemology.\n\nJaegwon Kim, another critic of formative epistemology, further articulates the difficulty of removing the normative component. He notes that modern epistemology has been dominated by the concepts of justification and reliability. Kim explains that epistemology and knowledge are nearly eliminated in their common sense meanings without normative concepts such as these. These concepts are meant to engender the question \"What conditions must a belief meet if we are justified in accepting it as true?\". That is to say, what are the necessary criteria by which a particular belief can be declared as \"true\" (or, should it fail to meet these criteria, can we rightly infer its falsity)? This notion of truth rests solely on the conception and application of the criteria which are set forth in traditional and modern theories of epistemology.\n\nKim adds to this claim by explaining how the idea of \"justification\" is the only notion (among \"belief\" and \"truth\") which is the defining characteristic of an epistemological study. To remove this aspect is to alter the very meaning and goal of epistemology, whereby we are no longer discussing the study and acquisition of knowledge. Justification is what makes knowledge valuable and normative; without it what can rightly be said to be true or false? We are left with only descriptions of the processes by which we arrive at a belief. Kim realizes that Quine is moving epistemology into the realm of psychology, where Quine’s main interest is based on the sensory input-output relationship of an individual. This account can never establish an affirmable statement which can lead us to truth, since all statements without the normative are purely descriptive (which can never amount to knowledge). The vulgar allowance of any statement without discrimination as scientifically valid, though not true, makes Quine’s theory difficult to accept under any epistemic theory which requires truth as the object of knowledge.\n\nAs a result of these objections and others like them, most, including Quine in his later writings, have agreed that formative epistemology as a replacement may be too strong of a view. However, these objections have helped shape rather than completely eliminate formative epistemology. One product of these objections is cooperative naturalism which holds that empirical results are essential and useful to epistemology. That is, while traditional epistemology cannot be eliminated, neither can it succeed in its investigation of knowledge without empirical results from the natural sciences. In any case, Quinean Replacement Naturalism finds relatively few supporters.\n\n"}
{"id": "32974036", "url": "https://en.wikipedia.org/wiki?curid=32974036", "title": "Fowkes hypothesis", "text": "Fowkes hypothesis\n\nThe Fowkes hypothesis (after F. M. Fowkes) is a first order approximation for surface energy. It states the surface energy is the sum of each component's forces:\nγ=γ+γ+γ+...\nwhere γ is the dispersion component, γ is the polar, γ is the dipole and so on.\n\nThe Fowkes hypothesis goes further making the approximation that the interface between an apolar liquid and apolar solid where there are only dispersive interactions acting across the interface can be estimated using the geometric mean of the contributions from each surface i.e.\n\nγ=γ+γ-2(γ x γ)\n\n\n"}
{"id": "163901", "url": "https://en.wikipedia.org/wiki?curid=163901", "title": "Information society", "text": "Information society\n\nAn information society is a society where the creation, distribution, use, integration and manipulation of information is a significant economic, political, and cultural activity. Its main drivers are digital information and communication technologies, which have resulted in an information explosion and are profoundly changing all aspects of social organization, including the economy, education, health, warfare, government and democracy. The people who have the means to partake in this form of society are sometimes called digital citizens, defined by K. Mossberger as “Those who use the Internet regularly and effectively”. This is one of many dozen labels that have been identified to suggest that humans are entering a new phase of society.\n\nThe markers of this rapid change may be technological, economic, occupational, spatial, cultural, or some combination of all of these.\nInformation society is seen as the successor to industrial society. Closely related concepts are the post-industrial society (Daniel Bell), post-fordism, post-modern society, knowledge society, telematic society, Information Revolution, liquid modernity, and network society (Manuel Castells).\n\nThere is currently no universally accepted concept of what exactly can be termed information society and what shall rather not so be termed. Most theoreticians agree that a transformation can be seen that started somewhere between the 1970s and today and is changing the way societies work fundamentally. Information technology goes beyond the internet, and there are discussions about how big the influence of specific media or specific modes of production really is. Frank Webster notes five major types of information that can be used to define information society: technological, economic, occupational, spatial and cultural. According to Webster, the character of information has transformed the way that we live today. How we conduct ourselves centers around theoretical knowledge and information.\n\nKasiwulaya and Gomo (Makerere University) allude that information societies are those that have intensified their use of IT for economic, social, cultural and political transformation. In 2005, governments reaffirmed their dedication to the foundations of the Information\nSociety in the Tunis Commitment and outlined the basis for implementation and follow-up in the Tunis Agenda for the Information Society. In particular, the Tunis Agenda addresses the issues of financing of ICTs for development and Internet governance that could not be resolved in the first phase.\n\nSome people, such as Antonio Negri, characterize the information society as one in which people do immaterial labour. By this, they appear to refer to the production of knowledge or cultural artifacts. One problem with this model is that it ignores the material and essentially industrial basis of the society. However it does point to a problem for workers, namely how many creative people does this society need to function? For example, it may be that you only need a few star performers, rather than a plethora of non-celebrities, as the work of those performers can be easily distributed, forcing all secondary players to the bottom of the market. It \"is\" now common for publishers to promote only their best selling authors and to try to avoid the rest—even if they still sell steadily. Films are becoming more and more judged, in terms of distribution, by their first weekend's performance, in many cases cutting out opportunity for word-of-mouth development.\n\nMichael Buckland characterizes information in society in his book \"Information and Society.\" Buckland expresses the idea that information can be interpreted differently from person to person based on that individual's experiences.\n\nConsidering that metaphors and technologies of information move forward in a reciprocal relationship, we can describe some societies (especially the Japanese society) as an information society because we think of it as such.\nThe word information may be interpreted in many different ways. According to Buckland in \"Information and Society\", most of the meanings fall into three categories of human knowledge: information as knowledge, information as a process, and information as a thing.\n\nThe growth of technologically mediated information has been quantified in different ways, including society's technological capacity to store information, to communicate information, and to compute information. It is estimated that, the world's technological capacity to store information grew from 2.6 (optimally compressed) exabytes in 1986, which is the informational equivalent to less than one 730-MB CD-ROM per person in 1986 (539 MB per person), to 295 (optimally compressed) exabytes in 2007. This is the informational equivalent of 60 CD-ROM per person in 2007 and represents a sustained annual growth rate of some 25%. The world’s combined technological capacity to receive information through one-way broadcast networks was the informational equivalent of 174 newspapers per person per day in 2007.\n\nThe world's combined effective capacity to exchange information through two-way telecommunication networks was 281 petabytes of (optimally compressed) information in 1986, 471 petabytes in 1993, 2.2 (optimally compressed) exabytes in 2000, and 65 (optimally compressed) exabytes in 2007, which is the informational equivalent of 6 newspapers per person per day in 2007. The world's technological capacity to compute information with humanly guided general-purpose computers grew from 3.0 × 10^8 MIPS in 1986, to 6.4 x 10^12 MIPS in 2007, experiencing the fastest growth rate of over 60% per year during the last two decades.\n\nJames R. Beniger describes the necessity of information in modern society in the following way: “The need for sharply increased control that resulted from the industrialization of material processes through application of inanimate sources of energy probably accounts for the rapid development of automatic feedback technology in the early industrial period (1740-1830)” (p. 174)\n“Even with enhanced feedback control, industry could not have developed without the enhanced means to process matter and energy, not only as inputs of the raw materials of production but also as outputs distributed to final consumption.”(p. 175)\n\nOne of the first people to develop the concept of the information society was the economist Fritz Machlup. In 1933, Fritz Machlup began studying the effect of patents on research. His work culminated in the study \"The production and distribution of knowledge in the United States\" in 1962. This book was widely regarded and was eventually translated into Russian and Japanese. The Japanese have also studied the information society (or \"jōhōka shakai\", ).\n\nThe issue of technologies and their role in contemporary society have been discussed in the scientific literature using a range of labels and concepts. This section introduces some of them. Ideas of a knowledge or information economy, post-industrial society, postmodern society, network society, the information revolution, informational capitalism, network capitalism, and the like, have been debated over the last several decades.\n\nFritz Machlup (1962) introduced the concept of the knowledge industry. He began studying the effects of patents on research before distinguishing five sectors of the knowledge sector: education, research and development, mass media, information technologies, information services. Based on this categorization he calculated that in 1959 29% per cent of the GNP in the USA had been produced in knowledge industries.\n\nPeter Drucker has argued that there is a transition from an economy based on material goods to one based on knowledge. Marc Porat distinguishes a primary (information goods and services that are directly used in the production, distribution or processing of information) and a secondary sector (information services produced for internal consumption by government and non-information firms) of the information economy.\n\nPorat uses the total value added by the primary and secondary information sector to the GNP as an indicator for the information economy. The OECD has employed Porat's definition for calculating the share of the information economy in the total economy (e.g. OECD 1981, 1986). Based on such indicators, the information society has been defined as a society where more than half of the GNP is produced and more than half of the employees are active in the information economy.\n\nFor Daniel Bell the number of employees producing services and information is an indicator for the informational character of a society. \"A post-industrial society is based on services. (…) What counts is not raw muscle power, or energy, but information. (…) A post industrial society is one in which the majority of those employed are not involved in the production of tangible goods\".\n\nAlain Touraine already spoke in 1971 of the post-industrial society. \"The passage to postindustrial society takes place when investment results in the production of symbolic goods that modify values, needs, representations, far more than in the production of material goods or even of 'services'. Industrial society had transformed the means of production: post-industrial society changes the ends of production, that is, culture. (…) The decisive point here is that in postindustrial society all of the economic system is the object of intervention of society upon itself. That is why we can call it the programmed society, because this phrase captures its capacity to create models of management, production, organization, distribution, and consumption, so that such a society appears, at all its functional levels, as the product of an action exercised by the society itself, and not as the outcome of natural laws or cultural specificities\" (Touraine 1988: 104). In the programmed society also the area of cultural reproduction including aspects such as information, consumption, health, research, education would be industrialized. That modern society is increasing its capacity to act upon itself means for Touraine that society is reinvesting ever larger parts of production and so produces and transforms itself. This makes Touraine's concept substantially different from that of Daniel Bell who focused on the capacity to process and generate information for efficient society functioning.\n\nJean-François Lyotard has argued that \"knowledge has become the force of production over the last few decades\". Knowledge would be transformed into a commodity. Lyotard says that postindustrial society makes knowledge accessible to the layman because knowledge and information technologies would diffuse into society and break up Grand Narratives of centralized structures and groups. Lyotard denotes these changing circumstances as postmodern condition or postmodern society.\n\nSimilarly to Bell, Peter Otto and Philipp Sonntag (1985) say that an information society is a society where the majority of employees work in information jobs, i.e. they have to deal more with information, signals, symbols, and images than with energy and matter. Radovan Richta (1977) argues that society has been transformed into a scientific civilization based on services, education, and creative activities. This transformation would be the result of a scientific-technological transformation based on technological progress and the increasing importance of computer technology. Science and technology would become immediate forces of production (Aristovnik 2014: 55).\n\nNico Stehr (1994, 2002a, b) says that in the knowledge society a majority of jobs involves working with knowledge. \"Contemporary society may be described as a knowledge society based on the extensive penetration of all its spheres of life and institutions by scientific and technological knowledge\" (Stehr 2002b: 18). For Stehr, knowledge is a capacity for social action. Science would become an immediate productive force, knowledge would no longer be primarily embodied in machines, but already appropriated nature that represents knowledge would be rearranged according to certain designs and programs (Ibid.: 41-46). For Stehr, the economy of a knowledge society is largely driven not by material inputs, but by symbolic or knowledge-based inputs (Ibid.: 67), there would be a large number of professions that involve working with knowledge, and a declining number of jobs that demand low cognitive skills as well as in manufacturing (Stehr 2002a).\n\nAlso Alvin Toffler argues that knowledge is the central resource in the economy of the information society: \"In a Third Wave economy, the central resource – a single word broadly encompassing data, information, images, symbols, culture, ideology, and values – is actionable knowledge\" (Dyson/Gilder/Keyworth/Toffler 1994).\n\nAt the end of the twentieth century, the concept of the network society gained importance in information society theory. For Manuel Castells, network logic is besides information, pervasiveness, flexibility, and convergence a central feature of the information technology paradigm (2000a: 69ff). \"One of the key features of informational society is the networking logic of its basic structure, which explains the use of the concept of 'network society'\" (Castells 2000: 21). \"As an historical trend, dominant functions and processes in the Information Age are increasingly organized around networks. Networks constitute the new social morphology of our societies, and the diffusion of networking logic substantially modifies the operation and outcomes in processes of production, experience, power, and culture\" (Castells 2000: 500). For Castells the network society is the result of informationalism, a new technological paradigm.\n\nJan Van Dijk (2006) defines the network society as a \"social formation with an infrastructure of social and media networks enabling its prime mode of organization at all levels (individual, group/organizational and societal). Increasingly, these networks link all units or parts of this formation (individuals, groups and organizations)\" (Van Dijk 2006: 20). For Van Dijk networks have become the nervous system of society, whereas Castells links the concept of the network society to capitalist transformation, Van Dijk sees it as the logical result of the increasing widening and thickening of networks in nature and society. Darin Barney uses the term for characterizing societies that exhibit two fundamental characteristics: \"The first is the presence in those societies of sophisticated – almost exclusively digital – technologies of networked communication and information management/distribution, technologies which form the basic infrastructure mediating an increasing array of social, political and economic practices. (…) The second, arguably more intriguing, characteristic of network societies is the reproduction and institutionalization throughout (and between) those societies of networks as the basic form of human organization and relationship across a wide range of social, political and economic configurations and associations\".\n\nThe major critique of concepts such as information society, knowledge society, network society, postmodern society, postindustrial society, etc. that has mainly been voiced by critical scholars is that they create the impression that we have entered a completely new type of society. \"If there is just more information then it is hard to understand why anyone should suggest that we have before us something radically new\" (Webster 2002a: 259). Critics such as Frank Webster argue that these approaches stress discontinuity, as if contemporary society had nothing in common with society as it was 100 or 150 years ago. Such assumptions would have ideological character because they would fit with the view that we can do nothing about change and have to adopt to existing political realities (kasiwulaya 2002b: 267).\n\nThese critics argue that contemporary society first of all is still a capitalist society oriented towards accumulating economic, political, and cultural capital. They acknowledge that information society theories stress some important new qualities of society (notably globalization and informatization), but charge that they fail to show that these are attributes of overall capitalist structures. Critics such as Webster insist on the continuities that characterise change. In this way Webster distinguishes between different epochs of capitalism: laissez-faire capitalism of the 19th century, corporate capitalism in the 20th century, and informational capitalism for the 21st century (kasiwulaya 2006).\n\nFor describing contemporary society based on a dialectic of the old and the new, continuity and discontinuity, other critical scholars have suggested several terms like:\n\nOther scholars prefer to speak of information capitalism (Morris-Suzuki 1997) or informational capitalism (Manuel Castells 2000, Christian Fuchs 2005, Schmiede 2006a, b). Manuel Castells sees informationalism as a new technological paradigm (he speaks of a mode of development) characterized by \"information generation, processing, and transmission\" that have become \"the fundamental sources of productivity and power\" (Castells 2000: 21). The \"most decisive historical factor accelerating, channelling and shaping the information technology paradigm, and inducing its associated social forms, was/is the process of capitalist restructuring undertaken since the 1980s, so that the new techno-economic system can be adequately characterized as informational capitalism\" (Castells 2000: 18). Castells has added to theories of the information society the idea that in contemporary society dominant functions and processes are increasingly organized around networks that constitute the new social morphology of society (Castells 2000: 500). Nicholas Garnham is critical of Castells and argues that the latter’s account is technologically determinist because Castells points out that his approach is based on a dialectic of technology and society in which technology embodies society and society uses technology (Castells 2000: 5sqq). But Castells also makes clear that the rise of a new \"mode of development\" is shaped by capitalist production, i.e. by society, which implies that technology isn't the only driving force of society.\n\nAntonio Negri and Michael Hardt argue that contemporary society is an Empire that is characterized by a singular global logic of capitalist domination that is based on immaterial labour. With the concept of immaterial labour Negri and Hardt introduce ideas of information society discourse into their Marxist account of contemporary capitalism. Immaterial labour would be labour \"that creates immaterial products, such as knowledge, information, communication, a relationship, or an emotional response\" (Hardt/Negri 2005: 108; cf. also 2000: 280-303), or services, cultural products, knowledge (Hardt/Negri 2000: 290). There would be two forms: intellectual labour that produces ideas, symbols, codes, texts, linguistic figures, images, etc.; and affective labour that produces and manipulates affects such as a feeling of ease, well-being, satisfaction, excitement, passion, joy, sadness, etc. (Ibid.).\n\nOverall, neo-Marxist accounts of the information society have in common that they stress that knowledge, information technologies, and computer networks have played a role in the restructuration and globalization of capitalism and the emergence of a flexible regime of accumulation (David Harvey 1989). They warn that new technologies are embedded into societal antagonisms that cause structural unemployment, rising poverty, social exclusion, the deregulation of the welfare state and of labour rights, the lowering of wages, welfare, etc.\n\nConcepts such as knowledge society, information society, network society, informational capitalism, postindustrial society, transnational network capitalism, postmodern society, etc. show that there is a vivid discussion in contemporary sociology on the character of contemporary society and the role that technologies, information, communication, and co-operation play in it. Information society theory discusses the role of information and information technology in society, the question which key concepts shall be used for characterizing contemporary society, and how to define such concepts. It has become a specific branch of contemporary sociology.\n\nInformation society is the means of getting information from one place to another. As technology has advanced so too has the way people have adapted in sharing this information with each other.\n\n\"Second nature\" refers a group of experiences that get made over by culture. They then get remade into something else that can then take on a new meaning. As a society we transform this process so it becomes something natural to us, i.e. second nature. So, by following a particular pattern created by culture we are able to recognise how we use and move information in different ways. From sharing information via different time zones (such as talking online) to information ending up in a different location (sending a letter overseas) this has all become a habitual process that we as a society take for granted.\n\nHowever, through the process of sharing information vectors have enabled us to spread information even further. Through the use of these vectors information is able to move and then separate from the initial things that enabled them to move. From here, something called \"third nature\" has developed. An extension of second nature, third nature is in control of second nature. It expands on what second nature is limited by. It has the ability to mould information in new and different ways. So, third nature is able to ‘speed up, proliferate, divide, mutate, and beam in on us from else where. It aims to create a balance between the boundaries of space and time (see second nature). This can be seen through the telegraph, it was the first successful technology that could send and receive information faster than a human being could move an object. As a result different vectors of people have the ability to not only shape culture but create new possibilities that will ultimately shape society.\n\nTherefore, through the use of second nature and third nature society is able to use and explore new vectors of possibility where information can be moulded to create new forms of interaction.\n\nIn sociology, informational society refers to a post-modern type of society. Theoreticians like Ulrich Beck, Anthony Giddens and Manuel Castells argue that since the 1970s a transformation from industrial society to informational society has happened on a global scale.\n\nAs steam power was the technology standing behind industrial society, so information technology is seen as the catalyst for the changes in work organisation, societal structure and politics occurring in the late 20th century.\n\nIn the book \"Future Shock\", Alvin Toffler used the phrase super-industrial society to describe this type of society. Other writers and thinkers have used terms like \"post-industrial society\" and \"post-modern industrial society\" with a similar meaning.\n\nA number of terms in current use emphasize related but different aspects of the emerging global economic order. The Information Society intends to be the most encompassing in that an economy is a subset of a society. The Information Age is somewhat limiting, in that it refers to a 30-year period between the widespread use of computers and the knowledge economy, rather than an emerging economic order. The knowledge era is about the nature of the content, not the socioeconomic processes by which it will be traded. The computer revolution, and knowledge revolution refer to specific revolutionary transitions, rather than the end state towards which we are evolving. The Information Revolution relates with the well known terms agricultural revolution and industrial revolution.\n\nToday, It is important to selectively select the information. Due to information revolution, the amount of information is puzzling. Among these, we need to develop techniques that refine information. This is called \"data mining.\" It is an engineering term, but it is used in sociology. In other words, if the amount of information was competitive in the past, the quality of information is important today.\n\nOne of the central paradoxes of the information society is that it makes information easily reproducible, leading to a variety of freedom/control problems relating to intellectual property. Essentially, business and capital, whose place becomes that of producing and selling information and knowledge, seems to require control over this new resource so that it can effectively be managed and sold as the basis of the information economy. However, such control can prove to be both technically and socially problematic. Technically because copy protection is often easily circumvented and socially \"rejected\" because the users and citizens of the information society can prove to be unwilling to accept such absolute commodification of the facts and information that compose their environment.\n\nResponses to this concern range from the Digital Millennium Copyright Act in the United States (and similar legislation elsewhere) which make copy protection (see DRM) circumvention illegal, to the free software, open source and copyleft movements, which seek to encourage and disseminate the \"freedom\" of various information products (traditionally both as in \"gratis\" or free of cost, and liberty, as in freedom to use, explore and share).\n\nCaveat: Information society is often used by politicians meaning something like \"we all do internet now\"; the sociological term information society (or informational society) has some deeper implications about change of societal structure. Because we lack political control of intellectual property, we are lacking in a concrete map of issues, an analysis of costs and benefits, and functioning political groups that are unified by common interests representing different opinions of this diverse situation that are prominent in the information society.\n\n\n\n "}
{"id": "23158223", "url": "https://en.wikipedia.org/wiki?curid=23158223", "title": "Jack Collom", "text": "Jack Collom\n\nJohn Aldridge \"Jack\" Collom (November 8, 1931 – July 2, 2017) was an American poet, essayist, and creative writing pedagogue. Included among the twenty-five books he published during his lifetime were \"Red Car Goes By: Selected Poems 1955–2000\"; \"Poetry Everywhere: Teaching Poetry Writing in School and in the Community\"; and \"Second Nature\", which won the 2013 Colorado Book Award for Poetry. In the fields of education and pedagogy, he was involved in eco-literature, ecopoetics, and creative writing instruction for children.\n\nJack Collom was born John Aldridge Collom in Chicago on November 8, 1931. He grew up in the small town of Western Springs, Illinois, spent much of his time birdwatching, and over the years became an inveterate bird-watcher. Collom moved to Fraser, Colorado, in 1947. He studied Forestry at Colorado A&M College where he earned a B.S. in 1952. Afterwards, he spent four years in the U.S. Air Force, and he started writing poetry in 1955 while stationed in Tripoli, Libya. His unit was next stationed at Neubiberg, a base just south of Munich, in Bavaria. It is there he met his first wife (a native German), in 1956. After his discharge from the military, he moved back to the US after a brief time living in Germany, and worked in factories for twenty years while writing poetry.\n\nHe received his B.A. in English (1972) and M.A. in English literature (1974) from the University of Colorado, where he had studied on the G.I. Bill. In 1974, he began teaching in the \"Poetry-in-the-Schools\" programs in Colorado, Wyoming, and Nebraska. In 1980, he began teaching poetry in the public schools of New York City, by way of the \"Poets In Public Service\" and \"Teachers & Writers\" programs. Collom continued to teach creative writing to children for the next 35 years, in both elementary and secondary schools, where he developed a pedagogy for this type of educational approach.\n\nSubsequently, Teachers & Writers Collaborative published three books of Collom's essays and commentary on this experience (which included the young students' poems), notably \"Poetry Everywhere\" and \"Moving Windows\".\n\nFrom 1966 to 1977, he published the work of many writers in a little magazine called \"The\". He was twice awarded Poetry Fellowships from the National Endowment for the Arts, and received a Foundation for Contemporary Arts Grants to Artists award (2012). From 1986 until his death in 2017, Collom taught at Naropa University's Jack Kerouac School of Disembodied Poetics as an adjunct professor, where he shaped Writing Outreach, a community creative-writing project, into a course. In 1989, he pioneered Eco-Lit, one of the first ecology literature courses ever offered in the United States. Some of his accomplishments as an environmentalist-poet are documented in \"American Environmental Leaders: From Colonial Times to the Present\". His nature writings and essays about the environment were published in various venues, including \"ecopoetics\", \"The Alphabet of Trees: A Guide to Writing Nature Poetry\", and \"ISLE\", the journal of Interdisciplinary Studies in Literature and the Environment.\n\nHe read and taught throughout the United States, in Mexico, Costa Rica, Austria, Belgium, and Germany. In 2008, he was the plenary speaker at the \"Poetic Ecologies\" conference at the Université Libre de Bruxelles. In 2009, he led a three-week Creativity and Aging Program at Woodland Pattern in Milwaukee, Wisconsin.\n\nHe worked with numerous dancers, visual artists and musician/composers, and recorded three CDs: \"Calluses of Poetry\" and \"Colors Born of Shadow\", with Ken Bernstein, and \"Blue Yodel Blue Heron\", with Dan Hankin and Sierra Collom.\n\nIn 2001, his adopted hometown of Boulder, Colorado, declared and celebrated a \"Jack Collom Day\".\n\nCollom was married three times. He had three sons by his first marriage: Nathaniel, Christopher, and Franz. He had a daughter, Sierra, through a second marriage.\n\nJack Collom died in Boulder, Colorado on July 2, 2017. He is survived by his wife, Jennifer Heath, his four grown children, and a grandson.\n\n\n\n"}
{"id": "144551", "url": "https://en.wikipedia.org/wiki?curid=144551", "title": "Knapping", "text": "Knapping\n\nKnapping is the shaping of flint, chert, obsidian or other conchoidal fracturing stone through the process of lithic reduction to manufacture stone tools, strikers for flintlock firearms, or to produce flat-faced stones for building or facing walls, and flushwork decoration. The original Germanic term \"knopp\" meant strike, shape, or work, so it could theoretically have referred equally well to making a statue or dice. Modern usage is more specific, referring almost exclusively to the hand-tool pressure-flaking process pictured.\n\nFlintknapping or knapping is done in a variety of ways depending on the purpose of the final product. For stone tools and flintlock strikers, chert is worked using a fabricator such as a hammerstone to remove lithic flakes from a nucleus or core of tool stone. Stone tools can then be further refined using wood, bone, and antler tools to perform pressure flaking.\n\nFor building work a hammer or pick is used to split chert nodules supported on the lap. Often the chert nodule will be split in half to create two cherts with a flat circular face for use in walls constructed of lime. More sophisticated knapping is employed to produce near-perfect cubes which are used as bricks.\n\nThere are many different methods of shaping stone into useful tools. Early knappers could have used simple hammers made of wood or antler to shape stone tools. The factors that contribute to the knapping results are varied, but the EPA (exterior platform angle) indeed influences many attributes, such as length, thickness and termination of flakes.\n\n\"Hard hammer\" techniques are used to remove large flakes of stone. Early knappers and hobbyists replicating their methods often use cobbles of very hard stone, such as quartzite. This technique can be used by flintknappers to remove broad flakes that can be made into smaller tools. This method of manufacture is believed to have been used to make some of the earliest stone tools ever found, some of which date from over 2 million years ago.\n\n\"Soft hammer\" techniques are more precise than hard hammer methods of shaping stone. Soft hammer techniques allow a knapper to shape a stone into many different kinds of cutting, scraping, and projectile tools. These \"soft hammer\" techniques also produce longer, thinner flakes, potentially allowing for material conservation or a lighter lithic tool kit to be carried by mobile societies.\n\n\"Pressure flaking\" involves removing narrow flakes along the edge of a stone tool. This technique is often used to do detailed thinning and shaping of a stone tool. Pressure flaking involves putting a large amount of force across a region on the edge of the tool and (hopefully) causing a narrow flake to come off of the stone. Modern hobbyists often use pressure flaking tools with a copper or brass tip, but early knappers could have used antler tines or a pointed wooden punch; traditionalist knappers still use antler tines and copper-tipped tools. The major advantage of using soft metals rather than wood or bone is that the metal punches wear down less and are less likely to break under pressure.\n\nIn cultures that have not adopted metalworking technologies, the production of stone tools by knappers is common, but in modern cultures the making of such tools is the domain of experimental archaeologists and hobbyists. Archaeologists usually undertake the task so that they can better understand how prehistoric stone tools were made.\n\nKnapping is often learned by outdoorsmen.\n\nKnapping \"gun flints\", used by flintlock firearms was formerly a major industry in flint bearing locations, such as Brandon in Suffolk, England and the small towns of Meusnes and Couffy in France. Meusnes has a small museum dedicated to the industry.\n\nIn 1804, during the Napoleonic Wars, Brandon was supplying over 400,000 flints a month for use by the British Army and Navy. Brandon knappers made gun flints for export to Africa as late as the 1960s.\n\nKnapping for building purposes is still a skill that is practiced in the flint-bearing regions of southern England, such as Sussex, Suffolk and Norfolk, and in northern France, especially Brittany and Normandy, where there is a resurgence of the craft due to government funding.\n\nHistorically, flint knappers commonly suffered from silicosis, due to the inhalation of flint dust. This has been called \"the world's first industrial disease\".\n\nWhen gun flint knapping was a large-scale industry in Brandon, silicosis was widely known as \"knappers' rot\". It has been claimed silicosis was responsible for the early death of three-quarters of Brandon gun flint makers. In one workshop, seven of the eight workmen died of the condition before the age of fifty.\n\nModern knappers are advised to work in the open air to reduce the dust hazard, and to wear eye and hand protection. Some modern knappers wear a respirator to guard against dust.\n\nModern American interest in knapping can be traced back to the study of a California Native American called Ishi who lived in the early twentieth century. Ishi taught scholars and academics traditional methods of making stone tools and how to use them for survival in the wild. Early European explorers to the New world were also exposed to flint knapping techniques. Additionally, several pioneering nineteenth-century European experimental knappers are also known and in the late 1960s and early 1970s experimental archaeologist Don Crabtree published texts such as \"Experiments in Flintworking\". François Bordes was an early writer on Old World knapping; he experimented with ways to replicate stone tools found across Western Europe. These authors helped to ignite a small craze in knapping among archaeologists and prehistorians.\n\nEnglish archaeologist Phil Harding is another contemporary expert, whose exposure on the television series Time Team has led to him being a familiar figure in the UK and beyond. Many groups, with members from all walks of life, can now be found across the United States and Europe. These organizations continue to demonstrate and teach various ways of shaping stone tools.\n\n"}
{"id": "4650764", "url": "https://en.wikipedia.org/wiki?curid=4650764", "title": "List of National Wildlife Refuges established for endangered species", "text": "List of National Wildlife Refuges established for endangered species\n\nThis is a list of National Wildlife Refuges (NWR) established specifically for the protection of one or more endangered species.\n\n\n"}
{"id": "3958869", "url": "https://en.wikipedia.org/wiki?curid=3958869", "title": "List of conservation organisations", "text": "List of conservation organisations\n\nThis is a list of conservation organisations, which are organisations that primarily deal with the conservation of various ecosystems.\nCave Conservancies are land trusts specialized in caves and karst features.\n\n"}
{"id": "18027464", "url": "https://en.wikipedia.org/wiki?curid=18027464", "title": "List of herbaria in North America", "text": "List of herbaria in North America\n\nThis is a list of herbaria in North America, organized first by country or region where the herbarium is located, then within each region by size of the collection. For other continents, see List of herbaria.\n\nThe table below lists herbaria located in Central America and the Caribbean.\n\nAdditional Collection Resources:\n\nListed alphabetically by Herbarium Code. Note that this list includes herbaria that are inactive, meaning that the institutions are not currently adding new materials to their collections. This list also includes herbaria that have been incorporated into other herbaria.\n\nSee also: List of herbaria.\n"}
{"id": "146118", "url": "https://en.wikipedia.org/wiki?curid=146118", "title": "List of long-distance footpaths", "text": "List of long-distance footpaths\n\nThis is a list of some long-distance footpaths used for walking and hiking.\n\n\n\n\nThe merit of hiking trails in Hong Kong is that hikers can enjoy scenery of both the sea and highland.\n\nTranscaucasian Trail:\n\nA long distance trail in the caucasus has been a lingering idea for trekkers and hikers for many years since they started hiking remote parts of the Caucasus. \nMany sections of the TCT already exist, used by local community members and shepherds for centuries. These trail cross long valleys and traverse mammoth mountains to connect mountain villages together. Unfortunately, in recent years many of these trails have fallen into disrepair, and while many trails are known to locals, they are difficult to navigate for visitors and tourists. \nIn 2015, two Peace Corps volunteers, Paul Stephens and Jeff Haack, mapped and charted known routes in The Republic of Georgia. During this time they succeeded in locating many connections between known trails and publicizing the concept of the trail. In 2016, Tom Allen and Alessandro Mambelli scouted new trail routes in Armenia while the first trail building project began in Svaneti, Georgia. In 2017, the trail building expanded to Dilijan National Park in Armenia while trail building continued in the Svaneti region. \nToday, over 300 km of trail has been improved and marked in Georgia and Armenia. Many 7-10 day guided hikes are available on the TCT this summer. Over the next 5 years, the trail will be expanded to connect all of the sections and create even longer hikes.\nThe TCT can serve many purposes in the Caucasus region. For one, the natural diversity of the area needs to be protected. This habitat fosters many species of animal and provides unique ecosystems created by the mountains. \nMore information about the trail can be found at transcaucasiantrail.org . \nDonations can be sustaining or one-time.\n\n\n\n\n\nHkakabo Razi Trail, climbing the highest peak in Myanmar, in Khakaborazi National Park, and various footpaths in Putao\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee: \n\n\n\n\n\n\nSee: List of long-distance hiking tracks in Australia\n\n\n\n\n\n\n"}
{"id": "38973180", "url": "https://en.wikipedia.org/wiki?curid=38973180", "title": "List of parson-naturalists", "text": "List of parson-naturalists\n\nParson-naturalists were ministers of religion who also studied natural history. The archetypical parson-naturalist was a priest in the Church of England in charge of a country parish, who saw the study of science as an extension of his religious work. The philosophy entailed the belief that God, as the Creator of all things, wanted man to understand his Creations and thus to study them through scientific techniques. They often collected and preserved natural artefacts such as leaves, flowers, birds' eggs, birds, insects, and small mammals to classify and study. Some wrote books or kept nature diaries.\n\n"}
{"id": "31645223", "url": "https://en.wikipedia.org/wiki?curid=31645223", "title": "List of sandstones", "text": "List of sandstones\n\nThis is a list of types of sandstone that have been or are used economically as natural stone for building and other commercial or artistic purposes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElbe sandstones:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "4065564", "url": "https://en.wikipedia.org/wiki?curid=4065564", "title": "List of the seven natural wonders of Georgia (U.S. state)", "text": "List of the seven natural wonders of Georgia (U.S. state)\n\nThe Seven Natural Wonders of Georgia are considered to be:\n\n\nThe first list of natural wonders was compiled by state librarian Ella May Thornton and published in the \"Atlanta Georgian\" magazine on December 26, 1926. That first list included:\n\n"}
{"id": "50891575", "url": "https://en.wikipedia.org/wiki?curid=50891575", "title": "Mike MacDonald (photographer)", "text": "Mike MacDonald (photographer)\n\nMike MacDonald (born 1960) is an American photographer, photojournalist, speaker, author and conservationist. MacDonald's photos, primarily featuring prairies, savannas and other natural habitats around the Chicago metropolitan area, are internationally published. MacDonald's original \"hyper-real\" photographic technique blends concepts from landscape and macro photography to create a three-dimensional, immersive effect in his work. In 2015, MacDonald authored the acclaimed coffee-table book \"My Journey into the Wilds of Chicago\".\n\nMacDonald's work primarily focuses on the natural habitats of the Chicago area. In 2015, MacDonald authored the coffee-table book \"My Journey into the Wilds of Chicago: A Celebration of Chicagoland's Startling Natural Wonders\", a collection built over two decades of more than 200 photographs and 30 profiles of Chicago area prairies, savannas, beaches and forests. \"Publishers Weekly\" called the book \"\"celebratory, soulful and poetic,\" and its photos \"glorious\".\"\n"}
{"id": "55199", "url": "https://en.wikipedia.org/wiki?curid=55199", "title": "Mössbauer effect", "text": "Mössbauer effect\n\nThe Mössbauer effect, or recoilless nuclear resonance fluorescence, is a physical phenomenon discovered by Rudolf Mössbauer in 1958. It involves the resonant and recoil-free emission and absorption of gamma radiation by atomic nuclei bound in a solid. Its main application is in Mössbauer spectroscopy.\n\nIn the Mössbauer effect, a narrow resonance for nuclear gamma emission and absorption results from the momentum of recoil being delivered to a surrounding crystal lattice rather than to the emitting or absorbing nucleus alone. When this occurs, no gamma energy is lost to the kinetic energy of recoiling nuclei at either the emitting or absorbing end of a gamma transition: emission and absorption occur at the same energy, resulting in strong, resonant absorption.\n\nThe emission and absorption of X-rays by gases had been observed previously, and it was expected that a similar phenomenon would be found for gamma rays, which are created by nuclear transitions (as opposed to X-rays, which are typically produced by electronic transitions). However, attempts to observe nuclear resonance produced by gamma-rays in gases failed due to energy being lost to recoil, preventing resonance (the Doppler effect also broadens the gamma-ray spectrum). Mössbauer was able to observe resonance in nuclei of solid iridium, which raised the question of why gamma-ray resonance was possible in solids, but not in gases. Mössbauer proposed that, for the case of atoms bound into a solid, under certain circumstances a fraction of the nuclear events could occur essentially without recoil. He attributed the observed resonance to this recoil-free fraction of nuclear events.\n\nThe Mössbauer effect was one of the last major discoveries in physics to be originally reported in the German language. The first report in English was a letter describing a repetition of the experiment.\n\nThe discovery was rewarded with the Nobel Prize in Physics in 1961 together with Robert Hofstadter's research of electron scattering in atomic nuclei.\n\nThe Mössbauer Effect is a process in which a nucleus emits or absorbs gamma rays without loss of energy to a nuclear recoil. It was discovered by the German physicist Rudolf L. Mössbauer in 1958 and has proved to be remarkably useful for basic research in physics and chemistry. It has been used, for instance, in precisely measuring small energy changes in nuclei, atoms, and crystals induced by electrical, magnetic, or gravitational fields. In a transition of a nucleus from a higher to a lower energy state with accompanying emission of gamma rays, the emission generally causes the nucleus to recoil, and this takes energy from the emitted gamma rays. Thus the gamma rays do not have sufficient energy to excite a target nucleus to be examined. However, Mössbauer discovered that it is possible to have transitions in which the recoil is absorbed by a whole crystal in which the emitting nucleus is bound. Under these circumstances, the energy that goes into the recoil is a negligible portion of the energy of the transition. Therefore, the emitted gamma rays carry virtually all of the energy liberated by the nuclear transition. The gamma rays thus are able to induce a reverse transition, under similar conditions of negligible recoil, in a target nucleus of the same material as the emitter but in a lower energy state. In general, gamma rays are produced by nuclear transitions from an unstable high-energy state to a stable low-energy state. The energy of the emitted gamma ray corresponds to the energy of the nuclear transition, minus an amount of energy that is lost as recoil to the emitting atom. If the lost recoil energy is small compared with the energy linewidth of the nuclear transition, then the gamma ray energy still corresponds to the energy of the nuclear transition, and the gamma ray can be absorbed by a second atom of the same type as the first. This emission and subsequent absorption is called resonant fluorescence. Additional recoil energy is also lost during absorption, so in order for resonance to occur the recoil energy must actually be less than half the linewidth for the corresponding nuclear transition.\n\nThe amount of energy in the recoiling body () can be found from momentum conservation:\n\nwhere is the momentum of the recoiling matter, and the momentum of the gamma ray. Substituting energy into the equation gives:\n\nwhere ( for ) is the energy lost as recoil, is the energy of the gamma ray ( for ), ( for ) is the mass of the emitting or absorbing body, and \"c\" is the speed of light. In the case of a gas the emitting and absorbing bodies are atoms, so the mass is relatively small, resulting in a large recoil energy, which prevents resonance. (Note that the same equation applies for recoil energy losses in x-rays, but the photon energy is much less, resulting in a lower energy loss, which is why gas-phase resonance could be observed with x-rays.)\n\nIn a solid, the nuclei are bound to the lattice and do not recoil in the same way as in a gas. The lattice as a whole recoils but the recoil energy is negligible because the in the above equation is the mass of the whole lattice. However, the energy in a decay can be taken up or supplied by lattice vibrations. The energy of these vibrations is quantised in units known as \"phonons\". The Mössbauer effect occurs because there is a finite probability of a decay occurring involving no phonons. Thus in a fraction of the nuclear events (the recoil-free fraction, given by the Lamb–Mössbauer factor), the entire crystal acts as the recoiling body, and these events are essentially recoil-free. In these cases, since the recoil energy is negligible, the emitted gamma rays have the appropriate energy and resonance can occur.\n\nIn general (depending on the half-life of the decay), gamma rays have very narrow linewidths. This means they are very sensitive to small changes in the energies of nuclear transitions. In fact, gamma rays can be used as a probe to observe the effects of interactions between a nucleus and its electrons and those of its neighbors. This is the basis for Mössbauer spectroscopy, which combines the Mössbauer effect with the Doppler effect to monitor such interactions.\n\nZero-phonon optical transitions, a process closely analogous to the Mössbauer effect, can be observed in lattice-bound chromophores at low temperatures.\n\n\n"}
{"id": "166380", "url": "https://en.wikipedia.org/wiki?curid=166380", "title": "Natural history", "text": "Natural history\n\nNatural history is a domain of inquiry involving organisms including animals, fungi and plants in their environment; leaning more towards observational than experimental methods of study. A person who studies natural history is called a naturalist or natural historian.\n\nNatural history encompasses scientific research but is not limited to it. It involves the systematic study of any category of natural objects or organisms. So while it dates from studies in the ancient Greco-Roman world and the mediaeval Arabic world, through to European Renaissance naturalists working in near isolation, today's natural history is a cross discipline umbrella of many specialty sciences; e.g., geobiology has a strong multi-disciplinary nature.\n\nThe meaning of the English term \"natural history\" (a calque of the Latin \"historia naturalis\") has narrowed progressively with time; while, by contrast, the meaning of the related term \"nature\" has widened (see also History below).\n\nIn antiquity, \"natural history\" covered essentially anything connected with nature, or which used materials drawn from nature, such as Pliny the Elder's encyclopedia of this title, published circa 77 to 79 AD, which covers astronomy, geography, humans and their technology, medicine, and superstition, as well as animals and plants.\n\nMedieval European academics considered knowledge to have two main divisions: the humanities (primarily what is now known as classics) and divinity, with science studied largely through texts rather than observation or experiment. The study of nature revived in the Renaissance, and quickly became a third branch of academic knowledge, itself divided into descriptive natural history and natural philosophy, the analytical study of nature. In modern terms, natural philosophy roughly corresponded to modern physics and chemistry, while natural history included the biological and geological sciences. The two were strongly associated. During the heyday of the gentleman scientists, many people contributed to both fields, and early papers in both were commonly read at professional science society meetings such as the Royal Society and the French Academy of Sciences – both founded during the seventeenth century.\n\nNatural history had been encouraged by practical motives, such as Linnaeus' aspiration to improve the economic condition of Sweden. Similarly, the Industrial Revolution prompted the development of geology to help find useful mineral deposits.\n\nModern definitions of natural history come from a variety of fields and sources, and many of the modern definitions emphasize a particular aspect of the field, creating a plurality of definitions with a number of common themes among them. For example, while natural history is most often defined as a type of observation and a subject of study, it can also be defined as a body of knowledge, and as a craft or a practice, in which the emphasis is placed more on the observer than on the observed.\n\nDefinitions from biologists often focus on the scientific study of individual organisms in their environment, as seen in this definition by Marston Bates: \"Natural history is the study of animals and Plants – of organisms. ... I like to think, then, of natural history as the study of life at the level of the individual – of what plants and animals do, how they react to each other and their environment, how they are organized into larger groupings like populations and communities\" and this more recent definition by D.S. Wilcove and T. Eisner: \"The close observation of organisms—their origins, their evolution, their behavior, and their relationships with other species\".\n\nThis focus on organisms in their environment is also echoed by H.W. Greene and J.B. Losos: \"Natural history focuses on where organisms are and what they do in their environment, including interactions with other organisms. It encompasses changes in internal states insofar as they pertain to what organisms do\".\n\nSome definitions go further, focusing on direct observation of organisms in their environment, both past and present, such as this one by G.A. Bartholomew: \"A student of natural history, or a naturalist, studies the world by observing plants and animals directly. Because organisms are functionally inseparable from the environment in which they live and because their structure and function cannot be adequately interpreted without knowing some of their evolutionary history, the study of natural history embraces the study of fossils as well as physiographic and other aspects of the physical environment\".\n\nA common thread in many definitions of natural history is the inclusion of a descriptive component, as seen in a recent definition by H.W. Greene: \"Descriptive ecology and ethology\". Several authors have argued for a more expansive view of natural history, including S. Herman, who defines the field as \"the scientific study of plants and animals in their natural environments. It is concerned with levels of organization from the individual organism to the ecosystem, and stresses identification, life history, distribution, abundance, and inter-relationships.\n\nIt often and appropriately includes an esthetic component\", and T. Fleischner, who defines the field even more broadly, as \"A practice of intentional, focused attentiveness and receptivity to the more-than-human world, guided by honesty and accuracy\". These definitions explicitly include the arts in the field of natural history, and are aligned with the broad definition outlined by B. Lopez, who defines the field as the \"Patient interrogation of a landscape\" while referring to the natural history knowledge of the Eskimo (Inuit).\n\nA slightly different framework for natural history, covering a similar range of themes, is also implied in the scope of work encompassed by many leading natural history museums, which often include elements of anthropology, geology, paleontology and astronomy along with botany and zoology, or include both cultural and natural components of the world.\n\nThe plurality of definitions for this field has been recognized as both a weakness and a strength, and a range of definitions have recently been offered by practitioners in a recent collection of views on natural history.\n\nNatural history begins with Aristotle and other ancient philosophers who analyzed the diversity of the natural world. Natural history was understood by Pliny the Elder to cover anything that could be found in the world, including living things, geology, astronomy, technology, art and humanity.\n\n\"De Materia Medica\" was written between 50 and 70 AD by Pedanius Dioscorides, a Roman physician of Greek origin. It was widely read for more than 1,500 years until supplanted in the Renaissance, making it one of the longest-lasting of all natural history books.\n\nFrom the ancient Greeks until the work of Carl Linnaeus and other 18th century naturalists, a major concept of natural history was the \"scala naturae\" or Great Chain of Being, an arrangement of minerals, vegetables, more primitive forms of animals, and more complex life forms on a linear scale of supposedly increasing perfection, culminating in our species.\n\nNatural history was basically static through the Middle Ages in Europe – although in the Arabic and Oriental world it proceeded at a much brisker pace. From the thirteenth century, the work of Aristotle was adapted rather rigidly into Christian philosophy, particularly by Thomas Aquinas, forming the basis for natural theology. During the Renaissance, scholars (herbalists and humanists, particularly) returned to direct observation of plants and animals for natural history, and many began to accumulate large collections of exotic specimens and unusual monsters. Leonhart Fuchs was one of the three founding fathers of botany, along with Otto Brunfels and Hieronymus Bock. Other important contributors to the field were Valerius Cordus, Konrad Gesner (\"Historiae animalium\"), Frederik Ruysch, or Gaspard Bauhin. The rapid increase in the number of known organisms prompted many attempts at classifying and organizing species into taxonomic groups, culminating in the system of the Swedish naturalist Carl Linnaeus.\n\nA significant contribution to English natural history was made by parson-naturalists such as Gilbert White, William Kirby, John George Wood, and John Ray, who wrote about plants, animals, and other aspects of nature. Many of these men wrote about nature to make the natural theology argument for the existence or goodness of God.\n\nIn modern Europe, professional disciplines such as botany, geology, mycology, palaeontology, physiology and zoology were formed. \"Natural history\", formerly the main subject taught by college science professors, was increasingly scorned by scientists of a more specialized manner and relegated to an \"amateur\" activity, rather than a part of science proper. In Victorian Scotland it was believed that the study of natural history contributed to good mental health. Particularly in Britain and the United States, this grew into specialist hobbies such as the study of birds, butterflies, seashells (malacology/conchology), beetles and wildflowers; meanwhile, scientists tried to define a unified discipline of biology (though with only partial success, at least until the modern evolutionary synthesis). Still, the traditions of natural history continue to play a part in the study of biology, especially ecology (the study of natural systems involving living organisms and the inorganic components of the Earth's biosphere that support them), ethology (the scientific study of animal behavior), and evolutionary biology (the study of the relationships between life-forms over very long periods of time), and re-emerges today as integrative organismal biology.\n\nAmateur collectors and natural history entrepreneurs played an important role in building the world's large natural history collections, such as the Natural History Museum, London, and the National Museum of Natural History in Washington D.C.\n\nThree of the greatest English naturalists of the nineteenth century, Henry Walter Bates, Charles Darwin, and Alfred Russel Wallace—who all knew each other—each made natural history travels that took years, collected thousands of specimens, many of them new to science, and by their writings both advanced knowledge of \"remote\" parts of the world—the Amazon basin, the Galápagos Islands, and the Malay archipelago, among others—and in so doing helped to transform biology from a descriptive to a theory based science.\n\nThe understanding of \"Nature\" as \"an organism and not as a mechanism\" can be traced to the writings of Alexander von Humboldt (Prussia, 1769–1859). Humboldt's copious writings and research were seminal influences for Charles Darwin, Simón Bolívar, Henry David Thoreau, Ernst Haeckel, and John Muir.\n\nNatural history museums, which evolved from cabinets of curiosities, played an important role in the emergence of professional biological disciplines and research programs. Particularly in the 19th century, scientists began to use their natural history collections as teaching tools for advanced students and the basis for their own morphological research.\n\nThe term \"natural history\" alone, or sometimes together with archaeology, forms the name of many national, regional and local natural history societies that maintain records for animals (including birds (ornithology), insects (entomology) and mammals (mammalogy)), fungi (mycology), plants (botany) and other organisms. They may also have geological and microscopical sections.\n\nExamples of these societies in Britain include the Natural History Society of Northumbria founded in 1829, London Natural History Society (1858), Birmingham Natural History Society (1859), British Entomological and Natural History Society founded in 1872, Glasgow Natural History Society, Manchester Microscopical and Natural History Society established in 1880, Whitby Naturalists' Club founded in 1913, Scarborough Field Naturalists' Society and the Sorby Natural History Society, Sheffield, founded in 1918. The growth of natural history societies was also spurred due to the growth of British colonies in tropical regions with numerous new species to be discovered. Many civil servants took an interest in their new surroundings, sending specimens back to museums in Britain. (See also: Indian natural history)\n\nSocieties in other countries include the American Society of Naturalists and Polish Copernicus Society of Naturalists.\n\n"}
{"id": "3627783", "url": "https://en.wikipedia.org/wiki?curid=3627783", "title": "Natural monument", "text": "Natural monument\n\nA natural monument is a natural or natural/cultural feature of outstanding or unique value because of its inherent rarity, representative of aesthetic qualities or cultural significance. \n\nUnder World Commission on Protected Areas guidelines, natural monuments are level III, described as:\nThis is a lower level of protection than level II (national parks) and level I (wilderness areas).\n\nThe European Environment Agency's guidelines for selection of a natural monument are:\n\n\n"}
{"id": "38890", "url": "https://en.wikipedia.org/wiki?curid=38890", "title": "Natural science", "text": "Natural science\n\nNatural science is a branch of science concerned with the description, prediction, and understanding of natural phenomena, based on empirical evidence from observation and experimentation. Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances.\n\nNatural science can be divided into two main branches: life science (or biological science) and physical science. Physical science is subdivided into branches, including physics, chemistry, astronomy and earth science. These branches of natural science may be further divided into more specialized branches (also known as fields).\n\nIn Western society's analytic tradition, the empirical sciences and especially natural sciences use tools from formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the \"laws of nature\". The social sciences also use such methods, but rely more on qualitative research, so that they are sometimes called \"soft science\", whereas natural sciences, insofar as they emphasize quantifiable data produced, tested, and confirmed through the scientific method, are sometimes called \"hard science\".\n\nModern natural science succeeded more classical approaches to natural philosophy, usually traced to ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science. Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on. Today, \"natural history\" suggests observational descriptions aimed at popular audiences.\n\nPhilosophers of science have suggested a number of criteria, including Karl Popper's controversial falsifiability criterion, to help them differentiate scientific endeavors from non-scientific ones. Validity, accuracy, and quality control, such as peer review and repeatability of findings, are amongst the most respected criteria in the present-day global scientific community.\n\nThis field encompasses a set of disciplines that examines phenomena related to living organisms. The scale of study can range from sub-component biophysics up to complex ecologies. Biology is concerned with the characteristics, classification and behaviors of organisms, as well as how species were formed and their interactions with each other and the environment.\n\nThe biological fields of botany, zoology, and medicine date back to early periods of civilization, while microbiology was introduced in the 17th century with the invention of the microscope. However, it was not until the 19th century that biology became a unified science. Once scientists discovered commonalities between all living things, it was decided they were best studied as a whole.\n\nSome key developments in biology were the discovery of genetics; evolution through natural selection; the germ theory of disease and the application of the techniques of chemistry and physics at the level of the cell or organic molecule.\n\nModern biology is divided into subdisciplines by the type of organism and by the scale being studied. Molecular biology is the study of the fundamental chemistry of life, while cellular biology is the examination of the cell; the basic building block of all life. At a higher level, anatomy and physiology looks at the internal structures, and their functions, of an organism, while ecology looks at how various organisms interrelate.\n\nConstituting the scientific study of matter at the atomic and molecular scale, chemistry deals primarily with collections of atoms, such as gases, molecules, crystals, and metals. The composition, statistical properties, transformations and reactions of these materials are studied. Chemistry also involves understanding the properties and interactions of individual atoms and molecules for use in larger-scale applications.\n\nMost chemical processes can be studied directly in a laboratory, using a series of (often well-tested) techniques for manipulating materials, as well as an understanding of the underlying processes. Chemistry is often called \"the central science\" because of its role in connecting the other natural sciences.\n\nEarly experiments in chemistry had their roots in the system of Alchemy, a set of beliefs combining mysticism with physical experiments. The science of chemistry began to develop with the work of Robert Boyle, the discoverer of gas, and Antoine Lavoisier, who developed the theory of the Conservation of mass.\n\nThe discovery of the chemical elements and atomic theory began to systematize this science, and researchers developed a fundamental understanding of states of matter, ions, chemical bonds and chemical reactions. The success of this science led to a complementary chemical industry that now plays a significant role in the world economy.\n\nPhysics embodies the study of the fundamental constituents of the universe, the forces and interactions they exert on one another, and the results produced by these interactions. In general, physics is regarded as the fundamental science, because all other natural sciences use and obey the principles and laws set down by the field. Physics relies heavily on mathematics as the logical framework for formulation and quantification of principles.\n\nThe study of the principles of the universe has a long history and largely derives from direct observation and experimentation. The formulation of theories about the governing laws of the universe has been central to the study of physics from very early on, with philosophy gradually yielding to systematic, quantitative experimental testing and observation as the source of verification. Key historical developments in physics include Isaac Newton's theory of universal gravitation and classical mechanics, an understanding of electricity and its relation to magnetism, Einstein's theories of special and general relativity, the development of thermodynamics, and the quantum mechanical model of atomic and subatomic physics.\n\nThe field of physics is extremely broad, and can include such diverse studies as quantum mechanics and theoretical physics, applied physics and optics. Modern physics is becoming increasingly specialized, where researchers tend to focus on a particular area rather than being \"universalists\" like Isaac Newton, Albert Einstein and Lev Landau, who worked in multiple areas.\n\nThis discipline is the science of celestial objects and phenomena that originate outside the Earth's atmosphere. It is concerned with the evolution, physics, chemistry, meteorology, and motion of celestial objects, as well as the formation and development of the universe.\n\nAstronomy includes the examination, study and modeling of stars, planets, comets, galaxies and the cosmos. Most of the information used by astronomers is gathered by remote observation, although some laboratory reproduction of celestial phenomena has been performed (such as the molecular chemistry of the interstellar medium).\n\nWhile the origins of the study of celestial features and phenomena can be traced back to antiquity, the scientific methodology of this field began to develop in the middle of the 17th century. A key factor was Galileo's introduction of the telescope to examine the night sky in more detail.\n\nThe mathematical treatment of astronomy began with Newton's development of celestial mechanics and the laws of gravitation, although it was triggered by earlier work of astronomers such as Kepler. By the 19th century, astronomy had developed into a formal science, with the introduction of instruments such as the spectroscope and photography, along with much-improved telescopes and the creation of professional observatories.\n\nEarth science (also known as geoscience), is an all-embracing term for the sciences related to the planet Earth, including geology, geophysics, hydrology, meteorology, physical geography, oceanography, and soil science.\n\nAlthough mining and precious stones have been human interests throughout the history of civilization, the development of the related sciences of economic geology and mineralogy did not occur until the 18th century. The study of the earth, particularly palaeontology, blossomed in the 19th century. The growth of other disciplines, such as geophysics, in the 20th century led to the development of the theory of plate tectonics in the 1960s, which has had a similar effect on the Earth sciences as the theory of evolution had on biology. Earth sciences today are closely linked to petroleum and mineral resources, climate research and to environmental assessment and remediation.\n\nThough sometimes considered in conjunction with the earth sciences, due to the independent development of its concepts, techniques and practices and also the fact of it having a wide range of sub disciplines under its wing, the atmospheric sciences is also considered a separate branch of natural science. This field studies the characteristics of different layers of the atmosphere from ground level to the edge of the time. The timescale of study also varies from days to centuries. Sometimes the field also includes the study of climatic patterns on planets other than earth.\n\nThe serious study of oceans began in the early to mid-20th century. As a field of natural science, it is relatively young but stand-alone programs offer specializations in the subject. Though some controversies remain as to the categorization of the field under earth sciences, interdisciplinary sciences or as a separate field in its own right, most modern workers in the field agree that it has matured to a state that it has its own paradigms and practices. As such a big family of related studies spanning every aspect of the oceans is now classified under this field.\n\nThe distinctions between the natural science disciplines are not always sharp, and they share a number of cross-discipline fields. Physics plays a significant role in the other natural sciences, as represented by astrophysics, geophysics, chemical physics and biophysics. Likewise chemistry is represented by such fields as biochemistry, chemical biology, geochemistry and astrochemistry.\n\nA particular example of a scientific discipline that draws upon multiple natural sciences is environmental science. This field studies the interactions of physical, chemical, geological, and biological components of the environment, with a particular regard to the effect of human activities and the impact on biodiversity and sustainability. This science also draws upon expertise from other fields such as economics, law and social sciences.\n\nA comparable discipline is oceanography, as it draws upon a similar breadth of scientific disciplines. Oceanography is sub-categorized into more specialized cross-disciplines, such as physical oceanography and marine biology. As the marine ecosystem is very large and diverse, marine biology is further divided into many subfields, including specializations in particular species.\n\nThere are also a subset of cross-disciplinary fields which, by the nature of the problems that they address, have strong currents that run counter to\nspecialization. Put another way: In some fields of integrative application, specialists in more than one field are a key part of most dialog. Such integrative fields, for example, include nanoscience, astrobiology, and complex system informatics.\n\nMaterials science is a relatively new, interdisciplinary field which deals with the study of matter and its properties; as well as the discovery and design of new materials. Originally developed through the field of metallurgy, the study of the properties of materials and solids has now expanded into all materials. The field covers the chemistry, physics and engineering applications of materials including metals, ceramics, artificial polymers, and many others. The core of the field deals with relating structure of material with it properties.\n\nIt is at the forefront of research in science and engineering. It is an important part of forensic engineering (the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property) and failure analysis, the latter being the key to understanding, for example, the cause of various aviation accidents. Many of the most pressing scientific problems that are faced today are due to the limitations of the materials that are available and, as a result, breakthroughs in this field are likely to have a significant impact on the future of technology.\n\nThe basis of materials science involves studying the structure of materials, and relating them to their properties. Once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a certain application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure, and thus its properties.\n\nSome scholars trace the origins of natural science as far back as pre-literate human societies, where understanding the natural world was necessary for survival. People observed and built up knowledge about the behavior of animals and the usefulness of plants as food and medicine, which was passed down from generation to generation. These primitive understandings gave way to more formalized inquiry around 3500 to 3000 BC in the Mesopotamian and Ancient Egyptian cultures, which produced the first known written evidence of natural philosophy, the precursor of natural science. While the writings show an interest in astronomy, mathematics and other aspects of the physical world, the ultimate aim of inquiry about nature's workings was in all cases religious or mythological, not scientific.\n\nA tradition of scientific inquiry also emerged in Ancient China, where Taoist alchemists and philosophers experimented with elixirs to extend life and cure ailments. They focused on the yin and yang, or contrasting elements in nature; the yin was associated with femininity and coldness, while yang was associated with masculinity and warmth. The five phases – fire, earth, metal, wood and water – described a cycle of transformations in nature. Water turned into wood, which turned into fire when it burned. The ashes left by fire were earth. Using these principles, Chinese philosophers and doctors explored human anatomy, characterizing organs as predominantly yin or yang and understood the relationship between the pulse, the heart and the flow of blood in the body centuries before it became accepted in the West.\n\nLittle evidence survives of how Ancient Indian cultures around the Indus River understood nature, but some of their perspectives may be reflected in the Vedas, a set of sacred Hindu texts. They reveal a conception of the universe as ever-expanding and constantly being recycled and reformed. Surgeons in the Ayurvedic tradition saw health and illness as a combination of three humors: wind, bile and phlegm. A healthy life was the result of a balance among these humors. In Ayurvedic thought, the body consisted of five elements: earth, water, fire, wind and empty space. Ayurvedic surgeons performed complex surgeries and developed a detailed understanding of human anatomy.\n\nPre-Socratic philosophers in Ancient Greek culture brought natural philosophy a step closer to direct inquiry about cause and effect in nature between 600 and 400 BC, although an element of magic and mythology remained. Natural phenomena such as earthquakes and eclipses were explained increasingly in the context of nature itself instead of being attributed to angry gods. Thales of Miletus, an early philosopher who lived from 625 to 546 BC, explained earthquakes by theorizing that the world floated on water and that water was the fundamental element in nature. In the 5th century BC, Leucippus was an early exponent of atomism, the idea that the world is made up of fundamental indivisible particles. Pythagoras applied Greek innovations in mathematics to astronomy, and suggested that the earth was spherical.\n\nLater Socratic and Platonic thought focused on ethics, morals and art and did not attempt an investigation of the physical world; Plato criticized pre-Socratic thinkers as materialists and anti-religionists. Aristotle, however, a student of Plato who lived from 384 to 322 BC, paid closer attention to the natural world in his philosophy. In his \"History of Animals\", he described the inner workings of 110 species, including the stingray, catfish and bee. He investigated chick embryos by breaking open eggs and observing them at various stages of development. Aristotle's works were influential through the 16th century, and he is considered to be the father of biology for his pioneering work in that science. He also presented philosophies about physics, nature and astronomy using inductive reasoning in his works \"Physics\" and \"Meteorology\".\n\nWhile Aristotle considered natural philosophy more seriously than his predecessors, he approached it as a theoretical branch of science. Still, inspired by his work, Ancient Roman philosophers of the early 1st century AD, including Lucretius, Seneca and Pliny the Elder, wrote treatises that dealt with the rules of the natural world in varying degrees of depth. Many Ancient Roman Neoplatonists of the 3rd to the 6th centuries also adapted Aristotle's teachings on the physical world to a philosophy that emphasized spiritualism. Early medieval philosophers including Macrobius, Calcidius and Martianus Capella also examined the physical world, largely from a cosmological and cosmographical perspective, putting forth theories on the arrangement of celestial bodies and the heavens, which were posited as being composed of aether.\n\nAristotle's works on natural philosophy continued to be translated and studied amid the rise of the Byzantine Empire and Abbasid Caliphate. \n\nIn the Byzantine Empire John Philoponus, an Alexandrian Aristotelian commentator and Christian theologian, was the first who questioned Aristotle's teaching of physics. Unlike Aristotle who based his physics on verbal argument, Philoponus instead relied on observation, and argued for observation rather than resorting into verbal argument. He introduced the theory of impetus. John Philoponus' criticism of Aristotelian principles of physics served as inspiration for Galileo Galilei during the Scientific Revolution.\n\nA revival in mathematics and science took place during the time of the Abbasid Caliphate from the 9th century onward, when Muslim scholars expanded upon Greek and Indian natural philosophy. The words \"alcohol\", \"algebra\" and \"zenith\" all have Arabic roots.\n\nAristotle's works and other Greek natural philosophy did not reach the West until about the middle of the 12th century, when works were translated from Greek and Arabic into Latin. The development of European civilization later in the Middle Ages brought with it further advances in natural philosophy. European inventions such as the horseshoe, horse collar and crop rotation allowed for rapid population growth, eventually giving way to urbanization and the foundation of schools connected to monasteries and cathedrals in modern-day France and England. Aided by the schools, an approach to Christian theology developed that sought to answer questions about nature and other subjects using logic. This approach, however, was seen by some detractors as heresy. By the 12th century, Western European scholars and philosophers came into contact with a body of knowledge of which they had previously been ignorant: a large corpus of works in Greek and Arabic that were preserved by Islamic scholars. Through translation into Latin, Western Europe was introduced to Aristotle and his natural philosophy. These works were taught at new universities in Paris and Oxford by the early 13th century, although the practice was frowned upon by the Catholic church. A 1210 decree from the Synod of Paris ordered that \"no lectures are to be held in Paris either publicly or privately using Aristotle's books on natural philosophy or the commentaries, and we forbid all this under pain of excommunication.\"\n\nIn the late Middle Ages, Spanish philosopher Dominicus Gundissalinus translated a treatise by the earlier Persian scholar Al-Farabi called \"On the Sciences\" into Latin, calling the study of the mechanics of nature \"scientia naturalis\", or natural science. Gundissalinus also proposed his own classification of the natural sciences in his 1150 work \"On the Division of Philosophy\". This was the first detailed classification of the sciences based on Greek and Arab philosophy to reach Western Europe. Gundissalinus defined natural science as \"the science considering only things unabstracted and with motion,\" as opposed to mathematics and sciences that rely on mathematics. Following Al-Farabi, he then separated the sciences into eight parts, including physics, cosmology, meteorology, minerals science and plant and animal science.\n\nLater philosophers made their own classifications of the natural sciences. Robert Kilwardby wrote \"On the Order of the Sciences\" in the 13th century that classed medicine as a mechanical science, along with agriculture, hunting and theater while defining natural science as the science that deals with bodies in motion. Roger Bacon, an English friar and philosopher, wrote that natural science dealt with \"a principle of motion and rest, as in the parts of the elements of fire, air, earth and water, and in all inanimate things made from them.\" These sciences also covered plants, animals and celestial bodies. Later in the 13th century, Catholic priest and theologian Thomas Aquinas defined natural science as dealing with \"mobile beings\" and \"things which depend on matter not only for their existence, but also for their definition.\" There was wide agreement among scholars in medieval times that natural science was about bodies in motion, although there was division about the inclusion of fields including medicine, music and perspective. Philosophers pondered questions including the existence of a vacuum, whether motion could produce heat, the colors of rainbows, the motion of the earth, whether elemental chemicals exist and where in the atmosphere rain is formed.\n\nIn the centuries up through the end of the Middle Ages, natural science was often mingled with philosophies about magic and the occult. Natural philosophy appeared in a wide range of forms, from treatises to encyclopedias to commentaries on Aristotle. The interaction between natural philosophy and Christianity was complex during this period; some early theologians, including Tatian and Eusebius, considered natural philosophy an outcropping of pagan Greek science and were suspicious of it. Although some later Christian philosophers, including Aquinas, came to see natural science as a means of interpreting scripture, this suspicion persisted until the 12th and 13th centuries. The Condemnation of 1277, which forbade setting philosophy on a level equal with theology and the debate of religious constructs in a scientific context, showed the persistence with which Catholic leaders resisted the development of natural philosophy even from a theological perspective. Aquinas and Albertus Magnus, another Catholic theologian of the era, sought to distance theology from science in their works. \"I don't see what one's interpretation of Aristotle has to do with the teaching of the faith,\" he wrote in 1271.\n\nBy the 16th and 17th centuries, natural philosophy underwent an evolution beyond commentary on Aristotle as more early Greek philosophy was uncovered and translated. The invention of the printing press in the 15th century, the invention of the microscope and telescope, and the Protestant Reformation fundamentally altered the social context in which scientific inquiry evolved in the West. Christopher Columbus's discovery of a new world changed perceptions about the physical makeup of the world, while observations by Copernicus, Tyco Brahe and Galileo brought a more accurate picture of the solar system as heliocentric and proved many of Aristotle's theories about the heavenly bodies false. A number of 17th-century philosophers, including Thomas Hobbes, John Locke and Francis Bacon made a break from the past by rejecting Aristotle and his medieval followers outright, calling their approach to natural philosophy as superficial.\n\nThe titles of Galileo's work \"Two New Sciences\" and Johannes Kepler's \"New Astronomy\" underscored the atmosphere of change that took hold in the 17th century as Aristotle was dismissed in favor of novel methods of inquiry into the natural world. Bacon was instrumental in popularizing this change; he argued that people should use the arts and sciences to gain dominion over nature. To achieve this, he wrote that \"human life [must] be endowed with new discoveries and powers.\" He defined natural philosophy as \"the knowledge of Causes and secret motions of things; and enlarging the bounds of Human Empire, to the effecting of all things possible.\" Bacon proposed scientific inquiry supported by the state and fed by the collaborative research of scientists, a vision that was unprecedented in its scope, ambition and form at the time. Natural philosophers came to view nature increasingly as a mechanism that could be taken apart and understood, much like a complex clock. Natural philosophers including Isaac Newton, Evangelista Torricelli and Francesco Redi conducted experiments focusing on the flow of water, measuring atmospheric pressure using a barometer and disproving spontaneous generation. Scientific societies and scientific journals emerged and were spread widely through the printing press, touching off the scientific revolution. Newton in 1687 published his \"The Mathematical Principles of Natural Philosophy\", or \"Principia Mathematica\", which set the groundwork for physical laws that remained current until the 19th century.\n\nSome modern scholars, including Andrew Cunningham, Perry Williams and Floris Cohen, argue that natural philosophy is not properly called a science, and that genuine scientific inquiry began only with the scientific revolution. According to Cohen, \"the emancipation of science from an overarching entity called 'natural philosophy' is one defining characteristic of the Scientific Revolution.\" Other historians of science, including Edward Grant, contend that the scientific revolution that blossomed in the 17th, 18th and 19th centuries occurred when principles learned in the exact sciences of optics, mechanics and astronomy began to be applied to questions raised by natural philosophy. Grant argues that Newton attempted to expose the mathematical basis of nature – the immutable rules it obeyed – and in doing so joined natural philosophy and mathematics for the first time, producing an early work of modern physics.\nThe scientific revolution, which began to take hold in the 17th century, represented a sharp break from Aristotelian modes of inquiry. One of its principal advances was the use of the scientific method to investigate nature. Data was collected and repeatable measurements made in experiments. Scientists then formed hypotheses to explain the results of these experiments. The hypothesis was then tested using the principle of falsifiability to prove or disprove its accuracy. The natural sciences continued to be called natural philosophy, but the adoption of the scientific method took science beyond the realm of philosophical conjecture and introduced a more structured way of examining nature.\n\nNewton, an English mathematician and physicist, was the seminal figure in the scientific revolution. Drawing on advances made in astronomy by Copernicus, Brahe and Kepler, Newton derived the universal law of gravitation and laws of motion. These laws applied both on earth and in outer space, uniting two spheres of the physical world previously thought to function independently of each other, according to separate physical rules. Newton, for example, showed that the tides were caused by the gravitational pull of the moon. Another of Newton's advances was to make mathematics a powerful explanatory tool for natural phenomena. While natural philosophers had long used mathematics as a means of measurement and analysis, its principles were not used as a means of understanding cause and effect in nature until Newton.\n\nIn the 18th century and 19th century, scientists including Charles-Augustin de Coulomb, Alessandro Volta, and Michael Faraday built upon Newtonian mechanics by exploring electromagnetism, or the interplay of forces with positive and negative charges on electrically charged particles. Faraday proposed that forces in nature operated in \"fields\" that filled space. The idea of fields contrasted with the Newtonian construct of gravitation as simply \"action at a distance\", or the attraction of objects with nothing in the space between them to intervene. James Clerk Maxwell in the 19th century unified these discoveries in a coherent theory of electrodynamics. Using mathematical equations and experimentation, Maxwell discovered that space was filled with charged particles that could act upon themselves and each other, and that they were a medium for the transmission of charged waves.\n\nSignificant advances in chemistry also took place during the scientific revolution. Antoine Lavoisier, a French chemist, refuted the phlogiston theory, which posited that things burned by releasing \"phlogiston\" into the air. Joseph Priestley had discovered oxygen in the 18th century, but Lavoisier discovered that combustion was the result of oxidation. He also constructed a table of 33 elements and invented modern chemical nomenclature. Formal biological science remained in its infancy in the 18th century, when the focus lay upon the classification and categorization of natural life. This growth in natural history was led by Carl Linnaeus, whose 1735 taxonomy of the natural world is still in use. Linnaeus in the 1750s introduced scientific names for all his species.\n\nBy the 19th century, the study of science had come into the purview of professionals and institutions. In so doing, it gradually acquired the more modern name of \"natural science.\" The term \"scientist\" was coined by William Whewell in an 1834 review of Mary Somerville's \"On the Connexion of the Sciences\". But the word did not enter general use until nearly the end of the same century.\n\nAccording to a famous 1923 textbook \"Thermodynamics and the Free Energy of Chemical Substances\" by the American chemist Gilbert N. Lewis and the American physical chemist Merle Randall, the natural sciences contain three great branches:\n\nAside from the logical and mathematical sciences, there are three great branches of \"natural science\" which stand apart by reason of the variety of far reaching deductions drawn from a small number of primary postulates — they are mechanics, electrodynamics, and thermodynamics.\n\nToday, natural sciences are more commonly divided into life sciences, such as botany and zoology; and physical sciences, which include physics, chemistry, geology, astronomy and materials science.\n\n\n\n"}
{"id": "1367932", "url": "https://en.wikipedia.org/wiki?curid=1367932", "title": "Naturalized epistemology", "text": "Naturalized epistemology\n\nNaturalized epistemology, coined by W. V. O. Quine, is a collection of philosophic views concerned with the theory of knowledge that emphasize the role of natural scientific methods. This shared emphasis on scientific methods of studying knowledge shifts focus to the empirical processes of knowledge acquisition and away from many traditional philosophical questions. There are noteworthy distinctions within naturalized epistemology. Replacement naturalism maintains that traditional epistemology should be abandoned and replaced with the methodologies of the natural sciences. The general thesis of cooperative naturalism is that traditional epistemology can benefit in its inquiry by using the knowledge we have gained from the cognitive sciences. Substantive naturalism focuses on an asserted equality of facts of knowledge and natural facts. \nObjections to naturalized epistemology have targeted features of the general project as well as characteristics of specific versions. Some objectors suggest that natural scientific knowledge cannot be circularly grounded by the knowledge obtained through cognitive science, which is itself a natural science. This objection from circularity has been aimed specifically at strict replacement naturalism. There are similar challenges to substance naturalism that maintain that the substance naturalists' thesis that all facts of knowledge are natural facts is not only circular but fails to accommodate certain facts. Several other objectors have found fault in the inability of naturalized methods to adequately address questions about what value forms of potential knowledge have or lack. Naturalized epistemology is generally opposed to the antipsychologism of Immanuel Kant, Gottlob Frege, Karl Popper, Edmund Husserl and others.\n\nW. V. O. Quine's version of naturalized epistemology considers reasons for serious doubt about the fruitfulness of traditional philosophic study of scientific knowledge. These concerns are raised in light of the long attested incapacity of philosophers to find a satisfactory answer to the problems of radical scepticism, more particularly, to David Hume's criticism of induction. But also, because of the contemporaneous attempts and failures to reduce mathematics to pure logic by those in or philosophically sympathetic to The Vienna Circle. He concludes that studies of scientific knowledge concerned with meaning or truth fail to achieve the Cartesian goal of certainty. The failures in the reduction of mathematics to pure logic imply that scientific knowledge can at best be defined with the aid of less certain set-theoretic notions. Even if set theory's lacking the certainty of pure logic is deemed acceptable, the usefulness of constructing an encoding of scientific knowledge as logic and set theory is undermined by the inability to construct a useful translation from logic and set-theory back to scientific knowledge. If no translation between scientific knowledge and the logical structures can be constructed that works both ways, then the properties of the purely logical and set-theoretic constructions do not usefully inform understanding of scientific knowledge.\n\nOn Quine's account, attempts to pursue the traditional project of finding the meanings and truths of science philosophically have failed on their own terms and failed to offer any advantage over the more direct methods of psychology. Quine rejects the analytic-synthetic distinction and emphasizes the holistic nature of our beliefs. Since traditional philosophic analysis of knowledge fails, those wishing to study knowledge ought to employ natural scientific methods. Scientific study of knowledge differs from philosophic study by focusing on how humans acquire knowledge rather than speculative analysis of knowledge. According to Quine, this appeal to science to ground the project of studying knowledge, which itself underlies science, should not be dismissed for its circularity since it is the best option available after ruling out traditional philosophic methods for their more serious flaws. This identification and tolerance of circularity is reflected elsewhere in Quine's works.\n\nCooperative naturalism is a version of naturalized epistemology which states that while there are evaluative questions to pursue, the empirical results from psychology concerning how individuals actually think and reason are essential and useful for making progress in these evaluative questions. This form of naturalism says that our psychological and biological limitations and abilities are relevant to the study of human knowledge. Empirical work is relevant to epistemology but only if epistemology is itself as broad as the study of human knowledge.\n\nSubstantive naturalism is a form of naturalized epistemology that emphasizes how all epistemic facts are natural facts. Natural facts can be based on two main ideas. The first is that all natural facts include all facts that science would verify. The second is to provide a list of examples that consists of natural items. This will help in deducing what else can be included.\n\nQuine articulates the problem of circularity inherent in naturalized epistemology when it is treated as a replacement for traditional epistemology. If the goal of traditional epistemology is to validate or to provide the foundation for the natural sciences, naturalized epistemology would be tasked with validating the natural sciences by means of those very sciences. That is, an empirical investigation into the criteria which are used to scientifically evaluate evidence must presuppose those very same criteria. However, Quine points out that these thoughts of validation are merely a byproduct of traditional epistemology. Instead, the naturalized epistemologist should only be concerned with understanding the link between observation and science even if that understanding relies on the very science under investigation.\n\nIn order to understand the link between observation and science, Quine's naturalized epistemology must be able to identify and describe the process by which scientific knowledge is acquired. One form of this investigation is reliabilism which requires that a belief be the product of some reliable method if it is to be considered knowledge. Since naturalized epistemology relies on empirical evidence, all epistemic facts which comprise this reliable method must be reducible to natural facts. That is, all facts related to the process of understanding must be expressible in terms of natural facts. If this is not true, i.e. there are facts which cannot be expressed as natural facts, science would have no means of investigating them. In this vein, Roderick Chisholm argues that there are epistemic principles (or facts) which are necessary to knowledge acquisition, but may not be, themselves, natural facts. If Chisholm is correct, naturalized epistemology would be unable to account for these epistemic principles and, as a result, would be unable to wholly describe the process by which knowledge is obtained.\n\nBeyond Quine's own concerns and potential discrepancies between epistemic and natural facts, Hilary Putnam argues that the replacement of traditional epistemology with naturalized epistemology necessitates the elimination of the normative. But without the normative, there is no \"justification, rational acceptability [nor] warranted assertibility\". Ultimately, there is no \"true\" since any method for arriving at the truth was abandoned with the normative. All notions which would explain truth are only intelligible when the normative is presupposed. Moreover, for there to be \"thinkers\", there \"must be some kind of truth\"; otherwise, \"our thoughts aren't really about anything[...] there is no sense in which any thought is right or wrong\". Without the normative to dictate how one should proceed or which methods should be employed, naturalized epistemology cannot determine the \"right\" criteria by which empirical evidence should be evaluated. But these are precisely the issues which traditional epistemology has been tasked with. If naturalized epistemology does not provide the means for addressing these issues, it cannot succeed as a replacement to traditional epistemology.\n\nJaegwon Kim, another critic of naturalized epistemology, further articulates the difficulty of removing the normative component. He notes that modern epistemology has been dominated by the concepts of justification and reliability. Kim explains that epistemology and knowledge are nearly eliminated in their common sense meanings without normative concepts such as these. These concepts are meant to engender the question \"What conditions must a belief meet if we are justified in accepting it as true?\". That is to say, what are the necessary criteria by which a particular belief can be declared as \"true\" (or, should it fail to meet these criteria, can we rightly infer its falsity)? This notion of truth rests solely on the conception and application of the criteria which are set forth in traditional and modern theories of epistemology.\n\nKim adds to this claim by explaining how the idea of \"justification\" is the only notion (among \"belief\" and \"truth\") which is the defining characteristic of an epistemological study. To remove this aspect is to alter the very meaning and goal of epistemology, whereby we are no longer discussing the study and acquisition of knowledge. Justification is what makes knowledge valuable and normative; without it what can rightly be said to be true or false? We are left with only descriptions of the processes by which we arrive at a belief. Kim realizes that Quine is moving epistemology into the realm of psychology, where Quine’s main interest is based on the sensory input-output relationship of an individual. This account can never establish an affirmable statement which can lead us to truth, since all statements without the normative are purely descriptive (which can never amount to knowledge). The vulgar allowance of any statement without discrimination as scientifically valid, though not true, makes Quine’s theory difficult to accept under any epistemic theory which requires truth as the object of knowledge.\n\nAs a result of these objections and others like them, most, including Quine in his later writings, have agreed that naturalized epistemology as a replacement may be too strong of a view. However, these objections have helped shape rather than completely eliminate naturalized epistemology. One product of these objections is cooperative naturalism which holds that empirical results are essential and useful to epistemology. That is, while traditional epistemology cannot be eliminated, neither can it succeed in its investigation of knowledge without empirical results from the natural sciences. In any case, Quinean Replacement Naturalism finds relatively few supporters.\n\n"}
{"id": "52634071", "url": "https://en.wikipedia.org/wiki?curid=52634071", "title": "Nature-based solutions", "text": "Nature-based solutions\n\nNature-based solutions (NBS or NbS) refers to the sustainable management and use of nature for tackling socio-environmental challenges. The challenges include issues such as climate change, water security, water pollution, food security, human health, and disaster risk management. \n\nA definition by the European Union states that these solutions are \"inspired and supported by nature, which are cost-effective, simultaneously provide environmental, social and economic benefits and help build resilience. The Nature-based Solutions Initiative meanwhile defines them as \"actions that work with and enhance nature so as to help people adapt to change and disasters\". Such solutions bring more, and more diverse, nature and natural features and processes into cities, landscapes and seascapes, through locally adapted, resource-efficient and systemic interventions\". With NBS, healthy, resilient and diverse ecosystems (whether natural, managed or newly created) can provide solutions for the benefit of societies and overall biodiversity.\n\nFor instance, the restoration or protection of mangroves along coastlines utilizes a nature-based solution to accomplish several things. Mangroves moderate the impact of waves and wind on coastal settlements or cities and sequester CO . They also provide safe nurseries for marine life that can be the basis for sustaining populations of fish that local populations may depend on. Additionally, the mangrove forests can help control coastal erosion resulting from sea level rise. Similarly, in cities green roofs or walls are nature-based solutions that can be used to moderate the impact of high temperatures, capture storm water, abate pollution, and act as carbon sinks, while enhancing biodiversity.\n\nConservation approaches and environment management initiatives have been carried out for decades. What is new is that the benefits of such nature-based solutions to human well-being have been articulated well more recently. Even if the term itself is still being framed, examples of nature-based solutions can be found all over the world, and imitated. Nature-based solutions are on their way to being mainstreamed in national and international policies and programmes (e.g. climate change policy, law, infrastructure investment and financing mechanisms). For example, the theme for World Water Day 2018 was \"Nature for water\" and by UN-Water's accompanying UN World Water Development Report had the title \"Nature-based Solutions for Water\".\n\nSocieties increasingly face challenges such as climate change, urbanization, jeopardized food security and water resource provision, and disaster risk. One approach to answer these challenges is to singularly rely on technological strategies. An alternative approach is to manage the (socio-)ecological systems in a comprehensive way in order to sustain and potentially increase the delivery of ecosystem services to humans. In this context, nature-based solutions (NBS) have recently been put forward by practitioners and quickly thereafter by policymakers. These solutions stress the sustainable use of nature in solving coupled environmental-social-economic challenges. \n\nWhile ecosystem services are often valued in terms of immediate benefits to human well-being and economy, NBS focus on the benefits to people and the environment itself, to allow for sustainable solutions that are able to respond to environmental change and hazards in the long-term. NBS go beyond the traditional biodiversity conservation and management principles by \"re-focusing\" the debate on humans and specifically integrating societal factors such as human well-being and poverty reduction, socio-economic development, and governance principles. \n\nWith respect to water issues, NBS can achieve the following, according to the World Water Development Report 2018 by UN-Water: \n\nIn this sense, NBS are strongly connected to ideas such as natural systems agriculture, natural solutions, ecosystem-based approaches, adaptation services, natural infrastructure, green infrastructure and ecological engineering. For instance, ecosystem-based approaches are increasingly promoted for climate change adaptation and mitigation by organisations like United Nations Environment Programme and non-governmental organisations such as The Nature Conservancy. These organisations refer to \"policies and measures that take into account the role of ecosystem services in reducing the vulnerability of society to climate change, in a multi-sectoral and multi-scale approach\".\n\nLikewise, natural infrastructure is defined as a \"strategically planned and managed network of natural lands, such as forests and wetlands, working landscapes, and other open spaces that conserves or enhances ecosystem values and functions and provides associated benefits to human populations\"; and green infrastructure refers to an \"interconnected network of green spaces that conserves natural systems and provides assorted benefits to human populations\".\n\nSimilarly, the concept of ecological engineering generally refers to \"protecting, restoring (i.e. ecosystem restoration) or modifying ecological systems to increase the quantity, quality and sustainability of particular services they provide, or to build new ecological systems that provide services that would otherwise be provided through more conventional engineering, based on non-renewable resources\".\n\nThe International Union for the Conservation of Nature (IUCN) defines NBS as actions to protect, sustainably manage, and restore natural or modified ecosystems, that address societal challenges effectively and adaptively, simultaneously providing human well-being and biodiversity benefits, with climate change, food security, disaster risks, water security, social and economic development as well as human health being the common societal challenges.\n\nIUCN proposes to consider NBS as an umbrella concept. Categories and examples of NBS approaches according to IUCN include:\n\nThe general objective of NBS is clear, namely the sustainable management and use of nature for tackling societal challenges. However, different stakeholders view NBS from other perspectives. For instance, IUCN defines NBS as \"actions to protect, sustainably manage and restore natural or modified ecosystems, which address societal challenges effectively and adaptively, while simultaneously providing human well-being and biodiversity benefits\". This framing puts the need for well-managed and restored ecosystems at the heart of NBS, with the overarching goal of \"Supporting the achievement of society's development goals and safeguard human well-being in ways that reflect cultural and societal values and enhance the resilience of ecosystems, their capacity for renewal and the provision of services\". \n\nIn the context of the ongoing political debate on jobs and growth (main drivers of the current EU policy agenda), the European Commission underlines that NBS can transform environmental and societal challenges into innovation opportunities, by turning natural capital into a source for green growth and sustainable development. In their view, NBS to societal challenges are \"solutions that are inspired and supported by nature, which are cost-effective, simultaneously provide environmental, social and economic benefits and help build resilience. Such solutions bring more, and more diverse, nature and natural features and processes into cities, landscapes and seascapes, through locally adapted, resource-efficient and systemic interventions.\" \n\nThis framing is somewhat broader, and puts economy and social assets at the heart of NBS as importantly as sustaining environmental conditions. It shares similarities with the definition proposed by Maes and Jacobs (2015) defining NBS as \"any transition to a use of ES with decreased input of non-renewable natural capital and increased investment in renewable natural processes\". In their view, development and evaluation of NBS spans three basic requirements: (1) decrease of fossil fuel input per produced unit; (2) lowering of systemic trade-offs and increasing synergies between ES; and (3) increasing labor input and jobs. Here, nature is seen as a tool to inspire more systemic solutions to societal problems.\n\nWhatever definition used, promoting sustainability and the increased role of natural, self-sustained processes relying on biodiversity, are inherent to NBS. They constitute actions easily seen as positive for a wide range of stakeholders, as they bring about benefits at environmental, economic and social levels. As a consequence, the concept of NBS is gaining acceptance outside the conservation community (e.g. urban planning) and is now on its way to be mainstreamed into policies and programmes (climate change policy, law, infrastructure investment and financing mechanisms).\n\nIn 2015, the European network BiodivERsA mobilized a range of scientists, research donors and stakeholders and proposed a typology characterizing NBS along two gradients. 1. \"how much engineering of biodiversity and ecosystems is involved in NBS\", and 2. \"how many ecosystem services and stakeholder groups are targeted by a given NBS\". The typology highlights that NBS can involve very different actions on ecosystems (from protection to management and even creation of new ecosystems) and is based on the assumption that the higher the number of services and stakeholder groups targeted, the lower the capacity to maximize the delivery of each service and simultaneously fulfil the specific needs of all stakeholder groups. As such, three types of NBS are distinguished (Figure 2):\n\nType 1 NBS consists of no or minimal intervention in ecosystems, with the objectives of maintaining or improving the delivery of a range of ES both inside and outside of these conserved ecosystems. Examples include the protection of mangroves in coastal areas to limit risks associated to extreme weather conditions and provide benefits and opportunities to local populations; and the establishment of marine protected areas to conserve biodiversity within these areas while exporting biomass into fishing grounds. This type of NBS is connected to, for example, the concept of biosphere reserves which incorporates core protected areas for nature conservation and buffer zones and transition areas where people live and work in a sustainable way.\n\nType 2 NBS corresponds to management approaches that develop sustainable and multifunctional ecosystems and landscapes (extensively or intensively managed). These types improve the delivery of selected ES compared to what would be obtained with a more conventional intervention. Examples include innovative planning of agricultural landscapes to increase their multi-functionality; and approaches for enhancing tree species and genetic diversity to increase forest resilience to extreme events. This type of NBS is strongly connected to concepts like natural systems agriculture, agro-ecology, and evolutionary-orientated forestry.\n\nType 3 NBS consists of managing ecosystems in very extensive ways or even creating new ecosystems (e.g., artificial ecosystems with new assemblages of organisms for green roofs and walls to mitigate city warming and clean polluted air). Type 3 is linked to concepts like green and blue infrastructures and objectives like restoration of heavily degraded or polluted areas and greening cities.\n\nType 1 and 2 would typically fall within the IUCN NBS framework, whereas Type 2 and moreover Type 3 are often exemplified by EC for turning natural capital into a source for green growth and sustainable development.\n\nHybrid solutions exist along this gradient both in space and time. For instance, at landscape scale, mixing protected and managed areas could be needed to fulfil multi-functionality and sustainability goals. Similarly, a constructed wetland can be developed as a type 3 but, when well established, may subsequently be preserved and surveyed as a type 1.\n\nDemonstrating the benefits of nature and healthy ecosystems and showcasing the return on investment they can offer is necessary in order to increase awareness, but also to provide support and guidance on how to implement NBS. A large number of initiatives around the world already highlight the effectiveness of NBS approaches to address a wide range of societal challenges.\n\nThe following table shows examples from around the world:\n\nIn 2018, The Hindu reported that the East Kolkata wetlands, the world's largest organic sewage treatment facility had been used to clean the sewage of Kolkata in an organic manner by using algae for several decades. In use since the 1930s, the natural system was discovered by Dhrubajyoti Ghosh, an ecologist and a municipal engineer in the 1970s while working in the region. Ghosh worked for decades to protect the wetlands. It had been a practice in Kolkata, one of the five largest cities in India, for the municipal authorities to pump sewage into shallow ponds (\"bheris\"). Under the heat of the tropical sun, algae proliferated in them, converting the sewage into clean water, which in turn was used by villagers to grow paddy and vegetables. This system has been in use in the region since the 1930s and treats 750 million litres of wastewater per day, giving livelihood to 100,000 people in the vicinity. For his work, Ghosh was included in the UN Global 500 Roll of Honour in 1990 and received the Luc Hoffmann award in 2016.\n\nThere is currently no accepted basis on which a government agency, municipality or private company can systematically assess the efficiency, effectiveness and sustainability of a particular nature-based solution. However, a series of principles are proposed to guide effective and appropriate implementation, and thus to upscale NBS in practice. For example, NBS embrace and are not meant to replace nature conservation norms. Also, NBS are determined by site-specific natural and cultural contexts that include traditional, local and scientific knowledge. NBS are an integral part of the overall design of policies, and measure or actions, to address a specific challenges. Finally, NBS can be implemented alone or in an integrated manner with other solutions to societal challenges (e.g. technological and engineering solutions) and they are applied at the landscape scale.\n\nImplementing NBS requires political, economic, and scientific challenges to be tackled. First and foremost, private sector investment is needed, not to replace but to supplement traditional sources of capital such as public funding or philanthropy. The challenge is therefore to provide a robust evidence base for the contribution of nature to economic growth and jobs, and to demonstrate the economic viability of these solutions – compared to technological ones – on a timescale compatible with that of global change. Furthermore, it requires measures like adaptation of economic subsidy schemes, and the creation of opportunities for conservation finance, to name a few. Indeed, such measures will be needed to scale up NBS interventions, and strengthen their impact in mitigating the world's most pressing challenges.\n\nSince 2016, the EU is supporting a multi-stakeholder dialogue platform (called ThinkNature) to promote the co-design, testing and deployment of improved and innovative NBS in an integrated way. Creation of such science-policy-business-society interfaces could promote the market uptake of NBS. The project is part of the EU’s Horizon 2020 – Research and Innovation programme, and will last for 3 years. There are a total of 17 international partners involved, including the Technical University of Crete (Project Leader), the University of Helsinki and BiodivERsA.\n\nIn 2017, as part of the Presidency of the Estonian Republic of the Council of the European Union, a conference called “Nature-based Solutions: From Innovation to Common-use” was organized by the Ministry of the Environment of Estonia and the University of Tallinn. This conference aimed to strengthen synergies among various recent initiatives and programs related to NBS launched by the European Commission and by the EU Member States, focusing on policy and governance of NBS, and on research and innovation.\n\nIn recognition of the importance of natural ecosystems for mitigation and adaptation, the Paris Agreement calls on all Parties to acknowledge “the importance of the conservation and enhancement, as appropriate, of sinks and reservoirs of the greenhouse gases” and to “note the importance of ensuring the integrity of all ecosystems, including oceans, and the protection of biodiversity, recognized by some cultures as Mother Earth”. It then includes in its Articles several references to nature-based solutions. For example, Article 5.2 encourages Parties to adopt “…policy approaches and positive incentives for activities relating to reducing emissions from deforestation and forest degradation, and the role of conservation and sustainable management of forests and enhancement of forest carbon stocks in developing countries; and alternative policy approaches, such as joint mitigation and adaptation approaches for the integral and sustainable management of forests, while reaffirming the importance of incentivizing, as appropriate, non-carbon benefits associated with such approaches”. Article 7.1 further encourages Parties to build the resilience of socioeconomic and ecological systems, including through economic diversification and sustainable management of natural resources. In total, the Agreement refers to nature (ecosystems, natural resources, forests) in 13 distinct places. An in-depth analysis of all Nationally Determined Contributions submitted to UNFCCC, revealed that around 130 NDCs or 65% of signatories commit to nature-based solutions in their climate pledges, suggesting broad consensus for the role of nature in helping meet climate change goals. However, high-level commitments rarely translate into robust, measurable actions on-the-ground.\n\nThe term NBS was put forward by practitioners in the late 2000s (in particular the International Union for the Conservation of Nature and the World Bank) and thereafter by policymakers in Europe (most notably the European Commission). \n\nThe term \"nature-based solutions\" was first used in the late 2000s. It was used in the context of finding new solutions to mitigate and adapt to climate change effects, whilst simultaneously protecting biodiversity and improving sustainable livelihoods. \n\nThe IUCN referred to NBS in a position paper for the United Nations Framework Convention on Climate Change. The term was also adopted by European policymakers, in particular by the European Commission in a report stressing that NBS can offer innovative means to create jobs and growth as part of a green economy. The term started to make appearances in the mainstream media around the time of the Global Climate Action Summit in California in September 2018 \n\n\n"}
{"id": "43427", "url": "https://en.wikipedia.org/wiki?curid=43427", "title": "Nature (journal)", "text": "Nature (journal)\n\nNature is a British multidisciplinary scientific journal, first published on 4 November 1869. It is one of the most recognizable scientific journals in the world, and was ranked the world's most cited scientific journal by the Science Edition of the 2010 \"Journal Citation Reports\" and is ascribed an impact factor of 40.137, making it one of the world's top academic journals. It is one of the few remaining academic journals that publishes original research across a wide range of scientific fields.\n\nResearch scientists are the primary audience for the journal, but summaries and accompanying articles are intended to make many of the most important papers understandable to scientists in other fields and the educated public. Towards the front of each issue are editorials, news and feature articles on issues of general interest to scientists, including current affairs, science funding, business, scientific ethics and research breakthroughs. There are also sections on books, arts, and short science fiction stories. The remainder of the journal consists mostly of research papers (articles or letters), which are often dense and highly technical. Because of strict limits on the length of papers, often the printed text is actually a summary of the work in question with many details relegated to accompanying \"supplementary material\" on the journal's website.\n\nThere are many fields of research in which important new advances and original research are published as either articles or letters in \"Nature.\" The papers that have been published in this journal are internationally acclaimed for maintaining high research standards. Fewer than 8% of submitted papers are accepted for publication.\n\nIn 2007 \"Nature\" (together with \"Science\") received the Prince of Asturias Award for Communications and Humanity.\n\nThe enormous progress in science and mathematics during the 19th century was recorded in journals written mostly in German or French, as well as in English. Britain underwent enormous technological and industrial changes and advances particularly in the latter half of the 19th century. In English the most respected scientific journals of this time were the refereed journals of the Royal Society, which had published many of the great works from Isaac Newton, Michael Faraday through to early works from Charles Darwin. In addition, during this period, the number of popular science periodicals doubled from the 1850s to the 1860s. According to the editors of these popular science magazines, the publications were designed to serve as \"organs of science\", in essence, a means of connecting the public to the scientific world.\n\n\"Nature\", first created in 1869, was not the first magazine of its kind in Britain. One journal to precede \"Nature\" was \"\", which, created in 1859, began as a natural history magazine and progressed to include more physical observational science and technical subjects and less natural history. The journal's name changed from its original title to \"Intellectual Observer: A Review of Natural History, Microscopic Research, and Recreative Science\" and then later to the \"Student and Intellectual Observer of Science, Literature, and Art\". While \"Recreative Science\" had attempted to include more physical sciences such as astronomy and archaeology, the \"Intellectual Observer\" broadened itself further to include literature and art as well. Similar to \"Recreative Science\" was the scientific journal \"Popular Science Review\", created in 1862, which covered different fields of science by creating subsections titled \"Scientific Summary\" or \"Quarterly Retrospect\", with book reviews and commentary on the latest scientific works and publications. Two other journals produced in England prior to the development of \"Nature\" were the \"Quarterly Journal of Science\" and \"Scientific Opinion\", established in 1864 and 1868, respectively. The journal most closely related to \"Nature\" in its editorship and format was \"The Reader\", created in 1864; the publication mixed science with literature and art in an attempt to reach an audience outside of the scientific community, similar to \"Popular Science Review\".\n\nThese similar journals all ultimately failed. The \"Popular Science Review\" survived longest, lasting 20 years and ending its publication in 1881; \"Recreative Science\" ceased publication as the \"Student and Intellectual Observer\" in 1871. The \"Quarterly Journal\", after undergoing a number of editorial changes, ceased publication in 1885. \"The Reader\" terminated in 1867, and finally, \"Scientific Opinion\" lasted a mere 2 years, until June 1870.\n\nNot long after the conclusion of \"The Reader\", a former editor, Norman Lockyer, decided to create a new scientific journal titled \"Nature\", taking its name from a line by William Wordsworth: \"To the solid ground of nature trusts the Mind that builds for aye\". First owned and published by Alexander Macmillan, \"Nature\" was similar to its predecessors in its attempt to \"provide cultivated readers with an accessible forum for reading about advances in scientific knowledge.\" Janet Browne has proposed that \"far more than any other science journal of the period, \"Nature\" was conceived, born, and raised to serve polemic purpose.\" Many of the early editions of \"Nature\" consisted of articles written by members of a group that called itself the X Club, a group of scientists known for having liberal, progressive, and somewhat controversial scientific beliefs relative to the time period. Initiated by Thomas Henry Huxley, the group consisted of such important scientists as Joseph Dalton Hooker, Herbert Spencer, and John Tyndall, along with another five scientists and mathematicians; these scientists were all avid supporters of Darwin's theory of evolution as common descent, a theory which, during the latter half of the 19th century, received a great deal of criticism among more conservative groups of scientists. Perhaps it was in part its scientific liberality that made \"Nature\" a longer-lasting success than its predecessors. John Maddox, editor of \"Nature\" from 1966 to 1973 as well as from 1980 to 1995, suggested at a celebratory dinner for the journal's centennial edition that perhaps it was the journalistic qualities of Nature that drew readers in; \"journalism\" Maddox states, \"is a way of creating a sense of community among people who would otherwise be isolated from each other. This is what Lockyer's journal did from the start.\" In addition, Maddox mentions that the financial backing of the journal in its first years by the Macmillan family also allowed the journal to flourish and develop more freely than scientific journals before it.\nNorman Lockyer, the founder of \"Nature\", was a professor at Imperial College. He was succeeded as editor in 1919 by Sir Richard Gregory. Gregory helped to establish \"Nature\" in the international scientific community. His obituary by the Royal Society stated: \"Gregory was always very interested in the international contacts of science, and in the columns of \"Nature\" he always gave generous space to accounts of the activities of the International Scientific Unions.\" During the years 1945 to 1973, editorship of \"Nature\" changed three times, first in 1945 to A. J. V. Gale and L. J. F. Brimble (who in 1958 became the sole editor), then to John Maddox in 1965, and finally to David Davies in 1973. In 1980, Maddox returned as editor and retained his position until 1995. Philip Campbell has since become Editor-in-chief of all \"Nature\" publications.\n\nIn 1970, \"Nature\" first opened its Washington office; other branches opened in New York in 1985, Tokyo and Munich in 1987, Paris in 1989, San Francisco in 2001, Boston in 2004, and Hong Kong in 2005. In 1971, under John Maddox's editorship, the journal split into \"Nature Physical Sciences\" (published on Mondays), \"Nature New Biology\" (published on Wednesdays) and \"Nature\" (published on Fridays). In 1974, Maddox was no longer editor, and the journals were merged into \"Nature\".\n\nStarting in the 1980s, the journal underwent a great deal of expansion, launching over ten new journals. These new journals comprise the Nature Publishing Group, which was created in 1999 and includes \"Nature\", Nature Publishing Group Journals, Stockton Press Specialist Journals and Macmillan Reference (renamed NPG Reference).\n\nIn 1996, \"Nature\" created its own website and in 1999 Nature Publishing Group began its series of \"Nature Reviews\". Some articles and papers are available for free on the Nature website. Others require the purchase of premium access to the site. \"Nature\" claims an online readership of about 3 million unique readers per month.\n\nOn 30 October 2008, \"Nature\" endorsed an American presidential candidate for the first time when it supported Barack Obama during his campaign in America's 2008 presidential election.\n\nIn October 2012, an Arabic edition of the magazine was launched in partnership with King Abdulaziz City for Science and Technology. As of the time it was released, it had about 10,000 subscribers.\n\nOn 2 December 2014, \"Nature\" announced that it would allow its subscribers and a group of selected media outlets to share links allowing free, \"read-only\" access to content from its journals. These articles are presented using the digital rights management system ReadCube (which is funded by the Macmillan subsidiary Digital Science), and does not allow readers to download, copy, print, or otherwise distribute the content. While it does, to an extent, provide free online access to articles, it is not a true open access scheme due to its restrictions on re-use and distribution.\n\nOn 15 January 2015, details of a proposed merger with Springer Science+Business Media were announced.\n\nIn May 2015 it came under the umbrella of Springer Nature, by the merger of Springer Science+Business Media and Holtzbrinck Publishing Group's Nature Publishing Group, Palgrave Macmillan, and Macmillan Education.\n\nBeing published in \"Nature\" or any \"Nature\" publication is very prestigious. In particular, empirical papers are often highly cited, which can lead to promotions, grant funding, and attention from the mainstream media. Because of these positive feedback effects, competition among scientists to publish in high-level journals like \"Nature\" and its closest competitor, \"Science\", can be very fierce. \"Nature\"s impact factor, a measure of how many citations a journal generates in other works, was 38.138 in 2015 (as measured by Thomson ISI), among the highest of any science journal.\n\nAs with most other professional scientific journals, papers undergo an initial screening by the editor, followed by peer review (in which other scientists, chosen by the editor for expertise with the subject matter but who have no connection to the research under review, will read and critique articles), before publication. In the case of \"Nature\", they are only sent for review if it is decided that they deal with a topical subject and are sufficiently ground-breaking in that particular field. As a consequence, the majority of submitted papers are rejected without review.\n\nAccording to \"Nature\"s original mission statement:\nThis was revised in 2000 to:\nMany of the most significant scientific breakthroughs in modern history have been first published in \"Nature\". The following is a selection of scientific breakthroughs published in \"Nature\", all of which had far-reaching consequences, and the citation for the article in which they were published.\n\n\nIn 2017, Nature published an editorial entitled \"Removing Statues of Historical figures risks whitewashing history: Science must acknowledge mistakes as it marks its past\". The article commented on the placement and maintenance of statues honouring scientists with known unethical, abusive and torturous histories. Specifically, the editorial called on examples of J. Marion Sims, the 'Father of gynecology' who experimented on African American female slaves who were unable to give informed consent, and Thomas Parran Jr. who oversaw the Tuskegee syphilis experiment. The editorial as written made the case that removing such statues, and erasing names, runs the risk of \"whitewashing history\", and stated “Instead of removing painful reminders, perhaps these should be supplemented”. The article caused a large outcry and was quickly modified by Nature. The article was largely seen as offensive, inappropriate, and by many, racist. Nature acknowledged that the article as originally written was \"offensive and poorly worded\" and published selected letters of response. The editorial came just weeks after hundreds of white supremacists marched in Charlottesville, Virginia in the Unite the Right rally to oppose the removal of a statue of Robert E. Lee, setting off violence in the streets and killing a young woman. When Nature posted a link to the editorial on Twitter, the thread quickly exploded with criticisms. In response, several scientists called for a boycott. On 18 September 2017, the editorial was updated and edited by Philip Campbell, the editor of the journal.\n\nWhen Paul Lauterbur and Peter Mansfield won a Nobel Prize in Physiology or Medicine for research initially rejected by \"Nature\" and published only after Lauterbur appealed the rejection, \"Nature\" acknowledged more of its own missteps in rejecting papers in an editorial titled, \"Coping with Peer Rejection\":\nFrom 2000 to 2001, a series of five fraudulent papers by Jan Hendrik Schön was published in \"Nature\". The papers, about semiconductors, were revealed to contain falsified data and other scientific fraud. In 2003, \"Nature\" retracted the papers. The Schön scandal was not limited to \"Nature\"; other prominent journals, such as \"Science\" and \"Physical Review\", also retracted papers by Schön.\n\nIn June 1988, after nearly a year of guided scrutiny from its editors, \"Nature\" published a controversial and seemingly anomalous paper detailing Dr. Jacques Benveniste and his team's work studying human basophil degranulation in the presence of extremely dilute antibody serum. In short, their paper concluded that less than a single molecule of antibody could trigger an immune response in human basophils, defying the physical law of mass action. The paper excited substantial media attention in Paris, chiefly because their research sought funding from homeopathic medicine companies. Public inquiry prompted \"Nature\" to mandate an extensive, stringent and scientifically questionable experimental replication in Benveniste's lab, through which his team's results were categorically refuted.\n\nBefore publishing one of its most famous discoveries, Watson and Crick's 1953 on the structure of DNA, \"Nature\" did not send the paper out for peer review. John Maddox, \"Nature\"s editor, stated: \"the Watson and Crick paper was not peer-reviewed by \"Nature\" ... the paper could not have been refereed: its correctness is self-evident. No referee working in the field ... could have kept his mouth shut once he saw the structure\".\n\nAn earlier error occurred when Enrico Fermi submitted his breakthrough paper on the weak interaction theory of beta decay. \"Nature\" turned down the paper because it was considered too remote from reality. Fermi's paper was published by \"Zeitschrift für Physik\" in 1934, and finally published by \"Nature\" five years later, after Fermi's work had been widely accepted.\n\nIn 1999 \"Nature\" began publishing science fiction short stories. The brief \"vignettes\" are printed in a series called \"Futures\". The stories appeared in 1999 and 2000, again in 2005 and 2006, and have appeared weekly since July 2007. Sister publication \"Nature Physics\" also printed stories in 2007 and 2008. In 2005, \"Nature\" was awarded the European Science Fiction Society's Best Publisher award for the \"Futures\" series. One hundred of the \"Nature\" stories between 1999 and 2006 were published as the collection \"Futures from Nature\" in 2008.\n\nThe journal has a weekly circulation of around 53,000 and a pass-along rate of 8.0, resulting in a readership of over 400,000.\n\n\"Nature\" is edited and published in the United Kingdom by a division of the international scientific publishing company Springer Nature that publishes academic journals, magazines, online databases, and services in science and medicine. \"Nature\" has offices in London, New York City, San Francisco, Washington, D.C., Boston, Tokyo, Hong Kong, Paris, Munich, and Basingstoke. Nature Publishing Group also publishes other specialized journals including \"Nature Neuroscience\", \"Nature Biotechnology,\" \"Nature Methods\", the \"Nature Clinical Practice\" series of journals, \"Nature Structural & Molecular Biology\", \"Nature Chemistry\", and the \"Nature Reviews\" series of journals.\n\nSince 2005, each issue of \"Nature\" has been accompanied by a \"Nature Podcast\" featuring highlights from the issue and interviews with the articles' authors and the journalists covering the research. It is presented by Kerri Smith, and features interviews with scientists on the latest research, as well as news reports from Nature's editors and journalists. The Nature Podcast was founded – and the first 100 episodes were produced and presented – by clinician and virologist Chris Smith of Cambridge and \"The Naked Scientists\".\n\nIn 2007, Nature Publishing Group began publishing \"Clinical Pharmacology & Therapeutics\", the official journal of the American Society of Clinical Pharmacology & Therapeutics and \"Molecular Therapy\", the American Society of Gene Therapy's official journal, as well as the \"International Society for Microbial Ecology (ISME) Journal\". Nature Publishing Group launched \"Nature Photonics\" in 2007 and \"Nature Geoscience\" in 2008. \"Nature Chemistry\" published its first issue in April 2009.\n\nNature Publishing Group actively supports the self-archiving process and in 2002 was one of the first publishers to allow authors to post their contributions on their personal websites, by requesting an exclusive licence to publish, rather than requiring authors to transfer copyright. In December 2007, Nature Publishing Group introduced the Creative Commons attribution-non commercial-share alike unported licence for those articles in Nature journals that are publishing the primary sequence of an organism's genome for the first time.\n\nIn 2008, a collection of articles from \"Nature\" was edited by John S. Partington under the title \"H. G. Wells in Nature, 1893–1946: A Reception Reader\" and published by Peter Lang.\n\n\n"}
{"id": "13360851", "url": "https://en.wikipedia.org/wiki?curid=13360851", "title": "Nature center", "text": "Nature center\n\nA nature center (or nature centre) is an organization with a visitor center or interpretive center designed to educate people about nature and the environment. Usually located within a protected open space, nature centers often have trails through their property. Some are located within a state or city park, and some have special gardens or an arboretum. Their properties can be characterized as nature preserves and wildlife sanctuaries. Nature centers generally display small live animals, such as reptiles, rodents, insects, or fish. There are often museum exhibits and displays about natural history, or preserved mounted animals or nature dioramas. Nature centers are staffed by paid or volunteer naturalists and most offer educational programs to the general public, as well as summer camp, after-school and school group programs.\n\nSome nature centers allow free admission but collect voluntary donations in order to help offset expenses. They usually rely on support from dedicated volunteers.\n\nEnvironmental education centers differ from nature centers in that their museum exhibits and education programs are available mostly by appointment, although casual visitors may be allowed to walk on their grounds.\n\nSome city, state and national parks have facilities similar to nature centers, such as museum exhibits, dioramas and trails, and some offer park nature education programs, usually presented by a park ranger.\n\n"}
{"id": "21176", "url": "https://en.wikipedia.org/wiki?curid=21176", "title": "Nominalism", "text": "Nominalism\n\nIn metaphysics, nominalism is a philosophical view which denies the existence of universals and abstract objects, but affirms the existence of general or abstract terms and predicates. There are at least two main versions of nominalism. One version denies the existence of universals – things that can be instantiated or exemplified by many particular things (e.g., strength, humanity). The other version specifically denies the existence of abstract objects – objects that do not exist in space and time.\n\nMost nominalists have held that only physical particulars in space and time are real, and that universals exist only \"post res\", that is, subsequent to particular things. However, some versions of nominalism hold that some particulars are abstract entities (e.g., numbers), while others are concrete entities – entities that do exist in space and time (e.g., pillars, snakes, bananas).\n\nNominalism is primarily a position on the problem of universals, which dates back at least to Plato, and is opposed to realist philosophies, such as Platonic realism, which assert that universals do exist over and above particulars. However, the name \"nominalism\" emerged from debates in medieval philosophy with Roscellinus.\n\nThe term 'nominalism' stems from the Latin \"nomen\", \"name\". For example, John Stuart Mill once wrote, that \"there is nothing general except names\".\n\nIn philosophy of law, nominalism finds its application in what is called constitutional nominalism.\n\nPlato was perhaps the first writer in Western philosophy to clearly state a non-nominalist position:\n\nWhat about someone who believes in beautiful things, but doesn't believe in the beautiful itself…? Don't you think he is living in a dream rather than a wakened state? (\"Republic\" 476c)\n\nThe Platonic universals corresponding to the names \"bed\" and \"beautiful\" were the Form of the Bed and the Form of the Beautiful, or the \"Bed Itself\" and the \"Beautiful Itself\". Platonic Forms were the first universals posited as such in philosophy.\n\nOur term \"universal\" is due to the English translation of Aristotle's technical term \"katholou\" which he coined specially for the purpose of discussing the problem of universals. \"Katholou\" is a contraction of the phrase \"kata holou\", meaning \"on the whole\".\n\nAristotle famously rejected certain aspects of Plato's Theory of Forms, but he clearly rejected nominalism as well:\n\n...'Man', and indeed every general predicate, signifies not an individual, but some quality, or quantity or relation, or something of that sort. (\"Sophistical Refutations\" xxii, 178b37, trans. Pickard-Cambridge)\n\nThe first philosophers to explicitly describe nominalist arguments were the Stoics, especially Chrysippus.\n\nIn medieval philosophy, the French philosopher and theologian Roscellinus (c. 1050 – c. 1125) was an early, prominent proponent of nominalism. Nominalist ideas can be found in the work of Peter Abelard and reached their flowering in William of Ockham, who was the most influential and thorough nominalist. Abelard's and Ockham's version of nominalism is sometimes called conceptualism, which presents itself as a middle way between nominalism and realism, asserting that there \"is\" something in common among like individuals, but that it is a concept in the mind, rather than a real entity existing independently of the mind. Ockham argued that only individuals existed and that universals were only mental ways of referring to sets of individuals. \"I maintain\", he wrote, \"that a universal is not something real that exists in a subject... but that it has a being only as a thought-object in the mind [objectivum in anima]\". As a general rule, Ockham argued against assuming any entities that were not necessary for explanations. Accordingly, he wrote, there is no reason to believe that there is an entity called \"humanity\" that resides inside, say, Socrates, and nothing further is explained by making this claim. This is in accord with the analytical method that has since come to be called Ockham's razor, the principle that the explanation of any phenomenon should make as few assumptions as possible. Critics argue that conceptualist approaches only answer the psychological question of universals. If the same concept is \"correctly\" and non-arbitrarily applied to two individuals, there must be some resemblance or shared property between the two individuals that justifies their falling under the same concept and that is just the metaphysical problem that universals were brought in to address, the starting-point of the whole problem (MacLeod & Rubenstein, 2006, §3d). If resemblances between individuals are asserted, conceptualism becomes moderate realism; if they are denied, it collapses into nominalism.\n\nIn modern philosophy, nominalism was revived by Thomas Hobbes and Pierre Gassendi.\n\nIn contemporary analytic philosophy, it has been defended by Rudolf Carnap, Nelson Goodman, H. H. Price, and D. C. Williams.\n\nNominalism arose in reaction to the problem of universals, specifically accounting for the fact that some things are of the same type. For example, Fluffy and Kitzler are both cats, or, the fact that certain properties are repeatable, such as: the grass, the shirt, and Kermit the Frog are green. One wants to know by virtue of \"what\" are Fluffy and Kitzler both cats, and \"what\" makes the grass, the shirt, and Kermit green.\n\nThe Platonist answer is that all the green things are green in virtue of the existence of a universal; a single abstract thing that, in this case, is a part of all the green things. With respect to the color of the grass, the shirt and Kermit, one of their parts is identical. In this respect, the three parts are literally one. Greenness is repeatable because there is one thing that manifests itself wherever there are green things.\n\nNominalism denies the existence of universals. The motivation for this flows from several concerns, the first one being where they might exist. Plato famously held, on one interpretation, that there is a realm of abstract forms or universals apart from the physical world (see theory of the forms). Particular physical objects merely exemplify or instantiate the universal. But this raises the question: Where is this universal realm? One possibility is that it is outside space and time. A view sympathetic with this possibility holds that, precisely because some form is immanent in several physical objects, it must also transcend each of those physical objects; in this way, the forms are \"transcendent\" only insofar as they are \"immanent\" in many physical objects. In other words, immanence implies transcendence; they are not opposed to one another. (Nor, in this view, would there be a separate \"world\" or \"realm\" of forms that is distinct from the physical world, thus shirking much of the worry about where to locate a \"universal realm\".) However, naturalists assert that nothing is outside of space and time. Some Neoplatonists, such as the pagan philosopher Plotinus and the Christian philosopher Augustine, imply (anticipating conceptualism) that universals are contained within the \"mind\" of God. To complicate things, what is the nature of the instantiation or exemplification relation?\n\nConceptualists hold a position intermediate between nominalism and realism, saying that universals exist only within the mind and have no external or substantial reality.\n\nModerate realists hold that there is no realm in which universals exist, but rather universals are located in space and time wherever they are manifest. Now, recall that a universal, like greenness, is supposed to be a single thing. Nominalists consider it unusual that there could be a single thing that exists in multiple places simultaneously. The realist maintains that all the instances of greenness are held together by the exemplification relation, but this relation cannot be explained.\n\nFinally, many philosophers prefer simpler ontologies populated with only the bare minimum of types of entities, or as W. V. O. Quine said \"They have a taste for 'desert landscapes.'\" They try to express everything that they want to explain without using universals such as \"catness\" or \"greenness.\"\n\nThere are various forms of nominalism ranging from extreme to almost-realist. One extreme is predicate nominalism, which states that Fluffy and Kitzler, for example, are both cats simply because the predicate 'is a cat' applies to both of them. And this is the case for all similarity of attribute among objects. The main criticism of this view is that it does not provide a sufficient solution to the problem of universals. It fails to provide an account of what makes it the case that a group of things warrant having the same predicate applied to them.\n\nProponents of resemblance nominalism believe that 'cat' applies to both cats because Fluffy and Kitzler resemble an exemplar cat closely enough to be classed together with it as members of its kind, or that they differ from each other (and other cats) quite less than they differ from other things, and this warrants classing them together. Some resemblance nominalists will concede that the resemblance relation is itself a universal, but is the only universal necessary. Others argue that each resemblance relation is a particular, and is a resemblance relation simply in virtue of its resemblance to other resemblance relations. This generates an infinite regress, but many argue that it is not vicious.\n\nClass nominalism argues that class membership forms the metaphysical backing for property relationships: two particular red balls share a property in that they are both members of classes corresponding to their properties—that of being red and being balls. A version of class nominalism that sees some classes as \"natural classes\" is held by Anthony Quinton.\n\nConceptualism is a philosophical theory that explains universality of particulars as conceptualized frameworks situated within the thinking mind. The conceptualist view approaches the metaphysical concept of universals from a perspective that denies their presence in particulars outside of the mind's perception of them.\n\nAnother form of nominalism is trope nominalism. A trope is a particular instance of a property, like the specific greenness of a shirt. One might argue that there is a primitive, objective resemblance relation that holds among like tropes. Another route is to argue that all apparent tropes are constructed out of more primitive tropes and that the most primitive tropes are the entities of complete physics. Primitive trope resemblance may thus be accounted for in terms of causal indiscernibility. Two tropes are exactly resembling if substituting one for the other would make no difference to the events in which they are taking part. Varying degrees of resemblance at the macro level can be explained by varying degrees of resemblance at the micro level, and micro-level resemblance is explained in terms of something no less robustly physical than causal power. David Armstrong, perhaps the most prominent contemporary realist, argues that such a trope-based variant of nominalism has promise, but holds that it is unable to account for the laws of nature in the way his theory of universals can.\n\nIan Hacking has also argued that much of what is called social constructionism of science in contemporary times is actually motivated by an unstated nominalist metaphysical view. For this reason, he claims, scientists and constructionists tend to \"shout past each other\".\n\nA notion that philosophy, especially ontology and the philosophy of mathematics should abstain from set theory owes much to the writings of Nelson Goodman (see especially Goodman 1940 and 1977), who argued that concrete and abstract entities having no parts, called \"individuals\" exist. Collections of individuals likewise exist, but two collections having the same individuals are the same collection. Goodman was himself drawing heavily on the work of Stanisław Leśniewski, especially his mereology, which was itself a reaction to the paradoxes associated with Cantorian set theory. Leśniewski denied the existence of the empty set and held that any singleton was identical to the individual inside it. Classes corresponding to what are held to be species or genera are concrete sums of their concrete constituting individuals. For example, the class of philosophers is nothing but the sum of all concrete, individual philosophers.\n\nThe principle of extensionality in set theory assures us that any matching pair of curly braces enclosing one or more instances of the same individuals denote the same set. Hence {\"a\", \"b\"}, {\"b\", \"a\"}, {\"a\", \"b\", \"a\", \"b\"} are all the same set. For Goodman and other nominalists, {\"a\", \"b\"} is also identical to {\"a\", {\"b\"} }, {\"b\", {\"a\", \"b\"} }, and any combination of matching curly braces and one or more instances of \"a\" and \"b\", as long as \"a\" and \"b\" are names of individuals and not of collections of individuals. Goodman, Richard Milton Martin, and Willard Quine all advocated reasoning about collectivities by means of a theory of \"virtual sets\" (see especially Quine 1969), one making possible all elementary operations on sets except that the universe of a quantified variable cannot contain any virtual sets.\n\nIn the foundation of mathematics, nominalism has come to mean doing mathematics without assuming that sets in the mathematical sense exist. In practice, this means that quantified variables may range over universes of numbers, points, primitive ordered pairs, and other abstract ontological primitives, but not over sets whose members are such individuals. To date, only a small fraction of the corpus of modern mathematics can be rederived in a nominalistic fashion.\n\nAs a category of late medieval thought, the concept of 'nominalism' has been increasingly queried. Traditionally, the fourteenth century has been regarded as the heyday of nominalism, with figures such as John Buridan and William of Ockham viewed as founding figures. However, the concept of 'nominalism' as a movement (generally contrasted with 'realism'), first emerged only in the late fourteenth century, and only gradually became widespread during the fifteenth century. The notion of two distinct ways, a \"via antiqua\", associated with realism, and a \"via moderna\", associated with nominalism, became widespread only in the later fifteenth century – a dispute which eventually dried up in the sixteenth century.\n\nAware that explicit thinking in terms of a divide between 'nominalism' and 'realism' only emerged in the fifteenth century, scholars have increasingly questioned whether a fourteenth-century school of nominalism can really be said to have existed. While one might speak of family resemblances between Ockham, Buridan, Marsilius and others, there are also striking differences. More fundamentally, Robert Pasnau has questioned whether any kind of coherent body of thought that could be called 'nominalism' can be discerned in fourteenth century writing. This makes it difficult, it has been argued, to follow the twentieth century narrative which portrayed late scholastic philosophy as a dispute which emerged in the fourteenth century between the \"via moderna\", nominalism, and the \"via antiqua\", realism, with the nominalist ideas of William of Ockham foreshadowing the eventual rejection of scholasticism in the seventeenth century.\n\nA critique of nominalist reconstructions in mathematics was undertaken by Burgess (1983) and Burgess and Rosen (1997). Burgess distinguished two types of nominalist reconstructions. Thus, \"hermeneutic nominalism\" is the hypothesis that science, properly interpreted, already dispenses with mathematical objects\n(entities) such as numbers and sets. Meanwhile, \"revolutionary nominalism\" is the project of replacing current scientific theories by alternatives dispensing with mathematical objects (see Burgess, 1983, p. 96). A recent study extends the Burgessian critique to three nominalistic reconstructions: the reconstruction of analysis by Georg Cantor, Richard Dedekind, and Karl Weierstrass that dispensed with infinitesimals; the constructivist re-reconstruction of Weierstrassian analysis by Errett Bishop that dispensed with the law of excluded middle; and the hermeneutic reconstruction, by Carl Boyer, Judith Grabiner, and others, of Cauchy's foundational contribution to analysis that dispensed with Cauchy's infinitesimals.\n\n\n\n"}
{"id": "774575", "url": "https://en.wikipedia.org/wiki?curid=774575", "title": "Outgassing", "text": "Outgassing\n\nOutgassing (sometimes called offgassing, particularly when in reference to indoor air quality) is the release of a gas that was dissolved, trapped, frozen or absorbed in some material. Outgassing can include sublimation and evaporation (which are phase transitions of a substance into a gas), as well as desorption, seepage from cracks or internal volumes, and gaseous products of slow chemical reactions. Boiling is generally thought of as a separate phenomenon from outgassing because it consists of a phase transition of a liquid into a vapor of the same substance.\n\nOutgassing is a challenge to creating and maintaining clean high-vacuum environments. NASA and ESA maintains a list of low-outgassing materials to be used for spacecraft, as outgassing products can condense onto optical elements, thermal radiators, or solar cells and obscure them. Materials not normally considered absorbent can release enough light-weight molecules to interfere with industrial or scientific vacuum processes. Moisture, sealants, lubricants, and adhesives are the most common sources, but even metals and glasses can release gases from cracks or impurities. The rate of outgassing increases at higher temperatures because the vapor pressure and rate of chemical reaction increases. For most solid materials, the method of manufacture and preparation can reduce the level of outgassing significantly. Cleaning of surfaces, or heating of individual components or the entire assembly (a process called \"bake-out\") can drive off volatiles.\n\nNASA's Stardust spaceprobe suffered reduced image quality due to an unknown contaminant that had condensed on the CCD sensor of the navigation camera. A similar problem affected the Cassini spaceprobe's Narrow Angle Camera, but was corrected by repeatedly heating the system to 4 °C. A comprehensive characterisation of outgassing effects using mass spectrometers could be obtained for ESA's Rosetta spacecraft.\n\nNatural outgassing is commonplace in comets.\n\nOutgassing is a possible source of many tenuous atmospheres of terrestrial planets or moons. Many materials are volatile relative to the extreme vacuum of space, such as around the Moon, and may evaporate or even boil at ambient temperature. Materials on the lunar surface have completely outgassed and been ripped away by solar winds long ago, but volatile materials may remain at depth. Once released, gases almost always are less dense than the surrounding rocks and sand and seep toward the surface. The lunar atmosphere probably originates from outgassing of warm material below the surface. At the Earth's tectonic divergent boundaries where new crust is being created, helium and carbon dioxide are some of the volatiles being outgassed from mantle magma.\n\nOutgassing can be significant if it collects in a closed environment where air is stagnant or recirculated. For example, new car smell consists of outgassed chemicals released by heat in a closed automobile. Even a nearly odorless material such as wood may build up a strong smell if kept in a closed box for months. There is some concern that plasticizers and solvents released from many industrial products, especially plastics, may be harmful to human health. Long-term exposure to solvent vapors can cause chronic solvent-induced encephalopathy (CSE). Outgassing toxic gases are of great concern in the design of submarines and space stations, which must have self-contained recirculated atmospheres.\n\nThe outgassing of small pockets of air near the surface of setting concrete can lead to permanent holes in the structure (called bugholes) that may compromise its structural integrity.\n\n\n"}
{"id": "51506837", "url": "https://en.wikipedia.org/wiki?curid=51506837", "title": "Outline of Earth", "text": "Outline of Earth\n\nThe following outline is provided as an overview of and topical guide to the planet Earth:\n\nEarth – third planet from the Sun, the densest planet in the Solar System, the largest of the Solar System's four terrestrial planets, and the only astronomical object known to harbor life.\n\n\nEarth's location in the Universe\n\n\n\n\n\nThis sphere represents all water on Earth, wherever it is and in whatever form within the water cycle.\n\n\n\n\n\n\n\nHistory of Earth\n\nFuture of Earth\n\n"}
{"id": "49516442", "url": "https://en.wikipedia.org/wiki?curid=49516442", "title": "Phylogenetic inertia", "text": "Phylogenetic inertia\n\nPhylogenetic inertia or phylogenetic constraint refers to the limitations on the future evolutionary pathways that have been imposed by previous adaptations.\n\nCharles Darwin first recognized this phenomenon, though the term was later coined by Huber in 1939. Darwin explained the idea of phylogenetic inertia based on his observations; he spoke about it when explaining the \"Law of Conditions of Existence\". Darwin also suggested that, after speciation, the organisms do not start over from scratch, but have characteristics that are built upon already existing ones that were inherited from their ancestors; and these characteristics likely limit the amount of evolution seen in that new taxa. This is the main concept of phylogenetic inertia.\n\nRichard Dawkins also explained these constraints by likening natural selection to a river in his 1982 book \"The Extended Phenotype\".\n\n\nBirds are the only speciose group of vertebrates that are exclusively oviparous, or egg laying. It has been suggested that birds are phylogenetically constrained, as being derived from reptiles, and likely have not overcome this constraint or diverged far enough away to develop viviparity, or live birth.\n\n\n\nThere have been several studies that have been able to effectively test for phylogenetic inertia when looking into shared traits; predominantly with a comparative methods approach. Some have used comparative methods and found evidence for certain traits attributed to adaptation, and some to phylogeny; there were also numerous traits that could be attributed to both. Another study developed a new method of comparative examination that showed to be a powerful predictor of phylogenetic inertia in a variety of situations. It was called Phylogenetic Eigenvector Regression (PVR), which runs principal component analyses between species on a pairwise phylogenetic distance matrix. In another, different study, the authors described methods for measuring phylogenetic inertia, looked at effectiveness of various comparative methods, and found that different methods can reveal different aspects of drivers. Autoregression and PVR showed good results with morphological traits.\n"}
{"id": "33316231", "url": "https://en.wikipedia.org/wiki?curid=33316231", "title": "Pouillet effect", "text": "Pouillet effect\n\nIn physics, the term Pouillet effect refers to an exothermic reaction that takes place when a liquid is added to a powder. It was first observed by Leslie in 1802 when dry sawdust was wetted with water. Claude Pouillet later described this phenomenon in 1822 when it came to be known as the Pouillet effect in France.\n"}
{"id": "44120129", "url": "https://en.wikipedia.org/wiki?curid=44120129", "title": "Revolving rivers", "text": "Revolving rivers\n\nRevolving rivers are a surprising, uncommon way of sand pile growth that can be found in a few sands around the world, but has been studied in detail only for one Cuban sand from a place called Santa Teresa (Pinar del Rio province).\n\nWhen pouring \"revolving\" sand on a flat surface from a fixed position, the growth of a conical pile does not occur by the common avalanche mechanism, where sand slides down the pile in a more or less random fashion. What happens in that a relatively thin \"river\" of flowing sand travels from the pouring point at the apex of the pile to its base, while the rest of the sand at the surface is static. In addition, the river \"revolves\" around the pile either in clockwise or counter-clockwise directions (looking from top) depending on the initial conditions of the experiment. Actually the river constitutes the \"cutting edge\" of a layer of sand that deposits as a helix on the conical pile, and makes it grow.\nFor small sandpiles, rivers are continuous, but they become intermittent\nfor larger piles.\n\nThe phenomenon was observed first by E. Altshuler at the University of Havana in 1995, but at the time he assumed that it was well known, and temporarily forgot about it. In 2000, being at the University of Houston, he told K. E. Bassler, who showed a vivid interest in the matter. Embarrassingly enough, Altshuler was unable to demonstrate it before Bassler using a random sand from Houston, so he had to send him a video from Cuba after his return to the island.\n\nOnce the existence of the strange phenomenon was confirmed for everyone, E. Altshuler and a number of collaborators performed a systematic study in Havana, which was then jointly published with Bassler.\nFurther work has been done to understand in more detail the\nphenomenon, and it has been found in other sands from different parts of the world. \nHowever, the connection between the physical, chemical (and possibly biological) properties of the grains in a specific sand, the nature of the inter-grain interactions, and the emergence of the revolving rivers is still an open question.\n\nSand from Santa Teresa is made of almost pure silicon dioxide grains with an average grain size of 0.2 mm approximately and no visible special features regarding grain shape. But in spite of its apparent simplicity, many puzzles still remain. For example, after many experiments one batch of sand may stop showing revolving rivers (just as singing sand eventually stops singing), which suggests that the decay is connected to certain properties of the surface of the grains that degrade by continued friction.\n\nVideos of the effect are available on YouTube.\n"}
{"id": "3173180", "url": "https://en.wikipedia.org/wiki?curid=3173180", "title": "Sonochemistry", "text": "Sonochemistry\n\nIn chemistry, the study of sonochemistry is concerned with understanding the effect of ultrasound in forming acoustic cavitation in liquids, resulting in the initiation or enhancement of the chemical activity in the solution. Therefore, the chemical effects of ultrasound do not come from a\ndirect interaction of the ultrasonic sound wave with the molecules in the solution. Sound waves propagating through a liquid at ultrasonic frequencies do so with a wavelength that is dramatically longer than molecular dimensions or the bond length between atoms in the molecule. Therefore, the sound wave cannot affect that vibrational energy of the bond, and can therefore not directly increase the internal energy of a molecule. Instead, sonochemistry arises from acoustic cavitation: the formation, growth, and implosive collapse of bubbles in a liquid. The collapse of these bubbles is an almost adiabatic process, thereby resulting in the massive build-up of energy inside the bubble, resulting in extremely high temperatures and pressures in a microscopic region of the sonicated liquid. The high temperatures and pressures result in the chemical excitation of any matter that was inside of, or in the immediate surroundings of the bubble as it rapidly imploded. A broad variety of outcomes can result from acoustic cavitation, including sonoluminescence, increased chemical activity in the solution due to the formation of primary and secondary radical reactions, and increase chemical activity through the formation of new, relatively stable chemical species that can diffuse further into the solution to create chemical effects (for example, the formation of hydrogen peroxide from the combination of two hydroxyl radicals formed following the dissociation of water vapor inside the collapsing bubbles what water is exposed to ultrasound.\n\nThe influence of sonic waves traveling through liquids was first reported by Robert Williams Wood (1868–1955) and Alfred Lee Loomis (1887–1975) in 1927. The experiment was about the frequency of the energy that it took for sonic waves to \"penetrate\" the barrier of water. He came to the conclusion that sound does travel faster in water, but because of the water's density compared to our earth's atmosphere it was incredibly hard to get the sonic waves into the water. After lots of research they decided that the best way to disperse sound into the water was to make loud noises into the water by creating bubbles that were made at the same time as the sound. One of the easier ways that they put sound into the water was they simply yelled. But another road block they ran into was the ratio of the amount of time it took for the lower frequency waves to penetrate the bubbles walls and access the water around the bubble, and then time from that point to the point on the other end of the body of water. But despite the revolutionary ideas of this article it was left mostly unnoticed. Sonochemistry experienced a renaissance in the 1980s with the advent of inexpensive and reliable generators of high-intensity ultrasound.\n\nUpon irradiation with high intensity sound or ultrasound, acoustic cavitation usually occurs. Cavitation – the formation, growth, and implosive collapse of bubbles irradiated with sound — is the impetus for sonochemistry and sonoluminescence. Bubble collapse in liquids produces enormous amounts of energy from the conversion of kinetic energy of the liquid motion into heating the contents of the bubble. The compression of the bubbles during cavitation is more rapid than thermal transport, which generates a short-lived localized hot-spot. Experimental results have shown that these bubbles have temperatures around 5000 K, pressures of roughly 1000 atm, and heating and cooling rates above 10 K/s. These cavitations can create extreme physical and chemical conditions in otherwise cold liquids.\n\nWith liquids containing solids, similar phenomena may occur with exposure to ultrasound. Once cavitation occurs near an extended solid surface, cavity collapse is nonspherical and drives high-speed jets of liquid to the surface. These jets and associated shock waves can damage the now highly heated surface. Liquid-powder suspensions produce high velocity interparticle collisions. These collisions can change the surface morphology, composition, and reactivity.\n\nThree classes of sonochemical reactions exist: homogeneous sonochemistry of liquids, heterogeneous sonochemistry of liquid-liquid or solid–liquid systems, and, overlapping with the aforementioned, sonocatalysis. Sonoluminescence is a consequence of the same cavitation phenomena that is responsible for homogeneous sonochemistry. The chemical enhancement of reactions by ultrasound has been explored and has beneficial applications in mixed phase synthesis, materials chemistry, and biomedical uses. Because cavitation can only occur in liquids, chemical reactions are not seen in the ultrasonic irradiation of solids or solid–gas systems.\n\nFor example, in chemical kinetics, it has been observed that ultrasound can greatly enhance chemical reactivity in a number of systems by as much as a million-fold; effectively acting to activate heterogenous catalysts. In addition, in reactions at liquid-solid interfaces, ultrasound breaks up the solid pieces and exposes active clean surfaces through microjet pitting from cavitation near the surfaces and from fragmentation of solids by cavitation collapse nearby. This gives the solid reactant a larger surface area of active surfaces for the reaction to proceed over, increasing the observed rate of reaction. , \n\nWhile the application of ultrasound often generates mixtures of products, a paper published in 2007 in the journal \"Nature\" described the use of ultrasound to selectively affect a certain cyclobutane ring-opening reaction. Atul Kumar has reported multicomponent reaction Hantzsch ester synthesis in Aqueous Micelles using ultrasound.\n\nSome water pollutants, especially chlorinated organic compounds, can be destroyed sonochemically.\n\nSonochemistry can be performed by using a bath (usually used for ultrasonic cleaning) or with a high power probe, called an ultrasonic horn.\n\n\n"}
{"id": "3469441", "url": "https://en.wikipedia.org/wiki?curid=3469441", "title": "Sorbent", "text": "Sorbent\n\nA sorbent is a material used to absorb or adsorb liquids or gases. Examples include:\n\n"}
{"id": "233636", "url": "https://en.wikipedia.org/wiki?curid=233636", "title": "Spherical Earth", "text": "Spherical Earth\n\nThe earliest reliably documented mention of the spherical Earth concept dates from around the 6th century BC when it appeared in ancient Greek philosophy but remained a matter of speculation until the 3rd century BC, when Hellenistic astronomy established the spherical shape of the Earth as a physical given and calculated Earth's circumference. The paradigm was gradually adopted throughout the Old World during Late Antiquity and the Middle Ages. A practical demonstration of Earth's sphericity was achieved by Ferdinand Magellan and Juan Sebastián Elcano's expedition's circumnavigation (1519–1522).\n\nThe concept of a spherical Earth displaced earlier beliefs in a flat Earth: In early Mesopotamian mythology, the world was portrayed as a flat disk floating in the ocean with a hemispherical sky-dome above, and this forms the premise for early world maps like those of Anaximander and Hecataeus of Miletus. Other speculations on the shape of Earth include a seven-layered ziggurat or cosmic mountain, alluded to in the Avesta and ancient Persian writings (see seven climes).\n\nThe realization that the figure of the Earth is more accurately described as an ellipsoid dates to the 17th century, as described by Isaac Newton in \"Principia\". In the early 19th century, the flattening of the earth ellipsoid was determined to be of the order of 1/300 (Delambre, Everest). The modern value as determined by the US DoD World Geodetic System since the 1960s is close to 1/298.25.\n\nThe Earth is massive enough that gravity maintains it as a roughly spherical shape. Its formation into a sphere was made easy by its primordial hot, liquid phase.\n\nThe Solar System formed from a dust cloud that was at least partially the remnant of one or more supernovas that created heavy elements by nucleosynthesis. Grains of matter accreted through electrostatic interaction. As they grew in mass, gravity took over in gathering yet more mass, releasing the potential energy of their collisions and in-falling as heat. The protoplanetary disk also had a greater proportion of radioactive elements than the Earth today because, over time, those elements decayed. Their decay heated the early Earth even further, and continue to contribute to Earth's internal heat budget. The early Earth was thus mostly liquid.\n\nA sphere is the only stable shape for a non-rotating, gravitationally self-attracting liquid. The outward acceleration caused by the Earth's rotation is greater at the equator than at the poles (where is it zero), so the sphere gets deformed into an ellipsoid, which represents the shape having the lowest potential energy for a rotating, fluid body. This ellipsoid is slightly fatter around the equator than a perfect sphere would be. Earth's shape is also slightly lumpy because it is composed of different materials of different densities that exert slightly different amounts of gravitational force per volume.\n\nThe liquidity of a hot, newly formed planet allows heavier elements to sink down to the middle and forces lighter elements closer to the surface, a process known as planetary differentiation. This event is known as the iron catastrophe; the most abundant heavier elements were iron and nickel, which now form the Earth's core.\n\nThough the surface rocks of the Earth have cooled enough to solidify, the outer core of the planet is still hot enough to remain liquid. Energy is still being released; volcanic and tectonic activity has pushed rocks into hills and mountains and blown them out of calderas. Meteors also create impact craters and surrounding ridges. However, if the energy release ceases from these processes, then they tend to erode away over time and return toward the lowest potential-energy curve of the ellipsoid. Weather powered by solar energy can also move water, rock, and soil to make the Earth slightly out of round.\n\nEarth undulates as the shape of its lowest potential energy changes daily due to the gravity of the Sun and Moon as they move around with respect to the Earth. This is what causes tides in the oceans' water, which can flow freely along the changing potential.\n\nThe IAU definitions of planet and dwarf planet require that a Sun-orbiting body has undergone the rounding process to reach a roughly spherical shape, an achievement known as hydrostatic equilibrium. The same spheroidal shape can be seen from smaller rocky planets like Mars to gas giants like Jupiter.\n\nAny natural Sun-orbiting body that has not reached hydrostatic equilibrium is classified by the IAU as a small Solar System body (SSB). These come in many non-spherical shapes which are lumpy masses accreted haphazardly by in-falling dust and rock; not enough mass falls in to generate the heat needed to complete the rounding. Some SSSBs are just collections of relatively small rocks that are weakly held next to each other by gravity but are not actually fused into a single big bedrock. Some larger SSSBs are nearly round but have not reached hydrostatic equilibrium. The small Solar System body 4 Vesta is large enough to have undergone at least partial planetary differentiation.\n\nStars like the Sun are also spheroidal due to gravity's effects on their plasma, which is a free-flowing fluid. Ongoing stellar fusion is a much greater source of heat for stars compared to the initial heat released during formation.\n\nThe roughly spherical shape of the Earth can be confirmed by many different types of observation from ground level, aircraft, and spacecraft. The shape causes a number of phenomena that a flat Earth would not. Some of these phenomena and observations would be possible on other shapes, such as a curved disc or torus, but no other shape would explain all of them.\n\nMany pictures have been taken of the entire Earth by satellites launched by a variety of governments and private organizations. From high orbits, where half the planet can be seen at once, it is plainly spherical. The only way to piece together all the pictures taken of the ground from lower orbits so that all the surface features line up seamlessly and without distortion is to put them on an approximately spherical surface.\n\nAstronauts in low Earth orbit can personally see the curvature of the planet, and travel all the way around several times a day.\n\nThe astronauts who travelled to the Moon have seen the entire Moon-facing half at once, and can watch the sphere rotate once a day (approximately; the Moon is also moving with respect to the Earth).\n\nPeople in high-flying aircraft or skydiving from high-altitude balloons can plainly see the curvature of the Earth. Commercial aircraft do not necessarily fly high enough to make this obvious. Trying to measure the curvature of the horizon by taking a picture is complicated by the fact that camera lenses can produce distorted images depending on the angle used. An extreme version of this effect can be seen in the fisheye lens. Scientific measurements would require a carefully calibrated lens.\n\nThe fastest way for an airplane to travel between two distant cities is a great circle route, which deviates significantly from what would be the fastest straight-line travel path on a flat Earth.\n\nPhotos of the ground taken from airplanes over a large enough area also do not fit seamlessly together on a flat surface, but do fit on a roughly spherical surface. Aerial photographs of large areas must be corrected to account for curvature.\n\nOn a completely flat Earth with no visual interference (such as trees, hills, or atmospheric haze) the ground itself would never obscure distant objects; one would be able to see all the way to the edge of the surface. A spherical surface has a horizon which is closer when viewed from a lower altitude. In theory, a person standing on the surface with eyes above the ground can see the ground up to about away, but a person at the top of the Eiffel Tower at can see the ground up to about away.\n\nThis phenomenon would seem to present a method to verify that the Earth's surface is locally convex. If the degree of curvature was determined to be the same everywhere on the Earth's surface, and that surface was determined to be large enough, it would show that the Earth is spherical.\n\nIn practice, this turns out to be an unreliable method of measurement, due to variations in atmospheric refraction. This additional effect can give the impression that the earth's surface is flat, curved more convexly than it is, or even that it is concave, by bending light travelling near the surface of the earth (as happened in various trials of the famous Bedford Level experiment).\n\nThe phenomenon of variable atmospheric bending can be empirically confirmed by noting that sometimes the refractive layers of air can cause the image of a distant object to be broken into pieces or even turned upside down. This is commonly seen at sunset, when the sun's shape is distorted, but has also been photographed happening for ships, and has caused the city of Chicago to appear normally, upside down, and broken into pieces from across Lake Michigan (from where it is normally below the horizon). Because of their longer wavelengths, radio waves are even more susceptible to atmospheric refraction and reflection, which can cause radio and television signals to be received from towers thousands of miles away which cannot be seen with visible light.\n\nWhen the atmosphere is relatively well-mixed, the visual effects generally expected of a spherical Earth can be observed. For example, ships travelling on large bodies of water (such as the ocean) disappear over the horizon progressively, such that the highest part of the ship can still be seen even when lower parts cannot, proportional to distance from the observer. The same is true of the coastline or mountain when viewed from a ship or from across a large lake or flat terrain.\n\nThe shadow of the Earth on the Moon during a lunar eclipse is always a dark circle that moves from one side of the moon to the other (partially grazing it during a partial eclipse). This could be produced by a flat disc that always faces the Moon head-on during the eclipse, but this is inconsistent with the fact that the Moon is only rarely directly overhead during an eclipse. For each eclipse, the local surface of the Earth is pointed in a somewhat different direction. The shadow of a circular disc held at an angle is an oval, not a circle as is seen during the eclipse. The idea of the Earth being a flat disc is also inconsistent with the fact that a given lunar eclipse is only visible from half of the Earth at a time.\n\nThe only shape that casts a round shadow no matter which direction it is pointed is a sphere, and the ancient Greeks deduced that this must mean the Earth is spherical.\n\nOn a perfectly spherical Earth, flat terrain or ocean, when viewed from the surface, blocks exactly half the sky - a hemisphere of 180°. Moving away from the surface of the Earth means that the ground blocks less and less of the sky. For example, when viewed from the Moon, the Earth blocks only a small portion of the sky, because it is so distant. This phenomenon of geometry means that when viewed from a high mountain, flat ground or ocean blocks less than 180° of the sky. The rate of change in the angle blocked by the sky as altitude increases is different for a disc than a sphere, and values observed show that the Earth is locally convex. (The angles blocked would also be different for a mountain close to the edge of a flat Earth compared to a mountain in the middle of a flat Earth, and this is not observed.) In theory, measurements of this type from all around the Earth would confirm that it is a complete sphere (as opposed to some other shape with convex areas) though actually taking all those measurements would be very expensive.\n\nUsing other evidence to hypothesize a spherical shape, the medieval Iranian scholar Al-Biruni used this phenomenon to calculate the Earth's circumference to within of the correct value.\n\nThe fixed stars can be demonstrated to be very far away, by diurnal parallax measurements (a technique known at least as early as Ancient Greece). Unlike the Sun, Moon, and planets, they do not change position with respect to one another (at least not perceptibly over the span of a human lifetime); the shapes of the constellations are always the same. This makes them a convenient reference background for determining the shape of the Earth. Adding distance measurements on the ground allows calculation of the Earth's size.\n\nThe fact that different stars are visible from different locations on the Earth was noticed in ancient times. Aristotle wrote that some stars are visible from Egypt which are not visible from Europe. This would not be possible if the Earth was flat.\n\nAt the North Pole it is continuously nighttime for six months of the year and the same hemisphere of stars (a 180° view) are always visible making one counterclockwise rotation every 24 hours. The star Polaris (the \"North Star\") is almost at the center of this rotation (which is directly overhead). Some of the 88 modern constellations visible are Ursa Major (including the Big Dipper), Cassiopeia, and Andromeda. The other six months of the year, it is continuously daytime and the light from the Sun mostly blots out the stars. (The location of the poles can be defined by these phenomena, which only occur there; more than 24 hours of continuous daylight can occur north of the Arctic Circle and south of the Antarctic Circle.)\n\nAt the South Pole, a completely non-overlapping set of constellations are visible during the six months of continuous nighttime, including Orion, Crux, and Centaurus. This 180° hemisphere of stars rotate clockwise once every 24 hours, around a point directly overhead (where there do not happen to be any particularly bright stars).\n\nThe fact that the stars visible from the north and south poles do not overlap must mean that the two observation spots are on opposite sides of the Earth, which is not possible if the Earth is a single-sided disc, but is possible for other shapes (like a sphere, but also any other convex shape like a donut or dumbbell).\n\nFrom any point on the equator, 360° of stars are visible over the course of the night, as the sky rotates around a line drawn from due north to due south (which could be defined as \"the directions to walk to get to the poles in the shortest amount of time\"). When facing east, the stars visible from the north pole are on the left, and the stars visible from the south pole are on the right. This means the equator must be facing at a 90° angle from the poles.\n\nThe direction any intermediate spot on the Earth is facing can also be calculated by measuring the angles of the fixed stars and determining how much of the sky is visible. For example, New York City is about 40° north of the equator. The apparent motion of the Sun blots out slightly different parts of the sky from day to day, but over the course of the entire year it sees a dome of 280° (360° - 80°). So for example, both Orion and the Big Dipper are visible during at least part of the year.\n\nMaking stellar observations from a representative set of points across the Earth, combined with knowing the shortest on-the-ground distance between any two given points makes an approximate sphere the only possible shape for the Earth.\n\nKnowing the difference in angle between two points on the Earth's surface and the surface distance between them allows a calculation of the Earth's size. Using observations at Rhodes (in Greece) and Alexandria (in Egypt) and the distance between them, the Ancient Greek philosopher Posidonius actually did use this technique to calculate the circumference of the planet to within perhaps 4% of the correct value (though modern equivalents of his units of measure are not precisely known).\n\nSince the 1500s, many people have sailed or flown completely around the world in all directions, and none have discovered an edge or impenetrable barrier. (See Circumnavigation, Arctic exploration, and History of Antarctica.)\n\nSome flat Earth theories that propose the world is a north-pole-centered disc, conceive of Antarctica as an impenetrable ice wall that encircles the planet and hides any edges. This disc model explains east-west circumnavigation as simply moving around the disc in a circle. (East-west paths form a circle in both disc and spherical geometry.) It is possible in this model to traverse the North Pole, but it is not possible to perform a circumnavigation that includes the South Pole (which it posits does not exist).\n\nExplorers, government researchers, commercial pilots, and tourists have been to Antarctica and found that it is not a large ring that encircles the entire world, but actually a roughly disc-shaped continent smaller than South America but larger than Australia, with an interior that can in fact be traversed in order to take a shorter path from e.g. the tip of South America to Australia than would be possible on a disc.\n\nThe first land crossing of the entirety of Antarctica was the Commonwealth Trans-Antarctic Expedition in 1955-58, and many exploratory airplanes have since passed over the continent in various directions.\n\nOn a flat Earth, an omnidirectional Sun (emitting light in all directions, as it does) would illuminate the entire surface at the same time, and all places would experience sunrise and sunset at the horizon at the same time (with some small variations due to mountains and valleys). With a spherical Earth, half the planet is in daylight at any given time (the hemisphere facing the Sun) and the other half is experiencing nighttime. When a given location on the spherical Earth is in sunlight, its antipode - the location exactly on the opposite side of the Earth - is always experiencing nighttime. The spherical shape of the Earth causes the Sun to rise and set at different times in different places, and different locations get different amounts of sunlight each day. \n\nIn order to explain day and night, time zones, and the seasons, some flat Earth theorists propose that the Sun does not emit light in all directions, but acts more like a spotlight, only illuminating part of the flat Earth at a time. This theory is not consistent with observation; at sunrise and sunset, a spotlight Sun would be up in the sky at least a little bit, rather than at the horizon where it is always actually observed. A spotlight Sun would also appear at different angles in the sky with respect to a flat ground than it does with respect to a curved ground. Assuming light travels in straight lines, actual measurements of the Sun's angle in the sky from locations very distant from each other are only consistent with a geometry where the Sun is very far away and is being seen from a hemispherical surface (the daylight half of the Earth). These two phenomena are related: a low-altitude spotlight Sun would spent most of the day near the horizon for most locations on Earth (which is not observed), but rise and set fairly close to the horizon. A high-altitude Sun would spend more of the day away from the horizon, but rise and set fairly far from the horizon (which is not observed).\n\nAncient timekeeping reckoned \"noon\" as the time of day when the sun is highest in the sky, with the rest of the hours in the day measured against that. During the day, the apparent solar time can be measured directly with a sundial. In ancient Egypt, the first known sundials divided the day into 12 hours, though because the length of the day changed with the season, the length of the hours also changed. Sundials that defined hours as always being the same duration appeared in the Renaissance. In Western Europe, clock towers and striking clocks were used in the Middle Ages to keep people nearby appraised of the local time, though compared to modern times this was less important in a largely agrarian society.\n\nBecause the Sun reaches its highest point at different times for different longitudes (about four minutes of time for every degree of longitude difference east or west), the local solar noon in each city is different except for those directly north or south of each other. This means that the clocks in different cities could be offset from each other by minutes or hours. As clocks became more precise and industrialization made timekeeping more important, cities switched to mean solar time, which ignores minor variations in the timing of local solar noon over the year, due to the elliptical nature of the Earth's orbit, and its tilt.\n\nThe differences in clock time between cities was not generally a problem until the advent of railroad travel in the 1800s, which both made travel between distant cities much faster than by walking or horse, and also required passengers to show up at specific times to meet their desired trains. In the United Kingdom, railroads gradually switched to Greenwich Mean Time (set from local time at the Greenwich observatory in London), followed by public clocks across the country generally, forming a single time zone. In the United States, railroads published schedules based on local time, then later based on standard time for that railroad (typically the local time at the railroad's headquarters), and then finally based on four standard time zones shared across all railroads, where neighboring zones differed by exactly one hour. At first railroad time was synchronized by portable chronometers, and then later by telegraph and radio signals.\n\nSan Francisco is at 122.41°W longitude and Richmond, Virginia is at 77.46°W longitude. They are both at about 37.6°N latitude (±.2°). The approximately 45° of longitude difference translates into about 180 minutes, or 3 hours, of time between sunsets in the two cities, for example. San Francisco is in the Pacific Time zone, and Richmond is in the Eastern Time zone, which are three hours apart, so the local clocks in each city show that the sun sets at about the same time when using the local time zone. But a phone call from Richmond to San Francisco at sunset will reveal that there are still three hours of daylight left in California.\n\nOn a flat Earth with an omnidirectional Sun, all places would experience the same amount of daylight every day, and all places would get daylight at the same time. Actual day length varies considerably, with places closer to the poles getting very long days in the summer and very short days in the winter, with northerly summer happening at the same time as southerly winter. Places north of the Arctic Circle and south of the Antarctic Circle get no sunlight for at least one day a year, and get 24-hour sunlight for at least one day a year. Both the poles experience sunlight for 6 months and darkness for 6 months, at opposite times.\n\nThe movement of daylight between the northern and southern hemispheres happens because of the axial tilt of the Earth. The imaginary line around which the Earth spins, which goes between the North Pole and South Pole, is tilted about 23° from the oval that describes its orbit around the Sun. The Earth always points in the same direction as it moves around the Sun, so for half the year (summer in the Northern Hemisphere), the North Pole is pointed slightly toward the Sun, keeping it in daylight all the time because the Sun lights up the half of the Earth that is facing it (and the North Pole is always in that half due to the tilt). For the other half of the orbit, the South Pole is tilted slightly toward the Sun, and it is winter in the Northern Hemisphere. This means that at the equator, the Sun is not directly overhead at noon, except around the autumnal equinox and vernal equinox, when one spot on the equator is pointed directly at the Sun.\n\nThe length of the day varies because as the Earth rotates some places (near the poles) pass through only a short curve near the top or bottom of the sunlight half; other places (near the equator) travel along much longer curves through the middle.\n\nThe length of twilight would be very different on a flat Earth. On a round Earth, the atmosphere above the ground is lit for a while before sunrise and after sunset are observed at ground level, because the Sun is still visible from higher altitudes. Longer twilights are observed at higher latitudes (near the poles) due to a shallower angle of the Sun's apparent movement compared to the horizon. On a flat Earth, the Sun's shadow would reach the upper atmosphere very quickly, except near the closest edge of the Earth, and would always set at the same angle to the ground (which is not what is observed). The \"spotlight Sun\" theory is also not consistent with this observation, since the air cannot be lit without the ground below it also being lit (except for shadows of mountains and other surface obstacles).\n\nOn a given day, if many different cities measure the angle of the Sun at local noon, the resulting data, when combined with the known distances between cities, shows that the Earth has 180 degrees of north-south curvature. (A full range of angles will be observed if the north and south poles are included, and the day chosen is either the autumnal or spring equinox.) This is consistent with many rounded shapes, including a sphere, and is inconsistent with a flat shape.\n\nSome claim that this experiment assumes a very distant Sun, such that the incoming rays are essentially parallel, and if a flat Earth is assumed, that the measured angles can allow one to calculate the distance to the Sun, which must be small enough that its incoming rays are not very parallel. However, if more than two relatively well-separated cities are included in the experiment, the calculation will make clear whether the Sun is distant or nearby. For example, on the equinox, the 0 degree angle from the North Pole and the 90 degree angle from the equator predict a Sun which would have to be located essentially next to the surface of a flat Earth, but the difference in angle between the equator and New York City would predict a Sun much further away if the Earth is flat. Because these results are contradictory, the surface of the Earth cannot be flat; the data \"is\" consistent with a nearly spherical Earth and a Sun which is very far away compared with the diameter of the Earth.\n\nUsing the knowledge that the Sun is very far away, the ancient Greek geographer Eratosthenes performed an experiment using the differences in the observed angle of the Sun from two different locations to calculate the circumference of the Earth. Though modern telecommunications and timekeeping were not available, he was able to make sure the measurements happened at the same time by having them taken when the Sun was highest in the sky (local noon) at both locations. Using slightly inaccurate assumptions about the locations of two cities, he came to a result within 15% of the correct value.\n\nOn level ground, the difference in the distance to the horizon between lying down and standing up is large enough to watch the Sun set twice by quickly standing up immediately after seeing it set for the first time while lying down. This also can be done with a cherry picker or a tall building with a fast elevator. On a flat Earth, one would not be able to see the Sun again (unless standing near the edge closest to the Sun) due to a much faster-moving Sun shadow. \n\nWhen the supersonic Concorde took off not long after sunset from London and flew westward to New York faster than the sunset was advancing westward on the ground, passengers observed a sunrise in the west. After landing in New York, passengers saw a second sunset in the west.\n\nBecause the speed of the Sun's shadow is slower in polar regions (due to the steeper angle), even a subsonic aircraft can overtake the sunset when flying at high latitudes. One photographer used a roughly circular route around the North Pole to take pictures of 24 sunsets in the same 24-hour period, pausing westward progress in each time zone to let the shadow of the Sun catch up. The surface of the Earth rotates at at 80° north or south, and at the equator.\n\nBecause the Earth is spherical, long-distance travel sometimes requires heading in different directions than one would head on a flat Earth.\n\nFor example, consider an airplane that travels in a straight line, takes a 90-degree right turn, travels another , takes another 90-degree right turn, and travels a third time. On a flat Earth, the aircraft would have travelled along three sides of a square, and arrive at a spot about from where it started. But because the Earth is spherical, in reality it will have travelled along three sides of a triangle, and arrive back very close to its starting point. If the starting point is the North Pole, it would have travelled due south from the North Pole to the equator, then west for a quarter of the way around the Earth, and then due north back to the North Pole.\n\nIn spherical geometry, the sum of angles inside a triangle is greater than 180° (in this example 270°, having arrived back at the north pole a 90° angle to the departure path) unlike on a flat surface, where it is always exactly 180°.\n\nA meridian of longitude is a line where local solar noon occurs at the same time each day. These lines define \"north\" and \"south\". These are perpendicular to lines of latitude that define \"east\" and \"west\", where the Sun is at the same angle at local noon on the same day. If the Sun were travelling from east to west over a flat Earth, meridian lines would always be the same distance apart - they would form a square grid when combined with lines of latitude. In reality, meridian lines get farther apart as one travels toward the equator, which is only possible on a round Earth. In places where land is plotted on a grid system, this causes discontinuities in the grid. For example, in areas of the Midwestern United States that use the Public Land Survey System, the northernmost and westernmost sections of a township deviate from what would otherwise be an exact square mile. The resulting discontinuities are sometimes reflected directly in local roads, which have kinks where the grid cannot follow completely straight lines.\n\nLow-pressure weather systems with inward winds (such as a hurricane) spin counterclockwise north of the equator, but clockwise south of the equator. This is due to the Coriolis force, and requires that (assuming they are attached to each other and rotating in the same direction) the north and southern halves of the Earth are angled in opposite directions (e.g. the north is facing toward Polaris and the south is facing away from it).\n\nThe laws of gravity, chemistry, and physics that explain the formation and rounding of the Earth are well-tested through experiment, and applied successfully to many engineering tasks.\n\nFrom these laws, we know the amount of mass the Earth contains, and that a non-spherical planet the size of the Earth would not be able to support itself against its own gravity. A flat disc the size of the Earth, for example, would likely crack, heat up, liquefy, and re-form into a roughly spherical shape. On a disc strong enough to maintain its shape, gravity would not pull downward with respect to the surface, but would pull toward the center of the disc, contrary to what is observed on level terrain (and which would create major problems with oceans flowing toward the center of the disk).\n\nIgnoring the other concerns, some flat Earth theorists explain the observed surface \"gravity\" by proposing that the flat Earth is constantly accelerating upwards. Such a theory would also leave open for explanation the tides seen in Earth's oceans, which are conventionally explained by the gravity exerted by the Sun and Moon.\n\nObservation of Foucault pendulums, popular in science museums around the world, demonstrate both that the world is spherical and that it rotates (not that the stars are rotating around it).\n\nThe mathematics of navigation by GPS assume that satellites are moving in known orbits around an approximately spherical surface. The accuracy of GPS navigation in determining latitude and longitude and the way these numbers map onto locations on the ground show that these assumptions are correct. The same is true for the operational GLONASS system run by Russia, and the in-development European Galileo, Chinese BeiDou, and Indian IRNSS.\n\nSatellites, including communications satellites used for television, telephone, and Internet connections, would not stay in orbit unless the modern theory of gravitation were correct. The details of which satellites are visible from which places on the ground at which times prove an approximately spherical shape of the Earth. (Undersea cables are also used for intercontinental communications.)\n\nRadio transmitters are mounted on tall towers because they generally rely on line-of-sight propagation. The distance to the horizon is further at higher altitude, so mounting them higher significantly increases the area they can serve. Some signals can be transmitted at much longer distances, but only if they are at frequencies where they can use groundwave propagation, tropospheric propagation, tropospheric scatter, or ionospheric propagation to reflect or refract signals around the curve of the Earth.\n\nThe design of some large structures needs to take the shape of the Earth into account. For example, the towers of the Humber Bridge, although both vertical with respect to gravity, are farther apart at the top than the bottom due to the local curvature.\n\nThe Hebrew Bible imagined a three-part world, with the heavens (\"shamayim\") above, earth (\"eres\") in the middle, and the underworld (\"sheol\") below. After the 4th century BCE this was gradually replaced by a Greek scientific cosmology of a spherical earth surrounded by multiple concentric heavens.\n\nThough the earliest written mention of a spherical Earth comes from ancient Greek sources, there is no account of how the sphericity of the Earth was discovered. A plausible explanation is that it was \"the experience of travellers that suggested such an explanation for the variation in the observable altitude of the pole and the change in the area of circumpolar stars, a change that was quite drastic between Greek settlements\" around the eastern Mediterranean Sea, particularly those between the Nile Delta and Crimea.\n\nIn \"The Histories\", written 431–425 BC, Herodotus cast doubt on a report of the Sun observed shining from the north. He stated that the phenomenon was observed during a circumnavigation of Africa undertaken by Phoenician explorers employed by Egyptian pharaoh Necho II c. 610–595 BC (, 4.42) who claimed to have had the Sun on their right when circumnavigating in a clockwise direction. To modern historians, these details confirm the truth of the Phoenicians' report and even open the possibility that the Phoenicians knew about the spherical model. However, nothing certain about their knowledge of geography and navigation has survived.\n\nEarly Greek philosophers alluded to a spherical Earth, though with some ambiguity. Pythagoras (6th century BC) was among those said to have originated the idea, but this might reflect the ancient Greek practice of ascribing every discovery to one or another of their ancient wise men. Some idea of the sphericity of the Earth seems to have been known to both Parmenides and Empedocles in the 5th century BC, and although the idea cannot reliably be ascribed to Pythagoras, it might nevertheless have been formulated in the Pythagorean school in the 5th century BC although some disagree. After the 5th century BC, no Greek writer of repute thought the world was anything but round.\n\nPlato (427–347 BC) travelled to southern Italy to study Pythagorean mathematics. When he returned to Athens and established his school, Plato also taught his students that Earth was a sphere, though he offered no justifications. \"My conviction is that the Earth is a round body in the centre of the heavens, and therefore has no need of air or of any similar force to be a support\". If man could soar high above the clouds, Earth would resemble \"one of those balls which have leather coverings in twelve pieces, and is decked with various colours, of which the colours used by painters on Earth are in a manner samples.\"\nIn Timaeus, his one work that was available throughout the Middle Ages in Latin, we read that the Creator \"made the world in the form of a globe, round as from a lathe, having its extremes in every direction equidistant from the centre, the most perfect and the most like itself of all figures\", though the word \"world\" here refers to the heavens.\n\nAristotle (384–322 BC) was Plato's prize student and \"the mind of the school\". Aristotle observed \"there are stars seen in Egypt and [...] Cyprus which are not seen in the northerly regions.\" Since this could only happen on a curved surface, he too believed Earth was a sphere \"of no great size, for otherwise the effect of so slight a change of place would not be quickly apparent.\" (\"De caelo\", 298a2–10)\n\nAristotle provided physical and observational arguments supporting the idea of a spherical Earth:\n\n\nThe concepts of symmetry, equilibrium and cyclic repetition permeated Aristotle's work. In his \"Meteorology\" he divided the world into five climatic zones: two temperate areas separated by a torrid zone near the equator, and two cold inhospitable regions, \"one near our upper or northern pole and the other near the ... southern pole,\" both impenetrable and girdled with ice (\"Meteorologica\", 362a31–35). Although no humans could survive in the frigid zones, inhabitants in the southern temperate regions could exist.\n\nAristotle's theory of natural place relied on a spherical Earth to explain why heavy things go down (toward what Aristotle believed was the center of the Universe), and things like air and fire go up. In this geocentric model, the structure of the universe was believed to be a series of perfect spheres. The Sun, Moon, planets and fixed stars were believed to move on celestial spheres around a stationary Earth.\n\nThough Aristotle's theory of physics survived in the Christian world for many centuries, the heliocentric model was eventually shown to be a more correct explanation of the Solar System than the geocentric model, and atomic theory was shown to be a more correct explanation of the nature of matter than classical elements like earth, water, air, fire, and aether.\n\nIn proposition 2 of the First Book of his treatise \"On floating bodies,\" Archimedes demonstrates that \"The surface of any fluid at rest is the surface of a sphere whose centre is the same as that of the earth,\". Subsequently, in propositions 8 and 9 of the same work, he assumes the result of proposition 2 that the Earth is a sphere and that the surface of a fluid on it is a sphere centered on the center of the Earth.\n\nEratosthenes, a Greek astronomer from Hellenistic Cyrenaica (276–194 BC), estimated Earth's circumference around 240 BC. He had heard that in Syene the Sun was directly overhead at the summer solstice whereas in Alexandria it still cast a shadow. Using the differing angles the shadows made as the basis of his trigonometric calculations he estimated a circumference of around 250,000 \"stades\". The length of a 'stade' is not precisely known, but Eratosthenes's figure only has an error of around five to fifteen percent. Eratosthenes used rough estimates and round numbers, but depending on the length of the stadion, his result is within a margin of between 2% and 20% of the actual meridional circumference, . Note that Eratosthenes could only measure the circumference of the Earth by assuming that the distance to the Sun is so great that the rays of sunlight are practically parallel.\n\nSeventeen hundred years after Eratosthenes, Christopher Columbus studied Eratosthenes's findings before sailing west for the Indies. However, ultimately he rejected Eratosthenes in favour of other maps and arguments that interpreted Earth's circumference to be a third smaller than reality. If, instead, Columbus had accepted Eratosthenes findings, then he may have never gone west, since he didn't have the supplies or funding needed for the much longer voyage.\n\nSeleucus of Seleucia (c. 190 BC), who lived in the city of Seleucia in Mesopotamia, wrote that the Earth is spherical (and actually orbits the Sun, influenced by the heliocentric theory of Aristarchus of Samos).\n\nPosidonius (c. 135 – 51 BC) put faith in Eratosthenes's method, though by observing the star Canopus, rather than the sun in establishing the Earth's circumference. In Ptolemy's \"Geographia\", his result was favoured over that of Eratosthenes. Posidonius furthermore expressed the distance of the sun in earth radii.\n\nFrom its Greek origins, the idea of a spherical earth, along with much of Greek astronomical thought, slowly spread across the globe and ultimately became the adopted view in all major astronomical traditions.\n\nIn the West, the idea came to the Romans through the lengthy process of cross-fertilization with Hellenistic civilization. Many Roman authors such as Cicero and Pliny refer in their works to the rotundity of the earth as a matter of course.\n\nIt has been suggested that seafarers probably provided the first observational evidence that the Earth was not flat, based on observations of the horizon. This argument was put forward by the geographer Strabo (c. 64 BC – 24 AD), who suggested that the spherical shape of the Earth was probably known to seafarers around the Mediterranean Sea since at least the time of Homer, citing a line from the \"Odyssey\" as indicating that the poet Homer knew of this as early as the 7th or 8th century BC. Strabo cited various phenomena observed at sea as suggesting that the Earth was spherical. He observed that elevated lights or areas of land were visible to sailors at greater distances than those less elevated, and stated that the curvature of the sea was obviously responsible for this.\n\nClaudius Ptolemy (90–168 AD) lived in Alexandria, the centre of scholarship in the 2nd century. In the \"Almagest,\" which remained the standard work of astronomy for 1,400 years, he advanced many arguments for the spherical nature of the Earth. Among them was the observation that when a ship is sailing towards mountains, observers note these seem to rise from the sea, indicating that they were hidden by the curved surface of the sea. He also gives separate arguments that the Earth is curved north-south and that it is curved east-west.\n\nHe compiled an eight-volume \"Geographia\" covering what was known about the earth. The first part of the \"Geographia\" is a discussion of the data and of the methods he used. As with the model of the Solar System in the \"Almagest\", Ptolemy put all this information into a grand scheme. He assigned coordinates to all the places and geographic features he knew, in a grid that spanned the globe (although most of this has been lost). Latitude was measured from the equator, as it is today, but Ptolemy preferred to express it as the length of the longest day rather than degrees of arc (the length of the midsummer day increases from 12h to 24h as you go from the equator to the polar circle). He put the meridian of 0 longitude at the most western land he knew, the Canary Islands.\n\n\"Geographia\" indicated the countries of \"Serica\" and \"Sinae\" (China) at the extreme right, beyond the island of \"Taprobane\" (Sri Lanka, oversized) and the \"Aurea Chersonesus\" (Southeast Asian peninsula).\n\nPtolemy also devised and provided instructions on how to create maps both of the whole inhabited world (\"oikoumenè\") and of the Roman provinces. In the second part of the \"Geographia,\" he provided the necessary topographic lists, and captions for the maps. His \"oikoumenè\" spanned 180 degrees of longitude from the Canary Islands in the Atlantic Ocean to China, and about 81 degrees of latitude from the Arctic to the East Indies and deep into Africa. Ptolemy was well aware that he knew about only a quarter of the globe.\n\nKnowledge of the spherical shape of the Earth was received in scholarship of Late Antiquity as a matter of course, in both Neoplatonism and Early Christianity. Calcidius's fourth-century Latin commentary on and translation of Plato's \"Timaeus\", which was one of the few examples of Greek scientific thought that was known in the Early Middle Ages in Western Europe, discussed Hipparchus's use of the geometrical circumstances of eclipses to compute the relative diameters of the Sun, Earth, and Moon.\n\nTheological doubt informed by the flat Earth model implied in the Hebrew Bible inspired some early Christian scholars such as Lactantius, John Chrysostom and Athanasius of Alexandria, but this remained an eccentric current. Learned Christian authors such as Basil of Caesarea, Ambrose and Augustine of Hippo were clearly aware of the sphericity of the Earth. \"Flat Earthism\" lingered longest in Syriac Christianity, which tradition laid greater importance on a literalist interpretation of the Old Testament. Authors from that tradition, such as Cosmas Indicopleustes, presented the Earth as flat as late as in the 6th century. This last remnant of the ancient model of the cosmos disappeared during the 7th century. From the 8th century and the beginning medieval period, \"no cosmographer worthy of note has called into question the sphericity of the Earth.\"\n\nGreek ethnographer Megasthenes, c. 300 BC, has been interpreted as stating that the contemporary Brahmans believed in a spherical earth as the center of the universe. With the spread of Greek culture in the east, Hellenistic astronomy filtered eastwards to ancient India where its profound influence became apparent in the early centuries AD. The Greek concept of an Earth surrounded by the spheres of the planets and that of the fixed stars, vehemently supported by astronomers like Varahamihir and Brahmagupta, strengthened the astronomical principles. Some ideas were found possible to preserve, although in altered form.\n\nThe works of the classical Indian astronomer and mathematician, Aryabhatta (476–550 AD), deal with the sphericity of the Earth and the motion of the planets. The final two parts of his Sanskrit magnum opus, the \"Aryabhatiya\", which were named the \"Kalakriya\" (\"reckoning of time\") and the \"Gol\" (\"sphere\"), state that the Earth is spherical and that its circumference is 4,967 yojanas. In modern units this is , close to the current equatorial value of .\n\nKnowledge of the sphericity of the Earth survived into the medieval corpus of knowledge by direct transmission of the texts of Greek antiquity (Aristotle), and via authors such as Isidore of Seville and Beda Venerabilis.\nIt became increasingly traceable with the rise of scholasticism and medieval learning.\nSpread of this knowledge beyond the immediate sphere of Greco-Roman scholarship was necessarily gradual, associated with the pace of Christianisation of Europe. For example, the first evidence of knowledge of the spherical shape of the Earth in Scandinavia is a 12th-century Old Icelandic translation of \"Elucidarius\".\n\nA non-exhaustive list of more than a hundred Latin and vernacular writers from Late Antiquity and the Middle Ages who were aware that the earth was spherical has been compiled by Reinhard Krüger, professor for Romance literature at the University of Stuttgart.\nAmpelius, Chalcidius, Macrobius, Martianus Capella,\nBasil of Caesarea, Ambrose of Milan, Aurelius Augustinus, Paulus Orosius, Jordanes, Cassiodorus, Boethius, Visigoth king Sisebut.\n\nIsidore of Seville, Beda Venerabilis, Theodulf of Orléans, Vergilius of Salzburg,\nIrish monk Dicuil, Rabanus Maurus, King Alfred of England, Remigius of Auxerre, Johannes Scotus Eriugena, , Gerbert d’Aurillac (Pope Sylvester II).\n\nNotker the German of Sankt-Gallen, Hermann of Reichenau, Hildegard von Bingen, Petrus Abaelardus, Honorius Augustodunensis, Gautier de Metz, Adam of Bremen, Albertus Magnus, Thomas Aquinas, Berthold of Regensburg, Guillaume de Conches, , Abu-Idrisi, Bernardus Silvestris, Petrus Comestor, Thierry de Chartres, Gautier de Châtillon, Alexander Neckam, Alain de Lille, Averroes, Snorri Sturluson, Moshe ben Maimon, Lambert of Saint-Omer, Gervasius of Tilbury, Robert Grosseteste, Johannes de Sacrobosco, Thomas de Cantimpré, Peire de Corbian, Vincent de Beauvais, Robertus Anglicus, , Ristoro d'Arezzo, Roger Bacon, Jean de Meung, Brunetto Latini, Alfonso X of Castile.\n\nMarco Polo, Dante Alighieri, Meister Eckhart, Enea Silvio Piccolomini (Pope Pius II), Perot de Garbalei (\"divisiones mundi\"), Cecco d'Ascoli, , Levi ben Gershon, Konrad of Megenberg, Nicole Oresme, Petrus Aliacensis, , Toscanelli, , Jean de Mandeville, Christine de Pizan, Geoffrey Chaucer, William Caxton, Martin Behaim, Christopher Columbus.\n\nBishop Isidore of Seville (560–636) taught in his widely read encyclopedia, \"The Etymologies,\" that the Earth was \"round\". The bishop's confusing exposition and choice of imprecise Latin terms have divided scholarly opinion on whether he meant a sphere or a disk or even whether he meant anything specific. Notable recent scholars claim that he taught a spherical earth. Isidore did not admit the possibility of people dwelling at the antipodes, considering them as legendary and noting that there was no evidence for their existence.\n\n\nThe monk Bede (c. 672–735) wrote in his influential treatise on computus, \"The Reckoning of Time\", that the Earth was round. He explained the unequal length of daylight from \"the roundness of the Earth, for not without reason is it called 'the orb of the world' on the pages of Holy Scripture and of ordinary literature. It is, in fact, set like a sphere in the middle of the whole universe.\" (De temporum ratione, 32). The large number of surviving manuscripts of \"The Reckoning of Time,\" copied to meet the Carolingian requirement that all priests should study the computus, indicates that many, if not most, priests were exposed to the idea of the sphericity of the Earth. Ælfric of Eynsham paraphrased Bede into Old English, saying, \"Now the Earth's roundness and the Sun's orbit constitute the obstacle to the day's being equally long in every land.\"\n\nBede was lucid about earth's sphericity, writing \"We call the earth a globe, not as if the shape of a sphere were expressed in the diversity of plains and mountains, but because, if all things are included in the outline, the earth's circumference will represent the figure of a perfect globe... For truly it is an orb placed in the centre of the universe; in its width it is like a circle, and not circular like a shield but rather like a ball, and it extends from its centre with perfect roundness on all sides.\"\n\nThe 7th-century Armenian scholar Anania Shirakatsi described the world as \"being like an egg with a spherical yolk (the globe) surrounded by a layer of white (the atmosphere) and covered with a hard shell (the sky).\"\n\nIslamic astronomy was developed on the basis of a spherical earth inherited from Hellenistic astronomy. The Islamic theoretical framework largely relied on the fundamental contributions of Aristotle (\"De caelo\") and Ptolemy (\"Almagest\"), both of whom worked from the premise that the earth was spherical and at the centre of the universe (geocentric model).\n\nEarly Islamic scholars recognized Earth's sphericity, leading Muslim mathematicians to develop spherical trigonometry in order to further mensuration and to calculate the distance and direction from any given point on the Earth to Mecca. This determined the \"Qibla,\" or Muslim direction of prayer.\n\nAround 830 AD, Caliph al-Ma'mun commissioned a group of Muslim astronomers and geographers to measure the distance from Tadmur (Palmyra) to Raqqa in modern Syria. They found the cities to be separated by one degree of latitude and the meridian arc distance between them to be 66 miles and thus calculated the Earth's circumference to be 24,000 miles.\n\nAnother estimate given by his astronomers was 56 Arabic miles (111.8 km) per degree, which corresponds to a circumference of 40,248 km, very close to the currently modern values of 111.3 km per degree and 40,068 km circumference, respectively.\n\nAndalusian polymath Ibn Hazm stated that the proof of the Earth's sphericity \"is that the Sun is always vertical to a particular spot on Earth\".\n\nAl-Farghānī (Latinized as Alfraganus) was a Persian astronomer of the 9th century involved in measuring the diameter of the Earth, and commissioned by Al-Ma'mun. His estimate given above for a degree (56 Arabic miles) was much more accurate than the 60 Roman miles (89.7 km) given by Ptolemy. Christopher Columbus uncritically used Alfraganus's figure as if it were in Roman miles instead of in Arabic miles, in order to prove a smaller size of the Earth than that propounded by Ptolemy.\n\n\nAbu Rayhan Biruni (973–1048) used a new method to accurately compute the Earth's circumference, by which he arrived at a value that was close to modern values for the Earth's circumference. His estimate of 6,339.6 km for the Earth radius was only 31.4 km less than the modern mean value of 6,371.0 km. In contrast to his predecessors, who measured the Earth's circumference by sighting the Sun simultaneously from two different locations, Biruni developed a new method of using trigonometric calculations based on the angle between a plain and mountain top. This yielded more accurate measurements of the Earth's circumference and made it possible for a single person to measure it from a single location.\nBiruni's method was intended to avoid \"walking across hot, dusty deserts,\" and the idea came to him when he was on top of a tall mountain in India. From the top of the mountain, he sighted the angle to the horizon which, along with the mountain's height (which he calculated beforehand), allowed him to calculate the curvature of the Earth.\nHe also made use of algebra to formulate trigonometric equations and used the astrolabe to measure angles.\n\nAccording to John J. O'Connor and Edmund F. Robertson,\n\nMuslim scholars who held to the round Earth theory used it for a quintessentially Islamic purpose: to calculate the distance and direction from any given point on the Earth to Mecca. This determined the Qibla, or Muslim direction of prayer.\n\nA terrestrial globe (Kura-i-ard) was among the presents sent by the Persian Muslim astronomer Jamal-al-Din to Kublai Khan's Chinese court in 1267. It was made of wood on which \"seven parts of water are represented in green, three parts of land in white, with rivers, lakes etc.\" Ho Peng Yoke remarks that \"it did not seem to have any general appeal to the Chinese in those days\".\n\nDuring the High Middle Ages, the astronomical knowledge in Christian Europe was extended beyond what was transmitted directly from ancient authors by transmission of learning from Medieval Islamic astronomy. An early student of such learning was Gerbert d'Aurillac, the later Pope Sylvester II.\n\nSaint Hildegard (Hildegard von Bingen, 1098–1179), depicted the spherical earth several times in her work \"Liber Divinorum Operum\".\n\nJohannes de Sacrobosco (c. 1195 – c. 1256 AD) wrote a famous work on Astronomy called \"Tractatus de Sphaera,\" based on Ptolemy, which primarily considers the sphere of the sky. However, it contains clear proofs of the earth's sphericity in the first chapter.\n\nMany scholastic commentators on Aristotle's \"On the Heavens\" and Sacrobosco's \"Treatise on the Sphere\" unanimously agreed that the earth is spherical or round. Grant observes that no author who had studied at a medieval university thought that the earth was flat.\n\nThe \"Elucidarium\" of Honorius Augustodunensis (c. 1120), an important manual for the instruction of lesser clergy, which was translated into Middle English, Old French, Middle High German, Old Russian, Middle Dutch, Old Norse, Icelandic, Spanish, and several Italian dialects, explicitly refers to a spherical Earth. Likewise, the fact that Bertold von Regensburg (mid-13th century) used the spherical Earth as an illustration in a sermon shows that he could assume this knowledge among his congregation. The sermon was preached in the vernacular German, and thus was not intended for a learned audience.\n\nDante's \"Divine Comedy,\" written in Italian in the early 14th century, portrays Earth as a sphere, discussing implications such as the different stars visible in the southern hemisphere, the altered position of the sun, and the various timezones of the Earth.\n\nThe Portuguese exploration of Africa and Asia, Columbus's voyage to the Americas (1492) and, finally, Ferdinand Magellan's circumnavigation of the earth (1519–21) provided practical evidence of the global shape of the earth.\n\nThe first direct demonstration of Earth's sphericity came in the form of the first circumnavigation in history, an expedition captained by Portuguese explorer Ferdinand Magellan. The expedition was financed by the Spanish Crown. On August 10, 1519, the five ships under Magellan's command departed from Seville. They crossed the Atlantic Ocean, passed through what is now called the Strait of Magellan, crossed the Pacific, and arrived in Cebu, where Magellan was killed by Philippine natives in a battle. His second in command, the Spaniard Juan Sebastián Elcano, continued the expedition and, on September 6, 1522, arrived at Seville, completing the circumnavigation. Charles I of Spain, in recognition of his feat, gave Elcano a coat of arms with the motto \"Primus circumdedisti me\" (in Latin, \"You went around me first\").\n\nA circumnavigation alone does not prove that the earth is spherical. It could be cylindric or irregularly globular or one of many other shapes. Still, combined with trigonometric evidence of the form used by Eratosthenes 1,700 years prior, the Magellan expedition removed any reasonable doubt in educated circles in Europe. The Transglobe Expedition (1979–1982) was the first expedition to make a circumpolar circumnavigation, traveling the world \"vertically\" traversing both of the poles of rotation using only surface transport.\n\nIn the 17th century, the idea of a spherical Earth, now considerably advanced by Western astronomy, ultimately spread to Ming China, when Jesuit missionaries, who held high positions as astronomers at the imperial court, successfully challenged the Chinese belief that the Earth was flat and square.\n\nThe \"Ge zhi cao\" (格致草) treatise of Xiong Mingyu (熊明遇) published in 1648 showed a printed picture of the Earth as a spherical globe, with the text stating that \"the round Earth certainly has no square corners\". The text also pointed out that sailing ships could return to their port of origin after circumnavigating the waters of the Earth.\n\nThe influence of the map is distinctly Western, as traditional maps of Chinese cartography held the graduation of the sphere at 365.25 degrees, while the Western graduation was of 360 degrees. Also of interest to note is on one side of the world, there is seen towering Chinese pagodas, while on the opposite side (upside-down) there were European cathedrals. The adoption of European astronomy, facilitated by the failure of indigenous astronomy to make progress, was accompanied by a sinocentric reinterpretation that declared the imported ideas Chinese in origin:\n\nEuropean astronomy was so much judged worth consideration that numerous Chinese authors developed the idea that the Chinese of antiquity had anticipated most of the novelties presented by the missionaries as European discoveries, for example, the rotundity of the Earth and the \"heavenly spherical star carrier model.\" Making skillful use of philology, these authors cleverly reinterpreted the greatest technical and literary works of Chinese antiquity. From this sprang a new science wholly dedicated to the demonstration of the Chinese origin of astronomy and more generally of all European science and technology.\n\nAlthough mainstream Chinese science until the 17th century held the view that the earth was flat, square, and enveloped by the celestial sphere, this idea was criticized by the Jin-dynasty scholar Yu Xi (fl. 307–345), who suggested that the Earth could be either square or round, in accordance with the shape of the heavens. The Yuan-dynasty mathematician Li Ye (c. 1192–1279) firmly argued that the Earth was spherical, just like the shape of the heavens only smaller, since a square Earth would hinder the movement of the heavens and celestial bodies in his estimation. The 17th-century \"Ge zhi cao\" treatise also used the same terminology to describe the shape of the Earth that the Eastern-Han scholar Zhang Heng (78–139 AD) had used to describe the shape of the sun and moon (i.e. that the former was as round as a crossbow bullet, and the latter was the shape of a ball).\n\nGeodesy, also called geodetics, is the scientific discipline that deals with the measurement and representation of the Earth, its gravitational field and geodynamic phenomena (polar motion, Earth tides, and crustal motion) in three-dimensional time-varying space.\n\nGeodesy is primarily concerned with positioning and the gravity field and geometrical aspects of their temporal variations, although it can also include the study of Earth's magnetic field. Especially in the German speaking world, geodesy is divided into geomensuration (\"Erdmessung\" or \"höhere Geodäsie\"), which is concerned with measuring the Earth on a global scale, and surveying (\"Ingenieurgeodäsie\"), which is concerned with measuring parts of the surface.\n\nThe Earth's shape can be thought of in at least two ways;\n\nAs the science of geodesy measured Earth more accurately, the shape of the geoid was first found not to be a perfect sphere but to approximate an oblate spheroid, a specific type of ellipsoid. More recent measurements have measured the geoid to unprecedented accuracy, revealing mass concentrations beneath Earth's surface.\n\n\n\n"}
{"id": "31880880", "url": "https://en.wikipedia.org/wiki?curid=31880880", "title": "Theoretical foundations of evolutionary psychology", "text": "Theoretical foundations of evolutionary psychology\n\nThe theoretical foundations of evolutionary psychology are the general and specific scientific theories that explain the ultimate origins of psychological traits in terms of evolution. These theories originated with Charles Darwin's work, including his speculations about the evolutionary origins of social instincts in humans. Modern evolutionary psychology, however, is possible only because of advances in evolutionary theory in the 20th century.\n\nEvolutionary psychologists say that natural selection has provided humans with many psychological adaptations, in much the same way that it generated humans' anatomical and physiological adaptations. As with adaptations in general, psychological adaptations are said to be specialized for the environment in which an organism evolved, the environment of evolutionary adaptedness, or EEA. Sexual selection provides organisms with adaptations related to mating. For male mammals, which have a relatively fast reproduction rate, sexual selection leads to adaptations that help them compete for females. For female mammals, with a relatively slow reproduction rate, sexual selection leads to choosiness, which helps females select higher quality mates. Charles Darwin described both natural selection and sexual selection, but he relied on group selection to explain the evolution of self-sacrificing behavior. Group selection is a weak explanation because in any group the less self-sacrificing animals will be more likely to survive and the group will become less self-sacrificing.\n\nIn 1964, William D. Hamilton proposed inclusive fitness theory, emphasizing a \"gene's-eye\" view of evolution. Hamilton noted that individuals can increase the replication of their genes into the next generation by helping close relatives with whom they share genes survive and reproduce. According to \"Hamilton's rule\", a self-sacrificing behavior can evolve if it helps close relatives so much that it more than compensates for the individual animal's sacrifice. Inclusive fitness theory resolved the issue of how \"altruism\" evolved. Other theories also help explain the evolution of altruistic behavior, including evolutionary game theory, tit-for-tat reciprocity, and generalized reciprocity. These theories not only help explain the development of altruistic behavior but also account for hostility toward cheaters (individuals that take advantage of others' altruism).\n\nSeveral mid-level evolutionary theories inform evolutionary psychology. The r/K selection theory proposes that some species prosper by having many offspring while others follow the strategy of having fewer offspring but investing much more in each one. Humans follow the second strategy. Parental investment theory explains how parents invest more or less in individual offspring based on how successful those offspring are likely to be, and thus how much they might improve the parents' inclusive fitness. According to the Trivers-Willard hypothesis, parents in good conditions tend to invest more in sons (who are best able to take advantage of good conditions), while parents in poor conditions tend to invest more in daughters (who are best able to have successful offspring even in poor conditions). According to life history theory, animals evolve life histories to match their environments, determining details such as age at first reproduction and number of offspring. Dual inheritance theory posits that genes and human culture have interacted, with genes affecting the development of culture and culture, in turn, affecting human evolution on a genetic level (see also the Baldwin effect).\n\nCritics of evolutionary psychology have sometimes challenged its theoretical underpinnings, saying that humans never developed powerful social instincts through natural selection and that the hypotheses of evolutionary psychologists are merely just-so-stories.\n\nEvolutionary psychology primarily uses the theories of natural selection, sexual selection, and inclusive fitness to explain the evolution of psychological adaptations.\n\nEvolutionary psychology is sometimes seen not simply as a subdiscipline of psychology but as a metatheoretical framework in which \"the entire field of psychology can be examined.\"\n\nEvolutionary psychologists consider Charles Darwin's theory of natural selection to be important to an understanding of psychology. Natural selection occurs because individual organisms who are genetically better suited to the current environment leave more descendants, and their genes spread through the population, thus explaining why organisms fit their environments so closely. This process is slow and cumulative, with new traits layered over older traits. The advantages created by natural selection are known as adaptations. Evolutionary psychologists say that animals, just as they evolve physical adaptations, evolve psychological adaptations.\n\nEvolutionary psychologists emphasize that natural selection mostly generates specialized adaptations, which are more efficient than general adaptations. They point out that natural selection operates slowly, and that adaptations are sometimes out of date when the environment changes rapidly. In the case of humans, evolutionary psychologists say that much of human nature was shaped during the stone age and may not match the contemporary environment.\n\nSexual selection favors traits that provide mating advantages, such as the peacock's tail, even if these same traits are usually hindrances. Evolutionary psychologists point out that, unlike natural selection, sexual selection typically leads to the evolution of sex differences. Sex differences typically make reproduction faster for one sex and slower for the other, in which case mates are relatively scarce for the faster sex. Sexual selection favors traits that increase the number of mates for the fast sex and the quality of mates for the slow sex. For mammals, the female has the slower reproduction rate. Males typically evolve either traits to help them fight other males or traits to impress females. Females typically evolve greater abilities to discern the qualities of males, such as choosiness in mating.\n\nInclusive fitness theory, proposed by William D. Hamilton, emphasized a \"gene's-eye\" view of evolution. Hamilton noted that what evolution ultimately selects are genes, not groups or species. From this perspective, individuals can increase the replication of their genes into the next generation not only directly via reproduction, by also indirectly helping close relatives with whom they share genes survive and reproduce. General evolutionary theory, in its modern form, \"is\" essentially inclusive fitness theory.\n\nInclusive fitness theory resolved the issue of how \"altruism\" evolved. The dominant, pre-Hamiltonian view was that altruism evolved via group selection: the notion that altruism evolved for the benefit of the group. The problem with this was that if one organism in a group incurred any fitness costs on itself for the benefit of others in the group, (i.e. acted \"altruistically\"), then that organism would reduce its own ability to survive and/or reproduce, therefore reducing its chances of passing on its altruistic traits.\n\nFurthermore, the organism that benefited from that altruistic act and only acted on behalf of its own fitness would increase its own chance of survival and/or reproduction, thus increasing its chances of passing on its \"selfish\" traits.\nInclusive fitness resolved \"the problem of altruism\" by demonstrating that altruism can evolve via kin selection as expressed in Hamilton's rule:\ncost < relatedness × benefit\nIn other words, altruism can evolve as long as the fitness \"cost\" of the altruistic act on the part of the actor is less than the \"degree of genetic relatedness\" of the recipient times the fitness \"benefit\" to that recipient.\nThis perspective reflects what is referred to as the gene-centered view of evolution and demonstrates that group selection is a very weak selective force.\n\nMiddle-level evolutionary theories are consistent with general evolutionary theory, but focus on certain domains of functioning (Buss, 2011) Specific evolutionary psychology hypotheses may be derivative from a mid-level theory (Buss, 2011). Three very important middle-level evolutionary theories were contributed by Robert Trivers as well as Robert MacArthur and E. O. Wilson\n\n"}
{"id": "10340923", "url": "https://en.wikipedia.org/wiki?curid=10340923", "title": "Vacuum consolidation", "text": "Vacuum consolidation\n\nVacuum consolidation (or vacuum preloading) is a soft soil improvement method that has been successfully used by geotechnical engineers and specialists of ground improvement companies in countries such as Australia, China, Korea, Thailand and France for soil improvement or land reclamation (Chu et al., 2005). It does not necessarily require surcharge fill and vacuum loads of 80kPa or greater can, typically, be maintained for as long as required. However, if loads of 80kPa or greater are needed in order to achieve the target soil improvement, additional surcharge may be placed on top of the vacuum system. The vacuum preloading method is cheaper and faster than the fill surcharge method for an equivalent load in suitable areas. Where the underlying ground consists of permeable materials, such as sand or sandy clay, the cost of the technique will be significantly increased due to the requirement of cut-off walls into non-permeable layers to seal off the vacuum. It has been suggested by Carter et al. (2005) that the settlement resulting from vacuum preloading is less than that from a surcharge load of the same magnitude as vacuum consolidation is influenced by drainage boundary conditions.\n\n"}
{"id": "5547312", "url": "https://en.wikipedia.org/wiki?curid=5547312", "title": "Vacuum deposition", "text": "Vacuum deposition\n\nVacuum deposition is a family of processes used to deposit layers of material atom-by-atom or molecule-by-molecule on a solid surface. These processes operate at pressures well below atmospheric pressure (i.e., vacuum). The deposited layers can range from a thickness of one atom up to millimeters, forming freestanding structures. Multiple layers of different materials can be used, for example to form optical coatings. The process can be qualified based on the vapor source; physical vapor deposition uses a liquid or solid source and chemical vapor deposition uses a chemical vapor.\n\nThe vacuum environment may serve one or more purposes:\n\nCondensing particles can be generated in various ways:\n\nIn reactive deposition, the depositing material reacts either with a component of the gaseous environment (Ti + N → TiN) or with a co-depositing species (Ti + C → TiC). A plasma environment aids in activating gaseous species (N → 2N) and in decomposition of chemical vapor precursors (SiH → Si + 4H). The plasma may also be used to provide ions for vaporization by sputtering or for bombardment of the substrate for sputter cleaning and for bombardment of the depositing material to densify the structure and tailor properties (ion plating).\n\nWhen the vapor source is a liquid or solid the process is called physical vapor deposition (PVD). When the source is a chemical vapor precursor, the process is called chemical vapor deposition (CVD). The latter has several variants: \"low-pressure chemical vapor deposition\" (LPCVD), Plasma-enhanced chemical vapor deposition (PECVD), and \"plasma-assisted CVD\" (PACVD). Often a combination of PVD and CVD processes are used in the same or connected processing chambers.\n\n\nA thickness of less than one micrometre is generally called a thin film while a thickness greater than one micrometre is called a coating.\n\n\n"}
{"id": "73231", "url": "https://en.wikipedia.org/wiki?curid=73231", "title": "Weather forecasting", "text": "Weather forecasting\n\nWeather forecasting is the application of science and technology to predict the conditions of the atmosphere for a given location and time. People have attempted to predict the weather informally for millennia and formally since the 19th century. Weather forecasts are made by collecting quantitative data about the current state of the atmosphere at a given place and using meteorology to project how the atmosphere will change.\n\nOnce calculated by hand based mainly upon changes in barometric pressure, current weather conditions, and sky condition or cloud cover, weather forecasting now relies on computer-based models that take many atmospheric factors into account. Human input is still required to pick the best possible forecast model to base the forecast upon, which involves pattern recognition skills, teleconnections, knowledge of model performance, and knowledge of model biases. The inaccuracy of forecasting is due to the chaotic nature of the atmosphere, the massive computational power required to solve the equations that describe the atmosphere, the error involved in measuring the initial conditions, and an incomplete understanding of atmospheric processes. Hence, forecasts become less accurate as the difference between current time and the time for which the forecast is being made (the \"range\" of the forecast) increases. The use of ensembles and model consensus help narrow the error and pick the most likely outcome.\n\nThere are a variety of end uses to weather forecasts. Weather warnings are important forecasts because they are used to protect life and property. Forecasts based on temperature and precipitation are important to agriculture, and therefore to traders within commodity markets. Temperature forecasts are used by utility companies to estimate demand over coming days. On an everyday basis, people use weather forecasts to determine what to wear on a given day. Since outdoor activities are severely curtailed by heavy rain, snow and wind chill, forecasts can be used to plan activities around these events, and to plan ahead and survive them. In 2009, the US spent $5.1 billion on weather forecasting.\n\nFor millennia people have tried to forecast the weather. In 650 BC, the Babylonians predicted the weather from cloud patterns as well as astrology. In about 350 BC, Aristotle described weather patterns in \"Meteorologica\". Later, Theophrastus compiled a book on weather forecasting, called the \"Book of Signs\". Chinese weather prediction lore extends at least as far back as 300 BC, which was also around the same time ancient Indian astronomers developed weather-prediction methods. In New Testament times, Christ himself referred to deciphering and understanding local weather patterns, by saying, \"When evening comes, you say, 'It will be fair weather, for the sky is red', and in the morning, 'Today it will be stormy, for the sky is red and overcast.' You know how to interpret the appearance of the sky, but you cannot interpret the signs of the times.\"\n\nIn 904 AD, Ibn Wahshiyya's \"Nabatean Agriculture\", translated into Arabic from an earlier Aramaic work, discussed the weather forecasting of atmospheric changes and signs from the planetary astral alterations; signs of rain based on observation of the lunar phases; and weather forecasts based on the movement of winds.\n\nAncient weather forecasting methods usually relied on observed patterns of events, also termed pattern recognition. For example, it might be observed that if the sunset was particularly red, the following day often brought fair weather. This experience accumulated over the generations to produce weather lore. However, not all of these predictions prove reliable, and many of them have since been found not to stand up to rigorous statistical testing.\n\nIt was not until the invention of the electric telegraph in 1835 that the modern age of weather forecasting began. Before that, the fastest that distant weather reports could travel was around 100 miles per day (160 km/d), but was more typically 40–75 miles per day (60–120 km/day) (whether by land or by sea). By the late 1840s, the telegraph allowed reports of weather conditions from a wide area to be received almost instantaneously, allowing forecasts to be made from knowledge of weather conditions further upwind.\n\nThe two men credited with the birth of forecasting as a science were an officer of the Royal Navy Francis Beaufort and his protégé Robert FitzRoy. Both were influential men in British naval and governmental circles, and though ridiculed in the press at the time, their work gained scientific credence, was accepted by the Royal Navy, and formed the basis for all of today's weather forecasting knowledge.\n\nBeaufort developed the Wind Force Scale and Weather Notation coding, which he was to use in his journals for the remainder of his life. He also promoted the development of reliable tide tables around British shores, and with his friend William Whewell, expanded weather record-keeping at 200 British Coast guard stations.\n\nRobert FitzRoy was appointed in 1854 as chief of a new department within the Board of Trade to deal with the collection of weather data at sea as a service to mariners. This was the forerunner of the modern Meteorological Office. All ship captains were tasked with collating data on the weather and computing it, with the use of tested instruments that were loaned for this purpose.\nA storm in 1859 that caused the loss of the \"Royal Charter\" inspired FitzRoy to develop charts to allow predictions to be made, which he called \"forecasting the weather\", thus coining the term \"weather forecast\". Fifteen land stations were established to use the telegraph to transmit to him daily reports of weather at set times leading to the first gale warning service. His warning service for shipping was initiated in February 1861, with the use of telegraph communications. The first daily weather forecasts were published in \"The Times\" in 1861. In the following year a system was introduced of hoisting storm warning cones at the principal ports when a gale was expected. The \"Weather Book\" which FitzRoy published in 1863 was far in advance of the scientific opinion of the time.\n\nAs the electric telegraph network expanded, allowing for the more rapid dissemination of warnings, a national observational network was developed, which could then be used to provide synoptic analyses. Instruments to continuously record variations in meteorological parameters using photography were supplied to the observing stations from Kew Observatory – these cameras had been invented by Francis Ronalds in 1845 and his barograph had earlier been used by FitzRoy.\n\nTo convey accurate information, it soon became necessary to have a standard vocabulary describing clouds; this was achieved by means of a series of classifications first achieved by Luke Howard in 1802, and standardized in the \"International Cloud Atlas\" of 1896.\n\nIt was not until the 20th century that advances in the understanding of atmospheric physics led to the foundation of modern numerical weather prediction. In 1922, English scientist Lewis Fry Richardson published \"Weather Prediction By Numerical Process\", after finding notes and derivations he worked on as an ambulance driver in World War I. He described therein how small terms in the prognostic fluid dynamics equations governing atmospheric flow could be neglected, and a finite differencing scheme in time and space could be devised, to allow numerical prediction solutions to be found.\n\nRichardson envisioned a large auditorium of thousands of people performing the calculations and passing them to others. However, the sheer number of calculations required was too large to be completed without the use of computers, and the size of the grid and time steps led to unrealistic results in deepening systems. It was later found, through numerical analysis, that this was due to numerical instability. The first computerised weather forecast was performed by a team composed of American meteorologists Jule Charney, Philip Thompson, Larry Gates, and Norwegian meteorologist Ragnar Fjørtoft, applied mathematician John von Neumann, and ENIAC programmer Klara Dan von Neumann. Practical use of numerical weather prediction began in 1955, spurred by the development of programmable electronic computers.\n\nThe first ever daily weather forecasts were published in \"The Times\" on August 1, 1861, and the first weather maps were produced later in the same year. In 1911, the Met Office began issuing the first marine weather forecasts via radio transmission. These included gale and storm warnings for areas around Great Britain. In the United States, the first public radio forecasts were made in 1925 by Edward B. \"E.B.\" Rideout, on WEEI, the Edison Electric Illuminating station in Boston. Rideout came from the U.S. Weather Bureau, as did WBZ weather forecaster G. Harold Noyes in 1931.\n\nThe world's first televised weather forecasts, including the use of weather maps, were experimentally broadcast by the BBC in 1936. This was brought into practice in 1949 after World War II. George Cowling gave the first weather forecast while being televised in front of the map in 1954. In America, experimental television forecasts were made by James C Fidler in Cincinnati in either 1940 or 1947 on the DuMont Television Network. In the late 1970s and early 80s, John Coleman, the first weatherman on ABC-TV's Good Morning America, pioneered the use of on-screen weather satellite information and computer graphics for television forecasts. Coleman was a co-founder of The Weather Channel (TWC) in 1982. TWC is now a 24-hour cable network. Some weather channels have started broadcasting on live broadcasting programs such as YouTube and Periscope to reach more viewers.\n\nThe basic idea of numerical weather prediction is to sample the state of the fluid at a given time and use the equations of fluid dynamics and thermodynamics to estimate the state of the fluid at some time in the future. The main inputs from country-based weather services are surface observations from automated weather stations at ground level over land and from weather buoys at sea. The World Meteorological Organization acts to standardize the instrumentation, observing practices and timing of these observations worldwide. Stations either report hourly in METAR reports, or every six hours in SYNOP reports. Sites launch radiosondes, which rise through the depth of the troposphere and well into the stratosphere. Data from weather satellites are used in areas where traditional data sources are not available. Compared with similar data from radiosondes, the satellite data has the advantage of global coverage, however at a lower accuracy and resolution. Meteorological radar provide information on precipitation location and intensity, which can be used to estimate precipitation accumulations over time. Additionally, if a pulse Doppler weather radar is used then wind speed and direction can be determined.\nCommerce provides pilot reports along aircraft routes, and ship reports along shipping routes. Research flights using reconnaissance aircraft fly in and around weather systems of interest such as tropical cyclones. Reconnaissance aircraft are also flown over the open oceans during the cold season into systems that cause significant uncertainty in forecast guidance, or are expected to be of high impact 3–7 days into the future over the downstream continent.\n\nModels are \"initialized\" using this observed data. The irregularly spaced observations are processed by data assimilation and objective analysis methods, which perform quality control and obtain values at locations usable by the model's mathematical algorithms (usually an evenly spaced grid). The data are then used in the model as the starting point for a forecast. Commonly, the set of equations used to predict the known as the physics and dynamics of the atmosphere are called primitive equations. These equations are initialized from the analysis data and rates of change are determined. The rates of change predict the state of the atmosphere a short time into the future. The equations are then applied to this new atmospheric state to find new rates of change, and these new rates of change predict the atmosphere at a yet further time into the future. This \"time stepping\" procedure is continually repeated until the solution reaches the desired forecast time.\n\nThe length of the time step chosen within the model is related to the distance between the points on the computational grid, and is chosen to maintain numerical stability. Time steps for global models are on the order of tens of minutes, while time steps for regional models are between one and four minutes. The global models are run at varying times into the future. The Met Office's Unified Model is run six days into the future, the European Centre for Medium-Range Weather Forecasts model is run out to 10 days into the future, while the Global Forecast System model run by the Environmental Modeling Center is run 16 days into the future. The visual output produced by a model solution is known as a prognostic chart, or \"prog\". The raw output is often modified before being presented as the forecast. This can be in the form of statistical techniques to remove known biases in the model, or of adjustment to take into account consensus among other numerical weather forecasts. MOS or model output statistics is a technique used to interpret numerical model output and produce site-specific guidance. This guidance is presented in coded numerical form, and can be obtained for nearly all National Weather Service reporting stations in the United States. As proposed by Edward Lorenz in 1963, long range forecasts, those made at a range of two weeks or more, are impossible to definitively predict the state of the atmosphere, owing to the chaotic nature of the fluid dynamics equations involved. In numerical models, extremely small errors in initial values double roughly every five days for variables such as temperature and wind velocity.\n\nEssentially, a model is a computer program that produces meteorological information for future times at given locations and altitudes. Within any modern model is a set of equations, known as the primitive equations, used to predict the future state of the atmosphere. These equations—along with the ideal gas law—are used to evolve the density, pressure, and potential temperature scalar fields and the velocity vector field of the atmosphere through time. Additional transport equations for pollutants and other aerosols are included in some primitive-equation mesoscale models as well. The equations used are nonlinear partial differential equations, which are impossible to solve exactly through analytical methods, with the exception of a few idealized cases. Therefore, numerical methods obtain approximate solutions. Different models use different solution methods: some global models use spectral methods for the horizontal dimensions and finite difference methods for the vertical dimension, while regional models and other global models usually use finite-difference methods in all three dimensions.\n\nThe simplest method of forecasting the weather, persistence, relies upon today's conditions to forecast the conditions tomorrow. This can be a valid way of forecasting the weather when it is in a steady state, such as during the summer season in the tropics. This method of forecasting strongly depends upon the presence of a stagnant weather pattern. Therefore, when in a fluctuating weather pattern, this method of forecasting becomes inaccurate. It can be useful in both short range forecasts and long range forecasts.\n\nMeasurements of barometric pressure and the pressure tendency (the change of pressure over time) have been used in forecasting since the late 19th century. The larger the change in pressure, especially if more than , the larger the change in weather can be expected. If the pressure drop is rapid, a low pressure system is approaching, and there is a greater chance of rain. Rapid pressure rises are associated with improving weather conditions, such as clearing skies.\n\nAlong with pressure tendency, the condition of the sky is one of the more important parameters used to forecast weather in mountainous areas. Thickening of cloud cover or the invasion of a higher cloud deck is indicative of rain in the near future. High thin cirrostratus clouds can create halos around the sun or moon, which indicates an approach of a warm front and its associated rain. Morning fog portends fair conditions, as rainy conditions are preceded by wind or clouds that prevent fog formation. The approach of a line of thunderstorms could indicate the approach of a cold front. Cloud-free skies are indicative of fair weather for the near future. A bar can indicate a coming tropical cyclone. The use of sky cover in weather prediction has led to various weather lore over the centuries.\n\nThe forecasting of the weather within the next six hours is often referred to as nowcasting. In this time range it is possible to forecast smaller features such as individual showers and thunderstorms with reasonable accuracy, as well as other features too small to be resolved by a computer model. A human given the latest radar, satellite and observational data will be able to make a better analysis of the small scale features present and so will be able to make a more accurate forecast for the following few hours. However, there are now expert systems using those data and mesoscale numerical model to make better extrapolation, including evolution of those features in time.\n\nIn the past, the human forecaster was responsible for generating the entire weather forecast based upon available observations. Today, human input is generally confined to choosing a model based on various parameters, such as model biases and performance. Using a consensus of forecast models, as well as ensemble members of the various models, can help reduce forecast error. However, regardless how small the average error becomes with any individual system, large errors within any particular piece of guidance are still possible on any given model run. Humans are required to interpret the model data into weather forecasts that are understandable to the end user. Humans can use knowledge of local effects that may be too small in size to be resolved by the model to add information to the forecast. While increasing accuracy of forecast models implies that humans may no longer be needed in the forecast process at some point in the future, there is currently still a need for human intervention.\n\nThe analog technique is a complex way of making a forecast, requiring the forecaster to remember a previous weather event that is expected to be mimicked by an upcoming event. What makes it a difficult technique to use is that there is rarely a perfect analog for an event in the future. Some call this type of forecasting pattern recognition. It remains a useful method of observing rainfall over data voids such as oceans, as well as the forecasting of precipitation amounts and distribution in the future. A similar technique is used in medium range forecasting, which is known as teleconnections, when systems in other locations are used to help pin down the location of another system within the surrounding regime. An example of teleconnections are by using El Niño-Southern Oscillation (ENSO) related phenomena.\n\nMost end users of forecasts are members of the general public. Thunderstorms can create strong winds and dangerous lightning strikes that can lead to deaths, power outages, and widespread hail damage. Heavy snow or rain can bring transportation and commerce to a stand-still, as well as cause flooding in low-lying areas. Excessive heat or cold waves can sicken or kill those with inadequate utilities, and droughts can impact water usage and destroy vegetation.\n\nSeveral countries employ government agencies to provide forecasts and watches/warnings/advisories to the public in order to protect life and property and maintain commercial interests. Knowledge of what the end user needs from a weather forecast must be taken into account to present the information in a useful and understandable way. Examples include the National Oceanic and Atmospheric Administration's National Weather Service (NWS) and Environment Canada's Meteorological Service (MSC). Traditionally, newspaper, television, and radio have been the primary outlets for presenting weather forecast information to the public. In addition, some cities had weather beacons. Increasingly, the internet is being used due to the vast amount of specific information that can be found. In all cases, these outlets update their forecasts on a regular basis.\n\nA major part of modern weather forecasting is the severe weather alerts and advisories that the national weather services issue in the case that severe or hazardous weather is expected. This is done to protect life and property. Some of the most commonly known of severe weather advisories are the severe thunderstorm and tornado warning, as well as the severe thunderstorm and tornado watch. Other forms of these advisories include winter weather, high wind, flood, tropical cyclone, and fog. Severe weather advisories and alerts are broadcast through the media, including radio, using emergency systems as the Emergency Alert System, which break into regular programming.\n\nThe low temperature forecast for the current day is calculated using the lowest temperature found between 7pm that evening through 7am the following morning. So, in short, today's forecasted low is most likely tomorrow's low temperature.\n\nThere are a number of sectors with their own specific needs for weather forecasts and specialist services are provided to these users.\n\nBecause the aviation industry is especially sensitive to the weather, accurate weather forecasting is essential. Fog or exceptionally low ceilings can prevent many aircraft from landing and taking off. Turbulence and icing are also significant in-flight hazards. Thunderstorms are a problem for all aircraft because of severe turbulence due to their updrafts and outflow boundaries, icing due to the heavy precipitation, as well as large hail, strong winds, and lightning, all of which can cause severe damage to an aircraft in flight. Volcanic ash is also a significant problem for aviation, as aircraft can lose engine power within ash clouds. On a day-to-day basis airliners are routed to take advantage of the jet stream tailwind to improve fuel efficiency. Aircrews are briefed prior to takeoff on the conditions to expect en route and at their destination. Additionally, airports often change which runway is being used to take advantage of a headwind. This reduces the distance required for takeoff, and eliminates potential crosswinds.\n\nCommercial and recreational use of waterways can be limited significantly by wind direction and speed, wave periodicity and heights, tides, and precipitation. These factors can each influence the safety of marine transit. Consequently, a variety of codes have been established to efficiently transmit detailed marine weather forecasts to vessel pilots via radio, for example the MAFOR (marine forecast). Typical weather forecasts can be received at sea through the use of RTTY, Navtex and Radiofax.\n\nFarmers rely on weather forecasts to decide what work to do on any particular day. For example, drying hay is only feasible in dry weather. Prolonged periods of dryness can ruin cotton, wheat, and corn crops. While corn crops can be ruined by drought, their dried remains can be used as a cattle feed substitute in the form of silage. Frosts and freezes play havoc with crops both during the spring and fall. For example, peach trees in full bloom can have their potential peach crop decimated by a spring freeze. Orange groves can suffer significant damage during frosts and freezes, regardless of their timing.\n\nWeather forecasting of wind, precipitations and humidity is essential for preventing and controlling wildfires. Different indices, like the \"Forest fire weather index\" and the \"Haines Index\", have been developed to predict the areas more at risk to experience fire from natural or human causes. Conditions for the development of harmful insects can be predicted by forecasting the evolution of weather, too.\n\nElectricity and gas companies rely on weather forecasts to anticipate demand, which can be strongly affected by the weather. They use the quantity termed the degree day to determine how strong of a use there will be for heating (heating degree day) or cooling (cooling degree day). These quantities are based on a daily average temperature of . Cooler temperatures force heating degree days (one per degree Fahrenheit), while warmer temperatures force cooling degree days. In winter, severe cold weather can cause a surge in demand as people turn up their heating. Similarly, in summer a surge in demand can be linked with the increased use of air conditioning systems in hot weather. By anticipating a surge in demand, utility companies can purchase additional supplies of power or natural gas before the price increases, or in some circumstances, supplies are restricted through the use of brownouts and blackouts.\n\nIncreasingly, private companies pay for weather forecasts tailored to their needs so that they can increase their profits or avoid large losses. For example, supermarket chains may change the stocks on their shelves in anticipation of different consumer spending habits in different weather conditions. Weather forecasts can be used to invest in the commodity market, such as futures in oranges, corn, soybeans, and oil.\n\nRoyal Navy\n\nThe UK Royal Navy, working with the UK Met Office, has its own specialist branch of weather observers and forecasters, as part of the Hydrographic and Meteorological (HM) specialisation, who monitor and forecast operational conditions across the globe, to provide accurate and timely weather and oceanographic information to submarines, ships and Fleet Air Arm aircraft.\n\nA mobile unit in the RAF, working with the UK Met Office, forecasts the weather for regions in which British, allied servicemen and women are deployed. A group based at Camp Bastion provides forecasts for the British armed forces in Afghanistan.\n\nSimilar to the private sector, military weather forecasters present weather conditions to the war fighter community. Military weather forecasters provide pre-flight and in-flight weather briefs to pilots and provide real time resource protection services for military installations. Naval forecasters cover the waters and ship weather forecasts. The United States Navy provides a special service to both themselves and the rest of the federal government by issuing forecasts for tropical cyclones across the Pacific and Indian Oceans through their Joint Typhoon Warning Center.\n\nWithin the United States, Air Force Weather provides weather forecasting for the Air Force and the Army. Air Force forecasters cover air operations in both wartime and peacetime operations and provide Army support; United States Coast Guard marine science technicians provide ship forecasts for ice breakers and other various operations within their realm; and Marine forecasters provide support for ground- and air-based United States Marine Corps operations. All four military branches take their initial enlisted meteorology technical training at Keesler Air Force Base. Military and civilian forecasters actively cooperate in analyzing, creating and critiquing weather forecast products.\n\n\nThese are academic or governmental meteorology organizations. Most provide at least a limited forecast for their area of interest on their website.\n\n"}
{"id": "469990", "url": "https://en.wikipedia.org/wiki?curid=469990", "title": "Whirlpool", "text": "Whirlpool\n\nA whirlpool is a body of rotating water produced by the meeting of opposing currents. The vast majority of whirlpools are not very powerful and very small whirlpools can be easily seen when a bath or a sink is draining. More powerful ones in seas or oceans may be termed maelstroms. \"Vortex\" is the proper term for any whirlpool that has a downdraft.\n\nIn oceans, in narrow straits with fast flowing water, whirlpools are normally caused by tides; there are few stories of large ships ever being sucked into such a maelstrom, although smaller craft are in danger. Smaller whirlpools also appear at the base of many waterfalls and can also be observed downstream from manmade structures such as weirs and dams. In the case of powerful waterfalls, like Niagara Falls, these whirlpools can be quite strong.\n\nThe Maelstrom of Saltstraumen is the Earth's strongest maelstrom, and is located close to the Arctic Circle, round the bay on the Highway 17, south-east of the city of Bodø, Norway. The strait at its narrowest is in width and water \"funnels\" through the channel four times a day. It is estimated that of water passes the narrow strait during this event. The water is creamy in colour and most turbulent during high tide, which is witnessed by thousands of tourists. It reaches speeds of , with mean speed of about . As navigation is dangerous in this strait only a small slot of time is available for large ships to pass through. Its impressive strength is caused by the world's strongest tide occurring in the same location during the new and full moon. A narrow channel of length connects the outer Saltfjord with its extension, the large Skjerstadfjord, causing a colossal tide which in turn produces the Saltstraumen maelstrom.\n\nMoskstraumen is an unusual system of whirlpools in the open seas in the Lofoten Islands off the Norwegian coast. It is the second strongest whirlpool in the world with flow currents reaching speeds as high as . It finds mention in several books and movies.\n\nThe Moskstraumen is formed by the combination of powerful semi-diurnal tides and the unusual shape of the seabed, with a shallow ridge between the Moskenesøya and Værøy islands which amplifies and whirls the tidal currents.\n\nThe fictional depictions of the Maelstrom by Edgar Allan Poe, Jules Verne, and Cixin Liu describe it as a gigantic circular vortex that reaches the bottom of the ocean, when in fact it is a set of currents and crosscurrents with a rate of . Poe described this phenomenon in his short story \"A Descent into the Maelstrom,\" which in 1841 was the first to use the word \"maelstrom\" in the English language; in this story related to the Lofoten Maelstrom, two fishermen are swallowed by the maelstrom while one survives miraculously.\n\nThe Corryvreckan is a narrow strait between the islands of Jura and Scarba, in Argyll and Bute, on the northern side of the Gulf of Corryvreckan, Scotland. It is the third-largest whirlpool in the world. Flood tides and inflow from the Firth of Lorne to the west can drive the waters of Corryvreckan to waves of over , and the roar of the resulting maelstrom, which reaches speeds of , can be heard away. Though it was initially classified as non-navigable by the British navy it was later categorized as \"extremely dangerous\".\n\nA documentary team from Scottish independent producers Northlight Productions once threw a mannequin into the Corryvreckan (\"the Hag\") with a life jacket and depth gauge. The mannequin was swallowed and spat up far down current with a depth gauge reading of with evidence of being dragged along the bottom for a great distance.\n\nOld Sow whirlpool is located between Deer Island, New Brunswick, Canada, and Moose Island, Eastport, Maine, USA. It is given the epithet \"pig-like\" as it makes a screeching noise when the vortex is at its full fury and reaches speeds of up to . The smaller whirlpools around this Old Sow are known as \"Piglets.\n\nThe Naruto whirlpools are located in the Naruto Strait near Awaji Island in Japan, which have speeds of .\n\nSkookumchuck Narrows is a tidal rapids that develops whirlpools, on the Sunshine Coast, Canada with current speeds exceeding .\n\nFrench Pass () is a narrow and treacherous stretch of water that separates D'Urville Island from the north end of the South Island of New Zealand. In 2000 a whirlpool there caught student divers, resulting in fatalities.\n\nThere was a short-lived whirlpool that sucked in a portion of the 1300 acre (~530 hectares) Lake Peigneur in Louisiana, United States after a drilling mishap in November 1980. This was not a naturally occurring whirlpool, but a man-made disaster caused by underwater drillers breaking through the roof of a salt mine. The lake then drained into the mine until the mine filled and the water levels equalized but the ten-foot deep lake was now 1,300 feet deep. This mishap resulted in destruction of five houses, loss of nineteen barges and eight tug boats, oil rigs, a mobile home, and most of a botanical garden. The adjacent settlement of Jefferson Island was reduced in area by 10%. A crater 0.5-mile (~1km) across was left behind. Nine of the barges which had sunk floated back.\n\nA more recent example of a man-made whirlpool that received significant media coverage was in early June 2015, when an intake vortex formed in Lake Texoma, on the Oklahoma–Texas border, near the floodgates of the dam that forms the lake. At the time of the whirlpool's formation, the lake was being drained after reaching its highest level ever. The Army Corps of Engineers, which operates the dam and lake, expected that the whirlpool would last until the lake reached normal seasonal levels by late July.\n\nPowerful whirlpools have killed unlucky seafarers, but their power tends to be exaggerated by laymen. There are virtually no stories of large ships ever being sucked into a whirlpool. Tales like those by Paul the Deacon, Edgar Allan Poe, and Jules Verne are entirely fictional.\n\nHowever, temporary whirlpools caused by major engineering disasters are capable of submerging large ships. A prominent example is the drilling disaster that occurred on November 20, 1980, in Lake Peigneur. A drilling platform, eleven barges, several trees, and multiple acres of the surrounding terrain were submerged by the resulting whirlpool. Days after the disaster, once the water pressure equalized, nine of the eleven sunken barges popped out of the whirlpool and refloated on the lake's surface.\n\nApart from Poe and Verne other literary source is of the 1500s, of Olaus Magnus, a Swedish Bishop, who had stated that the maelstrom which was more powerful than \"The Odyssey\" destroyed ships which sank to the bottom of the sea, and even whales were sucked in. Pytheas, the Greek historian, also mentioned that maelstroms swallowed ships and threw them up again.\n\nCharybdis in Greek mythology was later rationalized as a whirlpool, which sucked entire ships into its fold in the narrow coast of Sicily, a disaster faced by navigators.\n\nIn the 8th century, Paul the Deacon, who had lived among the Belgii, described tidal bores and the maelstrom for a Mediterranean audience unused to such violent tidal surges:\n\nThree of the most notable literary references to the Lofoten Maelstrom date from the nineteenth century. The first is the Edgar Allan Poe short story \"A Descent into the Maelström\" (1841). The second is \"20,000 Leagues Under the Sea\" (1870), the famous novel by Jules Verne. At the end of this novel, Captain Nemo seems to commit suicide, sending his \"Nautilus\" submarine into the Maelstrom (although in Verne's sequel Nemo and the Nautilus were seen to have survived). The \"Norway maelstrom\" is also mentioned in Herman Melville's \"Moby-Dick\".\n\nIn the 'Life of St Columba', the author, Adomnan of Iona', attributes to the saint miraculous knowledge of a particular bishop who ran into a whirlpool off the coast of Ireland. In Adomnan's narrative, he quotes Columba saying\n\nOne of the earliest uses in English of the Scandinavian word (\"malström\" or \"malstrøm\") was by Edgar Allan Poe in his short story \"A Descent into the Maelström\" (1841). In turn, the Nordic word is derived from the Dutch \"maelstrom\", modern spelling \"maalstroom\", from \"malen\" (\"to grind\") and \"stroom\" (\"stream\"), to form the meaning \"grinding current\" or literally \"mill-stream\", in the sense of milling (grinding) grain.\n\n\n\n"}
