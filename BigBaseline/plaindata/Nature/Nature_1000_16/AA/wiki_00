{"id": "1758866", "url": "https://en.wikipedia.org/wiki?curid=1758866", "title": "Accelerating change", "text": "Accelerating change\n\nIn futures studies and the history of technology, accelerating change is a perceived increase in the rate of technological change throughout history, which may suggest faster and more profound change in the future and may or may not be accompanied by equally profound social and cultural change.\n\nIn 1910 during the town planning conference of London Daniel Burnham noted that \"But it is not merely in the number of facts or sorts of knowledge that progress lies: it is still more in the geometric ratio of sophistication, in the geometric widening of the sphere of knowledge, which every year is taking in a larger percentage of people as time goes on.\" and later on \"It is the argument with which I began, that a mighty change having come about in fifty years, and our pace of development having immensely accelerated, our sons and grandsons are going to demand and get results that would stagger us.\"\n\nIn 1938, Buckminster Fuller introduced the word ephemeralization to describe the trends of \"doing more with less\" in chemistry, health and other areas of industrial development. In 1946, Fuller published a chart of the discoveries of the chemical elements over time to highlight the development of accelerating acceleration in human knowledge acquisition.\n\nIn 1958, Stanislaw Ulam wrote in reference to a conversation with John von Neumann: \n\nIn a series of published articles from 1974–1979, and then in his 1988 book \"Mind Children\", computer scientist and futurist Hans Moravec generalizes Moore's law to make predictions about the future of artificial life. Moore's law describes an exponential growth pattern in the complexity of integrated semiconductor circuits. Moravec extends this to include technologies from long before the integrated circuit to future forms of technology. Moravec outlines a timeline and a scenario in which robots will evolve into a new series of artificial species, starting around 2030–2040.\nIn \"Robot: Mere Machine to Transcendent Mind\", published in 1998, Moravec further considers the implications of evolving robot intelligence, generalizing Moore's Law to technologies predating the integrated circuit, and also plotting the exponentially increasing computational power of the brains of animals in evolutionary history. Extrapolating these trends, he speculates about a coming \"mind fire\" of rapidly expanding superintelligence similar to the explosion of intelligence predicted by Vinge.\n\nIn his TV series \"Connections\" (1978)—and sequels \"Connections²\" (1994) and \"Connections³\" (1997)—James Burke explores an \"Alternative View of Change\" (the subtitle of the series) that rejects the conventional linear and teleological view of historical progress. Burke contends that one cannot consider the development of any particular piece of the modern world in isolation. Rather, the entire gestalt of the modern world is the result of a web of interconnected events, each one consisting of a person or group acting for reasons of their own motivations (e.g., profit, curiosity, religious) with no concept of the final, modern result to which the actions of either them or their contemporaries would lead. The interplay of the results of these isolated events is what drives history and innovation, and is also the main focus of the series and its sequels.\n\nBurke also explores three corollaries to his initial thesis. The first is that, if history is driven by individuals who act only on what they know at the time, and not because of any idea as to where their actions will eventually lead, then predicting the future course of technological progress is merely conjecture. Therefore, if we are astonished by the connections Burke is able to weave among past events, then we will be equally surprised to what the events of today eventually will lead, especially events we weren't even aware of at the time.\n\nThe second and third corollaries are explored most in the introductory and concluding episodes, and they represent the downside of an interconnected history. If history progresses because of the synergistic interaction of past events and innovations, then as history does progress, the number of these events and innovations increases. This increase in possible connections causes the process of innovation to not only continue, but to accelerate. Burke poses the question of what happens when this rate of innovation, or more importantly change itself, becomes too much for the average person to handle, and what this means for individual power, liberty, and privacy.\n\nIn his book \"Mindsteps to the Cosmos\" (HarperCollins, August 1983), Gerald S. Hawkins elucidated his notion of 'mindsteps', dramatic and irreversible changes to paradigms or world views. He identified five distinct mindsteps in human history, and the technology that accompanied these \"new world views\": the invention of imagery, writing, mathematics, printing, the telescope, rocket, radio, TV, computer... \"Each one takes the collective mind closer to reality, one stage further along in its understanding of the relation of humans to the cosmos.\" He noted: \"The waiting period between the mindsteps is getting shorter. One can't help noticing the acceleration.\" Hawkins' empirical 'mindstep equation' quantified this, and gave dates for future mindsteps. The date of the next mindstep (5; the series begins at 0) is given as 2021, with two further, successively closer mindsteps in 2045 and 2051, until the limit of the series in 2053. His speculations ventured beyond the technological:\n\nThe mathematician Vernor Vinge popularized his ideas about exponentially accelerating technological change in the science fiction novel \"Marooned in Realtime\" (1986), set in a world of rapidly accelerating progress leading to the emergence of more and more sophisticated technologies separated by shorter and shorter time intervals, until a point beyond human comprehension is reached. His subsequent Hugo award-winning novel \"A Fire Upon the Deep\" (1992) starts with an imaginative description of the evolution of a superintelligence passing through exponentially accelerating developmental stages ending in a transcendent, almost omnipotent power unfathomable by mere humans. His already mentioned influential 1993 paper on the technological singularity compactly summarizes the basic ideas.\n\nIn his 1999 book \"The Age of Spiritual Machines\", Ray Kurzweil proposed \"The Law of Accelerating Returns\", according to which the rate of change in a wide variety of evolutionary systems (including but not limited to the growth of technologies) tends to increase exponentially. He gave further focus to this issue in a 2001 essay entitled \"The Law of Accelerating Returns\". In it, Kurzweil, after Moravec, argued for extending Moore's Law to describe exponential growth of diverse forms of technological progress. Whenever a technology approaches some kind of a barrier, according to Kurzweil, a new technology will be invented to allow us to cross that barrier. He cites numerous past examples of this to substantiate his assertions. He predicts that such paradigm shifts have and will continue to become increasingly common, leading to \"technological change so rapid and profound it represents a rupture in the fabric of human history.\" He believes the Law of Accelerating Returns implies that a technological singularity will occur before the end of the 21st century, around 2045. The essay begins:\n\nThe Law of Accelerating Returns has in many ways altered public perception of Moore's law. It is a common (but mistaken) belief that Moore's law makes predictions regarding all forms of technology, when really it only concerns semiconductor circuits. Many futurists still use the term \"Moore's law\" to describe ideas like those put forth by Moravec, Kurzweil and others.\n\nAccording to Kurzweil, since the beginning of evolution, more complex life forms have been evolving exponentially faster, with shorter and shorter intervals between the emergence of radically new life forms, such as human beings, who have the capacity to engineer (i.e. intentionally design with efficiency) a new trait which replaces relatively blind evolutionary mechanisms of selection for efficiency. By extension, the rate of technical progress amongst humans has also been exponentially increasing, as we discover more effective ways to do things, we also discover more effective ways to learn, i.e. language, numbers, written language, philosophy, scientific method, instruments of observation, tallying devices, mechanical calculators, computers, each of these major advances in our ability to account for information occur increasingly close together. Already within the past sixty years, life in the industrialized world has changed almost beyond recognition except for living memories from the first half of the 20th century. This pattern will culminate in unimaginable technological progress in the 21st century, leading to a singularity. Kurzweil elaborates on his views in his books \"The Age of Spiritual Machines\" and \"The Singularity Is Near\".\n\nAccelerating change may not be restricted to the Anthropocene Epoch, but a general and predictable developmental feature of the universe. The physical processes that generate an acceleration such as Moore's law are positive feedback loops giving rise to exponential or superexponential technological change. These dynamics lead to increasingly efficient and dense configurations of Space, Time, Energy, and Matter (STEM efficiency and density, or STEM \"compression\"). At the physical limit, this developmental process of accelerating change leads to black hole density organizations, a conclusion also reached by studies of the ultimate physical limits of computation in the universe.\n\nApplying this vision to the search for extraterrestrial intelligence leads to the idea that advanced intelligent life reconfigures itself into a black hole. Such advanced life forms would be interested in inner space, rather than outer space and interstellar expansion. They would thus in some way transcend reality, not be observable and it would be a solution to Fermi's paradox called the \"transcension hypothesis\", Another solution is that the black holes we observe could actually be interpreted as intelligent super-civilizations feeding on stars, or \"stellivores\". \nThis dynamics of evolution and development is an invitation to study the universe itself as evolving, developing. If the universe is a kind of superorganism, it may possibly tend to reproduce, naturally or artificially, with intelligent life playing a role.\n\nExamples of large human \"buy-ins\" into technology include the computer revolution, as well as massive government projects like the Manhattan Project and the Human Genome Project. The foundation organizing the Methuselah Mouse Prize believes aging research could be the subject of such a massive project if substantial progress is made in slowing or reversing cellular aging in mice.\n\nBoth Theodore Modis and Jonathan Huebner have argued—each from different perspectives—that the rate of technological innovation has not only ceased to rise, but is actually now declining.\n\nIn fact, \"technological singularity\" is just one of a few singularities detected through the analysis of a number of characteristics of the World System development, for example, with respect to the world population, world GDP, and some other economic indices. It has been shown that the hyperbolic pattern of the world demographic, economic, cultural, urbanistic, and technological growth (observed for many centuries, if not millennia prior to the 1970s) could be accounted for by a rather simple mechanism, the nonlinear second-order positive feedback, that was shown long ago to generate precisely the hyperbolic growth, known also as the \"blow-up regime\" (implying just finite-time singularities). In our case this nonlinear second order positive feedback looks as follows: more people – more potential inventors – faster technological growth – the carrying capacity of the Earth grows faster – faster population growth – more people – more potential inventors – faster technological growth, and so on. On the other hand, this research has shown that since the 1970s the World System does not develop hyperbolically any more, its development diverges more and more from the blow-up regime, and at present it is moving \"from singularity\", rather than \"toward singularity\".\n\nJürgen Schmidhuber calls the Singularity \"Omega\", referring to Teilhard de Chardin's Omega Point (1916). For Omega = 2040, he says the series Omega - 2 human lifetimes (n < 10; one lifetime = 80 years) roughly matches the most important events in human history.\n\nKurzweil created the following graphs to illustrate his beliefs concerning and his justification for his Law of Accelerating Returns.\n\n\n"}
{"id": "4327451", "url": "https://en.wikipedia.org/wiki?curid=4327451", "title": "Adolf Dygasiński", "text": "Adolf Dygasiński\n\nAdolf Dygasiński (March 7, 1839, Niegosławice–June 3, 1902, Grodzisk Mazowiecki) was a Polish novelist, publicist and educator. In Polish literature, he was one of the leading representatives of Naturalism. \nDuring his literary career, Dygasiński wrote forty-two short stories and novels. \nSince 1884 his works were being published in book-form and enjoyed considerable success. \nThey were translated into Russian and German. \nIn 1891, Dygasiński went on a trip to Brazil on a trail of Polish emigrants from Partitioned Poland. \nHe produced a series of letters describing the tragic fate of Polish emigrees in South America. In the following years Dygasiński maintained a position of a tutor and coach for numerous wealthy landowning families. Late in life he settled in Warsaw, where he died on June 6, 1902, and was buried at the local Powązkowski Cemetery.\n\nIn his work Dygasiński often focused on topics of rural life and residents of small towns, highlighting the common fate of both, human and animal communities. Some of his most important work include: \n\n\n\n"}
{"id": "10038314", "url": "https://en.wikipedia.org/wiki?curid=10038314", "title": "Arizona flagstone", "text": "Arizona flagstone\n\nArizona flagstone is composed of rounded grains of quartz which are cemented by silica. Other minerals are present, mostly as thin seams of clay, mica, secondary calcite, and gypsum. Arizona flagstone is mainly quarried from the Coconino and Prescott National Forests.\n\nAlthough flagstone and dimension stone are quarried from all over the state of Arizona, the town of Ash Fork, Arizona, is well known as the center of production and has proclaimed itself \"The Flagstone Capital of the World\". Extensive outcrops of Arizona flagstone are also found in Mohave, Coconino, Yavapai, Navajo, Apache, and Gila counties.\n\n"}
{"id": "36438005", "url": "https://en.wikipedia.org/wiki?curid=36438005", "title": "Bliha Falls", "text": "Bliha Falls\n\nThe Bliha Falls (; or ), called by locals \"Blihin Skok\" (' or '), is a waterfall on the Bliha river located near Fajtovci, 14 kilometers west of Sanski Most, Bosnia and Herzegovina. At this point, the water of the Bliha drops from 56 meters high cliff. It is designated a natural monument since 1965.\n\n"}
{"id": "54474095", "url": "https://en.wikipedia.org/wiki?curid=54474095", "title": "Contained earth", "text": "Contained earth\n\nContained earth (CE) is a structurally designed natural building material that combines containment, inexpensive reinforcement, and strongly cohesive earthen walls. CE is earthbag construction that can be calibrated for several seismic risk levels based on building soil strength and plan standards for adequate bracing.\n\nThere is a recognized need for structural understanding of alternative building materials. Construction guidelines for CE are currently under development, based on the New Zealand's performance-based code for adobe and rammed earth. \n\nCE is differentiated from contained gravel (CG) or contained sand (CS) by the use of damp, tamped, cured cohesive fill. CE can be modular, built in poly-propylene rice bag material containers, or solid, built in mesh tubing that allows earthen fill to solidify between courses. \nCG, filled with pumice or ordinary gravel and/ or small stones, is often used as water-resistant base walls under CE, which also provides an effective capillary break. Soilbags used mostly in horizontal applications by civil engineers contain loose fill which includes both CG and CS. CG courses, like soilbags, may contribute base isolation and/or vibration damping qualities, although out-of-plane strength needs research. \n\nFor clarity, earthbag built with a low cohesion fill, or filled with dry soil that does not solidify, is not CE but CS. Uncured CE also performs structurally like CS.\n\nBuilders used to working without engineers are proud of earthbag's unlimited variations. Few trainers discuss risk levels of building sites, or recommend accurate tests of soil strength, even though soil strength is a key factor of improved seismic performance for earthen walls. \n\nNeed for or use of metal components are disputed, including rebar hammered into walls and barbed wire between courses, although static friction of smooth bag-to-bag surfaces of heavy modular CE walls is 0.4 with no adhesion. \n\nEngineering knowledge of earthbag has been growing. More is known about the performance of walls made with sand or dry or uncured soil than about the overwhelming majority of earthbag buildings which have used damp, cohesive soil fill. Reports based on tests of soilbags and loose or granular fill (or uncured fill) assumes that soil strength is less important to wall strength than bag fabric strength for. However, shear tests show clearly that stronger cured, cohesive fill increases contained earth wall strength substantially.\n\nEarthbag developed gradually without structural analysis, first for small domes, then for vertical wall buildings of many shapes. Although domes passed structural testing in California, no structural information was extracted from tests of the inherently stable shapes. Builders borrowed guidelines for adobe to recommend plan details, but code developed in low seismic risk New Mexico does not address issues for higher risk areas. California's seismic risk levels are almost three times as high as New Mexico's, and risk worldwide rises much higher. \n\nEarthbag is often tried after disasters in the developing world, including Sri Lanka's 2004 tsunami, Haiti's 2010 earthquake and Nepal's 2015 earthquake. \n\nCE walls fail in shear tests when barbs flex or bend back or (with weak soil fill) by chipping cured bag fill. CS walls or uncured CE walls fail differently, by slitting bag fabric as barbs move through loose fill.\n\nBecause no earthbag buildings were seriously damaged by seismic motion up to 0.8 g in Nepal's 2015 quakes, Nepal's building code recognizes earthbag, although the code does not discuss soil strengths or improved reinforcement. Nepal requires buildings to resist 1.5 g risk although hazard maps show higher values. Better trainers assume the use of cohesive soil and barbed wire, and recommend vertical rebar, buttresses, and bond beams, but rule of thumb earthbag techniques should be differentiated from contained earth that follows more complete guidelines.\n\nEarthquake damage results confirm the validity of New Zealand's detailed standards for non-engineered adobe and rammed earth which allow unreinforced buildings to 0.6 g force levels.\n\nAlthough earthbag without specific guidelines may often be this strong, conventional adobe can have severe damage at levels below 0.2 g forces. Non-traditional earthbag built with barbed wire, barely cohesive soil and no rebar can have half the shear strength of NZ's unreinforced adobe. Somewhere between 0.3 and 0.6 g forces, CE guidelines become important.\n\nBased on static shear testing (Stouter, P. May 2017):\nThe following approximate guidelines assume a single story of wide walls with 2 strands of 4 point barbed wire per course. Check NZS 4299 for bracing wall spacing and size of bracing walls and/ or buttresses. Vertical rebar must be spaced on center average and embedded in wall fill while damp. Follow NZS 4299 restrictions on building size, site slope, climate, and uses.\nDiscuss foundation concerns with an engineer, since NZS 4299 assumes a full reinforced concrete footing.\n\nFor comparison to NZS 4299 the following risk levels are based roughly on 0.2 second spectral acceleration (Ss) from 2% probability of exceedance in 50 years. Builders may refer to the Unified Facilities Handbook online for these values for some cities worldwide. These risk levels are based on ultimate strength, but deformation limits may require stiffer detailing or lower risk levels.\n\nMedium strength soil: unconfined compressive strength\n\nStrong soil: unconfined compressive strength\n\nAdditional research and engineering analysis is needed to create valid CE manuals.\n"}
{"id": "27233680", "url": "https://en.wikipedia.org/wiki?curid=27233680", "title": "Decomposed granite", "text": "Decomposed granite\n\nDecomposed granite is classification of rock that is derived from granite via its weathering to the point that the parent material readily fractures into smaller pieces of weaker rock. Further weathering yields material that easily crumbles into a mixtures of gravel-sized particles known as grus, that in turn may break down to produce a mixture of clay and silica sand or silt particles. Different specific granite types have differing propensities to weather, and so differing likelihoods of producing decomposed granite. It has practical uses that include its incorporation into paving and driveway materials, residential gardening materials in arid environments, as well as various types of walkways and heavy-use paths in parks. Different colors of decomposed granite are available, deriving from the natural range of granite colors from different quarry sources, and admixture of other natural and synthetic materials can extend the range of decomposed granite properties.\n\nDecomposed granite is rock of granitic origin that has weathered to the point that it readily fractures into smaller pieces of weak rock. Further weathering produces rock that easily crumbles into mixtures of gravel-sized particles, sand, and silt-sized particles with some clay. Eventually, the gravel may break down to produce a mixture of silica sand, silt particles, and clay. Different specific granite types have differing propensities to weather, and so differing likelihoods of producing decomposed granite.\n\nThe parent granite material is a common type of igneous rock that is granular, with its grains large enough to be distinguished with the unaided eye (i.e., it is phaneritic in texture); it is composed of plagioclase feldspar, orthoclase feldspar, quartz, mica, and possibly other minerals. The chemical transformation of feldspar, one of the primary constituents of granite, into the clay mineral kaolin is one of the important weathering processes. The presence of clay allows water to seep in and further weaken the rock allowing it to fracture or crumble into smaller particles, where, ultimately, the grains of silica produced from the granite are relatively resistant to weathering, and may remain almost unaltered.\n\nDecomposed granite, as a crushed stone form, is used as a pavement building material. It is used on driveways, garden walkways, bocce courts and pétanque terrains, and urban, regional, and national park walkways and heavy-use paths. DG can be installed and compacted to meet handicapped accessibility specifications and criteria, such as the ADA standards in the U.S. Different colors are available based on the various natural ranges available from different quarry sources, and polymeric stabilizers and other additives can be included to change the properties of the natural material. Decomposed granite is also sometimes used as a component of soil mixtures for cultivating bonsai.\n\n"}
{"id": "44701366", "url": "https://en.wikipedia.org/wiki?curid=44701366", "title": "Directed evolution (transhumanism)", "text": "Directed evolution (transhumanism)\n\nThe term directed evolution is used within the transhumanist community to refer to the idea of applying the principles of directed evolution and experimental evolution to the control of human evolution. In this sense, it is distinct from the use of the term in biochemistry, which refers only to the evolution of proteins and RNA. Maxwell J. Melhmanh has described directed evolution of humans as the Holy Grail of transhumanism.\nOxford philosopher Julian Savulescu wrote that:\nAccording to UCLA biophysicist Gregory Stock:\nRiccardo Campa, from the Institute for Ethics and Emerging Technologies, wrote that \"self-directed evolution\" can be coupled with many different political, philosophical, and religious views.\n\nAndrew Askland, from the Sandra Day O'Connor College of Law claims that referring to transhumanism as directed evolution is problematic because evolution is ateleological and transhumanism is teleological.\n\nParticipant evolution is an alternative term that refers to the process of deliberately redesigning the human body and brain using technological means, rather than through the natural processes of mutation and natural selection, with the goal of removing \"biological limitations\" and human enhancement. The idea of participant evolution was first put forward by Manfred Clynes and Nathan S. Kline in the 1960s in their article \"Cyborgs and Space\", where they argued that the human species was already on a path of participant evolution. Science fiction writers have speculated what the next stage of such participant evolution will be.\n\nWhilst Clynes and Kline saw participant evolution as the process of creating cyborgs, the idea has been adopted and propounded by transhumanists who argue that individuals should have the choice of using human enhancement technologies on themselves and their children, to progressively become transhuman and ultimately posthuman, as part of a voluntary regimen of participant evolution.\n"}
{"id": "9228", "url": "https://en.wikipedia.org/wiki?curid=9228", "title": "Earth", "text": "Earth\n\nEarth is the third planet from the Sun and the only astronomical object known to harbor life. According to radiometric dating and other sources of evidence, Earth formed over 4.5 billion years ago. Earth's gravity interacts with other objects in space, especially the Sun and the Moon, Earth's only natural satellite. Earth revolves around the Sun in 365.26 days, a period known as an Earth year. During this time, Earth rotates about its axis about 366.26 times.\n\nEarth's axis of rotation is tilted with respect to its orbital plane, producing seasons on Earth. The gravitational interaction between Earth and the Moon causes ocean tides, stabilizes Earth's orientation on its axis, and gradually slows its rotation. Earth is the densest planet in the Solar System and the largest of the four terrestrial planets.\n\nEarth's lithosphere is divided into several rigid tectonic plates that migrate across the surface over periods of many millions of years. About 71% of Earth's surface is covered with water, mostly by oceans. The remaining 29% is land consisting of continents and islands that together have many lakes, rivers and other sources of water that contribute to the hydrosphere. The majority of Earth's polar regions are covered in ice, including the Antarctic ice sheet and the sea ice of the Arctic ice pack. Earth's interior remains active with a solid iron inner core, a liquid outer core that generates the Earth's magnetic field, and a convecting mantle that drives plate tectonics.\n\nWithin the first billion years of Earth's history, life appeared in the oceans and began to affect the Earth's atmosphere and surface, leading to the proliferation of aerobic and anaerobic organisms. Some geological evidence indicates that life may have arisen as much as 4.1 billion years ago. Since then, the combination of Earth's distance from the Sun, physical properties, and geological history have allowed life to evolve and thrive. In the history of the Earth, biodiversity has gone through long periods of expansion, occasionally punctuated by mass extinction events. Over 99% of all species that ever lived on Earth are extinct. Estimates of the number of species on Earth today vary widely; most species have not been described. Over 7.6 billion humans live on Earth and depend on its biosphere and natural resources for their survival. Humans have developed diverse societies and cultures; politically, the world has about 200 sovereign states.\n\nThe modern English word \"Earth\" developed from a wide variety of Middle English forms, which derived from an Old English noun most often spelled '. It has cognates in every Germanic language, and their proto-Germanic root has been reconstructed as *\"erþō\". In its earliest appearances, \"eorðe\" was already being used to translate the many senses of Latin ' and Greek (\"gē\"): the ground, its soil, dry land, the human world, the surface of the world (including the sea), and the globe itself. As with Terra and Gaia, Earth was a personified goddess in Germanic paganism: the Angles were listed by Tacitus as among the devotees of Nerthus, and later Norse mythology included Jörð, a giantess often given as the mother of Thor.\n\nOriginally, \"earth\" was written in lowercase, and from early Middle English, its definite sense as \"the globe\" was expressed as \"the earth\". By Early Modern English, many nouns were capitalized, and \"the earth\" became (and often remained) \"the Earth\", particularly when referenced along with other heavenly bodies. More recently, the name is sometimes simply given as \"Earth\", by analogy with the names of the other planets. House styles now vary: Oxford spelling recognizes the lowercase form as the most common, with the capitalized form an acceptable variant. Another convention capitalizes \"Earth\" when appearing as a name (e.g. \"Earth's atmosphere\") but writes it in lowercase when preceded by \"the\" (e.g. \"the atmosphere of the earth\"). It almost always appears in lowercase in colloquial expressions such as \"what on earth are you doing?\"\n\nThe oldest material found in the Solar System is dated to (Bya). By the primordial Earth had formed. The bodies in the Solar System formed and evolved with the Sun. In theory, a solar nebula partitions a volume out of a molecular cloud by gravitational collapse, which begins to spin and flatten into a circumstellar disk, and then the planets grow out of that disk with the Sun. A nebula contains gas, ice grains, and dust (including primordial nuclides). According to nebular theory, planetesimals formed by accretion, with the primordial Earth taking 10– (Mys) to form.\n\nA subject of research is the formation of the Moon, some 4.53 Bya. A leading hypothesis is that it was formed by accretion from material loosed from Earth after a Mars-sized object, named Theia, hit Earth. In this view, the mass of Theia was approximately 10 percent of Earth, it hit Earth with a glancing blow and some of its mass merged with Earth. Between approximately 4.1 and , numerous asteroid impacts during the Late Heavy Bombardment caused significant changes to the greater surface environment of the Moon and, by inference, to that of Earth.\n\nEarth's atmosphere and oceans were formed by volcanic activity and outgassing. Water vapor from these sources condensed into the oceans, augmented by water and ice from asteroids, protoplanets, and comets. In this model, atmospheric \"greenhouse gases\" kept the oceans from freezing when the newly forming Sun had only 70% of its current luminosity. By , Earth's magnetic field was established, which helped prevent the atmosphere from being stripped away by the solar wind.\n\nA crust formed when the molten outer layer of Earth cooled to form a solid. The two models that explain land mass propose either a steady growth to the present-day forms or, more likely, a rapid growth early in Earth history followed by a long-term steady continental area. Continents formed by plate tectonics, a process ultimately driven by the continuous loss of heat from Earth's interior. Over the period of hundreds of millions of years, the supercontinents have assembled and broken apart. Roughly (Mya), one of the earliest known supercontinents, Rodinia, began to break apart. The continents later recombined to form Pannotia , then finally Pangaea, which also broke apart .\n\nThe present pattern of ice ages began about and then intensified during the Pleistocene about . High-latitude regions have since undergone repeated cycles of glaciation and thaw, repeating about every . The last continental glaciation ended ago.\n\nChemical reactions led to the first self-replicating molecules about four billion years ago. A half billion years later, the last common ancestor of all current life arose. The evolution of photosynthesis allowed the Sun's energy to be harvested directly by life forms. The resultant molecular oxygen () accumulated in the atmosphere and due to interaction with ultraviolet solar radiation, formed a protective ozone layer () in the upper atmosphere. The incorporation of smaller cells within larger ones resulted in the development of complex cells called eukaryotes. True multicellular organisms formed as cells within colonies became increasingly specialized. Aided by the absorption of harmful ultraviolet radiation by the ozone layer, life colonized Earth's surface. Among the earliest fossil evidence for life is microbial mat fossils found in 3.48 billion-year-old sandstone in Western Australia, biogenic graphite found in 3.7 billion-year-old metasedimentary rocks in Western Greenland, and remains of biotic material found in 4.1 billion-year-old rocks in Western Australia. The earliest direct evidence of life on Earth is contained in 3.45 billion-year-old Australian rocks showing fossils of microorganisms.\n\nDuring the Neoproterozoic, , much of Earth might have been covered in ice. This hypothesis has been termed \"Snowball Earth\", and it is of particular interest because it preceded the Cambrian explosion, when multicellular life forms significantly increased in complexity. Following the Cambrian explosion, , there have been five mass extinctions. The most recent such event was , when an asteroid impact triggered the extinction of the non-avian dinosaurs and other large reptiles, but spared some small animals such as mammals, which at the time resembled shrews. Mammalian life has diversified over the past , and several million years ago an African ape-like animal such as \"Orrorin tugenensis\" gained the ability to stand upright. This facilitated tool use and encouraged communication that provided the nutrition and stimulation needed for a larger brain, which led to the evolution of humans. The development of agriculture, and then civilization, led to humans having an influence on Earth and the nature and quantity of other life forms that continues to this day.\n\nEarth's expected long-term future is tied to that of the Sun. Over the next , solar luminosity will increase by 10%, and over the next by 40%. The Earth's increasing surface temperature will accelerate the inorganic carbon cycle, reducing concentration to levels lethally low for plants ( for C4 photosynthesis) in approximately . The lack of vegetation will result in the loss of oxygen in the atmosphere, making animal life impossible. After another billion years all surface water will have disappeared and the mean global temperature will reach . From that point, the Earth is expected to be habitable for another , possibly up to if nitrogen is removed from the atmosphere. Even if the Sun were eternal and stable, 27% of the water in the modern oceans will descend to the mantle in one billion years, due to reduced steam venting from mid-ocean ridges.\n\nThe Sun will evolve to become a red giant in about . Models predict that the Sun will expand to roughly , about 250 times its present radius. Earth's fate is less clear. As a red giant, the Sun will lose roughly 30% of its mass, so, without tidal effects, Earth will move to an orbit from the Sun when the star reaches its maximum radius. Most, if not all, remaining life will be destroyed by the Sun's increased luminosity (peaking at about 5,000 times its present level). A 2008 simulation indicates that Earth's orbit will eventually decay due to tidal effects and drag, causing it to enter the Sun's atmosphere and be vaporized.\n\nThe shape of Earth is approximately oblate spheroidal. Due to rotation, the Earth is flattened at the poles and bulging around the equator. The diameter of the Earth at the equator is larger than the pole-to-pole diameter. Thus the point on the surface farthest from Earth's center of mass is the summit of the equatorial Chimborazo volcano in Ecuador. The average diameter of the reference spheroid is . Local topography deviates from this idealized spheroid, although on a global scale these deviations are small compared to Earth's radius: The maximum deviation of only 0.17% is at the Mariana Trench ( below local sea level), whereas Mount Everest ( above local sea level) represents a deviation of 0.14%.\n\nIn geodesy, the exact shape that Earth's oceans would adopt in the absence of land and perturbations such as tides and winds is called the geoid. More precisely, the geoid is the surface of gravitational equipotential at mean sea level.\n\nEarth's mass is approximately (5,970 Yg). It is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminium (1.4%), with the remaining 1.2% consisting of trace amounts of other elements. Due to mass segregation, the core region is estimated to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements.\n\nThe most common rock constituents of the crust are nearly all oxides: chlorine, sulfur, and fluorine are the important exceptions to this and their total amount in any rock is usually much less than 1%. Over 99% of the crust is composed of 11 oxides, principally silica, alumina, iron oxides, lime, magnesia, potash, and soda.\n\nEarth's interior, like that of the other terrestrial planets, is divided into layers by their chemical or physical (rheological) properties. The outer layer is a chemically distinct silicate solid crust, which is underlain by a highly viscous solid mantle. The crust is separated from the mantle by the Mohorovičić discontinuity. The thickness of the crust varies from about under the oceans to for the continents. The crust and the cold, rigid, top of the upper mantle are collectively known as the lithosphere, and it is of the lithosphere that the tectonic plates are composed. Beneath the lithosphere is the asthenosphere, a relatively low-viscosity layer on which the lithosphere rides. Important changes in crystal structure within the mantle occur at below the surface, spanning a transition zone that separates the upper and lower mantle. Beneath the mantle, an extremely low viscosity liquid outer core lies above a solid inner core. The Earth's inner core might rotate at a slightly higher angular velocity than the remainder of the planet, advancing by 0.1–0.5° per year. The radius of the inner core is about one fifth of that of Earth.\n\nEarth's internal heat comes from a combination of residual heat from planetary accretion (about 20%) and heat produced through radioactive decay (80%). The major heat-producing isotopes within Earth are potassium-40, uranium-238, and thorium-232. At the center, the temperature may be up to , and the pressure could reach . Because much of the heat is provided by radioactive decay, scientists postulate that early in Earth's history, before isotopes with short half-lives were depleted, Earth's heat production was much higher. At approximately , twice the present-day heat would have been produced, increasing the rates of mantle convection and plate tectonics, and allowing the production of uncommon igneous rocks such as komatiites that are rarely formed today.\n\nThe mean heat loss from Earth is , for a global heat loss of . A portion of the core's thermal energy is transported toward the crust by mantle plumes, a form of convection consisting of upwellings of higher-temperature rock. These plumes can produce hotspots and flood basalts. More of the heat in Earth is lost through plate tectonics, by mantle upwelling associated with mid-ocean ridges. The final major mode of heat loss is through conduction through the lithosphere, the majority of which occurs under the oceans because the crust there is much thinner than that of the continents.\n\nEarth's mechanically rigid outer layer, the lithosphere, is divided into tectonic plates. These plates are rigid segments that move relative to each other at one of three boundaries types: At convergent boundaries, two plates come together; at divergent boundaries, two plates are pulled apart; and at transform boundaries, two plates slide past one another laterally. Along these plate boundaries, earthquakes, volcanic activity, mountain-building, and oceanic trench formation can occur. The tectonic plates ride on top of the asthenosphere, the solid but less-viscous part of the upper mantle that can flow and move along with the plates.\n\nAs the tectonic plates migrate, oceanic crust is subducted under the leading edges of the plates at convergent boundaries. At the same time, the upwelling of mantle material at divergent boundaries creates mid-ocean ridges. The combination of these processes recycles the oceanic crust back into the mantle. Due to this recycling, most of the ocean floor is less than old. The oldest oceanic crust is located in the Western Pacific and is estimated to be old. By comparison, the oldest dated continental crust is .\n\nThe seven major plates are the Pacific, North American, Eurasian, African, Antarctic, Indo-Australian, and South American. Other notable plates include the Arabian Plate, the Caribbean Plate, the Nazca Plate off the west coast of South America and the Scotia Plate in the southern Atlantic Ocean. The Australian Plate fused with the Indian Plate between . The fastest-moving plates are the oceanic plates, with the Cocos Plate advancing at a rate of and the Pacific Plate moving . At the other extreme, the slowest-moving plate is the Eurasian Plate, progressing at a typical rate of .\n\nThe total surface area of Earth is about . Of this, 70.8%, or , is below sea level and covered by ocean water. Below the ocean's surface are much of the continental shelf, mountains, volcanoes, oceanic trenches, submarine canyons, oceanic plateaus, abyssal plains, and a globe-spanning mid-ocean ridge system. The remaining 29.2%, or , not covered by water has terrain that varies greatly from place to place and consists of mountains, deserts, plains, plateaus, and other landforms. Tectonics and erosion, volcanic eruptions, flooding, weathering, glaciation, the growth of coral reefs, and meteorite impacts are among the processes that constantly reshape the Earth's surface over geological time.\n\nThe continental crust consists of lower density material such as the igneous rocks granite and andesite. Less common is basalt, a denser volcanic rock that is the primary constituent of the ocean floors. Sedimentary rock is formed from the accumulation of sediment that becomes buried and compacted together. Nearly 75% of the continental surfaces are covered by sedimentary rocks, although they form about 5% of the crust. The third form of rock material found on Earth is metamorphic rock, which is created from the transformation of pre-existing rock types through high pressures, high temperatures, or both. The most abundant silicate minerals on Earth's surface include quartz, feldspars, amphibole, mica, pyroxene and olivine. Common carbonate minerals include calcite (found in limestone) and dolomite.\n\nThe elevation of the land surface varies from the low point of at the Dead Sea, to a maximum altitude of at the top of Mount Everest. The mean height of land above sea level is about .\n\nThe pedosphere is the outermost layer of Earth's continental surface and is composed of soil and subject to soil formation processes. The total arable land is 10.9% of the land surface, with 1.3% being permanent cropland. Close to 40% of Earth's land surface is used for agriculture, or an estimated of cropland and of pastureland.\n\nThe abundance of water on Earth's surface is a unique feature that distinguishes the \"Blue Planet\" from other planets in the Solar System. Earth's hydrosphere consists chiefly of the oceans, but technically includes all water surfaces in the world, including inland seas, lakes, rivers, and underground waters down to a depth of . The deepest underwater location is Challenger Deep of the Mariana Trench in the Pacific Ocean with a depth of .\n\nThe mass of the oceans is approximately 1.35 metric tons or about 1/4400 of Earth's total mass. The oceans cover an area of with a mean depth of , resulting in an estimated volume of . If all of Earth's crustal surface were at the same elevation as a smooth sphere, the depth of the resulting world ocean would be .\n\nAbout 97.5% of the water is saline; the remaining 2.5% is fresh water. Most fresh water, about 68.7%, is present as ice in ice caps and glaciers.\n\nThe average salinity of Earth's oceans is about 35 grams of salt per kilogram of sea water (3.5% salt). Most of this salt was released from volcanic activity or extracted from cool igneous rocks. The oceans are also a reservoir of dissolved atmospheric gases, which are essential for the survival of many aquatic life forms. Sea water has an important influence on the world's climate, with the oceans acting as a large heat reservoir. Shifts in the oceanic temperature distribution can cause significant weather shifts, such as the El Niño–Southern Oscillation.\n\nThe atmospheric pressure at Earth's sea level averages , with a scale height of about . A dry atmosphere is composed of 78.084% nitrogen, 20.946% oxygen, 0.934% argon, and trace amounts of carbon dioxide and other gaseous molecules. Water vapor content varies between 0.01% and 4% but averages about 1%. The height of the troposphere varies with latitude, ranging between at the poles to at the equator, with some variation resulting from weather and seasonal factors.\n\nEarth's biosphere has significantly altered its atmosphere. Oxygenic photosynthesis evolved , forming the primarily nitrogen–oxygen atmosphere of today. This change enabled the proliferation of aerobic organisms and, indirectly, the formation of the ozone layer due to the subsequent conversion of atmospheric into. The ozone layer blocks ultraviolet solar radiation, permitting life on land. Other atmospheric functions important to life include transporting water vapor, providing useful gases, causing small meteors to burn up before they strike the surface, and moderating temperature. This last phenomenon is known as the greenhouse effect: trace molecules within the atmosphere serve to capture thermal energy emitted from the ground, thereby raising the average temperature. Water vapor, carbon dioxide, methane, nitrous oxide, and ozone are the primary greenhouse gases in the atmosphere. Without this heat-retention effect, the average surface temperature would be , in contrast to the current , and life on Earth probably would not exist in its current form. In May 2017, glints of light, seen as twinkling from an orbiting satellite a million miles away, were found to be reflected light from ice crystals in the atmosphere.\n\nEarth's atmosphere has no definite boundary, slowly becoming thinner and fading into outer space. Three-quarters of the atmosphere's mass is contained within the first of the surface. This lowest layer is called the troposphere. Energy from the Sun heats this layer, and the surface below, causing expansion of the air. This lower-density air then rises and is replaced by cooler, higher-density air. The result is atmospheric circulation that drives the weather and climate through redistribution of thermal energy.\n\nThe primary atmospheric circulation bands consist of the trade winds in the equatorial region below 30° latitude and the westerlies in the mid-latitudes between 30° and 60°. Ocean currents are also important factors in determining climate, particularly the thermohaline circulation that distributes thermal energy from the equatorial oceans to the polar regions.\n\nWater vapor generated through surface evaporation is transported by circulatory patterns in the atmosphere. When atmospheric conditions permit an uplift of warm, humid air, this water condenses and falls to the surface as precipitation. Most of the water is then transported to lower elevations by river systems and usually returned to the oceans or deposited into lakes. This water cycle is a vital mechanism for supporting life on land and is a primary factor in the erosion of surface features over geological periods. Precipitation patterns vary widely, ranging from several meters of water per year to less than a millimeter. Atmospheric circulation, topographic features, and temperature differences determine the average precipitation that falls in each region.\n\nThe amount of solar energy reaching Earth's surface decreases with increasing latitude. At higher latitudes, the sunlight reaches the surface at lower angles, and it must pass through thicker columns of the atmosphere. As a result, the mean annual air temperature at sea level decreases by about per degree of latitude from the equator. Earth's surface can be subdivided into specific latitudinal belts of approximately homogeneous climate. Ranging from the equator to the polar regions, these are the tropical (or equatorial), subtropical, temperate and polar climates.\n\nThis latitudinal rule has several anomalies:\n\nThe commonly used Köppen climate classification system has five broad groups (humid tropics, arid, humid middle latitudes, continental and cold polar), which are further divided into more specific subtypes. The Köppen system rates regions of terrain based on observed temperature and precipitation.\n\nThe highest air temperature ever measured on Earth was in Furnace Creek, California, in Death Valley, in 1913. The lowest air temperature ever directly measured on Earth was at Vostok Station in 1983, but satellites have used remote sensing to measure temperatures as low as in East Antarctica. These temperature records are only measurements made with modern instruments from the 20th century onwards and likely do not reflect the full range of temperature on Earth.\n\nAbove the troposphere, the atmosphere is usually divided into the stratosphere, mesosphere, and thermosphere. Each layer has a different lapse rate, defining the rate of change in temperature with height. Beyond these, the exosphere thins out into the magnetosphere, where the geomagnetic fields interact with the solar wind. Within the stratosphere is the ozone layer, a component that partially shields the surface from ultraviolet light and thus is important for life on Earth. The Kármán line, defined as 100 km above Earth's surface, is a working definition for the boundary between the atmosphere and outer space.\n\nThermal energy causes some of the molecules at the outer edge of the atmosphere to increase their velocity to the point where they can escape from Earth's gravity. This causes a slow but steady loss of the atmosphere into space. Because unfixed hydrogen has a low molecular mass, it can achieve escape velocity more readily, and it leaks into outer space at a greater rate than other gases. The leakage of hydrogen into space contributes to the shifting of Earth's atmosphere and surface from an initially reducing state to its current oxidizing one. Photosynthesis provided a source of free oxygen, but the loss of reducing agents such as hydrogen is thought to have been a necessary precondition for the widespread accumulation of oxygen in the atmosphere. Hence the ability of hydrogen to escape from the atmosphere may have influenced the nature of life that developed on Earth. In the current, oxygen-rich atmosphere most hydrogen is converted into water before it has an opportunity to escape. Instead, most of the hydrogen loss comes from the destruction of methane in the upper atmosphere.\n\nThe gravity of Earth is the acceleration that is imparted to objects due to the distribution of mass within the Earth. Near the Earth's surface, gravitational acceleration is approximately . Local differences in topography, geology, and deeper tectonic structure cause local and broad, regional differences in the Earth's gravitational field, known as gravity anomalies.\n\nThe main part of Earth's magnetic field is generated in the core, the site of a dynamo process that converts the kinetic energy of thermally and compositionally driven convection into electrical and magnetic field energy. The field extends outwards from the core, through the mantle, and up to Earth's surface, where it is, approximately, a dipole. The poles of the dipole are located close to Earth's geographic poles. At the equator of the magnetic field, the magnetic-field strength at the surface is , with global magnetic dipole moment of . The convection movements in the core are chaotic; the magnetic poles drift and periodically change alignment. This causes secular variation of the main field and field reversals at irregular intervals averaging a few times every million years. The most recent reversal occurred approximately 700,000 years ago.\n\nThe extent of Earth's magnetic field in space defines the magnetosphere. Ions and electrons of the solar wind are deflected by the magnetosphere; solar wind pressure compresses the dayside of the magnetosphere, to about 10 Earth radii, and extends the nightside magnetosphere into a long tail. Because the velocity of the solar wind is greater than the speed at which waves propagate through the solar wind, a supersonic bowshock precedes the dayside magnetosphere within the solar wind. Charged particles are contained within the magnetosphere; the plasmasphere is defined by low-energy particles that essentially follow magnetic field lines as Earth rotates; the ring current is defined by medium-energy particles that drift relative to the geomagnetic field, but with paths that are still dominated by the magnetic field, and the Van Allen radiation belt are formed by high-energy particles whose motion is essentially random, but otherwise contained by the magnetosphere.\n\nDuring magnetic storms and substorms, charged particles can be deflected from the outer magnetosphere and especially the magnetotail, directed along field lines into Earth's ionosphere, where atmospheric atoms can be excited and ionized, causing the aurora.\n\nEarth's rotation period relative to the Sun—its mean solar day—is of mean solar time (). Because Earth's solar day is now slightly longer than it was during the 19th century due to tidal deceleration, each day varies between longer.\n\nEarth's rotation period relative to the fixed stars, called its \"stellar day\" by the International Earth Rotation and Reference Systems Service (IERS), is of mean solar time (UT1), or Earth's rotation period relative to the precessing or moving mean vernal equinox, misnamed its \"sidereal day\", is of mean solar time (UT1) . Thus the sidereal day is shorter than the stellar day by about 8.4 ms. The length of the mean solar day in SI seconds is available from the IERS for the periods 1623–2005 and 1962–2005.\n\nApart from meteors within the atmosphere and low-orbiting satellites, the main apparent motion of celestial bodies in Earth's sky is to the west at a rate of 15°/h = 15'/min. For bodies near the celestial equator, this is equivalent to an apparent diameter of the Sun or the Moon every two minutes; from Earth's surface, the apparent sizes of the Sun and the Moon are approximately the same.\n\nEarth orbits the Sun at an average distance of about every 365.2564 mean solar days, or one sidereal year. This gives an apparent movement of the Sun eastward with respect to the stars at a rate of about 1°/day, which is one apparent Sun or Moon diameter every 12 hours. Due to this motion, on average it takes 24 hours—a solar day—for Earth to complete a full rotation about its axis so that the Sun returns to the meridian. The orbital speed of Earth averages about , which is fast enough to travel a distance equal to Earth's diameter, about , in seven minutes, and the distance to the Moon, , in about 3.5 hours.\n\nThe Moon and Earth orbit a common barycenter every 27.32 days relative to the background stars. When combined with the Earth–Moon system's common orbit around the Sun, the period of the synodic month, from new moon to new moon, is 29.53 days. Viewed from the celestial north pole, the motion of Earth, the Moon, and their axial rotations are all counterclockwise. Viewed from a vantage point above the north poles of both the Sun and Earth, Earth orbits in a counterclockwise direction about the Sun. The orbital and axial planes are not precisely aligned: Earth's axis is tilted some 23.44 degrees from the perpendicular to the Earth–Sun plane (the ecliptic), and the Earth–Moon plane is tilted up to ±5.1 degrees against the Earth–Sun plane. Without this tilt, there would be an eclipse every two weeks, alternating between lunar eclipses and solar eclipses.\n\nThe Hill sphere, or the sphere of gravitational influence, of the Earth is about in radius. This is the maximum distance at which the Earth's gravitational influence is stronger than the more distant Sun and planets. Objects must orbit the Earth within this radius, or they can become unbound by the gravitational perturbation of the Sun.\n\nEarth, along with the Solar System, is situated in the Milky Way and orbits about 28,000 light-years from its center. It is about 20 light-years above the galactic plane in the Orion Arm.\n\nThe axial tilt of the Earth is approximately 23.439281° with the axis of its orbit plane, always pointing towards the Celestial Poles. Due to Earth's axial tilt, the amount of sunlight reaching any given point on the surface varies over the course of the year. This causes the seasonal change in climate, with summer in the Northern Hemisphere occurring when the Tropic of Cancer is facing the Sun, and winter taking place when the Tropic of Capricorn in the Southern Hemisphere faces the Sun. During the summer, the day lasts longer, and the Sun climbs higher in the sky. In winter, the climate becomes cooler and the days shorter. In northern temperate latitudes, the Sun rises north of true east during the summer solstice, and sets north of true west, reversing in the winter. The Sun rises south of true east in the summer for the southern temperate zone and sets south of true west.\n\nAbove the Arctic Circle, an extreme case is reached where there is no daylight at all for part of the year, up to six months at the North Pole itself, a polar night. In the Southern Hemisphere, the situation is exactly reversed, with the South Pole oriented opposite the direction of the North Pole. Six months later, this pole will experience a midnight sun, a day of 24 hours, again reversing with the South Pole.\n\nBy astronomical convention, the four seasons can be determined by the solstices—the points in the orbit of maximum axial tilt toward or away from the Sun—and the equinoxes, when the direction of the tilt and the direction to the Sun are perpendicular. In the Northern Hemisphere, winter solstice currently occurs around 21 December; summer solstice is near 21 June, spring equinox is around 20 March and autumnal equinox is about 22 or 23 September. In the Southern Hemisphere, the situation is reversed, with the summer and winter solstices exchanged and the spring and autumnal equinox dates swapped.\n\nThe angle of Earth's axial tilt is relatively stable over long periods of time. Its axial tilt does undergo nutation; a slight, irregular motion with a main period of 18.6 years. The orientation (rather than the angle) of Earth's axis also changes over time, precessing around in a complete circle over each 25,800 year cycle; this precession is the reason for the difference between a sidereal year and a tropical year. Both of these motions are caused by the varying attraction of the Sun and the Moon on Earth's equatorial bulge. The poles also migrate a few meters across Earth's surface. This polar motion has multiple, cyclical components, which collectively are termed quasiperiodic motion. In addition to an annual component to this motion, there is a 14-month cycle called the Chandler wobble. Earth's rotational velocity also varies in a phenomenon known as length-of-day variation.\n\nIn modern times, Earth's perihelion occurs around 3 January, and its aphelion around 4 July. These dates change over time due to precession and other orbital factors, which follow cyclical patterns known as Milankovitch cycles. The changing Earth–Sun distance causes an increase of about 6.9% in solar energy reaching Earth at perihelion relative to aphelion. Because the Southern Hemisphere is tilted toward the Sun at about the same time that Earth reaches the closest approach to the Sun, the Southern Hemisphere receives slightly more energy from the Sun than does the northern over the course of a year. This effect is much less significant than the total energy change due to the axial tilt, and most of the excess energy is absorbed by the higher proportion of water in the Southern Hemisphere.\n\nA study from 2016 suggested that Planet Nine tilted all Solar System planets, including Earth's, by about six degrees.\n\nA planet that can sustain life is termed habitable, even if life did not originate there. Earth provides liquid water—an environment where complex organic molecules can assemble and interact, and sufficient energy to sustain metabolism. The distance of Earth from the Sun, as well as its orbital eccentricity, rate of rotation, axial tilt, geological history, sustaining atmosphere, and magnetic field all contribute to the current climatic conditions at the surface.\n\nA planet's life forms inhabit ecosystems, whose total is sometimes said to form a \"biosphere\". Earth's biosphere is thought to have begun evolving about . The biosphere is divided into a number of biomes, inhabited by broadly similar plants and animals. On land, biomes are separated primarily by differences in latitude, height above sea level and humidity. Terrestrial biomes lying within the Arctic or Antarctic Circles, at high altitudes or in extremely arid areas are relatively barren of plant and animal life; species diversity reaches a peak in humid lowlands at equatorial latitudes.\n\nIn July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all organisms living on Earth.\n\nEarth has resources that have been exploited by humans. Those termed non-renewable resources, such as fossil fuels, only renew over geological timescales.\n\nLarge deposits of fossil fuels are obtained from Earth's crust, consisting of coal, petroleum, and natural gas. These deposits are used by humans both for energy production and as feedstock for chemical production. Mineral ore bodies have also been formed within the crust through a process of ore genesis, resulting from actions of magmatism, erosion, and plate tectonics. These bodies form concentrated sources for many metals and other useful elements.\n\nEarth's biosphere produces many useful biological products for humans, including food, wood, pharmaceuticals, oxygen, and the recycling of many organic wastes. The land-based ecosystem depends upon topsoil and fresh water, and the oceanic ecosystem depends upon dissolved nutrients washed down from the land. In 1980, of Earth's land surface consisted of forest and woodlands, was grasslands and pasture, and was cultivated as croplands. The estimated amount of irrigated land in 1993 was . Humans also live on the land by using building materials to construct shelters.\n\nLarge areas of Earth's surface are subject to extreme weather such as tropical cyclones, hurricanes, or typhoons that dominate life in those areas. From 1980 to 2000, these events caused an average of 11,800 human deaths per year. Many places are subject to earthquakes, landslides, tsunamis, volcanic eruptions, tornadoes, sinkholes, blizzards, floods, droughts, wildfires, and other calamities and disasters.\n\nMany localized areas are subject to human-made pollution of the air and water, acid rain and toxic substances, loss of vegetation (overgrazing, deforestation, desertification), loss of wildlife, species extinction, soil degradation, soil depletion and erosion.\n\nThere is a scientific consensus linking human activities to global warming due to industrial carbon dioxide emissions. This is predicted to produce changes such as the melting of glaciers and ice sheets, more extreme temperature ranges, significant changes in weather and a global rise in average sea levels.\n\nCartography, the study and practice of map-making, and geography, the study of the lands, features, inhabitants and phenomena on Earth, have historically been the disciplines devoted to depicting Earth. Surveying, the determination of locations and distances, and to a lesser extent navigation, the determination of position and direction, have developed alongside cartography and geography, providing and suitably quantifying the requisite information.\n\nEarth's human population reached approximately seven billion on 31 October 2011. Projections indicate that the world's human population will reach 9.2 billion in 2050. Most of the growth is expected to take place in developing nations. Human population density varies widely around the world, but a majority live in Asia. By 2020, 60% of the world's population is expected to be living in urban, rather than rural, areas.\n\nIt is estimated that one-eighth of Earth's surface is suitable for humans to live on – three-quarters of Earth's surface is covered by oceans, leaving one-quarter as land. Half of that land area is desert (14%), high mountains (27%), or other unsuitable terrains. The northernmost permanent settlement in the world is Alert, on Ellesmere Island in Nunavut, Canada. (82°28′N) The southernmost is the Amundsen–Scott South Pole Station, in Antarctica, almost exactly at the South Pole. (90°S)\nIndependent sovereign nations claim the planet's entire land surface, except for some parts of Antarctica, a few land parcels along the Danube river's western bank, and the unclaimed area of Bir Tawil between Egypt and Sudan. , there are 193 sovereign states that are member states of the United Nations, plus two observer states and 72 dependent territories and states with limited recognition. Earth has never had a sovereign government with authority over the entire globe, although some nation-states have striven for world domination and failed.\n\nThe United Nations is a worldwide intergovernmental organization that was created with the goal of intervening in the disputes between nations, thereby avoiding armed conflict. The U.N. serves primarily as a forum for international diplomacy and international law. When the consensus of the membership permits, it provides a mechanism for armed intervention.\n\nThe first human to orbit Earth was Yuri Gagarin on 12 April 1961. In total, about 487 people have visited outer space and reached orbit , and, of these, twelve have walked on the Moon. Normally, the only humans in space are those on the International Space Station. The station's crew, made up of six people, is usually replaced every six months. The farthest that humans have traveled from Earth is , achieved during the Apollo 13 mission in 1970.\n\nThe Moon is a relatively large, terrestrial, planet-like natural satellite, with a diameter about one-quarter of Earth's. It is the largest moon in the Solar System relative to the size of its planet, although Charon is larger relative to the dwarf planet Pluto. The natural satellites of other planets are also referred to as \"moons\", after Earth's.\n\nThe gravitational attraction between Earth and the Moon causes tides on Earth. The same effect on the Moon has led to its tidal locking: its rotation period is the same as the time it takes to orbit Earth. As a result, it always presents the same face to the planet. As the Moon orbits Earth, different parts of its face are illuminated by the Sun, leading to the lunar phases; the dark part of the face is separated from the light part by the solar terminator.\nDue to their tidal interaction, the Moon recedes from Earth at the rate of approximately . Over millions of years, these tiny modifications—and the lengthening of Earth's day by about 23 µs/yr—add up to significant changes. During the Devonian period, for example, (approximately ) there were 400 days in a year, with each day lasting 21.8 hours.\n\nThe Moon may have dramatically affected the development of life by moderating the planet's climate. Paleontological evidence and computer simulations show that Earth's axial tilt is stabilized by tidal interactions with the Moon. Some theorists think that without this stabilization against the torques applied by the Sun and planets to Earth's equatorial bulge, the rotational axis might be chaotically unstable, exhibiting chaotic changes over millions of years, as appears to be the case for Mars.\n\nViewed from Earth, the Moon is just far enough away to have almost the same apparent-sized disk as the Sun. The angular size (or solid angle) of these two bodies match because, although the Sun's diameter is about 400 times as large as the Moon's, it is also 400 times more distant. This allows total and annular solar eclipses to occur on Earth.\n\nThe most widely accepted theory of the Moon's origin, the giant-impact hypothesis, states that it formed from the collision of a Mars-size protoplanet called Theia with the early Earth. This hypothesis explains (among other things) the Moon's relative lack of iron and volatile elements and the fact that its composition is nearly identical to that of Earth's crust.\n\nEarth has at least five co-orbital asteroids, including 3753 Cruithne and . A trojan asteroid companion, , is librating around the leading Lagrange triangular point, L4, in the Earth's orbit around the Sun.\n\nThe tiny near-Earth asteroid makes close approaches to the Earth–Moon system roughly every twenty years. During these approaches, it can orbit Earth for brief periods of time.\n\n, there are 1,886 operational, human-made satellites orbiting Earth. There are also inoperative satellites, including Vanguard 1, the oldest satellite currently in orbit, and over 16,000 pieces of tracked space debris. Earth's largest artificial satellite is the International Space Station.\n\nThe standard astronomical symbol of Earth consists of a cross circumscribed by a circle, , representing the four corners of the world.\n\nHuman cultures have developed many views of the planet. Earth is sometimes personified as a deity. In many cultures it is a mother goddess that is also the primary fertility deity, and by the mid-20th century, the Gaia Principle compared Earth's environments and life as a single self-regulating organism leading to broad stabilization of the conditions of habitability. Creation myths in many religions involve the creation of Earth by a supernatural deity or deities.\n\nScientific investigation has resulted in several culturally transformative shifts in people's view of the planet. Initial belief in a flat Earth was gradually displaced in the Greek colonies of southern Italy during the late 6th century BC by the idea of spherical Earth, which was attributed to both the philosophers Pythagoras and Parmenides. By the end of the 5th century BC, the sphericity of Earth was universally accepted among Greek intellectuals. Earth was generally believed to be the center of the universe until the 16th century, when scientists first conclusively demonstrated that it was a moving object, comparable to the other planets in the Solar System. Due to the efforts of influential Christian scholars and clerics such as James Ussher, who sought to determine the age of Earth through analysis of genealogies in Scripture, Westerners before the 19th century generally believed Earth to be a few thousand years old at most. It was only during the 19th century that geologists realized Earth's age was at least many millions of years.\n\nLord Kelvin used thermodynamics to estimate the age of Earth to be between 20 million and 400 million years in 1864, sparking a vigorous debate on the subject; it was only when radioactivity and radioactive dating were discovered in the late 19th and early 20th centuries that a reliable mechanism for determining Earth's age was established, proving the planet to be billions of years old. The perception of Earth shifted again in the 20th century when humans first viewed it from orbit, and especially with photographs of Earth returned by the Apollo program.\n\n</math>, where \"m\" is the mass of Earth, \"a\" is an astronomical unit, and \"M\" is the mass of the Sun. So the radius in AU is about formula_1.</ref>\n\n"}
{"id": "59216757", "url": "https://en.wikipedia.org/wiki?curid=59216757", "title": "Earth's circumference", "text": "Earth's circumference\n\nEarth's circumference is the distance around the Earth, either around the equator () or around the poles ().\n\nMeasurement of Earth's circumference has been important to navigation since ancient times. It was first calculated by Eratosthenes, which he did by comparing altitudes of the mid-day sun at two places a known North-South distance apart. In the Middle Ages, Al Biruni calculated a more accurate version, becoming the first person to perform the calculation based on data from a single location.\n\nIn modern times it has been used to define fundamental units of measurement of length – the nautical mile in the seventeenth century and the metre in the eighteenth. Earth's polar circumference is almost exactly 40,000 km because the metre was originally calibrated on this measurement (1/10-millionth of the distance between the poles and the equator), which is almost exactly 21,600 nautical miles (being defined as one minute of a degree – i.e. 360 multiplied by 60).\n\nAccording to Cleomedes' \"On the Circular Motions of the Celestial Bodies\", around 240 BC, Eratosthenes, the librarian of the Library of Alexandria, calculated the circumference of the Earth in Ptolemaic Egypt. Using a scaphe, he knew that at local noon on the summer solstice in Syene (modern Aswan, Egypt), the Sun was directly overhead. (Syene is at latitude 24°05′ North, near to the Tropic of Cancer, which was 23°42′ North in 100 BC) He knew this because the shadow of someone looking down a deep well at that time in Syene blocked the reflection of the Sun on the water. He then measured the Sun's angle of elevation at noon in Alexandria by using a vertical rod, known as a gnomon, and measuring the length of its shadow on the ground. Using the length of the rod, and the length of the shadow, as the legs of a triangle, he calculated the angle of the sun's rays. This angle was about 7°, or 1/50th the circumference of a circle; taking the Earth as perfectly spherical, he concluded that the Earth's circumference was 50 times the known distance from Alexandra to Syene (5,000 stadia, a figure that was checked yearly), i.e. 250,000 stadia. Depending on whether he used the \"Olympic stade\" (176.4m) or the Italian stade (184.8m), this would imply a circumference of 44,100 km (an error of 10%) or 46,100 km, an error of 15%. In 2012, Anthony Abreu Mora repeated Eratosthenes's calculation with more accurate data; the result was 40,074 km, which is 66 km different (0.16%) from the currently accepted polar circumference.\n\nPosidonius calculated the Earth's circumference by reference to the position of the star Canopus. As explained by Cleomedes, Posidonius observed Canopus on but never above the horizon at Rhodes, while at Alexandria he saw it ascend as far as 7½ degrees above the horizon (the meridian arc between the latitude of the two locales is actually 5 degrees 14 minutes). Since he thought Rhodes was 5,000 stadia due north of Alexandria, and the difference in the star's elevation indicated the distance between the two locales was 1/48 of the circle, he multiplied 5,000 by 48 to arrive at a figure of 240,000 stadia for the circumference of the earth. It is generally thought that the stadion used by Posidonius was almost exactly 1/10 of a modern statute mile. Thus Posidonius's measure of 240,000 stadia translates to , not much short of the actual circumference of .<ref name=\"fragment 202\"/ Strabo noted that the distance between Rhodes and Alexandria is 3,750 stadia, and reported Posidonius's estimate of the Earth's circumference to be 180,000 stadia or . Pliny the Elder mentions Posidonius among his sources and without naming him reported his method for estimating the Earth's circumference. He noted, however, that Hipparchus had added some 26,000 stadia to Eratosthenes's estimate. The smaller value offered by Strabo and the different lengths of Greek and Roman stadia have created a persistent confusion around Posidonius's result. Ptolemy used Posidonius's lower value of 180,000 stades (about 33% too low) for the earth's circumference in his \"Geography\". This was the number used by Christopher Columbus in order to underestimate the distance to India as 70,000 stades.\nA more accurate estimate was provided in Al-Biruni's \"Codex Masudicus\" (1037). In contrast to his predecessors, who measured the Earth's circumference by sighting the Sun simultaneously from two different locations, al-Biruni developed a new method of using trigonometric calculations, based on the angle between a plain and mountain top, which yielded more accurate measurements of the Earth's circumference, and made it possible for it to be measured by a single person from a single location.\n\n1,700 years after Eratosthenes's death, while Christopher Columbus studied what Eratosthenes had written about the size of the Earth, he chose to believe, based on a map by Toscanelli, that the Earth's circumference was one-third smaller. Had Columbus set sail knowing that Eratosthenes's larger circumference value was more accurate, he would have known that the place that he made landfall was not Asia, but rather the New World.\n\nBoth the metre and the nautical mile were once calculated as a subdivision of the Earth's circumference.\n\nIn 1617 the Dutch scientist Willebrord Snellius assessed the circumference of the Earth at 24,630 Roman miles (24,024 statute miles). Around that time British mathematician Edmund Gunter improved navigational tools including a new quadrant to determine latitude at sea. He reasoned that the lines of latitude could be used as the basis for a unit of measurement for distance and proposed the nautical mile as one minute or one-sixtieth () of one degree of latitude. As one degree is of a circle, one minute of arc is of a circle. Gunter used Snell's circumference to define a nautical mile as 6,080 feet, the length of one minute of arc at 48 degrees latitude.\n\nIn 1791, the French Academy of Sciences selected the circumference definition over the alternative pendular definition because the force of gravity varies slightly over the surface of the Earth, which affects the period of a pendulum. To establish a universally accepted foundation for the definition of the metre, more accurate measurements of this meridian were needed. The French Academy of Sciences commissioned an expedition led by Jean Baptiste Joseph Delambre and Pierre Méchain, lasting from 1792 to 1799, which attempted to accurately measure the distance between a belfry in Dunkerque and Montjuïc castle in Barcelona to estimate the length of the meridian arc through Dunkerque. This portion of the meridian, assumed to be the same length as the Paris meridian, was to serve as the basis for the length of the half meridian connecting the North Pole with the Equator. The problem with this approach is that the exact shape of the Earth is not a simple mathematical shape, such as a sphere or oblate spheroid, at the level of precision required for defining a standard of length. The irregular and particular shape of the Earth smoothed to sea level is represented by a mathematical model called a geoid, which literally means \"Earth-shaped\". Despite these issues, in 1793 France adopted this definition of the metre as its official unit of length based on provisional results from this expedition. However, it was later determined that the first prototype metre bar was short by about 200 micrometres because of miscalculation of the flattening of the Earth, making the prototype about 0.02% shorter than the original proposed definition of the metre. Regardless, this length became the French standard and was progressively adopted by other countries in Europe.\n\n\n"}
{"id": "944638", "url": "https://en.wikipedia.org/wiki?curid=944638", "title": "Earth's energy budget", "text": "Earth's energy budget\n\nEarth's energy budget accounts for the balance between the energy Earth receives from the Sun, the energy Earth radiates back into outer space after having been distributed throughout the five components of Earth's climate system and having thus powered the so-called Earth’s heat engine. This system is made up of earth's water, ice, atmosphere, rocky crust, and all living things.\n\nQuantifying changes in these amounts is required to accurately model the Earth's climate. \n\nReceived radiation is unevenly distributed over the planet, because the Sun heats equatorial regions more than polar regions. The atmosphere and ocean work non-stop to even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation. Earth is very close to being in radiative equilibrium, the situation where the incoming solar energy is balanced by an equal flow of heat to space; under that condition, global temperatures will be \"relatively\" stable. Globally, over the course of the year, the Earth system—land surfaces, oceans, and atmosphere—absorbs and then radiates back to space an average of about 240 watts of solar power per square meter. Anything that increases or decreases the amount of incoming or outgoing energy will change global temperatures in response.\n\nHowever, Earth's energy balance and heat fluxes depend on many factors, such as atmospheric composition (mainly aerosols and greenhouse gases), the albedo (reflectivity) of surface properties, cloud cover and vegetation and land use patterns.\n\nChanges in surface temperature due to Earth's energy budget do not occur instantaneously, due to the inertia of the oceans and the cryosphere. The net heat flux is buffered primarily by becoming part of the ocean's heat content, until a new equilibrium state is established between radiative forcings and the climate response.\n\nIn spite of the enormous transfers of energy into and from the Earth, it maintains a relatively constant temperature because, as a whole, there is little net gain or loss: Earth emits via atmospheric and terrestrial radiation (shifted to longer electromagnetic wavelengths) to space about the same amount of energy as it receives via insolation (all forms of electromagnetic radiation).\n\nTo quantify Earth's \"heat budget\" or \"heat balance\", let the insolation received at the top of the atmosphere be 100 units (100 units = about 1,360 watts per square meter facing the sun), as shown in the accompanying illustration. Called the albedo of Earth, around 35 units are reflected back to space: 27 from the top of clouds, 2 from snow and ice-covered areas, and 6 by other parts of the atmosphere. The 65 remaining units are absorbed: 14 within the atmosphere and 51 by the Earth’s surface. These 51 units are radiated to space in the form of terrestrial radiation: 17 directly radiated to space and 34 absorbed by the atmosphere (19 through latent heat of condensation, 9 via convection and turbulence, and 6 directly absorbed). The 48 units absorbed by the atmosphere (34 units from terrestrial radiation and 14 from insolation) are finally radiated back to space. These 65 units (17 from the ground and 48 from the atmosphere) balance the 65 units absorbed from the sun in order to maintain zero net gain of energy by the Earth.\n\nThe total amount of energy received per second at the top of Earth's atmosphere (TOA) is measured in watts and is given by the solar constant times the cross-sectional area of the Earth. Because the surface area of a sphere is four times the cross-sectional surface area of a sphere (i.e. the area of a circle), the average TOA flux is one quarter of the solar constant and so is approximately 340 W/m². Since the absorption varies with location as well as with diurnal, seasonal and annual variations, the numbers quoted are long-term averages, typically averaged from multiple satellite measurements.\n\nOf the ~340 W/m² of solar radiation received by the Earth, an average of ~77 W/m² is reflected back to space by clouds and the atmosphere and ~23 W/m² is reflected by the surface albedo, leaving ~240 W/m² of solar energy input to the Earth's energy budget. This gives the earth a mean net albedo of 0.29.\n\nThe geothermal heat flux from the Earth's interior is estimated to be 47 terawatts. This comes to 0.087 watt/square metre, which represents only 0.027% of Earth's total energy budget at the surface, which is dominated by 173,000 terawatts of incoming solar radiation.\n\nHuman production of energy is even lower, at an estimated 18 TW.\n\nPhotosynthesis has a larger effect: photosynthetic efficiency turns up to 2% of incoming sunlight into biomass, for a total photosynthetic productivity of earth between ~1500–2250 TW (~1%+/-0.26% solar energy hitting the Earth's surface).\n\nOther minor sources of energy are usually ignored in these calculations, including accretion of interplanetary dust and solar wind, light from stars other than the Sun and the thermal radiation from space. Earlier, Joseph Fourier had claimed that deep space radiation was significant in a paper often cited as the first on the greenhouse effect.\n\nLongwave radiation is usually defined as outgoing infrared energy leaving the planet. However, the atmosphere absorbs parts initially, or cloud cover can reflect radiation. Generally, heat energy is transported between the planet's surface layers (land and ocean) to the atmosphere, transported via evapotranspiration and latent heat fluxes or conduction/convection processes. Ultimately, energy is radiated in the form of longwave infrared radiation back into space.\n\nRecent satellite observations indicate additional precipitation, which is sustained by increased energy leaving the surface through evaporation (the latent heat flux), offsetting increases in longwave flux to the surface.\n\nIf the incoming energy flux is not equal to the outgoing energy flux, net heat is added to or lost by the planet (if the incoming flux is larger or smaller than the outgoing respectively).\n\nAn imbalance must show in something on Earth warming or cooling (depending on the direction of the imbalance), and the ocean being the larger thermal reservoir on Earth, is a prime candidate for measurements.\n\nEarth's energy imbalance measurements provided by Argo floats have detected an accumulation of ocean heat content (OHC). The estimated imbalance was measured during a deep solar minimum of 2005–2010 to be 0.58 ± 0.15 W/m². This level of detail cannot be inferred directly from measurements of surface energy fluxes, which have combined uncertainties of the order of ± 17 W/m².\n\nSeveral satellites indirectly measure the energy absorbed and radiated by Earth and by inference the energy imbalance. The NASA Earth Radiation Budget Experiment (ERBE) project involves three such satellites: the Earth Radiation Budget Satellite (ERBS), launched October 1984; NOAA-9, launched December 1984; and NOAA-10, launched September 1986.\n\nToday NASA's satellite instruments, provided by CERES, part of the NASA's Earth Observing System (EOS), are designed to measure both solar-reflected and Earth-emitted radiation.\n\nThe major atmospheric gases (oxygen and nitrogen) are transparent to incoming sunlight but are also transparent to outgoing thermal (infrared) radiation. However, water vapor, carbon dioxide, methane and other trace gases are opaque to many wavelengths of thermal radiation. The Earth's surface radiates the net equivalent of 17 percent of the incoming solar energy in the form of thermal infrared. However, the amount that directly escapes to space is only about 12 percent of incoming solar energy. The remaining fraction, 5 to 6 percent, is absorbed by the atmosphere by greenhouse gas molecules.\nWhen greenhouse gas molecules absorb thermal infrared energy, their temperature rises. Those gases then radiate an increased amount of thermal infrared energy in all directions. Heat radiated upward continues to encounter greenhouse gas molecules; those molecules also absorb the heat, and their temperature rises and the amount of heat they radiate increases. The atmosphere thins with altitude, and at roughly 5–6 kilometres, the concentration of greenhouse gases in the overlying atmosphere is so thin that heat can escape to space.\n\nBecause greenhouse gas molecules radiate infrared energy in all directions, some of it spreads downward and ultimately returns to the Earth's surface, where it is absorbed. The Earth's surface temperature is thus higher than it would be if it were heated only by direct solar heating. This supplemental heating is the natural greenhouse effect. It is as if the Earth is covered by a blanket that allows high frequency radiation (sunlight) to enter, but slows the rate at which the low frequency infrared radiant energy emitted by the Earth leaves.\n\nA change in the incident radiated portion of the energy budget is referred to as a radiative forcing.\n\nClimate sensitivity is the steady state change in the equilibrium temperature as a result of changes in the energy budget.\n\nClimate forcings are changes that cause temperatures to rise or fall, disrupting the energy balance. Natural climate forcings include changes in the Sun's brightness, Milankovitch cycles (small variations in the shape of Earth's orbit and its axis of rotation that occur over thousands of years) and volcanic eruptions that inject light-reflecting particles as high as the stratosphere. Man-made forcings include particle pollution (aerosols) that absorb and reflect incoming sunlight; deforestation, which changes how the surface reflects and absorbs sunlight; and the rising concentration of atmospheric carbon dioxide and other greenhouse gases, which decreases the rate at which heat is radiated to space.\n\nA forcing can trigger feedbacks that intensify (positive feedback) or weaken (negative feedback) the original forcing. For example, loss of ice at the poles, which makes them less reflective, causes greater absorption of energy and so increases the rate at which the ice melts, is an example of a positive feedback.\n\nThe observed planetary energy imbalance during the recent solar minimum shows that solar forcing of climate, although natural and significant, is overwhelmed by anthropogenic climate forcing.\n\nIn 2012, NASA scientists reported that to stop global warming atmospheric CO content would have to be reduced to 350 ppm or less, assuming all other climate forcings were fixed. The impact of anthropogenic aerosols has not been quantified, but individual aerosol types are thought to have substantial heating and cooling effects.\n\n\n"}
{"id": "38328166", "url": "https://en.wikipedia.org/wiki?curid=38328166", "title": "Energy broker", "text": "Energy broker\n\nEnergy brokers assist clients in procuring electric or natural gas from energy wholesalers/suppliers. Since electricity and natural gas are commodities, prices change daily with the market. It is challenging for most businesses without energy managers to obtain price comparisons from a variety of suppliers since prices must be compared on exactly the same day. In addition, the terms of the particular contract offered by the supplier influences the price that is quoted. An energy broker can provide a valuable service if they work with a large number of suppliers and can actually compile the sundry prices from suppliers. An important aspect of this consulting role is to assure that the client understands the differences between the contract offers. Under some State Laws they use the term \"Suppliers\" to refer to energy suppliers, brokers, and aggregators, however there are very important differences between them all.\n\nEnergy brokers do not own or distribute energy, nor are allowed to sell energy directly to you. They simply present the rates of a wholesaler, or supplier.\n\nEnergy consultants offer a lot more than procuring energy contracts from a supplier. In the UK and Europe where there is a lot of legislation and increasing pressure for businesses and countries to do more to reduce their energy consumption a lot of services from brokers now help ensure businesses meet a lot of compliance and accreditation requirements such as the ESOS (energy saving opportunity scheme), ISO 50001, ISO 14001, Energy Performance Certificates and Display Energy Certificates.\nOther services include helping companies reduce energy consumption with the aim of meeting national and international carbon emission standards. Services include, energy health checks, energy audits, carbon zero, carbon offsetting and energy saving consulting.\n\nAdditional services such as arranging a power purchase agreement, energy export contracts can be procured as well as energy monitoring and reporting technology and solutions are also offered by energy consultants.\n\nIn the USA, energy brokers can serve residential, commercial and government entities that reside in energy deregulated states. In the UK, and some countries in Europe, the entire market is deregulated.\n\nEnergy brokers typically do not charge up front fees to provide rates. If an entity purchases energy through a broker, the broker's fee is usually included in the rate the customer pays. Some brokers will charge a fixed fee for their consulting services.\n\nNot all energy brokers are consultants; However, the energy brokers who are also consultants will perform a more detailed analysis of a consumers' usage pattern in order to provide a custom rate, which typically results in more cost savings for the consumer. Typically, they do not need any more information than that of an energy broker, because they can pull usage information from the local utility company. There are some national energy brokers that use auditing teams to verify their client's invoices.\n"}
{"id": "321382", "url": "https://en.wikipedia.org/wiki?curid=321382", "title": "Energy flow (ecology)", "text": "Energy flow (ecology)\n\nIn ecology, energy flow, also called the calorific flow, refers to the flow of energy through a food chain, and is the focus of study in ecological energetics. In an ecosystem, ecologists seek to quantify the relative importance of different component species and feeding relationships.\n\nA general energy flow scenario follows:\n\nThe energy is passed on from trophic level to trophic level and each time about 90% of the energy is lost, with some being lost as heat into the environment (an effect of respiration) and some being lost as incompletely digested food (egesta). Therefore, primary consumers get about 10% of the energy produced by autotrophs, while secondary consumers get 1% and tertiary consumers get 0.1%. This means the top consumer of a food chain receives the least energy, as a lot of the food chain's energy has been lost between trophic levels. This loss of energy at each level limits typical food chains to only four to six links.\n\nEcological energetics appears to have grown out of the Age of Enlightenment and the concerns of the Physiocrats. It began in the works of Sergei Podolinsky in the late 1800s, and subsequently was developed by the Soviet ecologist Vladmir Stanchinsky, the Austro-American Alfred J. Lotka, and American limnologists, Raymond Lindeman and G. Evelyn Hutchinson. It underwent substantial development by Howard T. Odum and was applied by systems ecologists, and radiation ecologists.\n\n\n"}
{"id": "9285", "url": "https://en.wikipedia.org/wiki?curid=9285", "title": "Ethical naturalism", "text": "Ethical naturalism\n\nEthical naturalism (also called moral naturalism or naturalistic cognitivistic definism) is the meta-ethical view which claims that:\n\n\nIt is important to distinguish the versions of ethical naturalism which have received the most sustained philosophical interest, for example, Cornell realism, from the position that \"the way things are is always the way they ought to be\", which few ethical naturalists hold. Ethical naturalism does, however, reject the fact-value distinction: it suggests that inquiry into the natural world can increase our moral knowledge in just the same way it increases our scientific knowledge. Indeed, proponents of ethical naturalism have argued that humanity needs to invest in the science of morality, a broad and loosely defined field that uses evidence from biology, primatology, anthropology, psychology, neuroscience, and other areas to classify and describe moral behavior.\n\nEthical naturalism encompasses any reduction of ethical properties, such as 'goodness', to non-ethical properties; there are many different examples of such reductions, and thus many different varieties of ethical naturalism. Hedonism, for example, is the view that goodness is ultimately just pleasure.\n\n\nEthical naturalism has been criticized most prominently by ethical non-naturalist G. E. Moore, who formulated the open-question argument. Garner and Rosen say that a common definition of \"natural property\" is one \"which can be discovered by sense observation or experience, experiment, or through any of the available means of science.\" They also say that a good definition of \"natural property\" is problematic but that \"it is only in criticism of naturalism, or in an attempt to distinguish between naturalistic and nonnaturalistic definist theories, that such a concept is needed.\" R. M. Hare also criticised ethical naturalism because of its fallacious definition of the terms 'good' or 'right' explaining how value-terms being part of our prescriptive moral language are not reducible to descriptive terms: \"Value-terms have a special function in language, that of commending; and so they plainly cannot be defined in terms of other words which themselves do not perform this function\".\n\nWhen it comes to the moral questions that we might ask, it can be difficult to argue that there is not necessarily some level of meta-ethical relativism – and failure to address this matter is criticized as ethnocentrism. \n\nAs a broad example of relativism, we would no doubt see very different moral systems in an alien race that can only survive by occasionally ingesting one another. As a narrow example, there would be further specific moral opinions for each individual of that species.\n\nSome forms of moral realism are compatible with some degree of meta-ethical relativism. This argument rests on the assumption that one can have a \"moral\" discussion on various scales; that is, what is \"good\" for: a certain part of your being (leaving open the possibility of conflicting motives), you as a single individual, your family, your society, your species, your type of species. For example, a moral universalist (and certainly an absolutist) might argue that, just as one can discuss what is 'good and evil' at an individual's level, so too can one make certain \"moral\" propositions with truth values relative at the level of the species. In other words, the moral relativist need not deem \"all\" moral propositions as necessarily subjective. The answer to \"is free speech normally good for human societies?\" is relative in a sense, but the moral realist would argue that an individual can be incorrect in this matter. This may be the philosophical equivalent of the more pragmatic arguments made by some scientists.\n\nMoral nihilists maintain that any talk of an objective morality is incoherent and better off using other terms. Proponents of moral science like Ronald A. Lindsay have counter-argued that their way of understanding \"morality\" as a practical enterprise is the way we ought to have understood it in the first place. He holds the position that the alternative seems to be the elaborate philosophical reduction of the word \"moral\" into a vacuous, useless term. Lindsay adds that it is important to reclaim the specific word \"Morality\" because of the connotations it holds with many individuals.\n\nAuthor Sam Harris has argued that we overestimate the relevance of many arguments against the science of morality, arguments he believes scientists happily and rightly disregard in other domains of science like physics. For example, scientists may find themselves attempting to argue against philosophical skeptics, when Harris says they should be practically asking – as they would in any other domain – \"why would we listen to a solipsist in the first place?\" This, Harris contends, is part of what it means to practice a science of morality.\n\nPhysicist Sean Carroll believes that conceiving of morality as a science could be a case of scientific imperialism and insists that what is \"good for conscious creatures\" is not an adequate working definition of \"moral\". In opposition, Vice President at the Center for Inquiry, John Shook, claims that this working definition is more than adequate for science at present, and that disagreement should not immobilize the scientific study of ethics.\n\nIn the collective \"The End of Christianity\", Richard Carrier's chapter \"Moral Facts Naturally Exist (and Science Could Find Them)\" sets out to propose a form of moral realism centered on human satisfaction..\nIn modern times, many thinkers discussing the fact–value distinction and the is–ought problem have settled on the idea that one cannot derive \"ought\" from \"is\". Conversely, Harris maintains that the fact-value distinction is a confusion, proposing that values are really a certain kind of fact. Specifically, Harris suggests that values amount to empirical statements about \"the flourishing of conscious creatures in a society\". He argues that there are objective answers to moral questions, even if some are difficult or impossible to possess in practice. In this way, he says, science can tell us what to value. Harris adds that we do not demand absolute certainty from predictions in physics so we should not demand that of a science studying morality (see \"The Moral Landscape\").\n\n\n"}
{"id": "1960343", "url": "https://en.wikipedia.org/wiki?curid=1960343", "title": "Evolutionary epistemology", "text": "Evolutionary epistemology\n\nEvolutionary epistemology refers to three distinct topics: (1) the biological evolution of cognitive mechanisms in animals and humans, (2) a theory that knowledge itself evolves by natural selection, and (3) the study of the historical discovery of new abstract entities such as abstract number or abstract value that necessarily precede the individual acquisition and usage of such abstractions.\n\n\"Evolutionary epistemology\" can refer to a branch of epistemology that applies the concepts of biological evolution to the growth of animal and human cognition. It argues that the mind is in part genetically determined and that its structure and function reflect adaptation, a nonteleological process of interaction between the organism and its environment. A cognitive trait tending to increase inclusive fitness in a given population should therefore grow more common over time, and a trait tending to prevent its carriers from passing on their genes should show up less and less frequently.\n\n\"Evolutionary epistemology\" can also refer to a theory that applies the concepts of biological evolution to the growth of human knowledge, and argues that units of knowledge themselves, particularly scientific theories, evolve according to selection. In this case, a theory—like the germ theory of disease—becomes more or less credible according to changes in the body of knowledge surrounding it.\n\nOne of the hallmarks of evolutionary epistemology is the notion that empirical testing alone does not justify the pragmatic value of scientific theories, but rather that social and methodological processes select those theories with the closest \"fit\" to a given problem. The mere fact that a theory has survived the most rigorous empirical tests available does not, in the calculus of probability, predict its ability to survive future testing. Karl Popper used Newtonian physics as an example of a body of theories so thoroughly confirmed by testing as to be considered unassailable, but which were nevertheless overturned by Einstein's insights into the nature of space-time. For the evolutionary epistemologist, all theories are true only provisionally, regardless of the degree of empirical testing they have survived.\n\n\"Evolutionary epistemology\" can also refer to the opposite of (onto)genetic epistemology, namely phylogenetic epistemology as the historical discovery and reification of abstractions that necessarily precedes the learning of such abstractions by individuals. Piaget dismissed this possibility, stating\n\nPiaget was mistaken in so quickly dismissing the study of phylogenetic epistemology, as there is much historical data available about the origins and evolution of the various notational systems that reify different kinds of abstract entity.\n\nPopper is considered by many to have given evolutionary epistemology its first comprehensive treatment, though Donald T. Campbell coined the phrase in 1974 and Piaget alluded to it in 1974 and described the concept as one of five possible theories in \"The Origins of Intelligence in Children\" (1936).\n\n\n\n"}
{"id": "47971685", "url": "https://en.wikipedia.org/wiki?curid=47971685", "title": "Extended evolutionary synthesis", "text": "Extended evolutionary synthesis\n\nThe extended evolutionary synthesis consists of a set of theoretical concepts more comprehensive than the earlier modern synthesis of evolutionary biology that took place between 1918 and 1942. The extended evolutionary synthesis was called for in the 1950s by C. H. Waddington, argued for on the basis of punctuated equilibrium by Stephen Jay Gould and Niles Eldredge in the 1980s, and was reconceptualized in 2007 by Massimo Pigliucci and Gerd B. Müller.\n\nThe extended evolutionary synthesis revisits the relative importance of different factors at play, examining several assumptions of the earlier synthesis, and augmenting it with additional causative factors. It includes multilevel selection, transgenerational epigenetic inheritance, niche construction, evolvability, and several concepts from evo-devo.\n\nNot all biologists have agreed on the need for, or the scope of, an extended synthesis. Many have collaborated on another synthesis in evolutionary developmental biology, which concetrates on developmental molecular genetics and evolution to understand how natural selection operated on developmental processes and deep homologies between organisms at the level of highly conserved genes.\n\nThe modern synthesis was the widely accepted early-20th-century synthesis reconciling Charles Darwin's theory of evolution by natural selection and Gregor Mendel's theory of genetics in a joint mathematical framework. It established evolution as biology's central paradigm. The 19th-century ideas of natural selection by Darwin and Mendelian genetics were united by researchers who included Ronald Fisher, one of the three founders of population genetics, and J. B. S. Haldane and Sewall Wright, between 1918 and 1932. Julian Huxley introduced the phrase \"modern synthesis\" in his 1942 book, \"\".\n\nDuring the 1950s, the English biologist C. H. Waddington called for an extended synthesis based on his research on epigenetics and genetic assimilation. An extended synthesis was also proposed by the Austrian zoologist Rupert Riedl, with the study of evolvability. In 1978, Michael J. D. White wrote about an extension of the modern synthesis based on new research from speciation.\n\nIn the 1980s, the American palaeontologists Stephen Jay Gould and Niles Eldredge argued for an extended synthesis based on their idea of punctuated equilibrium, the role of species selection shaping large scale evolutionary patterns and natural selection working on multiple levels extending from genes to species.\nThe ethologist John Endler wrote a paper in 1988 discussing processes of evolution that he felt had been neglected.\n\nSome researchers in the field of evolutionary developmental biology proposed another synthesis. They argue that the modern and extended syntheses should mostly center on genes and suggest an integration of embryology with molecular genetics an evolution, aiming to understand how natural selection operates on gene regulation and deep homologies between organisms at the level of highly conserved genes, transcription factors and signalling pathways. By contrast, a different strand of evo-devo following an organismal approach contributes to the extended synthesis by emphasizing (amongst others) developmental bias (both through facilitation and constraint), evolvability, and inherency of form as primary factors in the evolution of complex structures and phenotypic novelties.\n\nThe idea of an extended synthesis was relaunched in 2007 by Massimo Pigliucci, and Gerd B. Müller with a book in 2010 titled \"Evolution: The Extended Synthesis\", which has served as a launching point for work on the extended synthesis. This includes:\n\n\nOther processes such as evolvability, phenotypic plasticity, reticulate evolution, sex evolution and symbiogenesis are said by proponents to have been excluded or missed from the modern synthesis. The goal of Piglucci's and Müller's extended synthesis is to take evolution beyond the gene-centered approach of population genetics to consider more organism- and ecology-centered approaches. Many of these causes are currently considered secondary in evolutionary causation, and proponents of the extended synthesis want them to be considered first-class evolutionary causes. The biologist Eugene Koonin wrote in 2009 that \"the new developments in evolutionary biology by no account should be viewed as refutation of Darwin. On the contrary, they are widening the trails that Darwin blazed 150 years ago and reveal the extraordinary fertility of his thinking.\"\n\nThe extended synthesis is characterized by its additional set of predictions that differ from the standard modern synthesis theory:\n\n\nThe extended evolutionary synthesis is currently being tested by a group of scientists from eight institutions in Britain, Sweden and the United States. The £7.7 million project is supported by a £5.7 million grant from the John Templeton Foundation.\n\nThe project is headed by Kevin N. Laland at the University of St Andrews and Tobias Uller at Lund University. According to Laland what the extended synthesis \"really boils down to is recognition that, in addition to selection, drift, mutation and other established evolutionary processes, other factors, particularly developmental influences, shape the evolutionary process in important ways.\"\n\nBiologists disagree on the need for an extended synthesis. Opponents contend that the modern synthesis is able to fully account for the newer observations, whereas others criticize that the Extended synthesis is not radical enough. Proponents think that the conceptions of evolution at the core of the modern synthesis are too narrow. Proponents argue that even when the modern synthesis allows for the ideas in the extended synthesis, using the modern synthesis affects the way that biologists think about evolution. For example, Denis Noble says that using terms and categories of the modern synthesis distort the picture of biology that modern experimentation has discovered. Proponents therefore claim that the extended synthesis is necessary to help expand the conceptions and framework of how evolution is considered throughout the biological disciplines.\n\nDefend the extended synthesis\n\nCriticism of the extended synthesis\n\n"}
{"id": "21873573", "url": "https://en.wikipedia.org/wiki?curid=21873573", "title": "Freddy Fox", "text": "Freddy Fox\n\nFreddy Fox () is a hardcover picture book written and illustrated by Ronald J. Meyer. It tells the story of a young fox and his family, following the daily activities of the fox kit, Freddy. Freddy Fox teaches the reader about red foxes, including that they are born shades of gray, how their den is built, what they eat, and family size. Then through a simple segue, Freddy's red fox mother teaches the young fox about other animals and traits attributed to them. By learning his lessons, Freddy will grow up to be a wise fox. All of the lessons, such as the bear eats a balanced diet or the raccoon washes his hands before eating, can easily be translated into lessons for young children.\n\nThe book was first published in 2004. It won the Silver Mom's Choice Award in the \"Animal Kingdom\" category in 2008.\n\nThe author is a wildlife photographer, and he used his wildlife photographs to illustrate the book. \n\n"}
{"id": "514231", "url": "https://en.wikipedia.org/wiki?curid=514231", "title": "Frequency-dependent selection", "text": "Frequency-dependent selection\n\nFrequency-dependent selection is an evolutionary process by which the fitness of a phenotype depends on its frequency relative to other phenotypes in a given population.\n\n\nFrequency-dependent selection is usually the result of interactions between species (predation, parasitism, or competition), or between genotypes within species (usually competitive or symbiotic), and has been especially frequently discussed with relation to anti-predator adaptations. Frequency-dependent selection can lead to polymorphic equilibria, which result from interactions among genotypes within species, in the same way that multi-species equilibria require interactions between species in competition (e.g. where \"α\" parameters in Lotka-Volterra competition equations are non-zero).\n\nThe first explicit statement of frequency-dependent selection appears to have been by Edward Bagnall Poulton in 1884, on the way that predators could maintain color polymorphisms in their prey.\n\nPerhaps the best known early modern statement of the principle is Bryan Clarke's 1962 paper on apostatic selection (a synonym of negative frequency-dependent selection). Clarke discussed predator attacks on polymorphic British snails, citing Luuk Tinbergen's classic work on searching images as support that predators such as birds tended to specialize in common forms of palatable species. Clarke later argued that frequency-dependent balancing selection could explain molecular polymorphisms (often in the absence of heterosis) in opposition to the neutral theory of molecular evolution.\n\nAnother example is plant self-incompatibility alleles. When two plants share the same incompatibility allele, they are unable to mate. Thus, a plant with a new (and therefore, rare) allele has more success at mating, and its allele spreads quickly through the population .\n\nIn human pathogens, such as the flu virus, once a particular strain has become common, most individuals have developed an immune response to that strain. But a rare, novel strain of the flu virus is able to spread quickly to almost any individual, causing continual evolution of viral strains.\n\nThe major histocompatibility complex (MHC) is involved in the recognition of foreign antigens and cells. Frequency-dependent selection may explain the high degree of polymorphism in the MHC.\n\nIn behavioral ecology, negative frequency-dependent selection often maintains multiple behavioral strategies within a species. A classic example is the Hawk-Dove model of interactions among individuals in a population. In a population with two traits A and B, being one form is better when most members are the other form. As another example, male common side-blotched lizards have three morphs, which either defend large territories and maintain large harems of females, defend smaller territories and keep one female, or mimic females in order to sneak matings from the other two morphs. These three morphs participate in a rock paper scissors sort of interaction such that no one morph completely outcompetes the other two. Another example occurs in the scaly-breasted munia, where certain individuals become scroungers and others become producers.\n\nPositive frequency-dependent selection gives an advantage to common phenotypes. A good example is warning coloration in aposematic species. Predators are more likely to remember a common color pattern that they have already encountered frequently than one that is rare. This means that new mutants or migrants that have color patterns other than the common type are eliminated from the population by differential predation. Positive frequency-dependent selection provides the basis for Müllerian mimicry, as described by Fritz Müller , because all species involved are aposematic and share the benefit of a common, honest signal to potential predators.\n\nAnother, rather complicated example occurs in the Batesian mimicry complex between a harmless mimic, the scarlet kingsnake (\"Lampropeltis elapsoides\"), and the model, the eastern coral snake (\"Micrurus fulvius\"), in locations where the model and mimic were in deep sympatry, the phenotype of the scarlet kingsnake was quite variable due to relaxed selection. But where the pattern was rare, the predator population was not 'educated', so the pattern brought no benefit. The scarlet kingsnake was much less variable on the allopatry/sympatry border of the model and mimic, most probably due to increased selection since the eastern coral snake is rare, but present, on this border. Therefore, the coloration is only advantageous once it has become common.\n\n\n"}
{"id": "1686779", "url": "https://en.wikipedia.org/wiki?curid=1686779", "title": "Fusion energy gain factor", "text": "Fusion energy gain factor\n\nThe fusion energy gain factor, usually expressed with the symbol Q, is the ratio of fusion power produced in a nuclear fusion reactor to the power required to maintain the plasma in steady state. The condition of \"Q\" = 1, when the power being released by the fusion reactions is equal to the required heating power, is referred to as breakeven.\n\nThe power given off by the fusion reactions may be captured within the fuel, leading to \"self-heating\". Most fusion reactions release at least some of their energy in a form that cannot be captured within the plasma, so a system at \"Q\" = 1 will cool without external heating. With typical fuels, self-heating in fusion reactors is not expected to match the external sources until at least \"Q\" = 5. If \"Q\" increases past this point, increasing self-heating eventually removes the need for external heating. At this point the reaction becomes self-sustaining, a condition called ignition. Ignition corresponds to infinite \"Q\", and is generally regarded as highly desirable for a practical reactor design.\n\nOver time, several related terms have entered the fusion lexicon. As a reactor does not cover its own heating losses until about \"Q\" = 5, the term engineering breakeven is sometimes used to describe a reactor that produces enough electricity to provide that heating. Above engineering breakeven a machine would produce more electricity than it uses, and could sell that excess. A machine that can sell enough electricity to cover its operating costs, estimated to require at least \"Q\" = 20, is sometimes known as economic breakeven.\n\n, the record for \"Q\" is held by the JET tokamak in the UK, at \"Q\" = (16 MW)/(24 MW) ≈ 0.67, first attained in 1997. ITER was originally designed to reach ignition, but is currently designed to reach \"Q\" = 10, producing 500 MW of fusion power from 50 MW of injected thermal power.\n\n\"Q\" is simply the comparison of the power being released by the fusion reactions in a reactor, \"P\", to the constant heating power being supplied, \"P\". However, there are several definitions of breakeven that consider additional power losses.\n\nIn 1955, John Lawson was the first to explore the energy balance mechanisms in detail, initially in classified works but published openly in a now-famous 1957 paper. In this paper he considered and refined work by earlier researchers, notably Hans Thirring, Peter Thonemann, and a review article by Richard Post. Expanding on all of these, Lawson's paper made detailed predictions for the amount of power that would be lost through various mechanisms, and compared that to the energy needed to sustain the reaction. This balance is today known as the Lawson criterion.\n\nIn a successful fusion reactor design, the fusion reactions generate an amount of power designated \"P\". Some amount of this energy, \"P\", is lost through a variety of mechanisms, mostly convection of the fuel to the walls of the reactor chamber and various forms of radiation that cannot be captured to generate power. In order to keep the reaction going, the system has to provide heating to make up for these losses, where \"P\" = \"P\" to maintain thermal equilibrium.\n\nThe most basic definition of breakeven is when \"Q\" = 1, that is, \"P\" = \"P\".\n\nSome works refer to this definition as scientific breakeven, to contrast it with similar terms. However, this usage is rare outside certain areas, specifically the inertial confinement fusion field, where the term is much more widely used.\n\nSince the 1950s, most commercial fusion reactor designs have been based on a mix of deuterium and tritium as their primary fuel; others fuels have been studied for a variety of reasons but are much harder to ignite. As tritium is radioactive, highly bioactive and highly mobile, it represents a significant safety concern and adds to the cost of designing and operating such a reactor.\n\nIn order to lower costs, many experimental machines are designed to run on test fuels of hydrogen or deuterium alone, leaving out the tritium. In this case, the term extrapolated breakeven is used to define the expected performance of the machine running on D-T fuel based on the performance when running on hydrogen or deuterium alone.\n\nThe records for extrapolated breakeven are slightly higher than the records for scientific breakeven. Both JET and JT-60 have reached values around 1.25 (see below for details) while running on D-D fuel. When running on D-T, only possible in JET, the maximum performance is about half the extrapolated value.\n\nAnother related term, engineering breakeven, considers the need to extract the energy from the reactor, turn that into electrical energy, and feed that back into the heating system. This closed loop is known as \"recirculation\". In this case, the basic definition changes by adding additional terms to the \"P\" side to consider the efficiencies of these processes.\n\nMost fusion reactions release energy in a variety of forms, mostly neutrons and a variety of charged particles like alpha particles. Neutrons are electrically neutral and will travel out of any magnetic confinement fusion (MFE) design, and in spite of the very high densities found in inertial confinement fusion (ICF) designs, they tend to easily escape the fuel mass in these designs as well. This means that only the charged particles from the reactions can be captured within the fuel mass and give rise to self-heating. If the fraction of the energy being released in the charged particles is \"f\", then the power in these particles is \"P\" = \"f\"\"P\". If this self-heating process is perfect, that is, all of \"P\" is captured in the fuel, that means the power available for generating electricity is the power that is not released in that form, or (1 − \"f\")\"P\".\n\nIn the case of neutrons carrying most of the practical energy, as is the case in the D-T fuel studied in most designs, this neutron energy is normally captured in a \"blanket\" of lithium that produces more tritium that is used to fuel the reactor. Due to various exothermic and endothermic reactions, the blanket may have a power gain factor a few percent higher or lower than 100%, but that will be neglected here. The blanket is then cooled and the cooling fluid used in a heat exchanger driving conventional steam turbines. These have an efficiency η which is around 35 to 40%.\n\nConsider a system that uses external heaters to heat the fusion fuel, then extracts the power from those reactions to generate electrical power. Some fraction of that power, \"f\", is needed to recirculate back into the heaters to close the loop. This is not the same as the \"P\" because the self-heating processes are providing some of the required energy. While the system as a whole requires additional power for building climate control, lighting, and the confinement system, these are generally much smaller than the plasma heating system requirements.\nConsidering all of these factors, the heating power can thus be related to the fusion power by the following equation:\n\nformula_1\n\nwhere formula_2 is the efficiency that power supplied to the heating systems is turned into heat in the fuel, as opposed to lost in the equipment itself, and formula_3 is the efficiency achieved when turning the heat into electrical power, for instance, through the Rankine cycle.\n\nThe fusion energy gain factor is then defined as:\n\nformula_4\n\nAs the temperature of the plasma increases, the rate of fusion reactions grows rapidly, and with it, the rate of self heating. In contrast, the non-capturable energy losses like x-rays do not grow at the same rate. Thus, in overall terms, the self-heating process becomes more efficient as the temperature increases, and less energy is needed from external sources to keep it hot.\n\nEventually \"P\" reaches zero, that is, all of the energy needed to keep the plasma at the operational temperature is being supplied by self-heating, and the amount of external energy that needs to be added drops to zero. This point is known as ignition.\n\nIgnition, by definition, corresponds to an infinite \"Q\", but it does not mean that \"f\" drops to zero as the other power sinks in the system, like the magnets and cooling systems, still need to be powered. Generally, however, these are much smaller than the energy in the heaters, and require a much smaller \"f\". More importantly, this number is more likely to be near constant, meaning that further improvements in plasma performance will result in more energy that can be directly used for commercial generation, as opposed to recirculation.\n\nThe final definition of breakeven is commercial breakeven, which occurs when the economic value of any net energy left over after recirculation is enough to finance the construction of the reactor. This value depends both on the reactor and the spot price of electrical power.\n\nCommercial breakeven relies on factors outside the technology of the reactor itself, and it is possible that even a reactor with a fully ignited plasma will not generate enough energy to pay for itself. Whether any of the mainline concepts like ITER can reach this goal is being debated in the field.\n\nMost fusion reactor designs being studied are based on the D-T reaction, as this is by far the easiest to ignite, and is energy dense. However, this reaction also gives off most of its energy in the form of a single highly energetic neutron, and only 20% of the energy in the form of an alpha. Thus, for the D-T reaction, \"f\" = 0.2. This means that self-heating does not become equal to the external heating until at least \"Q\" = 5. \n\nEfficiency values depend on design details but may be in the range of η = 0.7 (70%) and η = 0.4 (40%). The purpose of a fusion reactor is to produce power, not to recirculate it, so a practical reactor must have \"f\" = 0.2 approximately. Lower would be better but will be hard to achieve. Using these values we find for a practical reactor \"Q\" = 22.\n\nMany early fusion devices operated for microseconds, using some sort of pulsed power source to feed their magnetic confinement system and used the confinement as the heating source. Lawson defined breakeven in this context as the total energy released by the entire reaction cycle compared to the total energy supplied to the machine during the same cycle.\n\nOver time, as performance increased by orders of magnitude, the reaction times have extended from microseconds to seconds, and in ITER, on the order of minutes. In this case definition of \"the entire reaction cycle\" becomes blurred. In the case of an ignited plasma, for instance, P may be quite high while the system is being set up, and then drop to zero when it is fully developed, so one may be tempted to pick an instant in time when it is operating at its best to determine \"Q\". A better solution in these cases is to use the original Lawson definition averaged over the reaction to produce a similar value as the original definition.\n\nHowever, there is a complication. During the heating phase when the system is being brought up to operational conditions, some of the energy released by the fusion reactions will be used to heat the surrounding fuel, and thus not be released. This is no longer true when the plasma reaches its operational temperature and enters thermal equilibrium. Thus, if one averages over the entire cycle, this energy will be included as part of the heating term, that is, some of the energy that was captured for heating would otherwise have been released in P and is therefore not indicative of an operational \"Q\".\n\nOperators of the JET reactor argued that this input should be removed from the total:\n\nformula_5\n\nwhere:\n\nformula_6\n\nThat is, P is the amount of energy needed to raise the internal energy of the plasma. It is this definition that was used when reporting JET's record 0.67 value.\n\nSome debate over this definition continues. In 1998, the operators of the JT-60 claimed to have reached \"Q\" = 1.25 running on D-D fuel, thus reaching extrapolated breakeven. However, this measurement was based on the JET definition of Q*. Using this definition, JET had also reached extrapolated breakeven some time earlier. If one considers the energy balance in these conditions, and the analysis of previous machines, it is argued the original definition should be used, and thus both machines remain well below break-even of any sort.\n\nAlthough most fusion experiments use some form of magnetic confinement, another major branch is inertial confinement fusion (ICF) that mechanically presses together the fuel mass (the \"target\") to increase its density. This greatly increases the rate of fusion events and lowers the need to confine the fuel for long periods. This compression is accomplished by heating a lightweight capsule holding the fuel so rapidly that it explodes outwards, driving the fuel mass on the inside inward in accordance with Newton's third law. There are a variety of proposed \"drivers\" to cause the implosion process, but to date most experiments have used lasers. \n\nUsing the traditional definition of \"Q\", \"P\" / \"P\", ICF devices have extremely low \"Q\". This is because the laser is extremely inefficient; whereas formula_2 for the heaters used in magnetic systems might be on the order of 70%, lasers are on the order of 1.5%. For this reason, Lawrence Livermore National Laboratory (LLNL), the leader in ICF research, has proposed another modification of \"Q\" that defines \"P\" as the energy delivered by the driver, as opposed to the energy put into the driver. This definition produces much higher \"Q\" values, and changes the definition of breakeven to be \"P\" / \"P\" = 1. On occasion, they referred to this definition as \"scientific breakeven\". This term was not universally used, other groups adopted the redefinition of \"Q\" but continued to refer to \"P\" = \"P\" simply as breakeven. \n\nOn 7 October 2013, the BBC announced that LLNL had achieved scientific breakeven in the National Ignition Facility (NIF) on 29 September. In this experiment, \"P\" was approximately 14 kJ, while the laser output was 1.8 MJ. By their previous definition, this would be a \"Q\" of 0.0077. However, for this press release, they re-defined \"Q\" once again, this time equating \"P\" to be only the amount energy delivered to \"the hottest portion of the fuel\", calculating that only 10 kJ of the original laser energy reached the part of the fuel that was undergoing fusion reactions. This release has been heavily criticized in the field.\n\n"}
{"id": "3588836", "url": "https://en.wikipedia.org/wiki?curid=3588836", "title": "Hardness", "text": "Hardness\n\nHardness is a measure of the resistance to localized plastic deformation induced by either mechanical indentation or abrasion. Some materials (e.g. metals) are harder than others (e.g. plastics, wood). Macroscopic hardness is generally characterized by strong intermolecular bonds, but the behavior of solid materials under force is complex; therefore, there are different measurements of hardness: \"scratch hardness\", \"indentation hardness\", and \"rebound hardness\".\n\nHardness is dependent on ductility, elastic stiffness, plasticity, strain, strength, toughness, viscoelasticity, and viscosity.\n\nCommon examples of hard matter are ceramics, concrete, certain metals, and superhard materials, which can be contrasted with soft matter.\n\nThere are three main types of hardness measurements: \"scratch\", \"indentation\", and \"rebound\". Within each of these classes of measurement there are individual measurement scales. For practical reasons conversion tables are used to convert between one scale and another.\n\nScratch hardness is the measure of how resistant a sample is to fracture or permanent plastic deformation due to friction from a sharp object. The principle is that an object made of a harder material will scratch an object made of a softer material. When testing coatings, scratch hardness refers to the force necessary to cut through the film to the substrate. The most common test is Mohs scale, which is used in mineralogy. One tool to make this measurement is the sclerometer.\n\nAnother tool used to make these tests is the pocket hardness tester. This tool consists of a scale arm with graduated markings attached to a four-wheeled carriage. A scratch tool with a sharp rim is mounted at a predetermined angle to the testing surface. In order to use it a weight of known mass is added to the scale arm at one of the graduated markings, the tool is then drawn across the test surface. The use of the weight and markings allows a known pressure to be applied without the need for complicated machinery.\n\nIndentation hardness measures the resistance of a sample to material deformation due to a constant compression load from a sharp object. Tests for indentation hardness are primarily used in engineering and metallurgy fields. The tests work on the basic premise of measuring the critical dimensions of an indentation left by a specifically dimensioned and loaded indenter.\n\nCommon indentation hardness scales are Rockwell, Vickers, Shore, and Brinell, amongst others.\n\nRebound hardness, also known as \"dynamic hardness\", measures the height of the \"bounce\" of a diamond-tipped hammer dropped from a fixed height onto a material. This type of hardness is related to elasticity. The device used to take this measurement is known as a scleroscope.\n\nTwo scales that measures rebound hardness are the Leeb rebound hardness test and Bennett hardness scale.\n\nThere are five hardening processes: Hall-Petch strengthening, work hardening, solid solution strengthening, precipitation hardening, and martensitic transformation.\n\nIn solid mechanics, solids generally have three responses to force, depending on the amount of force and the type of material:\n\nStrength is a measure of the extent of a material's elastic range, or elastic and plastic ranges together. This is quantified as compressive strength, shear strength, tensile strength depending on the direction of the forces involved. Ultimate strength is an engineering measure of the maximum load a part of a specific material and geometry can withstand.\n\nBrittleness, in technical usage, is the tendency of a material to fracture with very little or no detectable plastic deformation beforehand. Thus in technical terms, a material can be both brittle and strong. In everyday usage \"brittleness\" usually refers to the tendency to fracture under a small amount of force, which exhibits both brittleness and a lack of strength (in the technical sense). For perfectly brittle materials, yield strength and ultimate strength are the same, because they do not experience detectable plastic deformation. The opposite of brittleness is ductility.\n\nThe toughness of a material is the maximum amount of energy it can absorb before fracturing, which is different from the amount of force that can be applied. Toughness tends to be small for brittle materials, because elastic and plastic deformations allow materials to absorb large amounts of energy.\n\nHardness increases with decreasing particle size. This is known as the Hall-Petch relationship. However, below a critical grain-size, hardness decreases with decreasing grain size. This is known as the inverse Hall-Petch effect.\n\nHardness of a material to deformation is dependent on its microdurability or small-scale shear modulus in any direction, not to any rigidity or stiffness properties such as its bulk modulus or Young's modulus. Stiffness is often confused for hardness. Some materials are stiffer than diamond (e.g. osmium) but are not harder, and are prone to spalling and flaking in squamose or acicular habits.\n\nThe key to understanding the mechanism behind hardness is understanding the metallic microstructure, or the structure and arrangement of the atoms at the atomic level. In fact, most important metallic properties critical to the manufacturing of today’s goods are determined by the microstructure of a material. At the atomic level, the atoms in a metal are arranged in an orderly three-dimensional array called a crystal lattice. In reality, however, a given specimen of a metal likely never contains a consistent single crystal lattice. A given sample of metal will contain many grains, with each grain having a fairly consistent array pattern. At an even smaller scale, each grain contains irregularities.\n\nThere are two types of irregularities at the grain level of the microstructure that are responsible for the hardness of the material. These irregularities are point defects and line defects. A point defect is an irregularity located at a single lattice site inside of the overall three-dimensional lattice of the grain. There are three main point defects. If there is an atom missing from the array, a vacancy defect is formed. If there is a different type of atom at the lattice site that should normally be occupied by a metal atom, a substitutional defect is formed. If there exists an atom in a site where there should normally not be, an interstitial defect is formed. This is possible because space exists between atoms in a crystal lattice. While point defects are irregularities at a single site in the crystal lattice, line defects are irregularities on a plane of atoms. Dislocations are a type of line defect involving the misalignment of these planes. In the case of an edge dislocation, a half plane of atoms is wedged between two planes of atoms. In the case of a screw dislocation two planes of atoms are offset with a helical array running between them.\n\nIn glasses, hardness seems to depend linearly on the number of topological constraints acting between the atoms of the network. Hence, the rigidity theory has allowed predicting hardness values with respect to composition.\n\nDislocations provide a mechanism for planes of atoms to slip and thus a method for plastic or permanent deformation. Planes of atoms can flip from one side of the dislocation to the other effectively allowing the dislocation to traverse through the material and the material to deform permanently. The movement allowed by these dislocations causes a decrease in the material's hardness.\n\nThe way to inhibit the movement of planes of atoms, and thus make them harder, involves the interaction of dislocations with each other and interstitial atoms. When a dislocation intersects with a second dislocation, it can no longer traverse through the crystal lattice. The intersection of dislocations creates an anchor point and does not allow the planes of atoms to continue to slip over one another A dislocation can also be anchored by the interaction with interstitial atoms. If a dislocation comes in contact with two or more interstitial atoms, the slip of the planes will again be disrupted. The interstitial atoms create anchor points, or pinning points, in the same manner as intersecting dislocations.\n\nBy varying the presence of interstitial atoms and the density of dislocations, a particular metal's hardness can be controlled. Although seemingly counter-intuitive, as the density of dislocations increases, there are more intersections created and consequently more anchor points. Similarly, as more interstitial atoms are added, more pinning points that impede the movements of dislocations are formed. As a result, the more anchor points added, the harder the material will become.\n\n\n\n\n"}
{"id": "48049760", "url": "https://en.wikipedia.org/wiki?curid=48049760", "title": "Ikranite", "text": "Ikranite\n\nIkranite is a member of the eudialyte group, named after the Shubinov Institute of Crystallography of the Russian Academy of Sciences. It is a cyclosilicate mineral that shows trigonal symmetry with the space group \"R\"3\"m\", and is often seen with a pseudo-hexagonal habit. Ikranite appears as translucent and ranges in color from yellow to a brownish yellow. This mineral ranks a 5 on Mohs Scale of Hardness, though it is considered brittle, exhibiting conchoidal fracture when broken.\n\nThe eudialyte group currently consists of 27 known minerals (see below), all considered rare, with the exception of eudialyte. The list below also includes one of around six unnamed (\"UM\") species listed by Mindat. This group is growing exponentially, with 17 members having been written about since 2000, and the chemical possibility of several thousand species that have yet to be discovered. Eudialyte group members are typically found as small crystals which have a complex crystal structure that is unique in that it consists of both 3- and 9-member SiO tetrahedra rings.\n\nIkranite separates itself from the other members of this group through both its physical and compositional properties. The most prominent of these characteristics is the absence of sodium in its structure, along with the replacement of divalent iron with the trivalent form. This replacement also causes the characteristic color change from the reddish color seen in eudialyte to the yellow brown of ikranite, which can be further examined in its IR spectrum.\n\nIkranite was first discovered on Mount Karnasurt (Kola Peninsula) in an agpaitic pegmatite, in the form of 1–2 cm grains. It is commonly associated with microcline, nepheline, lorenzenite, murmanite, lamprophyllite, and arfvedsonite. Tetranatrolite, and halloysite can also be found with it, though they occur at a later stage.\n\nThe crystal structure of ikranite can be described as a framework of three- and nine- member SiO tetrahedra rings, connected by Ca six-membered rings and Zn (Ti, or Nb) octahedra. Layers are constructed along the c axis as Si-Zr-Si-Ca. This repetition generates 12 layers, equal to ~30Å in size.\n\nDiffering types cations, anions, anionic groups, and water molecules fill any pockets within the framework. A defining feature is the location of the M(3) and M(4) vacancies in the nine-membered rings. These cavities may be occupied with Si in the M(3b) location, Zr in the M(4a), and Zr, Nb, or Ti in the (M4b), though the probability of occupancy is low. The M(2a) and M(2b) locations are also uniquely occupied in ikranite. The M(2a) vacancy is seen occupied by Fe octahedra. Typically holding a five-membered polyhedra with Fe, the M(2b) position is occupied by Na cations. Ikranite also holds a significant amount of water in the space between the rings where an Na molecule is usually found. A distinctive feature is the oxonium groups that can also be found occupying Na sites.\n\nIkranite's general formula thus becomes: (Na,HO)(Ca,Mn,REE)FeZr([ ],Zr)([ ],Si)SiO(O,OH)ClHO\n\n"}
{"id": "23721650", "url": "https://en.wikipedia.org/wiki?curid=23721650", "title": "Index of energy articles", "text": "Index of energy articles\n\nThis is an index of energy articles.\n\nActivation energy\n- Alternative energy\n- Alternative energy indexes\n- American Museum of Science and Energy (AMSE)\n- Anisotropy energy\n- Atomic energy\n\nBinding energy\n- Black hole\n- Breeder reactor\n- Brown energy\n\nCharacteristic energy\n- Conservation of energy\n- Consol Energy\n\nDark energy\n- Decay energy\n- Direct Energy\n- Dirichlet's energy\n- Dyson's sphere\n\nEcological energetics\n- Electric potential energy\n- Electrochemical energy conversion\n- Embodied energy\n- Encircled energy\n- Energy\n- Energy accidents\n- Energy accounting\n- Energy amplifier\n- Energy analyser\n- Energy applications of nanotechnology\n- Energy balance (biology)\n- Energy bar\n- Energy barrier\n- Energy being\n- Energy carrier\n- Energy Catalyzer\n- Energy cell\n- Energy charge\n- Energy conservation\n- Energy conversion efficiency\n- Energy crop\n- Energy current\n- Energy density\n- Energy-depth relationship in a rectangular channel\n- Energy development\n- Energy-dispersive X-ray spectroscopy\n- Energy distance\n- Energy drift\n- Energy drink\n- Energy efficiency gap\n- Energy-Efficient Ethernet\n- Energy-efficient landscaping\n- Energy elasticity\n- Energy engineering\n- Energy (esotericism)\n- Energy expenditure\n- Energy factor\n- Energy field disturbance\n- Energy filtered transmission electron microscopy\n- Energy transfer\n- Energy flow (ecology)\n- Energy flux\n- Energy forestry\n- Energy functional\n- Energy gel\n- Energy harvesting\n- Energy input labeling\n- Energy landscape\n- Energy level\n- Energy level splitting\n- Energy management software\n- Energy management system\n- Energy–maneuverability theory\n- Energy Manufacturing Co. Inc\n- Energy medicine\n- Energy–momentum relation\n- Energy monitoring and targeting\n- Energy Probe\n- Energy profile (chemistry)\n- Energy quality\n- Energy recovery ventilation\n- Energy security\n- Energy (signal processing)\n- Energy Slave\n- Energy Star\n- Energy statistics\n- Energy Storage Challenge\n- Energy storage\n- Energy system\n- Energy technology\n- Energy tower (downdraft)\n- Energy transfer\n- Energy transfer upconversion\n- Energy transformation\n- Energy value of coal\n- Energy vortex (stargate)\n- Enthalpy\n- Entropy\n- Equipartition theorem\n- E-statistic\n- Exertion\n\nFermi energy\n- Forms of energy\n- Fuel\n- Fusion power\n\nGeothermal energy\n- Gravitational energy\n- Gravitational potential\n\nHistory of energy\n- Hydroelectricity\n\nInteraction energy\n- Intermittent energy source\n- Internal energy\n- Invariant mass\n\nJosephson energy\n\nKinetic energy\n\nLatent heat\n\nMagnetic confinement fusion \n- Marine energy\n- Mass–energy equivalence\n- Mechanical energy\n- Möbius energy\n\nNegative energy\n- Nuclear fusion\n- Nuclear power\n- Nuclear reactor\n\nOrders of magnitude (energy)\n- Osmotic power\n\nPhotosynthesis\n- Potential energy\n- Power (physics)\n- Primary energy\n\nQi\n- Quasar\n\nRelativistic jet\n- Renewable energy - Rotational energy\n\nSeismic scale\n- Solar energy\n- Solar thermal energy\n- Sound energy\n- Specific energy\n- Specific kinetic energy\n- Specific orbital energy\n- Surface energy\n\nThermodynamic free energy\n- Threshold energy\n- Tidal power\n- Turbulence kinetic energy\n\nUnits of energy\n- Universe of Energy\n\nVacuum energy\n\nWork (physics)\n- World energy resources and consumption\n- World Forum on Energy Regulation\n\nZero-energy building\n- Zero-energy universe\n- Zero-point energy\n\n"}
{"id": "889726", "url": "https://en.wikipedia.org/wiki?curid=889726", "title": "Insect collecting", "text": "Insect collecting\n\nInsect collecting refers to the collection of insects and other arthropods for scientific study or as a hobby. Because most insects are small and the majority cannot be identified without the examination of minute morphological characters, entomologists often make and maintain insect collections. Very large collections are conserved in natural history museums or universities where they are maintained and studied by specialists. Many college courses require students to form small collections. There are also amateur entomologists and collectors who keep collections.\n\nHistorically, insect collecting has been widespread and was in the Victorian age a very popular educational hobby. Insect collecting has left traces in European cultural history, literature and songs (e.g., Georges Brassens's \"La chasse aux papillons\" (\"The Hunt for Butterflies\")). The practice is still widespread in many countries, and is particularly common among Japanese youths.\n\nInsects are passively caught using funnels, pitfall traps, bottle traps, malaise traps, flight interception traps and other passive types of insect traps, some of which are baited with small bits of sweet foods (such as honey). Different designs of ultraviolet light traps such as the Robinson trap are also used by entomologists for collecting nocturnal insects (especially moths) during faunistic survey studies. Aspirators or \"pooters\" suck up insects too small or delicate to handle with fingers.\nSeveral different types of nets are commonly used to actively collect insects. Aerial insect nets are used to collect flying insects. The bag of a butterfly net is generally constructed from a lightweight mesh to minimize damage to delicate butterfly wings. A sweep net is used to collect insects from grass and brush. It is similar to a butterfly net, except that the bag is generally constructed from more rugged material.The sweep net is swept back and forth through vegetation quickly turning the opening from side to side and following a shallow figure eight pattern. The collector walks forward while sweeping, and the net is moved through plants and grasses with force.This requires a heavy net fabric such as sailcloth to prevent tearing, although light nets can be used if swept less vigorously. Sweeping continues for some distance and then the net is flipped over, with the bag hanging over the rim, trapping the insects until they can be removed with a pooter. Other types of nets used for collecting insects include beating nets and aquatic nets. Leaf litter sieves are used by coleopterists and to collect larvae.\n\nOnce collected, a killing jar is used to kill required insects before they damage themselves trying to escape. However, killing jars are generally only used on hard-bodied insects. Soft-bodied insects, such as those in the larval stage, are generally fixed in a vial containing an ethanol and water solution. Another now mostly historical approach is Caterpillar inflation where the innards were removed and the skin dried.\n\nThe usual method of display is in a glass-covered box, with the insects mounted on specially made non corrosive insect pins stuck into suitable foam plastic or paper covered cork at the bottom of the box. Common pins are not used. Very small insects may be pinned on \"minuten\" (very tiny headless pins) stuck into a block of foam plastic on a standard insect pin. Alternatively they may be glued onto a small piece of card on the pin. There are specific procedures for proper mounting that are used to show off the insects' essential characteristics. Techniques and equipment may be varied to deal with various species or requirements. For example, one or both of the wings of a beetle or grasshopper can be pulled open and fanned out to show the wing structure that otherwise would be hidden. At least the date and place of capture should be written or computer printed onto a piece of paper or card transfixed by the pin. This is called a data label.\nRare insects, or those from distant parts of the world, may also be acquired from dealers or by trading. Some noted insect collections have been sold at auction.\n\n\"Pokémon\" creator Satoshi Tajiri's childhood hobby of insect collecting is the inspiration behind the popular video game series.\n\n\nPicture Guide series For college students. Out of date but very useful for beginners.\n\n"}
{"id": "18401357", "url": "https://en.wikipedia.org/wiki?curid=18401357", "title": "Light clay", "text": "Light clay\n\nLight clay (also light straw clay, light clay straw, slipstraw) is a natural building material used to infill between a wooden frame in a timber framed building using a combination of clay and straw, woodchips or some other lighter material.\n\nA mixture of clay and straw was used as an infill material for timber framed building from at least the 12th century in Germany and elsewhere in Europe. Renewed interest in traditional building methods developed from the 1980s after which various natural building architects and builders started promoting the use of light clay.\n\nLocal clay, often local subsoil, is mixed into a slurry with water and then combined with straw or wood chip or other similar material. Wood chips can vary in size from sawdust to chip 5cm in diameter. The ratio of clay to other ingredients can be adapted to either increase thermal mass or insulation properties. The mixture is provided with additional structural strength using wattles. When used externally it can be protected with a Lime render or a clay render.\n\n"}
{"id": "1760214", "url": "https://en.wikipedia.org/wiki?curid=1760214", "title": "List of natural phenomena", "text": "List of natural phenomena\n\nTypes of natural phenomena include:\n\nWeather, fog, thunder, tornadoes; biological processes, decomposition, germination; physical processes, wave propagation, erosion; tidal flow, and natural disasters such as electromagnetic pulses, volcanic eruptions, and earthquakes.\n\n\n\nGeological processes include erosion, sedimentation, and volcanic activities such as geysers and earthquakes.\n\nViolent meteorological phenomena are called storms. Regular, cyclical phenomena include seasons and atmospheric circulation. Climate change is often semi-regular.\n\n\n\n"}
{"id": "15911800", "url": "https://en.wikipedia.org/wiki?curid=15911800", "title": "List of reserves for waterbirds and migratory birds in Switzerland", "text": "List of reserves for waterbirds and migratory birds in Switzerland\n\nThis is a list of reserves for waterbirds and migratory birds in Switzerland. The nature reserves on this inventory are protected by Swiss federal legislation (\"Federal Inventory of Water and Migratory Birds Reserves of National and International Importance\").\n\n\n"}
{"id": "6748280", "url": "https://en.wikipedia.org/wiki?curid=6748280", "title": "Material", "text": "Material\n\nA material is a chemical substance or mixture of substances that constitute an object. Materials can be pure or impure, a singular composite or a complex mix, living or non-living matter, whether natural or man-made, either concrete or abstract. Materials can be classified based on different properties such as physical and chemical properties (see List of materials properties), geological, biological, choreographical, or philosophical properties. In the physical sense, materials are studied in the field of materials science.\n\nIn industry, materials are inputs to production or manufacturing processes. They may either be raw material, that is, unprocessed, or processed before being used in more advanced production processes, either by distillation or synthesis (synthetic materials).\n\nTypes of materials include:\n\nMaterials are classified according to many different criteria including their physical and chemical characteristics as well as their intended applications whether it is thermal, optical, electrical, magnetic, or combined. As their methods of usage dictate their physical appearance, they can be designed, tailored, and/or prepared in many forms such as powders, thin or thick films, and plates and could be introduced/studied in a single or multi layers. End products could be pure materials or doped ones with most useful compounds are those with controlled added impurities.The dopants could be added chemically or mixed and implanted physically. In case the impurities were added chemically, the dopants/co-dopants on substitutional/interstitial sites should be optimized and investigated thoroughly as well as any stresses instigated by their presence within the structure; whereas in the case of the physical mixing, the influence of the degree of heterogeneity of the prepared hybrid composites ought to be studied.The different physical and chemical preparation techniques can be used solely or combined including solid state synthesis, hydrothermal, sol-gel, precipitations and coprecipitations, spin coating, physical vapor deposition, and spray pyrolysis. Types of impurities along with their amounts are usually dictated by types of matrices to be added to, and their ability to maximize the desired products’ usefulness. Among the most commonly used characterization techniques are X-ray diffraction (XRD) either single crystal or powder, scanning electron microscopy (SEM), energy dispersive X-ray spectroscopy (EDS), X-ray fluorescence (XRF), differential scanning calorimetry (DSC), UV-Vis absorption Spectroscopy, Fourier transform infra-red (FTIR), and Photoluminescence spectrometry. In addition, it is usually considered of extreme importance to find theoretical models that can confirm and/or predict the experimental findings and assist in discussion, assignment, and the explanation of results and outcomes. Also, vision and room for future modification and development should always be pinpointed. Hence, one can classify the material as a smart one if its presence can serve multi purposes within the final product.\n\n"}
{"id": "38393", "url": "https://en.wikipedia.org/wiki?curid=38393", "title": "Natural rubber", "text": "Natural rubber\n\nNatural rubber, also called India rubber or caoutchouc, as initially produced, consists of polymers of the organic compound isoprene, with minor impurities of other organic compounds, plus water. Malaysia and Indonesia are two of the leading rubber producers. Forms of polyisoprene that are used as natural rubbers are classified as elastomers.\n\nCurrently, rubber is harvested mainly in the form of the latex from the rubber tree or others. The latex is a sticky, milky colloid drawn off by making incisions in the bark and collecting the fluid in vessels in a process called \"tapping\". The latex then is refined into rubber ready for commercial processing. In major areas, latex is allowed to coagulate in the collection cup. The coagulated lumps are collected and processed into dry forms for marketing.\n\nNatural rubber is used extensively in many applications and products, either alone or in combination with other materials. In most of its useful forms, it has a large stretch ratio and high resilience, and is extremely waterproof.\n\nThe major commercial source of natural rubber latex is the Pará rubber tree (\"Hevea brasiliensis\"), a member of the spurge family, \"Euphorbiaceae\". This species is preferred because it grows well under cultivation. A properly managed tree responds to wounding by producing more latex for several years.\n\nCongo rubber, formerly a major source of rubber, came from vines in the genus \"Landolphia\" (\"L. kirkii\", \"L. heudelotis\", and \"L. owariensis\").\n\nDandelion milk contains latex. The latex exhibits the same quality as the natural rubber from rubber trees. In the wild types of dandelion, latex content is low and varies greatly. In Nazi Germany, research projects tried to use dandelions as a base for rubber production, but failed. In 2013, by inhibiting one key enzyme and using modern cultivation methods and optimization techniques, scientists in the Fraunhofer Institute for Molecular Biology and Applied Ecology (IME) in Germany developed a cultivar that is suitable for commercial production of natural rubber. In collaboration with Continental Tires, IME began a pilot facility.\n\nMany other plants produce forms of latex rich in isoprene polymers, though not all produce usable forms of polymer as easily as the Pará. Some of them require more elaborate processing to produce anything like usable rubber, and most are more difficult to tap. Some produce other desirable materials, for example gutta-percha (\"Palaquium gutta\") and chicle from \"Manilkara\" species. Others that have been commercially exploited, or at least showed promise as rubber sources, include the rubber fig (\"Ficus elastica\"), Panama rubber tree (\"Castilla elastica\"), various spurges (\"Euphorbia\" spp.), lettuce (\"Lactuca\" species), the related \"Scorzonera tau-saghyz\", various \"Taraxacum\" species, including common dandelion (\"Taraxacum officinale\") and Russian dandelion (\"Taraxacum kok-saghyz\"), and perhaps most importantly for its hypoallergenic properties, guayule (\"Parthenium argentatum\"). The term gum rubber is sometimes applied to the tree-obtained version of natural rubber in order to distinguish it from the synthetic version.\n\nThe first use of rubber was by the indigenous cultures of Mesoamerica. The earliest archeological evidence of the use of natural latex from the \"Hevea\" tree comes from the Olmec culture, in which rubber was first used for making balls for the Mesoamerican ballgame. Rubber was later used by the Maya and Aztec cultures – in addition to making balls Aztecs used rubber for other purposes such as making containers and to make textiles waterproof by impregnating them with the latex sap.\n\nThe Pará rubber tree is indigenous to South America. Charles Marie de La Condamine is credited with introducing samples of rubber to the \"Académie Royale des Sciences\" of France in 1736. In 1751, he presented a paper by François Fresneau to the Académie (published in 1755) that described many of rubber's properties. This has been referred to as the first scientific paper on rubber. In England, Joseph Priestley, in 1770, observed that a piece of the material was extremely good for rubbing off pencil marks on paper, hence the name \"rubber\". It slowly made its way around England. In 1764 François Fresnau discovered that turpentine was a rubber solvent. Giovanni Fabbroni is credited with the discovery of naphtha as a rubber solvent in 1779.\n\nSouth America remained the main source of the limited amounts of latex rubber used during much of the 19th century. The trade was heavily protected and exporting seeds from Brazil was a capital offense, although no law prohibited it. Nevertheless, in 1876, Henry Wickham smuggled 70,000 Pará rubber tree seeds from Brazil and delivered them to Kew Gardens, England. Only 2,400 of these germinated. Seedlings were then sent to India, British Ceylon (Sri Lanka), Dutch East Indies (Indonesia), Singapore, and British Malaya. Malaya (now Peninsular Malaysia) was later to become the biggest producer of rubber.\n\nIn the early 1900s, the Congo Free State in Africa was also a significant source of natural rubber latex, mostly gathered by forced labor. King Leopold II's colonial state brutally enforced production quotas. Tactics to enforce the rubber quotas included removing the hands of victims to prove they had been killed. Soldiers often came back from raids with baskets full of chopped-off hands. Villages that resisted were razed to encourage better compliance locally. See Atrocities in the Congo Free State for more information on the rubber trade in the Congo Free State in the late 1800s and early 1900s. Liberia and Nigeria started production.\n\nIn India, commercial cultivation was introduced by British planters, although the experimental efforts to grow rubber on a commercial scale were initiated as early as 1873 at the Calcutta Botanical Gardens. The first commercial \"Hevea\" plantations were established at Thattekadu in Kerala in 1902. In later years the plantation expanded to Karnataka, Tamil Nadu and the Andaman and Nicobar Islands of India. India today is the world's 3rd largest producer and 4th largest consumer.\n\nIn Singapore and Malaya, commercial production was heavily promoted by Sir Henry Nicholas Ridley, who served as the first Scientific Director of the Singapore Botanic Gardens from 1888 to 1911. He distributed rubber seeds to many planters and developed the first technique for tapping trees for latex without causing serious harm to the tree. Because of his fervent promotion of this crop, he is popularly remembered by the nickname \"Mad Ridley\".\n\nCharles Goodyear developed vulcanization in 1839, although Mesoamericans used stabilized rubber for balls and other objects as early as 1600 BC.\n\nBefore World War II significant uses included door and window profiles, hoses, belts, gaskets, matting, flooring and dampeners (antivibration mounts) for the automotive industry. The use of rubber in car tires (initially solid rather than pneumatic) in particular consumed a significant amount of rubber. Gloves (medical, household and industrial) and toy balloons were large consumers of rubber, although the type of rubber used is concentrated latex. Significant tonnage of rubber was used as adhesives in many manufacturing industries and products, although the two most noticeable were the paper and the carpet industries. Rubber was commonly used to make rubber bands and pencil erasers.\n\nRubber produced as a fiber, sometimes called 'elastic', had significant value to the textile industry because of its excellent elongation and recovery properties. For these purposes, manufactured rubber fiber was made as either an extruded round fiber or rectangular fibers cut into strips from extruded film. Because of its low dye acceptance, feel and appearance, the rubber fiber was either covered by yarn of another fiber or directly woven with other yarns into the fabric. Rubber yarns were used in foundation garments. While rubber is still used in textile manufacturing, its low tenacity limits its use in lightweight garments because latex lacks resistance to oxidizing agents and is damaged by aging, sunlight, oil and perspiration. The textile industry turned to neoprene (polymer of chloroprene), a type of synthetic rubber, as well as another more commonly used elastomer fiber, spandex (also known as elastane), because of their superiority to rubber in both strength and durability.\n\nRubber exhibits unique physical and chemical properties. Rubber's stress–strain behavior exhibits the Mullins effect and the Payne effect and is often modeled as hyperelastic. Rubber strain crystallizes.\n\nDue to the presence of weakened allylic C-H bonds in each repeat unit, natural rubber is susceptible to vulcanisation as well as being sensitive to ozone cracking.\n\nThe two main solvents for rubber are turpentine and naphtha (petroleum). Because rubber does not dissolve easily, the material is finely divided by shredding prior to its immersion.\n\nAn ammonia solution can be used to prevent the coagulation of raw latex.\n\nRubber begins to melt at approximately .\n\nOn a microscopic scale, relaxed rubber is a disorganized cluster of erratically changing wrinkled chains. In stretched rubber, the chains are almost linear. The restoring force is due to the preponderance of wrinkled conformations over more linear ones. For the quantitative treatment see ideal chain, for more examples see entropic force.\n\nCooling below the glass transition temperature permits local conformational changes but a reordering is practically impossible because of the larger energy barrier for the concerted movement of longer chains. \"Frozen\" rubber's elasticity is low and strain results from small changes of bond lengths and angles: this caused the \"Challenger\" disaster, when the American Space Shuttle's flattened o-rings failed to relax to fill a widening gap. The glass transition is fast and reversible: the force resumes on heating.\n\nThe parallel chains of stretched rubber are susceptible to crystallization. This takes some time because turns of twisted chains have to move out of the way of the growing crystallites. Crystallization has occurred, for example, when, after days, an inflated toy balloon is found withered at a relatively large remaining volume. Where it is touched, it shrinks because the temperature of the hand is enough to melt the crystals.\n\nVulcanization of rubber creates di- and polysulfide bonds between chains, which limits the degrees of freedom and results in chains that tighten more quickly for a given strain, thereby increasing the elastic force constant and making the rubber harder and less extensible.\n\nRaw rubber storage depots and rubber processing can produce malodour that is serious enough to become a source of complaints and protest to those living in the vicinity.\n\nMicrobial impurities originate during the processing of block rubber. These impurities break down during storage or thermal degradation and produce volatile organic compounds. Examination of these compounds using gas chromatography/mass spectrometry (GC/MS) and gas chromatography (GC) indicates that they contain sulphur, ammonia, alkenes, ketones, esters, hydrogen sulphite, nitrogen, and low molecular weight fatty acids (C2-C5).\n\nWhen latex concentrate is produced from rubber, sulphuric acid is used for coagulation. This produces malodourous hydrogen sulphide.\n\nThe industry can mitigate these bad odours with scrubber systems.\n\nLatex is the polymer cis-1,4-polyisoprene – with a molecular weight of 100,000 to 1,000,000 daltons. Typically, a small percentage (up to 5% of dry mass) of other materials, such as proteins, fatty acids, resins, and inorganic materials (salts) are found in natural rubber. Polyisoprene can also be created synthetically, producing what is sometimes referred to as \"synthetic natural rubber\", but the synthetic and natural routes are different. Some natural rubber sources, such as gutta-percha, are composed of trans-1,4-polyisoprene, a structural isomer that has similar properties.\n\nNatural rubber is an elastomer and a thermoplastic. Once the rubber is vulcanized, it is a thermoset. Most rubber in everyday use is vulcanized to a point where it shares properties of both; i.e., if it is heated and cooled, it is degraded but not destroyed.\n\nThe final properties of a rubber item depend not just on the polymer, but also on modifiers and fillers, such as carbon black, factice, whiting and others.\n\nRubber particles are formed in the cytoplasm of specialized latex-producing cells called laticifers within rubber plants. Rubber particles are surrounded by a single phospholipid membrane with hydrophobic tails pointed inward. The membrane allows biosynthetic proteins to be sequestered at the surface of the growing rubber particle, which allows new monomeric units to be added from outside the biomembrane, but within the lacticifer. The rubber particle is an enzymatically active entity that contains three layers of material, the rubber particle, a biomembrane and free monomeric units. The biomembrane is held tightly to the rubber core due to the high negative charge along the double bonds of the rubber polymer backbone. Free monomeric units and conjugated proteins make up the outer layer. The rubber precursor is isopentenyl pyrophosphate (an allylic compound), which elongates by Mg-dependent condensation by the action of rubber transferase. The monomer adds to the pyrophosphate end of the growing polymer. The process displaces the terminal high-energy pyrophosphate. The reaction produces a cis polymer. The initiation step is catalyzed by prenyltransferase, which converts three monomers of isopentenyl pyrophosphate into farnesyl pyrophosphate. The farnesyl pyrophosphate can bind to rubber transferase to elongate a new rubber polymer.\n\nThe required isopentenyl pyrophosphate is obtained from the mevalonate pathway, which derives from acetyl-CoA in the cytosol. In plants, isoprene pyrophosphate can also be obtained from the 1-deox-D-xyulose-5-phosphate/2-C-methyl-D-erythritol-4-phosphate pathway within plasmids. The relative ratio of the farnesyl pyrophosphate initiator unit and isoprenyl pyrophosphate elongation monomer determines the rate of new particle synthesis versus elongation of existing particles. Though rubber is known to be produced by only one enzyme, extracts of latex host numerous small molecular weight proteins with unknown function. The proteins possibly serve as cofactors, as the synthetic rate decreases with complete removal.\n\nClose to 28 million tons of rubber were produced in 2013, of which approximately 44% was natural. Since the bulk is synthetic, which is derived from petroleum, the price of natural rubber is determined, to a large extent, by the prevailing global price of crude oil. Asia was the main source of natural rubber, accounting for about 94% of output in 2005. The three largest producers, Thailand, Indonesia (2.4 million tons) and Malaysia, together account for around 72% of all natural rubber production. Natural rubber is not cultivated widely in its native continent of South America due to the existence of South American leaf blight, and other natural predators.\n\nRubber latex is extracted from rubber trees. The economic life period of rubber trees in plantations is around 32 years — up to 7 years of immature phase and about 25 years of productive phase.\n\nThe soil requirement is well-drained, weathered soil consisting of laterite, lateritic types, sedimentary types, nonlateritic red or alluvial soils.\n\nThe climatic conditions for optimum growth of rubber trees are:\n\nMany high-yielding clones have been developed for commercial planting. These clones yield more than 2,000 kg of dry rubber per hectare per year, under ideal conditions.\n\nIn places such as Kerala and Sri Lanka where coconuts are in abundance, the half shell of coconut was used as the latex collection container. Glazed pottery or aluminium or plastic cups became more common in Kerala and other countries. The cups are supported by a wire that encircles the tree. This wire incorporates a spring so it can stretch as the tree grows. The latex is led into the cup by a galvanised \"spout\" knocked into the bark. Tapping normally takes place early in the morning, when the internal pressure of the tree is highest. A good tapper can tap a tree every 20 seconds on a standard half-spiral system, and a common daily \"task\" size is between 450 and 650 trees. Trees are usually tapped on alternate or third days, although many variations in timing, length and number of cuts are used. \"Tappers would make a slash in the bark with a small hatchet. These slanting cuts allowed latex to flow from ducts located on the exterior or the inner layer of bark (cambium) of the tree. Since the cambium controls the growth of the tree, growth stops if it is cut. Thus, rubber tapping demanded accuracy, so that the incisions would not be too many given the size of the tree, or too deep, which could stunt its growth or kill it.\"\n\nIt is usual to tap a pannel at least twice, sometimes three times, during the tree's life. The economic life of the tree depends on how well the tapping is carried out, as the critical factor is bark consumption. A standard in Malaysia for alternate daily tapping is 25 cm (vertical) bark consumption per year. The latex-containing tubes in the bark ascend in a spiral to the right. For this reason, tapping cuts usually ascend to the left to cut more tubes.\n\nThe trees drip latex for about four hours, stopping as latex coagulates naturally on the tapping cut, thus blocking the latex tubes in the bark. Tappers usually rest and have a meal after finishing their tapping work, then start collecting the liquid \"field latex\" at about midday.\n\nThe four types of field coagula are \"cuplump\", \"treelace\", \"smallholders' lump\" and \"earth scrap\". Each has significantly different properties. Some trees continue to drip after the collection leading to a small amount of \"cup lump\" that is collected at the next tapping. The latex that coagulates on the cut is also collected as \"tree lace\". Tree lace and cup lump together account for 10–20% of the dry rubber produced. Latex that drips onto the ground, \"earth scrap\", is also collected periodically for processing of low-grade product.\n\nCup lump is the coagulated material found in the collection cup when the tapper next visits the tree to tap it again. It arises from latex clinging to the walls of the cup after the latex was last poured into the bucket, and from late-dripping latex exuded before the latex-carrying vessels of the tree become blocked. It is of higher purity and of greater value than the other three types.\n\nTree lace is the coagulum strip that the tapper peels off the previous cut before making a new cut. It usually has higher copper and manganese contents than cup lump. Both copper and manganese are pro-oxidants and can damage the physical properties of the dry rubber.\n\nSmallholders' lump is produced by smallholders who collect rubber from trees far from the nearest factory. Many Indonesian smallholders, who farm paddies in remote areas, tap dispersed trees on their way to work in the paddy fields and collect the latex (or the coagulated latex) on their way home. As it is often impossible to preserve the latex sufficiently to get it to a factory that processes latex in time for it to be used to make high quality products, and as the latex would anyway have coagulated by the time it reached the factory, the smallholder will coagulate it by any means available, in any container available. Some smallholders use small containers, buckets etc., but often the latex is coagulated in holes in the ground, which are usually lined with plastic sheeting. Acidic materials and fermented fruit juices are used to coagulate the latex — a form of assisted biological coagulation. Little care is taken to exclude twigs, leaves, and even bark from the lumps that are formed, which may also include tree lace.\n\nEarth scrap is material that gathers around the base of the tree. It arises from latex overflowing from the cut and running down the bark, from rain flooding a collection cup containing latex, and from spillage from tappers' buckets during collection. It contains soil and other contaminants, and has variable rubber content, depending on the amount of contaminants. Earth scrap is collected by field workers two or three times a year and may be cleaned in a scrap-washer to recover the rubber, or sold to a contractor who cleans it and recovers the rubber. It is of low quality.\n\nLatex coagulates in the cups if kept for long and must be collected before this happens. The collected latex, \"field latex\", is transferred into coagulation tanks for the preparation of dry rubber or transferred into air-tight containers with sieving for ammoniation. Ammoniation preserves the latex in a colloidal state for longer periods of time.\n\nLatex is generally processed into either latex concentrate for manufacture of dipped goods or coagulated under controlled, clean conditions using formic acid. The coagulated latex can then be processed into the higher-grade, technically specified block rubbers such as SVR 3L or SVR CV or used to produce Ribbed Smoke Sheet grades.\n\nNaturally coagulated rubber (cup lump) is used in the manufacture of TSR10 and TSR20 grade rubbers. Processing for these grades is a size reduction and cleaning process to remove contamination and prepare the material for the final stage of drying.\n\nThe dried material is then baled and palletized for storage and shipment.\n\nNatural rubber is often vulcanized, a process by which the rubber is heated and sulfur, peroxide or bisphenol are added to improve resistance and elasticity and to prevent it from perishing. Before World War II, carbon black was often used as an additive to rubber to improve its strength, especially in vehicle tires.\n\nNatural rubber latex is shipped from factories in south-west Asia, South America, and West and Center Africa to destinations around the world. As the cost of natural rubber has risen significantly and rubber products are dense, the shipping methods offering the lowest cost per unit weight are preferred. Depending on destination, warehouse availability, and transportation conditions, some methods are preferred by certain buyers. In international trade, latex rubber is mostly shipped in 20-foot ocean containers. Inside the container, smaller containers are used to store the latex.\n\nUncured rubber is used for cements; for adhesive, insulating, and friction tapes; and for crepe rubber used in insulating blankets and footwear. Vulcanized rubber has many more applications. Resistance to abrasion makes softer kinds of rubber valuable for the treads of vehicle tires and conveyor belts, and makes hard rubber valuable for pump housings and piping used in the handling of abrasive sludge.\n\nThe flexibility of rubber is appealing in hoses, tires and rollers for devices ranging from domestic clothes wringers to printing presses; its elasticity makes it suitable for various kinds of shock absorbers and for specialized machinery mountings designed to reduce vibration. Its relative gas impermeability makes it useful in the manufacture of articles such as air hoses, balloons, balls and cushions. The resistance of rubber to water and to the action of most fluid chemicals has led to its use in rainwear, diving gear, and chemical and medicinal tubing, and as a lining for storage tanks, processing equipment and railroad tank cars. Because of their electrical resistance, soft rubber goods are used as insulation and for protective gloves, shoes and blankets; hard rubber is used for articles such as telephone housings, parts for radio sets, meters and other electrical instruments. The coefficient of friction of rubber, which is high on dry surfaces and low on wet surfaces, leads to its use for power-transmission belting and for water-lubricated bearings in deep-well pumps. Indian rubber balls or lacrosse balls are made of rubber.\n\nAround 25 million tonnes of rubber are produced each year, of which 30 percent is natural. The remainder is synthetic rubber derived from petrochemical sources. The top end of latex production results in latex products such as surgeons' gloves, condoms, balloons and other relatively high-value products. The mid-range which comes from the technically specified natural rubber materials ends up largely in tires but also in conveyor belts, marine products, windshield wipers and miscellaneous goods. Natural rubber offers good elasticity, while synthetic materials tend to offer better resistance to environmental factors such as oils, temperature, chemicals and ultraviolet light. \"Cured rubber\" is rubber that has been compounded and subjected to the vulcanisation process to create cross-links within the rubber matrix.\n\nSome people have a serious latex allergy, and exposure to natural latex rubber products such as latex gloves can cause anaphylactic shock. The antigenic proteins found in \"Hevea\" latex may be deliberately reduced (though not eliminated) through processing.\n\nLatex from non-\"Hevea\" sources, such as Guayule, can be used without allergic reaction by persons with an allergy to \"Hevea\" latex.\n\nSome allergic reactions are not to the latex itself, but from residues of chemicals used to accelerate the cross-linking process. Although this may be confused with an allergy to latex, it is distinct from it, typically taking the form of Type IV hypersensitivity in the presence of traces of specific processing chemicals.\n\nNatural rubber is susceptible to degradation by a wide range of bacteria.\nThe bacteria \"Streptomyces coelicolor\", \"Pseudomonas citronellolis\", and \"Nocardia\" spp. are capable of degrading vulcanized natural rubber.\n\n\n"}
{"id": "38890", "url": "https://en.wikipedia.org/wiki?curid=38890", "title": "Natural science", "text": "Natural science\n\nNatural science is a branch of science concerned with the description, prediction, and understanding of natural phenomena, based on empirical evidence from observation and experimentation. Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances.\n\nNatural science can be divided into two main branches: life science (or biological science) and physical science. Physical science is subdivided into branches, including physics, chemistry, astronomy and earth science. These branches of natural science may be further divided into more specialized branches (also known as fields).\n\nIn Western society's analytic tradition, the empirical sciences and especially natural sciences use tools from formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the \"laws of nature\". The social sciences also use such methods, but rely more on qualitative research, so that they are sometimes called \"soft science\", whereas natural sciences, insofar as they emphasize quantifiable data produced, tested, and confirmed through the scientific method, are sometimes called \"hard science\".\n\nModern natural science succeeded more classical approaches to natural philosophy, usually traced to ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science. Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on. Today, \"natural history\" suggests observational descriptions aimed at popular audiences.\n\nPhilosophers of science have suggested a number of criteria, including Karl Popper's controversial falsifiability criterion, to help them differentiate scientific endeavors from non-scientific ones. Validity, accuracy, and quality control, such as peer review and repeatability of findings, are amongst the most respected criteria in the present-day global scientific community.\n\nThis field encompasses a set of disciplines that examines phenomena related to living organisms. The scale of study can range from sub-component biophysics up to complex ecologies. Biology is concerned with the characteristics, classification and behaviors of organisms, as well as how species were formed and their interactions with each other and the environment.\n\nThe biological fields of botany, zoology, and medicine date back to early periods of civilization, while microbiology was introduced in the 17th century with the invention of the microscope. However, it was not until the 19th century that biology became a unified science. Once scientists discovered commonalities between all living things, it was decided they were best studied as a whole.\n\nSome key developments in biology were the discovery of genetics; evolution through natural selection; the germ theory of disease and the application of the techniques of chemistry and physics at the level of the cell or organic molecule.\n\nModern biology is divided into subdisciplines by the type of organism and by the scale being studied. Molecular biology is the study of the fundamental chemistry of life, while cellular biology is the examination of the cell; the basic building block of all life. At a higher level, anatomy and physiology looks at the internal structures, and their functions, of an organism, while ecology looks at how various organisms interrelate.\n\nConstituting the scientific study of matter at the atomic and molecular scale, chemistry deals primarily with collections of atoms, such as gases, molecules, crystals, and metals. The composition, statistical properties, transformations and reactions of these materials are studied. Chemistry also involves understanding the properties and interactions of individual atoms and molecules for use in larger-scale applications.\n\nMost chemical processes can be studied directly in a laboratory, using a series of (often well-tested) techniques for manipulating materials, as well as an understanding of the underlying processes. Chemistry is often called \"the central science\" because of its role in connecting the other natural sciences.\n\nEarly experiments in chemistry had their roots in the system of Alchemy, a set of beliefs combining mysticism with physical experiments. The science of chemistry began to develop with the work of Robert Boyle, the discoverer of gas, and Antoine Lavoisier, who developed the theory of the Conservation of mass.\n\nThe discovery of the chemical elements and atomic theory began to systematize this science, and researchers developed a fundamental understanding of states of matter, ions, chemical bonds and chemical reactions. The success of this science led to a complementary chemical industry that now plays a significant role in the world economy.\n\nPhysics embodies the study of the fundamental constituents of the universe, the forces and interactions they exert on one another, and the results produced by these interactions. In general, physics is regarded as the fundamental science, because all other natural sciences use and obey the principles and laws set down by the field. Physics relies heavily on mathematics as the logical framework for formulation and quantification of principles.\n\nThe study of the principles of the universe has a long history and largely derives from direct observation and experimentation. The formulation of theories about the governing laws of the universe has been central to the study of physics from very early on, with philosophy gradually yielding to systematic, quantitative experimental testing and observation as the source of verification. Key historical developments in physics include Isaac Newton's theory of universal gravitation and classical mechanics, an understanding of electricity and its relation to magnetism, Einstein's theories of special and general relativity, the development of thermodynamics, and the quantum mechanical model of atomic and subatomic physics.\n\nThe field of physics is extremely broad, and can include such diverse studies as quantum mechanics and theoretical physics, applied physics and optics. Modern physics is becoming increasingly specialized, where researchers tend to focus on a particular area rather than being \"universalists\" like Isaac Newton, Albert Einstein and Lev Landau, who worked in multiple areas.\n\nThis discipline is the science of celestial objects and phenomena that originate outside the Earth's atmosphere. It is concerned with the evolution, physics, chemistry, meteorology, and motion of celestial objects, as well as the formation and development of the universe.\n\nAstronomy includes the examination, study and modeling of stars, planets, comets, galaxies and the cosmos. Most of the information used by astronomers is gathered by remote observation, although some laboratory reproduction of celestial phenomena has been performed (such as the molecular chemistry of the interstellar medium).\n\nWhile the origins of the study of celestial features and phenomena can be traced back to antiquity, the scientific methodology of this field began to develop in the middle of the 17th century. A key factor was Galileo's introduction of the telescope to examine the night sky in more detail.\n\nThe mathematical treatment of astronomy began with Newton's development of celestial mechanics and the laws of gravitation, although it was triggered by earlier work of astronomers such as Kepler. By the 19th century, astronomy had developed into a formal science, with the introduction of instruments such as the spectroscope and photography, along with much-improved telescopes and the creation of professional observatories.\n\nEarth science (also known as geoscience), is an all-embracing term for the sciences related to the planet Earth, including geology, geophysics, hydrology, meteorology, physical geography, oceanography, and soil science.\n\nAlthough mining and precious stones have been human interests throughout the history of civilization, the development of the related sciences of economic geology and mineralogy did not occur until the 18th century. The study of the earth, particularly palaeontology, blossomed in the 19th century. The growth of other disciplines, such as geophysics, in the 20th century led to the development of the theory of plate tectonics in the 1960s, which has had a similar effect on the Earth sciences as the theory of evolution had on biology. Earth sciences today are closely linked to petroleum and mineral resources, climate research and to environmental assessment and remediation.\n\nThough sometimes considered in conjunction with the earth sciences, due to the independent development of its concepts, techniques and practices and also the fact of it having a wide range of sub disciplines under its wing, the atmospheric sciences is also considered a separate branch of natural science. This field studies the characteristics of different layers of the atmosphere from ground level to the edge of the time. The timescale of study also varies from days to centuries. Sometimes the field also includes the study of climatic patterns on planets other than earth.\n\nThe serious study of oceans began in the early to mid-20th century. As a field of natural science, it is relatively young but stand-alone programs offer specializations in the subject. Though some controversies remain as to the categorization of the field under earth sciences, interdisciplinary sciences or as a separate field in its own right, most modern workers in the field agree that it has matured to a state that it has its own paradigms and practices. As such a big family of related studies spanning every aspect of the oceans is now classified under this field.\n\nThe distinctions between the natural science disciplines are not always sharp, and they share a number of cross-discipline fields. Physics plays a significant role in the other natural sciences, as represented by astrophysics, geophysics, chemical physics and biophysics. Likewise chemistry is represented by such fields as biochemistry, chemical biology, geochemistry and astrochemistry.\n\nA particular example of a scientific discipline that draws upon multiple natural sciences is environmental science. This field studies the interactions of physical, chemical, geological, and biological components of the environment, with a particular regard to the effect of human activities and the impact on biodiversity and sustainability. This science also draws upon expertise from other fields such as economics, law and social sciences.\n\nA comparable discipline is oceanography, as it draws upon a similar breadth of scientific disciplines. Oceanography is sub-categorized into more specialized cross-disciplines, such as physical oceanography and marine biology. As the marine ecosystem is very large and diverse, marine biology is further divided into many subfields, including specializations in particular species.\n\nThere are also a subset of cross-disciplinary fields which, by the nature of the problems that they address, have strong currents that run counter to\nspecialization. Put another way: In some fields of integrative application, specialists in more than one field are a key part of most dialog. Such integrative fields, for example, include nanoscience, astrobiology, and complex system informatics.\n\nMaterials science is a relatively new, interdisciplinary field which deals with the study of matter and its properties; as well as the discovery and design of new materials. Originally developed through the field of metallurgy, the study of the properties of materials and solids has now expanded into all materials. The field covers the chemistry, physics and engineering applications of materials including metals, ceramics, artificial polymers, and many others. The core of the field deals with relating structure of material with it properties.\n\nIt is at the forefront of research in science and engineering. It is an important part of forensic engineering (the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property) and failure analysis, the latter being the key to understanding, for example, the cause of various aviation accidents. Many of the most pressing scientific problems that are faced today are due to the limitations of the materials that are available and, as a result, breakthroughs in this field are likely to have a significant impact on the future of technology.\n\nThe basis of materials science involves studying the structure of materials, and relating them to their properties. Once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a certain application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure, and thus its properties.\n\nSome scholars trace the origins of natural science as far back as pre-literate human societies, where understanding the natural world was necessary for survival. People observed and built up knowledge about the behavior of animals and the usefulness of plants as food and medicine, which was passed down from generation to generation. These primitive understandings gave way to more formalized inquiry around 3500 to 3000 BC in the Mesopotamian and Ancient Egyptian cultures, which produced the first known written evidence of natural philosophy, the precursor of natural science. While the writings show an interest in astronomy, mathematics and other aspects of the physical world, the ultimate aim of inquiry about nature's workings was in all cases religious or mythological, not scientific.\n\nA tradition of scientific inquiry also emerged in Ancient China, where Taoist alchemists and philosophers experimented with elixirs to extend life and cure ailments. They focused on the yin and yang, or contrasting elements in nature; the yin was associated with femininity and coldness, while yang was associated with masculinity and warmth. The five phases – fire, earth, metal, wood and water – described a cycle of transformations in nature. Water turned into wood, which turned into fire when it burned. The ashes left by fire were earth. Using these principles, Chinese philosophers and doctors explored human anatomy, characterizing organs as predominantly yin or yang and understood the relationship between the pulse, the heart and the flow of blood in the body centuries before it became accepted in the West.\n\nLittle evidence survives of how Ancient Indian cultures around the Indus River understood nature, but some of their perspectives may be reflected in the Vedas, a set of sacred Hindu texts. They reveal a conception of the universe as ever-expanding and constantly being recycled and reformed. Surgeons in the Ayurvedic tradition saw health and illness as a combination of three humors: wind, bile and phlegm. A healthy life was the result of a balance among these humors. In Ayurvedic thought, the body consisted of five elements: earth, water, fire, wind and empty space. Ayurvedic surgeons performed complex surgeries and developed a detailed understanding of human anatomy.\n\nPre-Socratic philosophers in Ancient Greek culture brought natural philosophy a step closer to direct inquiry about cause and effect in nature between 600 and 400 BC, although an element of magic and mythology remained. Natural phenomena such as earthquakes and eclipses were explained increasingly in the context of nature itself instead of being attributed to angry gods. Thales of Miletus, an early philosopher who lived from 625 to 546 BC, explained earthquakes by theorizing that the world floated on water and that water was the fundamental element in nature. In the 5th century BC, Leucippus was an early exponent of atomism, the idea that the world is made up of fundamental indivisible particles. Pythagoras applied Greek innovations in mathematics to astronomy, and suggested that the earth was spherical.\n\nLater Socratic and Platonic thought focused on ethics, morals and art and did not attempt an investigation of the physical world; Plato criticized pre-Socratic thinkers as materialists and anti-religionists. Aristotle, however, a student of Plato who lived from 384 to 322 BC, paid closer attention to the natural world in his philosophy. In his \"History of Animals\", he described the inner workings of 110 species, including the stingray, catfish and bee. He investigated chick embryos by breaking open eggs and observing them at various stages of development. Aristotle's works were influential through the 16th century, and he is considered to be the father of biology for his pioneering work in that science. He also presented philosophies about physics, nature and astronomy using inductive reasoning in his works \"Physics\" and \"Meteorology\".\n\nWhile Aristotle considered natural philosophy more seriously than his predecessors, he approached it as a theoretical branch of science. Still, inspired by his work, Ancient Roman philosophers of the early 1st century AD, including Lucretius, Seneca and Pliny the Elder, wrote treatises that dealt with the rules of the natural world in varying degrees of depth. Many Ancient Roman Neoplatonists of the 3rd to the 6th centuries also adapted Aristotle's teachings on the physical world to a philosophy that emphasized spiritualism. Early medieval philosophers including Macrobius, Calcidius and Martianus Capella also examined the physical world, largely from a cosmological and cosmographical perspective, putting forth theories on the arrangement of celestial bodies and the heavens, which were posited as being composed of aether.\n\nAristotle's works on natural philosophy continued to be translated and studied amid the rise of the Byzantine Empire and Abbasid Caliphate. \n\nIn the Byzantine Empire John Philoponus, an Alexandrian Aristotelian commentator and Christian theologian, was the first who questioned Aristotle's teaching of physics. Unlike Aristotle who based his physics on verbal argument, Philoponus instead relied on observation, and argued for observation rather than resorting into verbal argument. He introduced the theory of impetus. John Philoponus' criticism of Aristotelian principles of physics served as inspiration for Galileo Galilei during the Scientific Revolution.\n\nA revival in mathematics and science took place during the time of the Abbasid Caliphate from the 9th century onward, when Muslim scholars expanded upon Greek and Indian natural philosophy. The words \"alcohol\", \"algebra\" and \"zenith\" all have Arabic roots.\n\nAristotle's works and other Greek natural philosophy did not reach the West until about the middle of the 12th century, when works were translated from Greek and Arabic into Latin. The development of European civilization later in the Middle Ages brought with it further advances in natural philosophy. European inventions such as the horseshoe, horse collar and crop rotation allowed for rapid population growth, eventually giving way to urbanization and the foundation of schools connected to monasteries and cathedrals in modern-day France and England. Aided by the schools, an approach to Christian theology developed that sought to answer questions about nature and other subjects using logic. This approach, however, was seen by some detractors as heresy. By the 12th century, Western European scholars and philosophers came into contact with a body of knowledge of which they had previously been ignorant: a large corpus of works in Greek and Arabic that were preserved by Islamic scholars. Through translation into Latin, Western Europe was introduced to Aristotle and his natural philosophy. These works were taught at new universities in Paris and Oxford by the early 13th century, although the practice was frowned upon by the Catholic church. A 1210 decree from the Synod of Paris ordered that \"no lectures are to be held in Paris either publicly or privately using Aristotle's books on natural philosophy or the commentaries, and we forbid all this under pain of excommunication.\"\n\nIn the late Middle Ages, Spanish philosopher Dominicus Gundissalinus translated a treatise by the earlier Persian scholar Al-Farabi called \"On the Sciences\" into Latin, calling the study of the mechanics of nature \"scientia naturalis\", or natural science. Gundissalinus also proposed his own classification of the natural sciences in his 1150 work \"On the Division of Philosophy\". This was the first detailed classification of the sciences based on Greek and Arab philosophy to reach Western Europe. Gundissalinus defined natural science as \"the science considering only things unabstracted and with motion,\" as opposed to mathematics and sciences that rely on mathematics. Following Al-Farabi, he then separated the sciences into eight parts, including physics, cosmology, meteorology, minerals science and plant and animal science.\n\nLater philosophers made their own classifications of the natural sciences. Robert Kilwardby wrote \"On the Order of the Sciences\" in the 13th century that classed medicine as a mechanical science, along with agriculture, hunting and theater while defining natural science as the science that deals with bodies in motion. Roger Bacon, an English friar and philosopher, wrote that natural science dealt with \"a principle of motion and rest, as in the parts of the elements of fire, air, earth and water, and in all inanimate things made from them.\" These sciences also covered plants, animals and celestial bodies. Later in the 13th century, Catholic priest and theologian Thomas Aquinas defined natural science as dealing with \"mobile beings\" and \"things which depend on matter not only for their existence, but also for their definition.\" There was wide agreement among scholars in medieval times that natural science was about bodies in motion, although there was division about the inclusion of fields including medicine, music and perspective. Philosophers pondered questions including the existence of a vacuum, whether motion could produce heat, the colors of rainbows, the motion of the earth, whether elemental chemicals exist and where in the atmosphere rain is formed.\n\nIn the centuries up through the end of the Middle Ages, natural science was often mingled with philosophies about magic and the occult. Natural philosophy appeared in a wide range of forms, from treatises to encyclopedias to commentaries on Aristotle. The interaction between natural philosophy and Christianity was complex during this period; some early theologians, including Tatian and Eusebius, considered natural philosophy an outcropping of pagan Greek science and were suspicious of it. Although some later Christian philosophers, including Aquinas, came to see natural science as a means of interpreting scripture, this suspicion persisted until the 12th and 13th centuries. The Condemnation of 1277, which forbade setting philosophy on a level equal with theology and the debate of religious constructs in a scientific context, showed the persistence with which Catholic leaders resisted the development of natural philosophy even from a theological perspective. Aquinas and Albertus Magnus, another Catholic theologian of the era, sought to distance theology from science in their works. \"I don't see what one's interpretation of Aristotle has to do with the teaching of the faith,\" he wrote in 1271.\n\nBy the 16th and 17th centuries, natural philosophy underwent an evolution beyond commentary on Aristotle as more early Greek philosophy was uncovered and translated. The invention of the printing press in the 15th century, the invention of the microscope and telescope, and the Protestant Reformation fundamentally altered the social context in which scientific inquiry evolved in the West. Christopher Columbus's discovery of a new world changed perceptions about the physical makeup of the world, while observations by Copernicus, Tyco Brahe and Galileo brought a more accurate picture of the solar system as heliocentric and proved many of Aristotle's theories about the heavenly bodies false. A number of 17th-century philosophers, including Thomas Hobbes, John Locke and Francis Bacon made a break from the past by rejecting Aristotle and his medieval followers outright, calling their approach to natural philosophy as superficial.\n\nThe titles of Galileo's work \"Two New Sciences\" and Johannes Kepler's \"New Astronomy\" underscored the atmosphere of change that took hold in the 17th century as Aristotle was dismissed in favor of novel methods of inquiry into the natural world. Bacon was instrumental in popularizing this change; he argued that people should use the arts and sciences to gain dominion over nature. To achieve this, he wrote that \"human life [must] be endowed with new discoveries and powers.\" He defined natural philosophy as \"the knowledge of Causes and secret motions of things; and enlarging the bounds of Human Empire, to the effecting of all things possible.\" Bacon proposed scientific inquiry supported by the state and fed by the collaborative research of scientists, a vision that was unprecedented in its scope, ambition and form at the time. Natural philosophers came to view nature increasingly as a mechanism that could be taken apart and understood, much like a complex clock. Natural philosophers including Isaac Newton, Evangelista Torricelli and Francesco Redi conducted experiments focusing on the flow of water, measuring atmospheric pressure using a barometer and disproving spontaneous generation. Scientific societies and scientific journals emerged and were spread widely through the printing press, touching off the scientific revolution. Newton in 1687 published his \"The Mathematical Principles of Natural Philosophy\", or \"Principia Mathematica\", which set the groundwork for physical laws that remained current until the 19th century.\n\nSome modern scholars, including Andrew Cunningham, Perry Williams and Floris Cohen, argue that natural philosophy is not properly called a science, and that genuine scientific inquiry began only with the scientific revolution. According to Cohen, \"the emancipation of science from an overarching entity called 'natural philosophy' is one defining characteristic of the Scientific Revolution.\" Other historians of science, including Edward Grant, contend that the scientific revolution that blossomed in the 17th, 18th and 19th centuries occurred when principles learned in the exact sciences of optics, mechanics and astronomy began to be applied to questions raised by natural philosophy. Grant argues that Newton attempted to expose the mathematical basis of nature – the immutable rules it obeyed – and in doing so joined natural philosophy and mathematics for the first time, producing an early work of modern physics.\nThe scientific revolution, which began to take hold in the 17th century, represented a sharp break from Aristotelian modes of inquiry. One of its principal advances was the use of the scientific method to investigate nature. Data was collected and repeatable measurements made in experiments. Scientists then formed hypotheses to explain the results of these experiments. The hypothesis was then tested using the principle of falsifiability to prove or disprove its accuracy. The natural sciences continued to be called natural philosophy, but the adoption of the scientific method took science beyond the realm of philosophical conjecture and introduced a more structured way of examining nature.\n\nNewton, an English mathematician and physicist, was the seminal figure in the scientific revolution. Drawing on advances made in astronomy by Copernicus, Brahe and Kepler, Newton derived the universal law of gravitation and laws of motion. These laws applied both on earth and in outer space, uniting two spheres of the physical world previously thought to function independently of each other, according to separate physical rules. Newton, for example, showed that the tides were caused by the gravitational pull of the moon. Another of Newton's advances was to make mathematics a powerful explanatory tool for natural phenomena. While natural philosophers had long used mathematics as a means of measurement and analysis, its principles were not used as a means of understanding cause and effect in nature until Newton.\n\nIn the 18th century and 19th century, scientists including Charles-Augustin de Coulomb, Alessandro Volta, and Michael Faraday built upon Newtonian mechanics by exploring electromagnetism, or the interplay of forces with positive and negative charges on electrically charged particles. Faraday proposed that forces in nature operated in \"fields\" that filled space. The idea of fields contrasted with the Newtonian construct of gravitation as simply \"action at a distance\", or the attraction of objects with nothing in the space between them to intervene. James Clerk Maxwell in the 19th century unified these discoveries in a coherent theory of electrodynamics. Using mathematical equations and experimentation, Maxwell discovered that space was filled with charged particles that could act upon themselves and each other, and that they were a medium for the transmission of charged waves.\n\nSignificant advances in chemistry also took place during the scientific revolution. Antoine Lavoisier, a French chemist, refuted the phlogiston theory, which posited that things burned by releasing \"phlogiston\" into the air. Joseph Priestley had discovered oxygen in the 18th century, but Lavoisier discovered that combustion was the result of oxidation. He also constructed a table of 33 elements and invented modern chemical nomenclature. Formal biological science remained in its infancy in the 18th century, when the focus lay upon the classification and categorization of natural life. This growth in natural history was led by Carl Linnaeus, whose 1735 taxonomy of the natural world is still in use. Linnaeus in the 1750s introduced scientific names for all his species.\n\nBy the 19th century, the study of science had come into the purview of professionals and institutions. In so doing, it gradually acquired the more modern name of \"natural science.\" The term \"scientist\" was coined by William Whewell in an 1834 review of Mary Somerville's \"On the Connexion of the Sciences\". But the word did not enter general use until nearly the end of the same century.\n\nAccording to a famous 1923 textbook \"Thermodynamics and the Free Energy of Chemical Substances\" by the American chemist Gilbert N. Lewis and the American physical chemist Merle Randall, the natural sciences contain three great branches:\n\nAside from the logical and mathematical sciences, there are three great branches of \"natural science\" which stand apart by reason of the variety of far reaching deductions drawn from a small number of primary postulates — they are mechanics, electrodynamics, and thermodynamics.\n\nToday, natural sciences are more commonly divided into life sciences, such as botany and zoology; and physical sciences, which include physics, chemistry, geology, astronomy and materials science.\n\n\n\n"}
{"id": "52634071", "url": "https://en.wikipedia.org/wiki?curid=52634071", "title": "Nature-based solutions", "text": "Nature-based solutions\n\nNature-based solutions (NBS or NbS) refers to the sustainable management and use of nature for tackling socio-environmental challenges. The challenges include issues such as climate change, water security, water pollution, food security, human health, and disaster risk management. \n\nA definition by the European Union states that these solutions are \"inspired and supported by nature, which are cost-effective, simultaneously provide environmental, social and economic benefits and help build resilience. The Nature-based Solutions Initiative meanwhile defines them as \"actions that work with and enhance nature so as to help people adapt to change and disasters\". Such solutions bring more, and more diverse, nature and natural features and processes into cities, landscapes and seascapes, through locally adapted, resource-efficient and systemic interventions\". With NBS, healthy, resilient and diverse ecosystems (whether natural, managed or newly created) can provide solutions for the benefit of societies and overall biodiversity.\n\nFor instance, the restoration or protection of mangroves along coastlines utilizes a nature-based solution to accomplish several things. Mangroves moderate the impact of waves and wind on coastal settlements or cities and sequester CO . They also provide safe nurseries for marine life that can be the basis for sustaining populations of fish that local populations may depend on. Additionally, the mangrove forests can help control coastal erosion resulting from sea level rise. Similarly, in cities green roofs or walls are nature-based solutions that can be used to moderate the impact of high temperatures, capture storm water, abate pollution, and act as carbon sinks, while enhancing biodiversity.\n\nConservation approaches and environment management initiatives have been carried out for decades. What is new is that the benefits of such nature-based solutions to human well-being have been articulated well more recently. Even if the term itself is still being framed, examples of nature-based solutions can be found all over the world, and imitated. Nature-based solutions are on their way to being mainstreamed in national and international policies and programmes (e.g. climate change policy, law, infrastructure investment and financing mechanisms). For example, the theme for World Water Day 2018 was \"Nature for water\" and by UN-Water's accompanying UN World Water Development Report had the title \"Nature-based Solutions for Water\".\n\nSocieties increasingly face challenges such as climate change, urbanization, jeopardized food security and water resource provision, and disaster risk. One approach to answer these challenges is to singularly rely on technological strategies. An alternative approach is to manage the (socio-)ecological systems in a comprehensive way in order to sustain and potentially increase the delivery of ecosystem services to humans. In this context, nature-based solutions (NBS) have recently been put forward by practitioners and quickly thereafter by policymakers. These solutions stress the sustainable use of nature in solving coupled environmental-social-economic challenges. \n\nWhile ecosystem services are often valued in terms of immediate benefits to human well-being and economy, NBS focus on the benefits to people and the environment itself, to allow for sustainable solutions that are able to respond to environmental change and hazards in the long-term. NBS go beyond the traditional biodiversity conservation and management principles by \"re-focusing\" the debate on humans and specifically integrating societal factors such as human well-being and poverty reduction, socio-economic development, and governance principles. \n\nWith respect to water issues, NBS can achieve the following, according to the World Water Development Report 2018 by UN-Water: \n\nIn this sense, NBS are strongly connected to ideas such as natural systems agriculture, natural solutions, ecosystem-based approaches, adaptation services, natural infrastructure, green infrastructure and ecological engineering. For instance, ecosystem-based approaches are increasingly promoted for climate change adaptation and mitigation by organisations like United Nations Environment Programme and non-governmental organisations such as The Nature Conservancy. These organisations refer to \"policies and measures that take into account the role of ecosystem services in reducing the vulnerability of society to climate change, in a multi-sectoral and multi-scale approach\".\n\nLikewise, natural infrastructure is defined as a \"strategically planned and managed network of natural lands, such as forests and wetlands, working landscapes, and other open spaces that conserves or enhances ecosystem values and functions and provides associated benefits to human populations\"; and green infrastructure refers to an \"interconnected network of green spaces that conserves natural systems and provides assorted benefits to human populations\".\n\nSimilarly, the concept of ecological engineering generally refers to \"protecting, restoring (i.e. ecosystem restoration) or modifying ecological systems to increase the quantity, quality and sustainability of particular services they provide, or to build new ecological systems that provide services that would otherwise be provided through more conventional engineering, based on non-renewable resources\".\n\nThe International Union for the Conservation of Nature (IUCN) defines NBS as actions to protect, sustainably manage, and restore natural or modified ecosystems, that address societal challenges effectively and adaptively, simultaneously providing human well-being and biodiversity benefits, with climate change, food security, disaster risks, water security, social and economic development as well as human health being the common societal challenges.\n\nIUCN proposes to consider NBS as an umbrella concept. Categories and examples of NBS approaches according to IUCN include:\n\nThe general objective of NBS is clear, namely the sustainable management and use of nature for tackling societal challenges. However, different stakeholders view NBS from other perspectives. For instance, IUCN defines NBS as \"actions to protect, sustainably manage and restore natural or modified ecosystems, which address societal challenges effectively and adaptively, while simultaneously providing human well-being and biodiversity benefits\". This framing puts the need for well-managed and restored ecosystems at the heart of NBS, with the overarching goal of \"Supporting the achievement of society's development goals and safeguard human well-being in ways that reflect cultural and societal values and enhance the resilience of ecosystems, their capacity for renewal and the provision of services\". \n\nIn the context of the ongoing political debate on jobs and growth (main drivers of the current EU policy agenda), the European Commission underlines that NBS can transform environmental and societal challenges into innovation opportunities, by turning natural capital into a source for green growth and sustainable development. In their view, NBS to societal challenges are \"solutions that are inspired and supported by nature, which are cost-effective, simultaneously provide environmental, social and economic benefits and help build resilience. Such solutions bring more, and more diverse, nature and natural features and processes into cities, landscapes and seascapes, through locally adapted, resource-efficient and systemic interventions.\" \n\nThis framing is somewhat broader, and puts economy and social assets at the heart of NBS as importantly as sustaining environmental conditions. It shares similarities with the definition proposed by Maes and Jacobs (2015) defining NBS as \"any transition to a use of ES with decreased input of non-renewable natural capital and increased investment in renewable natural processes\". In their view, development and evaluation of NBS spans three basic requirements: (1) decrease of fossil fuel input per produced unit; (2) lowering of systemic trade-offs and increasing synergies between ES; and (3) increasing labor input and jobs. Here, nature is seen as a tool to inspire more systemic solutions to societal problems.\n\nWhatever definition used, promoting sustainability and the increased role of natural, self-sustained processes relying on biodiversity, are inherent to NBS. They constitute actions easily seen as positive for a wide range of stakeholders, as they bring about benefits at environmental, economic and social levels. As a consequence, the concept of NBS is gaining acceptance outside the conservation community (e.g. urban planning) and is now on its way to be mainstreamed into policies and programmes (climate change policy, law, infrastructure investment and financing mechanisms).\n\nIn 2015, the European network BiodivERsA mobilized a range of scientists, research donors and stakeholders and proposed a typology characterizing NBS along two gradients. 1. \"how much engineering of biodiversity and ecosystems is involved in NBS\", and 2. \"how many ecosystem services and stakeholder groups are targeted by a given NBS\". The typology highlights that NBS can involve very different actions on ecosystems (from protection to management and even creation of new ecosystems) and is based on the assumption that the higher the number of services and stakeholder groups targeted, the lower the capacity to maximize the delivery of each service and simultaneously fulfil the specific needs of all stakeholder groups. As such, three types of NBS are distinguished (Figure 2):\n\nType 1 NBS consists of no or minimal intervention in ecosystems, with the objectives of maintaining or improving the delivery of a range of ES both inside and outside of these conserved ecosystems. Examples include the protection of mangroves in coastal areas to limit risks associated to extreme weather conditions and provide benefits and opportunities to local populations; and the establishment of marine protected areas to conserve biodiversity within these areas while exporting biomass into fishing grounds. This type of NBS is connected to, for example, the concept of biosphere reserves which incorporates core protected areas for nature conservation and buffer zones and transition areas where people live and work in a sustainable way.\n\nType 2 NBS corresponds to management approaches that develop sustainable and multifunctional ecosystems and landscapes (extensively or intensively managed). These types improve the delivery of selected ES compared to what would be obtained with a more conventional intervention. Examples include innovative planning of agricultural landscapes to increase their multi-functionality; and approaches for enhancing tree species and genetic diversity to increase forest resilience to extreme events. This type of NBS is strongly connected to concepts like natural systems agriculture, agro-ecology, and evolutionary-orientated forestry.\n\nType 3 NBS consists of managing ecosystems in very extensive ways or even creating new ecosystems (e.g., artificial ecosystems with new assemblages of organisms for green roofs and walls to mitigate city warming and clean polluted air). Type 3 is linked to concepts like green and blue infrastructures and objectives like restoration of heavily degraded or polluted areas and greening cities.\n\nType 1 and 2 would typically fall within the IUCN NBS framework, whereas Type 2 and moreover Type 3 are often exemplified by EC for turning natural capital into a source for green growth and sustainable development.\n\nHybrid solutions exist along this gradient both in space and time. For instance, at landscape scale, mixing protected and managed areas could be needed to fulfil multi-functionality and sustainability goals. Similarly, a constructed wetland can be developed as a type 3 but, when well established, may subsequently be preserved and surveyed as a type 1.\n\nDemonstrating the benefits of nature and healthy ecosystems and showcasing the return on investment they can offer is necessary in order to increase awareness, but also to provide support and guidance on how to implement NBS. A large number of initiatives around the world already highlight the effectiveness of NBS approaches to address a wide range of societal challenges.\n\nThe following table shows examples from around the world:\n\nIn 2018, The Hindu reported that the East Kolkata wetlands, the world's largest organic sewage treatment facility had been used to clean the sewage of Kolkata in an organic manner by using algae for several decades. In use since the 1930s, the natural system was discovered by Dhrubajyoti Ghosh, an ecologist and a municipal engineer in the 1970s while working in the region. Ghosh worked for decades to protect the wetlands. It had been a practice in Kolkata, one of the five largest cities in India, for the municipal authorities to pump sewage into shallow ponds (\"bheris\"). Under the heat of the tropical sun, algae proliferated in them, converting the sewage into clean water, which in turn was used by villagers to grow paddy and vegetables. This system has been in use in the region since the 1930s and treats 750 million litres of wastewater per day, giving livelihood to 100,000 people in the vicinity. For his work, Ghosh was included in the UN Global 500 Roll of Honour in 1990 and received the Luc Hoffmann award in 2016.\n\nThere is currently no accepted basis on which a government agency, municipality or private company can systematically assess the efficiency, effectiveness and sustainability of a particular nature-based solution. However, a series of principles are proposed to guide effective and appropriate implementation, and thus to upscale NBS in practice. For example, NBS embrace and are not meant to replace nature conservation norms. Also, NBS are determined by site-specific natural and cultural contexts that include traditional, local and scientific knowledge. NBS are an integral part of the overall design of policies, and measure or actions, to address a specific challenges. Finally, NBS can be implemented alone or in an integrated manner with other solutions to societal challenges (e.g. technological and engineering solutions) and they are applied at the landscape scale.\n\nImplementing NBS requires political, economic, and scientific challenges to be tackled. First and foremost, private sector investment is needed, not to replace but to supplement traditional sources of capital such as public funding or philanthropy. The challenge is therefore to provide a robust evidence base for the contribution of nature to economic growth and jobs, and to demonstrate the economic viability of these solutions – compared to technological ones – on a timescale compatible with that of global change. Furthermore, it requires measures like adaptation of economic subsidy schemes, and the creation of opportunities for conservation finance, to name a few. Indeed, such measures will be needed to scale up NBS interventions, and strengthen their impact in mitigating the world's most pressing challenges.\n\nSince 2016, the EU is supporting a multi-stakeholder dialogue platform (called ThinkNature) to promote the co-design, testing and deployment of improved and innovative NBS in an integrated way. Creation of such science-policy-business-society interfaces could promote the market uptake of NBS. The project is part of the EU’s Horizon 2020 – Research and Innovation programme, and will last for 3 years. There are a total of 17 international partners involved, including the Technical University of Crete (Project Leader), the University of Helsinki and BiodivERsA.\n\nIn 2017, as part of the Presidency of the Estonian Republic of the Council of the European Union, a conference called “Nature-based Solutions: From Innovation to Common-use” was organized by the Ministry of the Environment of Estonia and the University of Tallinn. This conference aimed to strengthen synergies among various recent initiatives and programs related to NBS launched by the European Commission and by the EU Member States, focusing on policy and governance of NBS, and on research and innovation.\n\nIn recognition of the importance of natural ecosystems for mitigation and adaptation, the Paris Agreement calls on all Parties to acknowledge “the importance of the conservation and enhancement, as appropriate, of sinks and reservoirs of the greenhouse gases” and to “note the importance of ensuring the integrity of all ecosystems, including oceans, and the protection of biodiversity, recognized by some cultures as Mother Earth”. It then includes in its Articles several references to nature-based solutions. For example, Article 5.2 encourages Parties to adopt “…policy approaches and positive incentives for activities relating to reducing emissions from deforestation and forest degradation, and the role of conservation and sustainable management of forests and enhancement of forest carbon stocks in developing countries; and alternative policy approaches, such as joint mitigation and adaptation approaches for the integral and sustainable management of forests, while reaffirming the importance of incentivizing, as appropriate, non-carbon benefits associated with such approaches”. Article 7.1 further encourages Parties to build the resilience of socioeconomic and ecological systems, including through economic diversification and sustainable management of natural resources. In total, the Agreement refers to nature (ecosystems, natural resources, forests) in 13 distinct places. An in-depth analysis of all Nationally Determined Contributions submitted to UNFCCC, revealed that around 130 NDCs or 65% of signatories commit to nature-based solutions in their climate pledges, suggesting broad consensus for the role of nature in helping meet climate change goals. However, high-level commitments rarely translate into robust, measurable actions on-the-ground.\n\nThe term NBS was put forward by practitioners in the late 2000s (in particular the International Union for the Conservation of Nature and the World Bank) and thereafter by policymakers in Europe (most notably the European Commission). \n\nThe term \"nature-based solutions\" was first used in the late 2000s. It was used in the context of finding new solutions to mitigate and adapt to climate change effects, whilst simultaneously protecting biodiversity and improving sustainable livelihoods. \n\nThe IUCN referred to NBS in a position paper for the United Nations Framework Convention on Climate Change. The term was also adopted by European policymakers, in particular by the European Commission in a report stressing that NBS can offer innovative means to create jobs and growth as part of a green economy. The term started to make appearances in the mainstream media around the time of the Global Climate Action Summit in California in September 2018 \n\n\n"}
{"id": "43427", "url": "https://en.wikipedia.org/wiki?curid=43427", "title": "Nature (journal)", "text": "Nature (journal)\n\nNature is a British multidisciplinary scientific journal, first published on 4 November 1869. It is one of the most recognizable scientific journals in the world, and was ranked the world's most cited scientific journal by the Science Edition of the 2010 \"Journal Citation Reports\" and is ascribed an impact factor of 40.137, making it one of the world's top academic journals. It is one of the few remaining academic journals that publishes original research across a wide range of scientific fields.\n\nResearch scientists are the primary audience for the journal, but summaries and accompanying articles are intended to make many of the most important papers understandable to scientists in other fields and the educated public. Towards the front of each issue are editorials, news and feature articles on issues of general interest to scientists, including current affairs, science funding, business, scientific ethics and research breakthroughs. There are also sections on books, arts, and short science fiction stories. The remainder of the journal consists mostly of research papers (articles or letters), which are often dense and highly technical. Because of strict limits on the length of papers, often the printed text is actually a summary of the work in question with many details relegated to accompanying \"supplementary material\" on the journal's website.\n\nThere are many fields of research in which important new advances and original research are published as either articles or letters in \"Nature.\" The papers that have been published in this journal are internationally acclaimed for maintaining high research standards. Fewer than 8% of submitted papers are accepted for publication.\n\nIn 2007 \"Nature\" (together with \"Science\") received the Prince of Asturias Award for Communications and Humanity.\n\nThe enormous progress in science and mathematics during the 19th century was recorded in journals written mostly in German or French, as well as in English. Britain underwent enormous technological and industrial changes and advances particularly in the latter half of the 19th century. In English the most respected scientific journals of this time were the refereed journals of the Royal Society, which had published many of the great works from Isaac Newton, Michael Faraday through to early works from Charles Darwin. In addition, during this period, the number of popular science periodicals doubled from the 1850s to the 1860s. According to the editors of these popular science magazines, the publications were designed to serve as \"organs of science\", in essence, a means of connecting the public to the scientific world.\n\n\"Nature\", first created in 1869, was not the first magazine of its kind in Britain. One journal to precede \"Nature\" was \"\", which, created in 1859, began as a natural history magazine and progressed to include more physical observational science and technical subjects and less natural history. The journal's name changed from its original title to \"Intellectual Observer: A Review of Natural History, Microscopic Research, and Recreative Science\" and then later to the \"Student and Intellectual Observer of Science, Literature, and Art\". While \"Recreative Science\" had attempted to include more physical sciences such as astronomy and archaeology, the \"Intellectual Observer\" broadened itself further to include literature and art as well. Similar to \"Recreative Science\" was the scientific journal \"Popular Science Review\", created in 1862, which covered different fields of science by creating subsections titled \"Scientific Summary\" or \"Quarterly Retrospect\", with book reviews and commentary on the latest scientific works and publications. Two other journals produced in England prior to the development of \"Nature\" were the \"Quarterly Journal of Science\" and \"Scientific Opinion\", established in 1864 and 1868, respectively. The journal most closely related to \"Nature\" in its editorship and format was \"The Reader\", created in 1864; the publication mixed science with literature and art in an attempt to reach an audience outside of the scientific community, similar to \"Popular Science Review\".\n\nThese similar journals all ultimately failed. The \"Popular Science Review\" survived longest, lasting 20 years and ending its publication in 1881; \"Recreative Science\" ceased publication as the \"Student and Intellectual Observer\" in 1871. The \"Quarterly Journal\", after undergoing a number of editorial changes, ceased publication in 1885. \"The Reader\" terminated in 1867, and finally, \"Scientific Opinion\" lasted a mere 2 years, until June 1870.\n\nNot long after the conclusion of \"The Reader\", a former editor, Norman Lockyer, decided to create a new scientific journal titled \"Nature\", taking its name from a line by William Wordsworth: \"To the solid ground of nature trusts the Mind that builds for aye\". First owned and published by Alexander Macmillan, \"Nature\" was similar to its predecessors in its attempt to \"provide cultivated readers with an accessible forum for reading about advances in scientific knowledge.\" Janet Browne has proposed that \"far more than any other science journal of the period, \"Nature\" was conceived, born, and raised to serve polemic purpose.\" Many of the early editions of \"Nature\" consisted of articles written by members of a group that called itself the X Club, a group of scientists known for having liberal, progressive, and somewhat controversial scientific beliefs relative to the time period. Initiated by Thomas Henry Huxley, the group consisted of such important scientists as Joseph Dalton Hooker, Herbert Spencer, and John Tyndall, along with another five scientists and mathematicians; these scientists were all avid supporters of Darwin's theory of evolution as common descent, a theory which, during the latter half of the 19th century, received a great deal of criticism among more conservative groups of scientists. Perhaps it was in part its scientific liberality that made \"Nature\" a longer-lasting success than its predecessors. John Maddox, editor of \"Nature\" from 1966 to 1973 as well as from 1980 to 1995, suggested at a celebratory dinner for the journal's centennial edition that perhaps it was the journalistic qualities of Nature that drew readers in; \"journalism\" Maddox states, \"is a way of creating a sense of community among people who would otherwise be isolated from each other. This is what Lockyer's journal did from the start.\" In addition, Maddox mentions that the financial backing of the journal in its first years by the Macmillan family also allowed the journal to flourish and develop more freely than scientific journals before it.\nNorman Lockyer, the founder of \"Nature\", was a professor at Imperial College. He was succeeded as editor in 1919 by Sir Richard Gregory. Gregory helped to establish \"Nature\" in the international scientific community. His obituary by the Royal Society stated: \"Gregory was always very interested in the international contacts of science, and in the columns of \"Nature\" he always gave generous space to accounts of the activities of the International Scientific Unions.\" During the years 1945 to 1973, editorship of \"Nature\" changed three times, first in 1945 to A. J. V. Gale and L. J. F. Brimble (who in 1958 became the sole editor), then to John Maddox in 1965, and finally to David Davies in 1973. In 1980, Maddox returned as editor and retained his position until 1995. Philip Campbell has since become Editor-in-chief of all \"Nature\" publications.\n\nIn 1970, \"Nature\" first opened its Washington office; other branches opened in New York in 1985, Tokyo and Munich in 1987, Paris in 1989, San Francisco in 2001, Boston in 2004, and Hong Kong in 2005. In 1971, under John Maddox's editorship, the journal split into \"Nature Physical Sciences\" (published on Mondays), \"Nature New Biology\" (published on Wednesdays) and \"Nature\" (published on Fridays). In 1974, Maddox was no longer editor, and the journals were merged into \"Nature\".\n\nStarting in the 1980s, the journal underwent a great deal of expansion, launching over ten new journals. These new journals comprise the Nature Publishing Group, which was created in 1999 and includes \"Nature\", Nature Publishing Group Journals, Stockton Press Specialist Journals and Macmillan Reference (renamed NPG Reference).\n\nIn 1996, \"Nature\" created its own website and in 1999 Nature Publishing Group began its series of \"Nature Reviews\". Some articles and papers are available for free on the Nature website. Others require the purchase of premium access to the site. \"Nature\" claims an online readership of about 3 million unique readers per month.\n\nOn 30 October 2008, \"Nature\" endorsed an American presidential candidate for the first time when it supported Barack Obama during his campaign in America's 2008 presidential election.\n\nIn October 2012, an Arabic edition of the magazine was launched in partnership with King Abdulaziz City for Science and Technology. As of the time it was released, it had about 10,000 subscribers.\n\nOn 2 December 2014, \"Nature\" announced that it would allow its subscribers and a group of selected media outlets to share links allowing free, \"read-only\" access to content from its journals. These articles are presented using the digital rights management system ReadCube (which is funded by the Macmillan subsidiary Digital Science), and does not allow readers to download, copy, print, or otherwise distribute the content. While it does, to an extent, provide free online access to articles, it is not a true open access scheme due to its restrictions on re-use and distribution.\n\nOn 15 January 2015, details of a proposed merger with Springer Science+Business Media were announced.\n\nIn May 2015 it came under the umbrella of Springer Nature, by the merger of Springer Science+Business Media and Holtzbrinck Publishing Group's Nature Publishing Group, Palgrave Macmillan, and Macmillan Education.\n\nBeing published in \"Nature\" or any \"Nature\" publication is very prestigious. In particular, empirical papers are often highly cited, which can lead to promotions, grant funding, and attention from the mainstream media. Because of these positive feedback effects, competition among scientists to publish in high-level journals like \"Nature\" and its closest competitor, \"Science\", can be very fierce. \"Nature\"s impact factor, a measure of how many citations a journal generates in other works, was 38.138 in 2015 (as measured by Thomson ISI), among the highest of any science journal.\n\nAs with most other professional scientific journals, papers undergo an initial screening by the editor, followed by peer review (in which other scientists, chosen by the editor for expertise with the subject matter but who have no connection to the research under review, will read and critique articles), before publication. In the case of \"Nature\", they are only sent for review if it is decided that they deal with a topical subject and are sufficiently ground-breaking in that particular field. As a consequence, the majority of submitted papers are rejected without review.\n\nAccording to \"Nature\"s original mission statement:\nThis was revised in 2000 to:\nMany of the most significant scientific breakthroughs in modern history have been first published in \"Nature\". The following is a selection of scientific breakthroughs published in \"Nature\", all of which had far-reaching consequences, and the citation for the article in which they were published.\n\n\nIn 2017, Nature published an editorial entitled \"Removing Statues of Historical figures risks whitewashing history: Science must acknowledge mistakes as it marks its past\". The article commented on the placement and maintenance of statues honouring scientists with known unethical, abusive and torturous histories. Specifically, the editorial called on examples of J. Marion Sims, the 'Father of gynecology' who experimented on African American female slaves who were unable to give informed consent, and Thomas Parran Jr. who oversaw the Tuskegee syphilis experiment. The editorial as written made the case that removing such statues, and erasing names, runs the risk of \"whitewashing history\", and stated “Instead of removing painful reminders, perhaps these should be supplemented”. The article caused a large outcry and was quickly modified by Nature. The article was largely seen as offensive, inappropriate, and by many, racist. Nature acknowledged that the article as originally written was \"offensive and poorly worded\" and published selected letters of response. The editorial came just weeks after hundreds of white supremacists marched in Charlottesville, Virginia in the Unite the Right rally to oppose the removal of a statue of Robert E. Lee, setting off violence in the streets and killing a young woman. When Nature posted a link to the editorial on Twitter, the thread quickly exploded with criticisms. In response, several scientists called for a boycott. On 18 September 2017, the editorial was updated and edited by Philip Campbell, the editor of the journal.\n\nWhen Paul Lauterbur and Peter Mansfield won a Nobel Prize in Physiology or Medicine for research initially rejected by \"Nature\" and published only after Lauterbur appealed the rejection, \"Nature\" acknowledged more of its own missteps in rejecting papers in an editorial titled, \"Coping with Peer Rejection\":\nFrom 2000 to 2001, a series of five fraudulent papers by Jan Hendrik Schön was published in \"Nature\". The papers, about semiconductors, were revealed to contain falsified data and other scientific fraud. In 2003, \"Nature\" retracted the papers. The Schön scandal was not limited to \"Nature\"; other prominent journals, such as \"Science\" and \"Physical Review\", also retracted papers by Schön.\n\nIn June 1988, after nearly a year of guided scrutiny from its editors, \"Nature\" published a controversial and seemingly anomalous paper detailing Dr. Jacques Benveniste and his team's work studying human basophil degranulation in the presence of extremely dilute antibody serum. In short, their paper concluded that less than a single molecule of antibody could trigger an immune response in human basophils, defying the physical law of mass action. The paper excited substantial media attention in Paris, chiefly because their research sought funding from homeopathic medicine companies. Public inquiry prompted \"Nature\" to mandate an extensive, stringent and scientifically questionable experimental replication in Benveniste's lab, through which his team's results were categorically refuted.\n\nBefore publishing one of its most famous discoveries, Watson and Crick's 1953 on the structure of DNA, \"Nature\" did not send the paper out for peer review. John Maddox, \"Nature\"s editor, stated: \"the Watson and Crick paper was not peer-reviewed by \"Nature\" ... the paper could not have been refereed: its correctness is self-evident. No referee working in the field ... could have kept his mouth shut once he saw the structure\".\n\nAn earlier error occurred when Enrico Fermi submitted his breakthrough paper on the weak interaction theory of beta decay. \"Nature\" turned down the paper because it was considered too remote from reality. Fermi's paper was published by \"Zeitschrift für Physik\" in 1934, and finally published by \"Nature\" five years later, after Fermi's work had been widely accepted.\n\nIn 1999 \"Nature\" began publishing science fiction short stories. The brief \"vignettes\" are printed in a series called \"Futures\". The stories appeared in 1999 and 2000, again in 2005 and 2006, and have appeared weekly since July 2007. Sister publication \"Nature Physics\" also printed stories in 2007 and 2008. In 2005, \"Nature\" was awarded the European Science Fiction Society's Best Publisher award for the \"Futures\" series. One hundred of the \"Nature\" stories between 1999 and 2006 were published as the collection \"Futures from Nature\" in 2008.\n\nThe journal has a weekly circulation of around 53,000 and a pass-along rate of 8.0, resulting in a readership of over 400,000.\n\n\"Nature\" is edited and published in the United Kingdom by a division of the international scientific publishing company Springer Nature that publishes academic journals, magazines, online databases, and services in science and medicine. \"Nature\" has offices in London, New York City, San Francisco, Washington, D.C., Boston, Tokyo, Hong Kong, Paris, Munich, and Basingstoke. Nature Publishing Group also publishes other specialized journals including \"Nature Neuroscience\", \"Nature Biotechnology,\" \"Nature Methods\", the \"Nature Clinical Practice\" series of journals, \"Nature Structural & Molecular Biology\", \"Nature Chemistry\", and the \"Nature Reviews\" series of journals.\n\nSince 2005, each issue of \"Nature\" has been accompanied by a \"Nature Podcast\" featuring highlights from the issue and interviews with the articles' authors and the journalists covering the research. It is presented by Kerri Smith, and features interviews with scientists on the latest research, as well as news reports from Nature's editors and journalists. The Nature Podcast was founded – and the first 100 episodes were produced and presented – by clinician and virologist Chris Smith of Cambridge and \"The Naked Scientists\".\n\nIn 2007, Nature Publishing Group began publishing \"Clinical Pharmacology & Therapeutics\", the official journal of the American Society of Clinical Pharmacology & Therapeutics and \"Molecular Therapy\", the American Society of Gene Therapy's official journal, as well as the \"International Society for Microbial Ecology (ISME) Journal\". Nature Publishing Group launched \"Nature Photonics\" in 2007 and \"Nature Geoscience\" in 2008. \"Nature Chemistry\" published its first issue in April 2009.\n\nNature Publishing Group actively supports the self-archiving process and in 2002 was one of the first publishers to allow authors to post their contributions on their personal websites, by requesting an exclusive licence to publish, rather than requiring authors to transfer copyright. In December 2007, Nature Publishing Group introduced the Creative Commons attribution-non commercial-share alike unported licence for those articles in Nature journals that are publishing the primary sequence of an organism's genome for the first time.\n\nIn 2008, a collection of articles from \"Nature\" was edited by John S. Partington under the title \"H. G. Wells in Nature, 1893–1946: A Reception Reader\" and published by Peter Lang.\n\n\n"}
{"id": "1548703", "url": "https://en.wikipedia.org/wiki?curid=1548703", "title": "Nature worship", "text": "Nature worship\n\nNature worship is any of a variety of religious, spiritual and devotional practices that focus on the worship of the nature spirits considered to be behind the natural phenomena visible throughout nature. A nature deity can be in charge of nature, a place, a biotope, the biosphere, the cosmos, or the universe. Nature worship is often considered the primitive source of modern religious beliefs and can be found in theism, panentheism, pantheism, deism, polytheism, animism, totemism, shamanism, paganism. Common to most forms of nature worship is a spiritual focus on the individual's connection and influence on some aspects of the natural world and reverence towards it.\n"}
{"id": "4368966", "url": "https://en.wikipedia.org/wiki?curid=4368966", "title": "Nature writing", "text": "Nature writing\n\nNature writing is nonfiction or fiction prose or poetry about the natural environment. Nature writing encompasses a wide variety of works, ranging from those that place primary emphasis on natural history facts (such as field guides) to those in which philosophical interpretation predominate. It includes natural history essays, poetry, essays of solitude or escape, as well as travel and adventure writing.\n\nNature writing often draws heavily on scientific information and facts about the natural world; at the same time, it is frequently written in the first person and incorporates personal observations of and philosophical reflections upon nature.\n\nModern nature writing traces its roots to the works of natural history that were popular in the second half of the 18th century and throughout the 19th. An important early figures was the \"parson-naturalist\" Gilbert White (1720 – 1793), a pioneering English naturalist and ornithologist. He is best known for his \"Natural History and Antiquities of Selborne\" (1789).\n\nWilliam Bartram (1739 – 1823) is a significant early American pioneer naturalist who first work was published in 1791.\n\nGilbert White is regarded by many as England's first ecologist, and one of those who shaped the modern attitude of respect for nature. He said of the earthworm: \"Earthworms, though in appearance a small and despicable link in the chain of nature, yet, if lost, would make a lamentable chasm. [...] worms seem to be the great promoters of vegetation, which would proceed but lamely without them\" White and William Markwick collected records of the dates of emergence of more than 400 plant and animal species in Hampshire and Sussex between 1768 and 1793, which was summarised in \"The Natural History and Antiquities of Selborne\", as the earliest and latest dates for each event over the 25-year period, are among the earliest examples of modern phenology.\n\nThe tradition of clerical naturalists predates White and can be traced back to some monastic writings of the Middle Ages, although some argue that their writings about animals and plants cannot be correctly classified as natural history. Notable early parson-naturalists were William Turner (1508–1568), John Ray (1627–1705), William Derham (1657–1735).\n\nWilliam Bertram, in 1773, embarked on a four year journey through eight southern American colonies. Bartram made many drawings and took notes on the native flora and fauna, and the native American Indians. In 1774, he explored the St. Johns River. William Bartram wrote of his experiences exploring the Southeast in his book known today as \"Bartram's Travels\", published in 1791. Ephraim George Squier and Edwin Hamilton Davis, in their book, \"Ancient Monuments of the Mississippi Valley\", name Bartram as \"the first naturalist who penetrated the dense tropical forests of Florida.\"\n\nAfter Gilbert White and William Bertram, other significant writers include American ornithologist John James Audubon (1785 – 1851), Charles Darwin( (1809 – 1882), Richard Jefferies (1848 – 1887), Susan Fenimore Cooper (1813 – 1894), mother of American nature writting, and Henry David Thoreau (1817 – 1862), who is often considered the father of modern American nature writing, Ralph Waldo Emerson (1803 – 1882) John Burroughs, John Muir, Aldo Leopold, Rachel Carson, M. Krishnan, and Edward Abbey (although he rejected the term for himself).\n\nAnother important early work is \"A History of British Birds\" by Thomas Bewick, published in two volumes. Volume 1, \"Land Birds\", appeared in 1797. Volume 2, \"Water Birds\", appeared in 1804. The book was effectively the first \"field guide\" for non-specialists. Bewick provides an accurate illustration of each species, from life if possible, or from skins. The common and scientific name(s) are listed, citing the naming authorities. The bird is described, with its distribution and behaviour, often with extensive quotations from printed sources or correspondents. Critics note Bewick's skill as a naturalist as well as an engraver.\n\nSome important contemporary figures in Britain include Richard Mabey, Roger Deakin, Mark Cocker, and Oliver Rackham. Rackham's books included \"Ancient Woodland\" (1980) and \"The History of the Countryside\" (1986). Richard Maybey has been involved with radio and television programmes on nature, and his book \"Nature Cure\", describes his experiences and recovery from depression in the context of man’s relationship with landscape and nature. He has also edited and introduced editions of Richard Jefferies, Gilbert White, Flora Thompson and Peter Matthiessen. Mark Crocker has written extensively for British newspapers and magazines and his books include Birds Britannica (with Richard Mabey) (2005). and \"Crow Country\" (2007). He frequently writes about modern responses to the wild, whether found in landscape, human societies or in other species. Roger Deakin was an English writer, documentary-maker and environmentalist. In 1999, Deakin's acclaimed book \"Waterlog\" was published. Inspired in part by the short story \"The Swimmer\" by John Cheever, it describes his experiences of 'wild swimming' in Britain's rivers and lakes and advocates open access to the countryside and waterways. Deakin's book \"Wildwood\" appeared posthumously in 2007. It describes a series of journeys across the globe that Deakin made to meet people whose lives are intimately connected to trees and wood.\n\nIn 2017 the German book publishing company Matthes & Seitz Berlin started to grant the German Award for Nature Writing, an annual literary award for writers in German language that excellently fulfil the criteria of the literary genre. It comes with a price money of 10.000 Euro and additionallly an artist in residency grant of six weeks at the International Academy for Nature Conservation of Germany on the German island Vilm. The British Council in 2018 is offering an education bursary and workshops to six young German authors dedicated to Nature writing.\n\n\n\n"}
{"id": "774575", "url": "https://en.wikipedia.org/wiki?curid=774575", "title": "Outgassing", "text": "Outgassing\n\nOutgassing (sometimes called offgassing, particularly when in reference to indoor air quality) is the release of a gas that was dissolved, trapped, frozen or absorbed in some material. Outgassing can include sublimation and evaporation (which are phase transitions of a substance into a gas), as well as desorption, seepage from cracks or internal volumes, and gaseous products of slow chemical reactions. Boiling is generally thought of as a separate phenomenon from outgassing because it consists of a phase transition of a liquid into a vapor of the same substance.\n\nOutgassing is a challenge to creating and maintaining clean high-vacuum environments. NASA and ESA maintains a list of low-outgassing materials to be used for spacecraft, as outgassing products can condense onto optical elements, thermal radiators, or solar cells and obscure them. Materials not normally considered absorbent can release enough light-weight molecules to interfere with industrial or scientific vacuum processes. Moisture, sealants, lubricants, and adhesives are the most common sources, but even metals and glasses can release gases from cracks or impurities. The rate of outgassing increases at higher temperatures because the vapor pressure and rate of chemical reaction increases. For most solid materials, the method of manufacture and preparation can reduce the level of outgassing significantly. Cleaning of surfaces, or heating of individual components or the entire assembly (a process called \"bake-out\") can drive off volatiles.\n\nNASA's Stardust spaceprobe suffered reduced image quality due to an unknown contaminant that had condensed on the CCD sensor of the navigation camera. A similar problem affected the Cassini spaceprobe's Narrow Angle Camera, but was corrected by repeatedly heating the system to 4 °C. A comprehensive characterisation of outgassing effects using mass spectrometers could be obtained for ESA's Rosetta spacecraft.\n\nNatural outgassing is commonplace in comets.\n\nOutgassing is a possible source of many tenuous atmospheres of terrestrial planets or moons. Many materials are volatile relative to the extreme vacuum of space, such as around the Moon, and may evaporate or even boil at ambient temperature. Materials on the lunar surface have completely outgassed and been ripped away by solar winds long ago, but volatile materials may remain at depth. Once released, gases almost always are less dense than the surrounding rocks and sand and seep toward the surface. The lunar atmosphere probably originates from outgassing of warm material below the surface. At the Earth's tectonic divergent boundaries where new crust is being created, helium and carbon dioxide are some of the volatiles being outgassed from mantle magma.\n\nOutgassing can be significant if it collects in a closed environment where air is stagnant or recirculated. For example, new car smell consists of outgassed chemicals released by heat in a closed automobile. Even a nearly odorless material such as wood may build up a strong smell if kept in a closed box for months. There is some concern that plasticizers and solvents released from many industrial products, especially plastics, may be harmful to human health. Long-term exposure to solvent vapors can cause chronic solvent-induced encephalopathy (CSE). Outgassing toxic gases are of great concern in the design of submarines and space stations, which must have self-contained recirculated atmospheres.\n\nThe outgassing of small pockets of air near the surface of setting concrete can lead to permanent holes in the structure (called bugholes) that may compromise its structural integrity.\n\n\n"}
{"id": "26726375", "url": "https://en.wikipedia.org/wiki?curid=26726375", "title": "Planetary management", "text": "Planetary management\n\nPlanetary management is intentional global-scale management of Earth's biological, chemical and physical processes and cycles (water, carbon, nitrogen, sulfur, phosphorus, and others). Planetary management also includes managing humanity’s influence on planetary-scale processes. Effective planetary management aims to prevent destabilisation of Earth's climate, protect biodiversity and maintain or improve human well-being. More specifically, it aims to benefit society and the global economy, and safeguard the ecosystem services upon which humanity depends – global climate, freshwater supply, food, energy, clean air, fertile soil, pollinators, and so on.\n\nBecause of the sheer complexity and enormous scope of the task, it remains to be seen whether planetary management is a feasible paradigm for maintaining global sustainability. The concept currently has defenders and critics on both sides: environmentalist David W. Orr questions whether such a task can be accomplished with human help and technology or without first examining the underlying human causes, while geographer Vaclav Smil acknowledges that \"the idea of planetary management may seem preposterous to many, but at this time in history there is no rational alternative\".\n\nThe term has been around in science fiction novels since the 1970s. In 2004, the International Geosphere-Biosphere Programme published “Global Change and the Earth System, a planet under pressure.” The publication’s executive summary concluded: “An overall, comprehensive, internally consistent strategy for stewardship of the Earth system is required”. It stated that a research goal is to define and maintain a stable equilibrium in the global environment. In 2009, the planetary boundaries concept was published in the science journal Nature. The paper identifies nine boundaries in the Earth system. Remaining within these nine boundaries, the authors suggest, may safeguard the current equilibrium.\n\nIn 2007, France called for UNEP to be replaced by a new and more powerful organization, the United Nations Environment Organization. The rationale was that UNEP’s status as a programme, rather than an organization in the tradition of the World Health Organization or the World Meteorological Organization, weakened it to the extent that it was no longer fit for purpose given current knowledge of the state of Earth. The call was backed by 46 countries. Notably, the top five emitters of greenhouse gases failed to support the call.\n\nTogether with planetary management, stewardship and environmental wisdom are different ways to manage the Earth or \"environmental worldviews\".\n\nIn particular:\n\n\n"}
{"id": "44120129", "url": "https://en.wikipedia.org/wiki?curid=44120129", "title": "Revolving rivers", "text": "Revolving rivers\n\nRevolving rivers are a surprising, uncommon way of sand pile growth that can be found in a few sands around the world, but has been studied in detail only for one Cuban sand from a place called Santa Teresa (Pinar del Rio province).\n\nWhen pouring \"revolving\" sand on a flat surface from a fixed position, the growth of a conical pile does not occur by the common avalanche mechanism, where sand slides down the pile in a more or less random fashion. What happens in that a relatively thin \"river\" of flowing sand travels from the pouring point at the apex of the pile to its base, while the rest of the sand at the surface is static. In addition, the river \"revolves\" around the pile either in clockwise or counter-clockwise directions (looking from top) depending on the initial conditions of the experiment. Actually the river constitutes the \"cutting edge\" of a layer of sand that deposits as a helix on the conical pile, and makes it grow.\nFor small sandpiles, rivers are continuous, but they become intermittent\nfor larger piles.\n\nThe phenomenon was observed first by E. Altshuler at the University of Havana in 1995, but at the time he assumed that it was well known, and temporarily forgot about it. In 2000, being at the University of Houston, he told K. E. Bassler, who showed a vivid interest in the matter. Embarrassingly enough, Altshuler was unable to demonstrate it before Bassler using a random sand from Houston, so he had to send him a video from Cuba after his return to the island.\n\nOnce the existence of the strange phenomenon was confirmed for everyone, E. Altshuler and a number of collaborators performed a systematic study in Havana, which was then jointly published with Bassler.\nFurther work has been done to understand in more detail the\nphenomenon, and it has been found in other sands from different parts of the world. \nHowever, the connection between the physical, chemical (and possibly biological) properties of the grains in a specific sand, the nature of the inter-grain interactions, and the emergence of the revolving rivers is still an open question.\n\nSand from Santa Teresa is made of almost pure silicon dioxide grains with an average grain size of 0.2 mm approximately and no visible special features regarding grain shape. But in spite of its apparent simplicity, many puzzles still remain. For example, after many experiments one batch of sand may stop showing revolving rivers (just as singing sand eventually stops singing), which suggests that the decay is connected to certain properties of the surface of the grains that degrade by continued friction.\n\nVideos of the effect are available on YouTube.\n"}
{"id": "55613190", "url": "https://en.wikipedia.org/wiki?curid=55613190", "title": "Scotlandite", "text": "Scotlandite\n\nScotlandite is a sulfite mineral first discovered in a mine at Leadhills in South Lanarkshire, Scotland, an area known to mineralogists and geologists for its wide range of different mineral species found in the veins that lie deep in the mine shafts. This specific mineral is found in the Susanna vein of Leadhills, where the crystals are formed as chisel-shaped or bladed. Scotlandite was actually the first naturally occurring sulfite, which has the ideal chemical formula of PbSO. The mineral has been approved by the Commission on New Minerals and Mineral Names, IMA, to be named scotlandite for Scotland.\n\nScotlandite is found in association with pyromorphite, anglesite, lanarkite, leadhillite, susannite, and barite. It occurs in cavities in massive barite and anglesite, and is closely associated with lanarkite and susannite. Scotlandite represents the latest phase in the crystallization sequence of the associated lead secondary minerals. It can often be found in the vuggy anglesite as yellowish single crystals up to 1 millimeter in length that sometimes arrange in a fan-shaped aggregates. Anglesite can usually be recognized in a very thin coating on scotlandite which is used to protect the sulfite from further oxidation. A second variety of scotlandite can also occur in discontinuously distributed cavities between the anglesite mass containing the first variety and the barite matrix. This variety is characterized by tiny, whitish to water-clear crystals, and crystal clusters less than one millimeter in size, which encrust large portions of the interior of the cavities. Scotlandite is a uniquely rare mineral, as it occurs in small amounts in few locations around the world.\n\nScotlandite is a pale yellow, greyish-white, colorless, transparent mineral with an adamantine or pearly luster. It exhibits a hardness of 2 on the Mohs hardness scale. Scotlandite occurs as chisel-shaped or bladed crystals elongated along the c-axis, with a tendency to form radiating clusters. Its crystals are characterized by the {100}, {010}, {011}, {021}, {031}, and {032}. faces. Scotlandite shows perfect cleavage along the {100} plane and a less good one along the {010} plane. The measured density is 6.37 g/cm.\n\nScotlandite is biaxial positive, which means it will refract light along two axes. The mineral is optically biaxial positive, 2V 35° 24'(Na). The refractive indices are: α ~ 2.035, β ~ 2.040, and γ ~ 2.085 (Na). Dispersion is strong, v » r. The extinction is β//b, and α [001] = 20° (γ [100] = 4° in the obtuse angle β. H(Mohs) < 2. D = 6.37 and calculated D = 6.40 g cm. The infrared spectrum of scotlandite shows conclusively that it is an anhydrous sulfite, with no OH groups or other polyatomic anions being present. It is also proven by electron microprobe analysis and infrared spectroscopy that scotlandite must be a polymorph of lead sulfite.\n\nScotlandite is a sulfite compared with chemically related compounds, it is very close to the value of anglesite (6.38 g cm), but distinctly different from that of lanarkite (6.92 g cm). Orthorhombic lead sulfite is of higher density (D = 6.54, calculated D = 6.56 g cm), and has the same chemical properties as well. The empirical chemical formula for scotlandite calculated on the basis of Pb+S = 2, is PbSO or more ideally PbSO.\n\nA small crystal of scotlandite, showing some cleavage faces, was examined using Weissenberg and precession techniques. Scotlandite is in the monoclinic crystal system. The only systematic extinctions observed from the single crystal patterns were 0k0 where k was odd. Thus the possible space group is either P2 or P2/m. The unit cell parameters obtained from the single crystal study were used to index the X-ray powder pattern and were then refined with the indexed powder data. The results are: a = 4.505 Å, b = 5.333 Å, c = 6.405 Å; β= 106.24°; Z = 2. If the present a and c axes are interchanged, the unit cell of scotlandite is very similar, isotypic, to that of molybdomenite, PbSeO. Lead is coordinated to nine oxygen atoms with Pb-O=2.75 Å, and possibly further to one sulfur atom with Pb−S=3.46 Å. The average S−O distance in the pyramidal SO group is 1.52 Å.\n\nList of Minerals\n"}
{"id": "3469441", "url": "https://en.wikipedia.org/wiki?curid=3469441", "title": "Sorbent", "text": "Sorbent\n\nA sorbent is a material used to absorb or adsorb liquids or gases. Examples include:\n\n"}
{"id": "26686432", "url": "https://en.wikipedia.org/wiki?curid=26686432", "title": "Substorm", "text": "Substorm\n\nA substorm, sometimes referred to as a magnetospheric substorm or an auroral substorm, is a brief disturbance in the Earth's magnetosphere that causes energy to be released from the \"tail\" of the magnetosphere and injected into the high latitude ionosphere. Visually, a substorm is seen as a sudden brightening and increased movement of auroral arcs. Substorms were first described in qualitative terms by Kristian Birkeland which he called polar elementary storms. Sydney Chapman used the term substorm about 1960 which is now the standard term. The morphology of aurora during a substorm was first described by Syun-Ichi Akasofu in 1964 using data collected during the International Geophysical Year.\n\nSubstorms are distinct from geomagnetic storms in that the latter take place over a period of several days, are observable from anywhere on Earth, inject a large number of ions into the outer radiation belt, and occur once or twice a month during the maximum of the solar cycle and a few times a year during solar minimum. Substorms, on the other hand, take place over a period of a few hours, are observable primarily at the polar regions, do not inject many particles into the radiation belt, and are relatively frequent — often occurring only a few hours apart from each other. Substorms can be more intense and occur more frequently during a geomagnetic storm when one substorm may start before the previous one has completed. The source of the magnetic disturbances observed at the Earth's surface during geomagnetic storms is the ring current, whereas the sources of magnetic disturbances observed on the ground during substorms are electric currents in the ionosphere at high latitudes.\n\nSubstorms can cause magnetic field disturbances in the auroral zones up to a magnitude of 1000 nT, roughly 2% of the total magnetic field strength in that region. The disturbance is much greater in space, as some geosynchronous satellites have registered the magnetic field dropping to half of its normal strength during a substorm. The most visible indication of a substorm is an increase in the intensity and size of polar auroras. Substorms can be divided into three phases: the growth phase, the expansion phase, and the recovery phase.\n\nIn 2012, the THEMIS satellite mission observed the dynamics of rapidly developing substorms, confirming the existence of giant magnetic ropes and witnessed small explosions in the outskirts of Earth's magnetic field.\n"}
{"id": "235576", "url": "https://en.wikipedia.org/wiki?curid=235576", "title": "Suction", "text": "Suction\n\nSuction is the flow of a fluid into a partial vacuum, or region of low pressure. The pressure gradient between this region and the ambient pressure will propel matter toward the low pressure area. Dust is sucked into a vacuum cleaner when it is pushed in by the higher pressure air on the outside of the cleaner.\n\nThis is similar to what happens when humans breathe or drink through a straw. Both breathing and using a straw involve contracting the diaphragm and muscles around the rib cage. The increased volume in the chest cavity or thoracic cavity decreases the pressure inside, creating an imbalance with the ambient air pressure, or atmospheric pressure. This imbalance results in air pushing into the lungs or liquid pushing up through a straw and into the mouth.\n\nPumps typically have an inlet where the fluid (or air) enters the pump and an outlet where the fluid/air comes out. The inlet location is said to be at the suction side of the pump. The outlet location is said to be at the discharge side of the pump. Operation of the pump creates suction (a lower pressure) at the suction side so that fluid/air can enter the pump through the inlet. Pump operation also causes higher pressure at the discharge side by forcing the fluid/air out at the outlet. There may be pressure sensing devices at the pump's suction and/or discharge sides which control the operation of the pump. For example, if the suction pressure of a centrifugal pump is too high, a device may trigger the fluid pump to shut off to keep it from running dry; i. e. with no fluid entering.\n\nUnder normal conditions of atmospheric pressure suction can draw pure water up to a maximum height of approximately 10.3 m (33.9 feet). \n\nIn medicine, suction devices are used to clear airways of materials that would like to impede breathing or cause infections, to aid in surgery, and for other purposes.\n\n\n"}
{"id": "32840848", "url": "https://en.wikipedia.org/wiki?curid=32840848", "title": "The Cloud (poem)", "text": "The Cloud (poem)\n\n\"The Cloud\" is a major 1820 poem written by Percy Bysshe Shelley. \"The Cloud\" was written during late 1819 or early 1820, and submitted for publication on 12 July 1820. The work was published in the 1820 collection \"Prometheus Unbound, A Lyrical Drama, in Four Acts, With Other Poems\" by Charles and James Ollier in London in August 1820. The work was proof-read by John Gisborne. There were multiple drafts of the poem. The poem consists of six stanzas in anapestic or antidactylus meter, a foot with two unaccented syllables followed by an accented syllable.\n\nThe cloud is a metaphor for the unending cycle of nature: \"I silently laugh at my own cenotaph/ ... I arise and unbuild it again.\" As with the wind and the leaves in \"Ode to the West Wind\", the skylark in \"To a Skylark\", and the plant in \"The Sensitive Plant\", Shelley endows the cloud with sentient traits that personify the forces of nature.\n\nIn \"The Cloud\", Shelley relies on the imagery of transformation or metamorphosis, a cycle of birth, death, and rebirth: \"I change, but I cannot die.\" Mutability or change is a fact of physical nature.\n\nLightning or electricity is the \"pilot\" or guide for the cloud. Lightning is attracted to the \"genii\" in the earth which results in lightning flashes. The genii symbolize the positive charge of the surface of the earth while the cloud possesses a negative charge.\n\nBritish scientist and poet Erasmus Darwin, the grandfather of Charles Darwin, had written about plant life and science in the poem collection \"The Botanic Garden\" (1791) and on \"spontaneous vitality\", that \"microscopic animals are said to remain dead for many days or weeks ... and quickly to recover life and motion\" when water and heat are added, in \"The Temple of Nature\" (1803). Percy Bysshe Shelley had cited Darwin in his Preface to the anonymously published novel \"Frankenstein; or, The Modern Prometheus\" (1818), explaining how the novel was written and its meaning. He argued that imparting life to a corpse \"as not of impossible occurrence\".\n\nThe cloud is a personification and a metaphor for the perpetual cycle of transformation and change in nature. All life and matter are interconnected and undergo unending change and metamorphosis.\n\nA review of the 1820 \"Prometheus Unbound\" collection in the September and October 1821 issues of \"The London Magazine\" noted the originality of \"The Cloud\": \"It is impossible to peruse them without admiring the peculiar property of the author's mind, which can doff in an instant the cumbersome garments of metaphysical speculations, and throw itself naked as it were into the arms of nature and humanity. The beautiful and singularly original poem of 'The Cloud' will evince proofs of our opinion, and show the extreme force and freshness with which the writer can impregnate his poetry.\"\n\nIn the October 1821 issue of \"Quarterly Review\", W.S. Walker argued that \"The Cloud\" is related to \"Prometheus Unbound\" in that they are both absurd and \"galimatias\".\n\nJohn Todhunter wrote in 1880 that \"The Cloud\" and \"To a Skylark\" were \"the two most popular of Shelley's lyrics\".\n\nIn 1889, Francis Thompson asserted that \"The Cloud\" was the \"most typically Shelleyan of all the poems\" because it contained \"the child's faculty of make-believe raised to the nth power\" and that \"He is still at play, save only that his play is such as manhood stops to watch, and his playthings are those which the gods give their children. The universe is his box of toys. He dabbles his fingers in the dayfall. He is gold-dusty with tumbling amidst the stars.\"\n\nOn 20 April 1919, a silent black and white movie was released in the US entitled \"The Cloud\" which was \"a visual poem featuring clouds and landscapes in accompaniment to the words of Shelley's poem 'The Cloud'.\" The film was directed by W.A. Van Scoy and produced by the Post Nature Pictures company.\n\n\n"}
{"id": "23083720", "url": "https://en.wikipedia.org/wiki?curid=23083720", "title": "The Evolution of God", "text": "The Evolution of God\n\nThe Evolution of God is a 2009 book by Robert Wright, in which the author explores the history of the concept of God in the three Abrahamic religions through a variety of means, including archeology, history, theology, and evolutionary psychology. The patterns which link Judaism, Christianity, and Islam and the ways in which they have changed their concepts over time are explored as one of the central themes.\n\nOne of the conclusions of the book that Wright tries to make is a reconciliation between science and religion. He also speculates on the future of the concept of God.\n\nAmong other things, Wright discusses the role of evolutionary biology in the development of religion. Geneticist Dean Hamer hypothesized that some people have a specific gene that makes them prone to religious belief, which he calls the God gene, and that over time natural selection has favored these people because their spirituality leads to optimism. Wright, however, thinks the tendency towards religious belief is not an adaptive trait influenced by natural selection, but rather a spandrel - a trait that happens to be supported by adaptations originally selected for other purposes. Wright states that the human brain approaches religious belief based on how it adapted to survive and reproduce in early hunter-gatherer societies.\n\nHe points out four key traits of religion that align with the human brain's survival adaptations:\nHumans have adapted to pay attention to surprising and confusing information, because it could make the difference between life and death. (For instance, if a person left the campsite and mysteriously never returned, it would be wise for the others to be on guard for a predator or some other danger.) Understanding and controlling cause and effect also takes top priority in the human brain, since humans live in complex social groups where predicting and influencing the actions and thoughts of others gains them allies, status, and access to resources. As human cognitive abilities and curiosity expanded over the centuries, their investigation of cause and effect expanded from the strictly social context out into the world at large, opening the doors for religions to explain things like weather and disease.\n\nThough some of these explanations were strange and perhaps dubious, the fact that they could not be completely disproven lent them credibility; it was better to be cautious than dead. Wright uses an example from the Haida people, indigenous to the northwest coast of North America, who would try to appease killer whale deities to calm storms out at sea; they would pour fresh water into the ocean or tie tobacco or deer tallow to the end of a paddle. While some people certainly died despite these offerings, those who survived were a testament to the ritual's possible efficacy.\n\nMysterious and unproven beliefs can also persist in a culture because human brains have adapted to agree with the group consensus even if it goes against one's better judgment or personal beliefs, since a person alienated from the group loses protection, food, and mates. Wright cites the Asch conformity experiments and even posits that Stockholm syndrome is not so much a syndrome as a natural product of evolution, the brain's way of ensuring that a person accepts and is accepted by his or her new social group. In addition, beliefs can persist because once a person publicly announces a belief, social psychologists have found that he or she is inclined to focus on evidence supporting that belief while conveniently ignoring evidence contradicting it, a logical fallacy known as cherry picking.\n\nJournalist and political commentator Andrew Sullivan gave the book a positive review in \"The Atlantic\", saying that the book \"...gave me hope that we can avoid both the barrenness of a world without God and the horrible fusion of fundamentalism and weapons of mass destruction.\" \n\n\"Newsweek\" religion editor, Lisa Miller, described \"The Evolution of God\" as a reframing of the faith vs. reason debate. Drawing a contrast to such authors as Sam Harris, Richard Dawkins and Christopher Hitchens, Miller gives an overall positive review of the book's approach to the examination of the concept of God.\n\nIn a review for \"The New York Times\", Yale professor of psychology Paul Bloom said, \"In his brilliant new book, “The Evolution of God,” Robert Wright tells the story of how God grew up.\" Bloom sums up Wright's controversial stance as, \"Wright’s tone is reasoned and careful, even hesitant, throughout, and it is nice to read about issues like the morality of Christ and the meaning of jihad without getting the feeling that you are being shouted at. His views, though, are provocative and controversial. There is something here to annoy almost everyone.\"\n\nHowever, in a \"New York Times\" review that included a reply from Wright, Nicholas Wade, a writer for the \"Science Times\" section, notes the book is \"a disappointment from the Darwinian perspective\", because evolution \"provides a simpler explanation for moral progression than the deity Wright half invokes.\" Wright replied to Wade's comments, saying Wade had misunderstood Wright's argument and that \"The deity (if there is one–and I’m agnostic on that point) would be realizing moral progress through evolution’s creation of the human moral sense (and through the subsequent development of that moral sense via cultural evolution, particularly technological evolution).\" Wade replied that \"evolution seems to me a sufficient explanation for the moral progress that Mr. Wright correctly discerns in the human condition, so there seemed no compelling need to invoke a deity.\"\n\nTo promote the book, Wright did a variety of interviews, including with the \"New York Times\", \"Publishers Weekly\", and \"Bill Moyers Journal\".\nHe also did a series of videos on Bloggingheads.tv, a website he co-founded with Mickey Kaus. Wright also appeared on \"The Colbert Report\" on August 18, 2009.\n\n\n"}
{"id": "448321", "url": "https://en.wikipedia.org/wiki?curid=448321", "title": "Thermoelectric effect", "text": "Thermoelectric effect\n\nThe thermoelectric effect is the direct conversion of temperature differences to electric voltage and vice versa via a thermocouple. A thermoelectric device creates voltage when there is a different temperature on each side. Conversely, when a voltage is applied to it, heat is transferred from one side to the other, creating a temperature difference. At the atomic scale, an applied temperature gradient causes charge carriers in the material to diffuse from the hot side to the cold side.\n\nThis effect can be used to generate electricity, measure temperature or change the temperature of objects. Because the direction of heating and cooling is determined by the polarity of the applied voltage, thermoelectric devices can be used as temperature controllers.\n\nThe term \"thermoelectric effect\" encompasses three separately identified effects: the Seebeck effect, Peltier effect, and Thomson effect. The Seebeck and Peltier effects are different manifestations of the same physical process; textbooks may refer to this process as the Peltier–Seebeck effect (the separation derives from the independent discoveries by French physicist Jean Charles Athanase Peltier and Baltic German physicist Thomas Johann Seebeck). The Thomson effect is an extension of the Peltier–Seebeck model and is credited to Lord Kelvin.\n\nJoule heating, the heat that is generated whenever a current is passed through a resistive material, is related, though it is not generally termed a thermoelectric effect. The Peltier–Seebeck and Thomson effects are thermodynamically reversible, whereas Joule heating is not.\n\nThe Seebeck effect is the conversion of heat directly into electricity at the junction of different types of wire. Originally discovered in 1794 by Italian scientist Alessandro Volta, it is named after the Baltic German physicist Thomas Johann Seebeck, who in 1821 independently rediscovered it. It was observed that a compass needle would be deflected by a closed loop formed by two different metals joined in two places, with a temperature difference between the joints. This was because the electron energy levels in each metal shifted differently and a potential difference between the junctions created an electrical current and therefore a magnetic field around the wires. Seebeck did not recognize that there was an electric current involved, so he called the phenomenon \"thermomagnetic effect\". Danish physicist Hans Christian Ørsted rectified the oversight and coined the term \"thermoelectricity\".\n\nThe Seebeck effect is a classic example of an electromotive force (emf) and leads to measurable currents or voltages in the same way as any other emf. Electromotive forces modify Ohm's law by generating currents even in the absence of voltage differences (or vice versa); the local current density is given by\n\nwhere formula_2 is the local voltage, and formula_3 is the local conductivity. In general, the Seebeck effect is described locally by the creation of an electromotive field\n\nwhere formula_5 is the Seebeck coefficient (also known as thermopower), a property of the local material, and formula_6 is the temperature gradient.\n\nThe Seebeck coefficients generally vary as function of temperature and depend strongly on the composition of the conductor. For ordinary materials at room temperature, the Seebeck coefficient may range in value from −100 μV/K to +1,000 μV/K (see Seebeck coefficient article for more information).\n\nIf the system reaches a steady state, where formula_7, then the voltage gradient is given simply by the emf: formula_8. This simple relationship, which does not depend on conductivity, is used in the thermocouple to measure a temperature difference; an absolute temperature may be found by performing the voltage measurement at a known reference temperature. A metal of unknown composition can be classified by its thermoelectric effect if a metallic probe of known composition is kept at a constant temperature and held in contact with the unknown sample that is locally heated to the probe temperature. It is used commercially to identify metal alloys. Thermocouples in series form a thermopile. Thermoelectric generators are used for creating power from heat differentials.\nThe Peltier effect is the presence of heating or cooling at an electrified junction of two different conductors and is named after French physicist Jean Charles Athanase Peltier, who discovered it in 1834. When a current is made to flow through a junction between two conductors, A and B, heat may be generated or removed at the junction. The Peltier heat generated at the junction per unit time is\n\nwhere formula_10 and formula_11 are the Peltier coefficients of conductors A and B, and formula_12 is the electric current (from A to B). The total heat generated is not determined by the Peltier effect alone, as it may also be influenced by Joule heating and thermal-gradient effects (see below).\n\nThe Peltier coefficients represent how much heat is carried per unit charge. Since charge current must be continuous across a junction, the associated heat flow will develop a discontinuity if formula_10 and formula_11 are different. The Peltier effect can be considered as the back-action counterpart to the Seebeck effect (analogous to the back-emf in magnetic induction): if a simple thermoelectric circuit is closed, then the Seebeck effect will drive a current, which in turn (by the Peltier effect) will always transfer heat from the hot to the cold junction. The close relationship between Peltier and Seebeck effects can be seen in the direct connection between their coefficients: formula_15 (see below).\n\nA typical Peltier heat pump involves multiple junctions in series, through which a current is driven. Some of the junctions lose heat due to the Peltier effect, while others gain heat. Thermoelectric heat pumps exploit this phenomenon, as do thermoelectric cooling devices found in refrigerators.\n\nIn different materials, the Seebeck coefficient is not constant in temperature, and so a spatial gradient in temperature can result in a gradient in the Seebeck coefficient. If a current is driven through this gradient, then a continuous version of the Peltier effect will occur. This Thomson effect was predicted and subsequently observed in 1851 by Lord Kelvin (William Thomson). It describes the heating or cooling of a current-carrying conductor with a temperature gradient.\n\nIf a current density formula_16 is passed through a homogeneous conductor, the Thomson effect predicts a heat production rate per unit volume\n\nwhere formula_6 is the temperature gradient, and formula_19 is the Thomson coefficient. The Thomson coefficient is related to the Seebeck coefficient as formula_20 (see below). This equation, however, neglects Joule heating and ordinary thermal conductivity (see full equations below).\n\nOften, more than one of the above effects is involved in the operation of a real thermoelectric device. The Seebeck effect, Peltier effect, and Thomson effect can be gathered together in a consistent and rigorous way, described here; the effects of Joule heating and ordinary heat conduction are included as well. As stated above, the Seebeck effect generates an electromotive force, leading to the current equation\n\nTo describe the Peltier and Thomson effects the flow of energy must be considered. To start, the dynamic case where both temperature and charge may be varying with time can be considered. The full thermoelectric equation for the energy accumulation, formula_22, is\n\nwhere formula_24 is the thermal conductivity. The first term is the Fourier's heat conduction law, and the second term shows the energy carried by currents. The third term, formula_25, is the heat added from an external source (if applicable).\n\nIn the case where the material has reached a steady state, the charge and temperature distributions are stable, so one must have both formula_26 and formula_27. Using these facts and the second Thomson relation (see below), the heat equation then can be simplified to\n\nThe middle term is the Joule heating, and the last term includes both Peltier (formula_29 at junction) and Thomson (formula_29 in thermal gradient) effects. Combined with the Seebeck equation for formula_16, this can be used to solve for the steady-state voltage and temperature profiles in a complicated system.\n\nIf the material is not in a steady state, a complete description will also need to include dynamic effects such as relating to electrical capacitance, inductance, and heat capacity.\n\nIn 1854, Lord Kelvin found relationships between the three coefficients, implying that the Thomson, Peltier, and Seebeck effects are different manifestations of one effect (uniquely characterized by the Seebeck coefficient).\n\nThe first Thomson relation is\n\nwhere formula_33 is the absolute temperature, formula_19 is the Thomson coefficient, formula_35 is the Peltier coefficient, and formula_5 is the Seebeck coefficient. This relationship is easily shown given that the Thomson effect is a continuous version of the Peltier effect. Using the second relation (described next), the first Thomson relation becomes formula_20.\n\nThe second Thomson relation is\n\nThis relation expresses a subtle and fundamental connection between the Peltier and Seebeck effects. It was not satisfactorily proven until the advent of the Onsager relations, and it is worth noting that this second Thomson relation is only guaranteed for a time-reversal symmetric material; if the material is placed in a magnetic field or is itself magnetically ordered (ferromagnetic, antiferromagnetic, etc.), then the second Thomson relation does not take the simple form shown here.\n\nThe Thomson coefficient is unique among the three main thermoelectric coefficients because it is the only one directly measurable for individual materials. The Peltier and Seebeck coefficients can only be easily determined for pairs of materials; hence, it is difficult to find values of absolute Seebeck or Peltier coefficients for an individual material.\n\nIf the Thomson coefficient of a material is measured over a wide temperature range, it can be integrated using the Thomson relations to determine the absolute values for the Peltier and Seebeck coefficients. This needs to be done only for one material, since the other values can be determined by measuring pairwise Seebeck coefficients in thermocouples containing the reference material and then adding back the absolute Seebeck coefficient of the reference material. For more details on absolute Seebeck coefficient determination, see Seebeck coefficient.\n\nThe Seebeck effect is used in thermoelectric generators, which function like heat engines, but are less bulky, have no moving parts, and are typically more expensive and less efficient. They have a use in power plants for converting waste heat into additional electrical power (a form of energy recycling) and in automobiles as automotive thermoelectric generators (ATGs) for increasing fuel efficiency. Space probes often use radioisotope thermoelectric generators with the same mechanism but using radioisotopes to generate the required heat difference.\nRecent uses include stove fans, body-heat—powered lighting and a smartwatch powered by body heat.\n\nThe Peltier effect can be used to create a refrigerator that is compact and has no circulating fluid or moving parts. Such refrigerators are useful in applications where their advantages outweigh the disadvantage of their very low efficiency. The Peltier effect is also used by many thermal cyclers, laboratory devices used to amplify DNA by the polymerase chain reaction (PCR). PCR requires the cyclic heating and cooling of samples to specified temperatures. The inclusion of many thermocouples in a small space enables many samples to be amplified in parallel.\n\nThermocouples and thermopiles are devices that use the Seebeck effect to measure the temperature difference between two objects.\nThermocouples are often used to measure high temperatures, holding the temperature of one junction constant or measuring it independently (cold junction compensation). Thermopiles use many thermocouples electrically connected in series, for sensitive measurements of very small temperature difference.\n\n\n\n"}
{"id": "44244083", "url": "https://en.wikipedia.org/wiki?curid=44244083", "title": "Time and Eternity (philosophy book)", "text": "Time and Eternity (philosophy book)\n\nTime and Eternity - An Essay on the Philosophy of Religion (1st imp. Princeton New Jersey 1952, Princeton University Press, 169 pp) is a philosophy book written by Walter Terence Stace. At the time of writing, Stace was a professor of philosophy at Princeton University, where he had worked since 1932 after a 22-year career in the Ceylon Civil Service. \"Time and Eternity\" was one of his first books about the philosophy of religion and mysticism, after writing throughout most of the 1930s and 1940s that was influenced by phenomenalist philosophy.\n\nIn his introduction Stace writes that \"Time and Eternity\" is an attempt to set out the fundamental nature of religion, and to deal with the conflict between religion and naturalism. He explains that the basic idea set out in the book is that all religious thought is symbolic, and that his influences include Rudolf Otto, especially his \"Mysticism East and West\", and Immanuel Kant. He says he was motivated to write the book in an attempt to add to the \"other half of the truth which I now think naturalism [as espoused in his 1947 essay \"Man Against Darkness\"] misses\".\n\nThe book begins by looking at religion, specifically God as non-being and as being, put by Stace as the negative and positive divine. Stace then defines two orders of being - time and eternity, which he says intersect in the moment of mystic illumination. He goes on to say that the nature of God or eternity is such that all religious language is symbolic and that it is necessarily subject to contradictions.\n\nThe first chapter asks what religion is, stating that religious thought is contradictory, is rooted in intuition, and that God is fundamentally a mystery. The second and third chapters look at the negative divine - the characterisation of God as void, silence or non-being - which Stace maintains is an idea found in all religions. He maintains that mystical experience is shared by all mankind, it is only the theories about it that differ. On this point he says he is in agreement with Otto. In this experience the distinction between subject and object is overcome, indeed there is no difference between the experiencer and the experience.\n\nStace then goes on to explain that all religions say that religious revelation is ineffable, because no words or concepts can be applied to God who is without qualities or predicates. Thus, God cannot be comprehended by the intellect, but is apprehended by intuition. \"... it is of the very nature of intellect to involve the subject-object opposition. But in the mystic experience this opposition is transcended. Therefore the intellect is incapable of understanding it. Therefore it is incomprehensible, ineffable.\"\n\nStace then looks at the positive divine; he asks how concepts can be applied to that which is above all concepts and finds that all propositions about God are symbolical. He defines religious and non-religious symbolism as differing in two respects. Firstly, religious symbols cannot be translated into logical propositions because they refer to an (ineffable) experience rather than a proposition. Secondly, the relationship between the religious symbol and what is symbolised is one of evocation rather than \"meaning\", as meaning is a concept, which is absent in the mystical experience. \"Yet in some way this symbolic language evokes in us some glimpse, some hint, seen dimly through the mists and fogs which envelop us, of that being who stands above all human thought and conception.\" He goes on to write that some of these symbols feel more appropriate than others (e.g. God is love not hate).\n\nNext Stace explains that there are two orders of being: time (or the world) and eternity (or God), and these intersect in the moment of mystic illumination. He maintains these orders are distinct, so one order cannot dictate to the other. Here he says that he agrees with Kant, who made a distinction between the world of phenomena and the noumenon, although he is critical of Kant’s disregard for mystical experience.\n\nLooking at symbolism in religion, Stace states that there are two types of predicates applied to God: first, the ethically-neutral sort, such as God being mind, power or personhood. Secondly, the ethical kind, where he is love, mercy, or righteousness. He explains that the former qualities are justified by an appeal to a hierarchy of being, and the latter to a hierarchy of value. In both cases the more adequate symbol are those that are higher in each hierarchy. In rooting symbolism in hierarchies, Stace explicitly states he is in opposition to Otto who thought religious symbolism was based on analogy between the numen and qualities found in the natural world.\n\nStace next looks at religion’s claims to truth. He draws an analogy between mystical illumination and aesthetic truth, as the truths of both rest on revelation rather than reason. \"Either you directly perceive beauty, or you do not. And either you directly perceive God in intuition, or you do not.\" Further, he maintains the arguments of both mystics and naturalists in denying each other’s positions are invalid, as they concern different realities.\n\nThese separate spheres lead Stace to reflect on both proofs for God and acosmism. He writes that proofs and disproofs for God are equally false, as God is only accessible by intuition and not logic. \"… the production by philosophers of proofs of the unreality of space, time, and the temporal world generally, is a direct result of their mistaking of their mystical propositions for factual propositions.\" Further, proofs of God actually harm religion as they make him a part of the natural order - a point on which he says that he agrees with Kant. Conversely acosmism (the denial of the reality of the world) has its root in the mystical moment, within which there is no other truth, God is the supreme reality and there is no naturalistic world. However this is a symbolic truth, rather than a statement of fact. Its counterpart in naturalism is atheism, which denies the reality of God.\n\nIn the final chapter Stace looks at mysticism and logic. He returns to the idea that theology and mystical philosophies (he gives the examples of Vedanta, Spinoza, Hegel, and Bradley) will always contain contradictions. Known as the doctrine of the Mystery of God, he maintains this is because the intellect is inherently incapable of understanding the Ultimate. All attempts to state the nature of the ultimate necessarily produce contradictions.\n\nVirgil C Aldrich reviewed the book alongside \"Religion and the Modern Mind\" and \"The Gate of Silence\", also by Stace and published in 1952. He points out that all three books mark a new direction for Stace who was previously best known as an empiricist and naturalist. For Aldrich this new intellectual interest results in a sharp dualism in both Stace’s personality and his thought. However, he writes that fortunately Stace’s philosophical background prevents him from supposing that scientific empiricism can confirm religious experience, indeed his religious philosophy is the sort “that a Hume or a Kant can consort with.” Aldrich argues that Stace’s intellectual sophistication is most evident in his ideas about the negative divine, but his thought is liable to all the standard objections where he proposes notions of the positive divine and religious intuition. Specifically, the notion that religious language is evocative of the mystical experience is problematic, because it is difficult to determine what language is adequate without resorting to literal or abstract ideas. Rudolf Otto’s notion of analogy, rejected by Stace, is more robust. Aldrich points out a contradiction in Stace’s reliance on hierarchies of being and values to more adequately refer to God, as this implies continuity between the world and eternity, which Stace denies.\n\nJulius Seelye Bixler reviewed the book twice, in 1952 and 1953. In his first review he wrote that he believed Stace was trying to have his cake and eat it with regards to the truth of both naturalism and mysticism. Bixler also wonders whether the revelation of God can really be free of concepts and thus whether time and eternity are utterly unrelated as Stace maintains. He identifies points in Stace’s thought where there is continuity between these two states and mystical language does appear to refer to concepts. Finally he rejects the book’s analogy of mystical experience to the evocative power of art, maintaining that art must be somewhat related to logic. Nonetheless, Bixler does concur that the book is a fascinating confessio fidei and personal statement. A year later, he reviewed \"Time and Eternity\" alongside \"Religion and the Modern Mind\". As well as reiterating the points he had made earlier, Bixler judges the second book more favourably and recommends reading the two together to better understand the problems they address.\n\nStace was praised for his clarity and ambitious aims in \"Time and Eternity\" by Abraham Kaplan who believed the book was one of the best on the subject for many many years. He pointed out that the book’s distinction between the orders of time and eternity owed much to Kant (which Stace himself acknowledged). Kaplan reflected that it was the book’s emphasis placed on mysticism and a universal religious intuition that would be of particular interest to students of “Oriental and comparative philosophy”. The central idea upon which Stace’s thought stands or falls, for Kaplan, is that religious language is evocative rather than descriptive. In this both religionists and naturalists will find problems. For the former, Stace can only account for the appropriateness of religious language by relying on ‘nearness’ to the divine rather than on resemblance, and this relies on ‘a vague panpsychism’ and levels of being in the manner of Samuel Alexander. While for the naturalist, Stace’s system of religious symbolism is doomed to remain mysterious, because it does not allow religious metaphors to be translated literally and neither can it be said how they evoke the experience to which they refer.\n\nAlso noting the unachievable ambition of solving the conflict between naturalism and religion, Martin A Greenman, remarks that one must come to the book “with a certain mood”. Too critical a mood would blind the reader to its religious insights, while the sensitivity and depth of its philosophic insights would be lost if one were to approach it in a too enthusiastically religious mood. Greenman finishes by justifying Stace’s philosophy to logical positivists by quoting from Wittgenstein's \"Tractatus\": “My propositions are elucidatory in this way: he who understands me finally recognizes them as senseless, when he has climbed out through them, on them, over them….He must surmount these propositions: then he sees the world rightly” (6. 54.) Dorothy M. Emmet found issue with the notion that the mystical experience is the point of intersection between the temporal and eternal orders. She writes that there are difficulties in Stace defining these orders as two distinct “orders of being”, rather than just as a way of speaking, because this then means some statements about the temporal order are relevant to what is said about the eternal order and vice versa. Indeed, the interrelation between these two orders is difficult to maintain. She also questioned Stace’s characterisation of mystical consciousness as being the same everywhere.\n\nMore recently, Maurice Friedman writes about the book in the context of the various attempts to find a universal essence - or perennial philosophy - within religion. He finds that \"Time and Eternity\" is a more systematic attempt at this than those proposed by Aldous Huxley or Ananda Coomaraswamy, but no more successful. For Friedman, the philosophy that Stace lays out in the book is derived from metaphysical speculation (that, like the ideas of Huxley and Coomaraswamy, is influenced by Vedanta), rather than mystical experience. Central to Friedman’s critique is the notion that there is a vast gulf between the mystical experience which Stace defines as beyond thought, and his philosophical system built on this. He also mentions that mystics do not always agree on what experiences, symbols and philosophies are the closest to the divine.\n\nThe book has received more positive support however. Robert C Neville called \"Time and Eternity\" “the most sophisticated treatment of eternity and time in our century so far”. In his \"Thought: A Very Short Introduction\", Tim Bayne says the book contains a “classic” discussion of ineffability. American writer Arthur Goldwag has said that the phrase \"that than which there is no other\" that he encountered in \"Time and Eternity\" was one of a number of factors that contributed to him giving up praying.\n\n"}
{"id": "44078902", "url": "https://en.wikipedia.org/wiki?curid=44078902", "title": "Upstream contamination", "text": "Upstream contamination\n\nUpstream contamination by floating particles is a counterintuitive phenomenon in fluid dynamics. When pouring water from a higher container to a lower one, particles floating in the latter can climb upstream into the upper container. A definitive explanation is still lacking: experimental and computational evidence indicates that the contamination is chiefly driven by surface tension gradients, however the phenomenon is also affected by the dynamics of swirling flows that remain to be fully investigated.\n\nThe phenomenon was first observed in 2008 by the Argentinian \nS. Bianchini during mate tea preparation, while studying Physics \nat the University of Havana.\n\nIt rapidly attracted the interest of Prof. A. Lage, who performed,\nwith Bianchini, a series of controlled experiments. Later on\nProf. E. Altshuler completed the trio in Havana, which resulted in the\nDiploma thesis of Bianchini and \na short original paper posted in the web arxiv and commented as a surprising fact in some online journals.\n\nBianchini's Diploma thesis showed that the phenomenon could be reproduced \nin a controlled laboratory setting using mate leaves or chalk powder as contaminants, \nand that temperature gradients (hot in the top, cold in the bottom) were not necessary \nto generate the effect. The research also showed that surface tension was \na key element to the explanation through the so-called Marangoni effect, which was \nsuggested by two facts: (a) both mate and \nchalk lowered the surface tension of water, and (b) if an industrial surfactant was\nadded on the upper reservoir, the upstream motion of particles would stop.\n\nAfter a talk by A. Lage at the First Workshop on Complex Matter Physics\nin Havana (MarchCOMeeting'2012), Prof. T Shinbrot (Rutgers University)\ngot interested in the subject. Together with student T. Siu, \nCuban results were confirmed and expanded with new experiments and numerical\nsimulations at Rutgers,\nwhich resulted in a joint peer-reviewed paper.\n\nLater on, the phenomenon has been confirmed independently by others.\nWhether it is caused solely by surface tension gradients or depends also on dynamical \nbehaviors of the falling water still remains as an open question.\n\nVideos of the effect are available on YouTube.\n\nThe phenomenon of upstream contamination could be relevant to industrial and biotechnological processes, and may be\nconnected even to movements of the protoplasm. It could imply that some of the \"good practices\" in industrial and biotechnological procedures need revision.\n"}
{"id": "19678805", "url": "https://en.wikipedia.org/wiki?curid=19678805", "title": "Windows on Earth", "text": "Windows on Earth\n\nWindows on Earth is a museum exhibit, website, and exploration tool, developed by TERC, Inc. (an educational non-profit organization, previously called Technical Education Research Centers), and the Association of Space Explorers, that enables the public to explore an interactive, virtual view of Earth from space. In addition, the tool has been selected by NASA to help astronauts identify targets for photography from the International Space Station (ISS).\n\nThe program simulates the view of Earth as seen from a window aboard the ISS, in high-resolution, photographically accurate colors and 3D animations. The views include cloud cover, day and night cycles, night time lights, and other features that help make the exhibit realistic and interactive.\n\nWindows on Earth provides the user a view of Earth from an astronaut's viewpoint, with interactive photorealistic views of Earth as if seen from an altitude of 360 km. The program uses GeoFusion's digital Earth visualization system, which renders accurate views of Earth with terrain, satellite imagery, clouds, and other layers. The system is programmed for user interaction, allowing users to \"fly\" anywhere they wish to see, and zoom in or out to see details. The system's imagery is derived from Landsat, and features 3D perspective views. Former astronaut Jay Apt assisted with the color-correction of the images, to help get the most realistic colors as seen from space. The program is updated daily to include accurate cloud cover information.\n\nWindows on Earth was created by people from the Center for Earth and Space Science Education (CESSE) at TERC, a not-for profit math and science education company located in Cambridge, MA, in partnership with the Association of Space Explorers, GeoFusion's, and WorldSat International and with funding from the National Science Foundation, Informal Science Education.\n\nAdditional partners include the Challenger Learning Center and NASA's Johnson Space Center.\n\nThe Windows on Earth museum exhibit can be found in the Smithsonian National Air and Space Museum, Boston's Museum of Science, the Montshire Museum of Science in Vermont, the St. Louis Science Center, and the Connecticut Science Center.\n\nWindows on Earth flew on board the International Space Station (ISS). On October 12, 2008, Richard Garriott launched aboard Soyuz TMA-13 to the ISS as a Spaceflight participant. Garriott remained on board the station for 10 days, conducting educational and scientific programs and experiments. Earth observation was one of his prime tasks on this mission, and he used the Windows on Earth system to help him take pictures of specific targets.\n\nAfter the mission, Richard's photographs, along with ones taken by his astronaut father Owen Garriott, who flew on Skylab 3 (1973) and STS-9 (1983), were made available to the public through Windows on Earth. This provides a unique opportunity for comparing areas of Earth photographed by two generations of space explorers, showing how Earth's surface (and the technology of Earth observation) has changed over 35 years, from 1973 to 2008.\n\nIn May 2012, NASA selected Windows on Earth as the new tool to help astronauts identify targets for photography from the ISS.\n\n\n"}
{"id": "33959", "url": "https://en.wikipedia.org/wiki?curid=33959", "title": "Witchcraft", "text": "Witchcraft\n\nWitchcraft or witchery broadly means the practice of and belief in magical skills and abilities exercised by solitary practitioners and groups. \"Witchcraft\" is a broad term that varies culturally and societally, and thus can be difficult to define with precision, and cross-cultural assumptions about the meaning or significance of the term should be applied with caution. Witchcraft often occupies a religious divinatory or medicinal role, and is often present within societies and groups whose cultural framework includes a magical world view.\n\nThe concept of witchcraft and the belief in its existence have persisted throughout recorded history. They have been present or central at various times and in many diverse forms among cultures and religions worldwide, including both \"primitive\" and \"highly advanced\" cultures, and continue to have an important role in many cultures today. Scientifically, the existence of magical powers and witchcraft are generally believed to lack credence and to be unsupported by high-quality experimental testing, although individual witchcraft practices and effects may be open to scientific explanation or explained via mentalism and psychology.\n\nHistorically, the predominant concept of witchcraft in the Western world derives from Old Testament laws against witchcraft, and entered the mainstream when belief in witchcraft gained Church approval in the Early Modern Period. It posits a theosophical conflict between good and evil, where witchcraft was generally evil and often associated with the Devil and Devil worship. This culminated in deaths, torture and scapegoating (casting blame for human misfortune), and many years of large scale witch-trials and witch hunts, especially in Protestant Europe, before largely ceasing during the European Age of Enlightenment. Christian views in the modern day are diverse and cover the gamut of views from intense belief and opposition (especially from Christian fundamentalists) to non-belief, and in some churches even approval. From the mid-20th century, witchcraft – sometimes called contemporary witchcraft to clearly distinguish it from older beliefs – became the name of a branch of modern paganism. It is most notably practiced in the Wiccan and modern witchcraft traditions, and no longer practices in secrecy.\n\nThe Western mainstream Christian view is far from the only societal perspective about witchcraft. Many cultures worldwide continue to have widespread practices and cultural beliefs that are loosely translated into English as \"witchcraft\", although the English translation masks a very great diversity in their forms, magical beliefs, practices, and place in their societies. During the Age of Colonialism, many cultures across the globe were exposed to the modern Western world via colonialism, usually accompanied and often preceded by intensive Christian missionary activity \"(see \"Christianization\")\". Beliefs related to witchcraft and magic in these cultures were at times influenced by the prevailing Western concepts. Witch hunts, scapegoating, and killing or shunning of suspected witches still occurs in the modern era, with killings both of victims for their supposedly magical body parts, and of suspected witchcraft practitioners.\n\nSuspicion of modern medicine due to beliefs about illness being due to witchcraft also continues in many countries to this day, with tragic healthcare consequences. HIV/AIDS and Ebola virus disease are two examples of often-lethal infectious disease epidemics whose medical care and containment has been severely hampered by regional beliefs in witchcraft. Other severe medical conditions whose treatment is hampered in this way include tuberculosis, leprosy, epilepsy and the common severe bacterial Buruli ulcer. Public healthcare often requires considerable education work related to epidemology and modern health knowledge in many parts of the world where belief in witchcraft prevails, to encourage effective preventive health measures and treatments, to reduce victim blaming, shunning and stigmatization, and to prevent the killing of people and endangering of animal species for body parts believed to convey magical abilities.\n\nThe word \"witch\" is of uncertain origin. There are numerous etymologies that it could be derived from. One popular belief is that it is \"related to the English words wit, wise, wisdom [Germanic root *weit-, *wait-, *wit-; Indo-European root *weid-, *woid-, *wid-],\" so \"craft of the wise.\" Another is from the Old English wiccecræft, a compound of \"wicce\" (\"witch\") and \"cræft\" (\"craft\").\n\nIn anthropological terminology, witches differ from sorcerers in that they don't use physical tools or actions to curse; their maleficium is perceived as extending from some intangible inner quality, and one may be unaware of being a witch, or may have been convinced of his/her nature by the suggestion of others. This definition was pioneered in a study of central African magical beliefs by E. E. Evans-Pritchard, who cautioned that it might not correspond with normal English usage.\n\nHistorians of European witchcraft have found the anthropological definition difficult to apply to European witchcraft, where witches could equally use (or be accused of using) physical techniques, as well as some who really had attempted to cause harm by thought alone. European witchcraft is seen by historians and anthropologists as an ideology for explaining misfortune; however, this ideology has manifested in diverse ways, as described below.\n\nHistorically the witchcraft label has been applied to practices people believe influence the mind, body, or property of others against their will—or practices that the person doing the labeling believes undermine social or religious order. Some modern commentators believe the malefic nature of witchcraft is a Christian projection. The concept of a magic-worker influencing another person's body or property against their will was clearly present in many cultures, as traditions in both folk magic and religious magic have the purpose of countering malicious magic or identifying malicious magic users. Many examples appear in early texts, such as those from ancient Egypt and Babylonia. Malicious magic users can become a credible cause for disease, sickness in animals, bad luck, sudden death, impotence and other such misfortunes. Witchcraft of a more benign and socially acceptable sort may then be employed to turn the malevolence aside, or identify the supposed evil-doer so that punishment may be carried out. The folk magic used to identify or protect against malicious magic users is often indistinguishable from that used by the witches themselves.\n\nThere has also existed in popular belief the concept of white witches and white witchcraft, which is strictly benevolent. Many neopagan witches strongly identify with this concept, and profess ethical codes that prevent them from performing magic on a person without their request.\n\nWhere belief in malicious magic practices exists, such practitioners are typically forbidden by law as well as hated and feared by the general populace, while beneficial magic is tolerated or even accepted wholesale by the people – even if the orthodox establishment opposes it.\n\nProbably the most widely known characteristic of a witch was the ability to cast a spell, \"spell\" being the word used to signify the means employed to carry out a magical action. A spell could consist of a set of words, a formula or verse, or a ritual action, or any combination of these. Spells traditionally were cast by many methods, such as by the inscription of runes or sigils on an object to give it magical powers; by the immolation or binding of a wax or clay image (poppet) of a person to affect him or her magically; by the recitation of incantations; by the performance of physical rituals; by the employment of magical herbs as amulets or potions; by gazing at mirrors, swords or other specula (scrying) for purposes of divination; and by many other means.\n\nIn Christianity and Islam, sorcery came to be associated with heresy and apostasy and to be viewed as evil. Among the Catholics, Protestants, and secular leadership of the European Late Medieval/Early Modern period, fears about witchcraft rose to fever pitch and sometimes led to large-scale witch-hunts. The key century was the fifteenth, which saw a dramatic rise in awareness and terror of witchcraft, culminating in the publication of the \"Malleus Maleficarum\" but prepared by such fanatical popular preachers as Bernardino of Siena. Throughout this time, it was increasingly believed that Christianity was engaged in an apocalyptic battle against the Devil and his secret army of witches, who had entered into a diabolical pact. In total, tens or hundreds of thousands of people were executed, and others were imprisoned, tortured, banished, and had lands and possessions confiscated. The majority of those accused were women, though in some regions the majority were men. In early modern Scots, the word Warlock came to be used as the male equivalent of witch (which can be male or female, but is used predominantly for females). From this use, the word passed into Romantic literature and ultimately 20th-century popular culture. Accusations of witchcraft were often combined with other charges of heresy against such groups as the Cathars and Waldensians.\n\nThe \"Malleus Maleficarum,\" (Latin for \"Hammer of The Witches\") was a witch-hunting manual written in 1486 by two German monks, Heinrich Kramer and Jacob Sprenger. It was used by both Catholics and Protestants for several hundred years, outlining how to identify a witch, what makes a woman more likely than a man to be a witch, how to put a witch on trial, and how to punish a witch. The book defines a witch as evil and typically female. The book became the handbook for secular courts throughout Renaissance Europe, but was not used by the Inquisition, which even cautioned against relying on the work, and was later officially condemned by the Catholic Church in 1490.\n\nIn the modern Western world, witchcraft accusations have often accompanied the satanic ritual abuse moral panic. Such accusations are a counterpart to blood libel of various kinds, which may be found throughout history across the globe.\n\nThroughout the early modern period, the English term \"witch\" was not exclusively negative in meaning, and could also indicate cunning folk. As Alan McFarlane noted, \"There were a number of interchangeable terms for these practitioners, 'white', 'good', or 'unbinding' witches, blessers, wizards, sorcerers, however 'cunning-man' and 'wise-man' were the most frequent.\" The contemporary Reginald Scot explained, \"At this day it is indifferent to say in the English tongue, 'she is a witch' or 'she is a wise woman'\". Folk magicians throughout Europe were often viewed ambivalently by communities, and were considered as capable of harming as of healing, which could lead to their being accused as \"witches\" in the negative sense. Many English \"witches\" convicted of consorting with demons seem to have been cunning folk whose fairy familiars had been demonised; many French \"devins-guerisseurs\" (\"diviner-healers\") were accused of witchcraft, and over one half the accused witches in Hungary seem to have been healers.\n\nSome of the healers and diviners historically accused of witchcraft have considered themselves mediators between the mundane and spiritual worlds, roughly equivalent to shamans. Such people described their contacts with fairies, spirits often involving out-of-body experiences and travelling through the realms of an \"other-world\".\nBeliefs of this nature are implied in the folklore of much of Europe, and were explicitly described by accused witches in central and southern Europe. Repeated themes include participation in processions of the dead or large feasts, often presided over by a horned male deity or a female divinity who teaches magic and gives prophecies; and participation in battles against evil spirits, \"vampires\", or \"witches\" to win fertility and prosperity for the community.\n\nÉva Pócs states that reasons for accusations of witchcraft fall into four general categories:\n\nShe identifies three varieties of witch in popular belief:\n\"Neighbourhood witches\" are the product of neighbourhood tensions, and are found only in self-sufficient serf village communities where the inhabitants largely rely on each other. Such accusations follow the breaking of some social norm, such as the failure to return a borrowed item, and any person part of the normal social exchange could potentially fall under suspicion. Claims of \"sorcerer\" witches and \"supernatural\" witches could arise out of social tensions, but not exclusively; the supernatural witch in particular often had nothing to do with communal conflict, but expressed tensions between the human and supernatural worlds; and in Eastern and Southeastern Europe such supernatural witches became an ideology explaining calamities that befell entire communities.\n\nBelief in witchcraft continues to be present today in some societies and accusations of witchcraft are the trigger of serious forms of violence, including murder. Such incidents are common in places such as Burkina Faso, Ghana, India, Kenya, Malawi, Nepal and Tanzania. Accusations of witchcraft are sometimes linked to personal disputes, jealousy, and conflicts between neighbors or family over land or inheritance. Witchcraft-related violence is often discussed as a serious issue in the broader context of violence against women.\n\nIn Tanzania, about 500 older women are murdered each year following accusations against them of witchcraft or of being a witch. Apart from extrajudicial violence, there is also state-sanctioned violence in some jurisdictions. For instance, in Saudi Arabia practicing witchcraft and sorcery is a crime punishable by death and the country has executed people for this crime in 2011, 2012 and 2014.\n\nChildren in some regions of the world, such as parts of Africa, are also vulnerable to violence related to witchcraft accusations. Such incidents have also occurred in immigrant communities in the UK, including the much publicized case of the murder of Victoria Climbié.\n\nModern practices identified by their practitioners as \"witchcraft\" have grown dramatically since the early 20th century. Generally portrayed as revivals of pre-Christian European ritual and spirituality, they are understood to involve varying degrees of magic, shamanism, folk medicine, spiritual healing, calling on elementals and spirits, veneration of ancient deities and archetypes, and attunement with the forces of nature.\n\nThe first Neopagan groups to publicly appear, during the 1950s and 60s, were Gerald Gardner's Bricket Wood coven and Roy Bowers' Clan of Tubal Cain. They operated as initiatory secret societies. Other individual practitioners and writers such as Paul Huson also claimed inheritance to surviving traditions of witchcraft.\n\nDuring the 20th century, interest in witchcraft in English-speaking and European countries began to increase, inspired particularly by Margaret Murray's theory of a pan-European witch-cult originally published in 1921, since discredited by further careful historical research. Interest was intensified, however, by Gerald Gardner's claim in 1954 in \"Witchcraft Today\" that a form of witchcraft still existed in England. The truth of Gardner's claim is now disputed too, with different historians offering evidence for or against the religion's existence prior to Gardner.\n\nThe Wicca that Gardner initially taught was a witchcraft religion having a lot in common with Margaret Murray's hypothetically posited cult of the 1920s. Indeed, Murray wrote an introduction to Gardner's \"Witchcraft Today\", in effect putting her stamp of approval on it. Wicca is now practised as a religion of an initiatory secret society nature with positive ethical principles, organised into autonomous covens and led by a High Priesthood. There is also a large \"Eclectic Wiccan\" movement of individuals and groups who share key Wiccan beliefs but have no initiatory connection or affiliation with traditional Wicca. Wiccan writings and ritual show borrowings from a number of sources including 19th and 20th-century ceremonial magic, the medieval grimoire known as the Key of Solomon, Aleister Crowley's Ordo Templi Orientis and pre-Christian religions. Both men and women are equally termed \"witches.\" They practice a form of duotheistic universalism.\n\nSince Gardner's death in 1964, the Wicca that he claimed he was initiated into has attracted many initiates, becoming the largest of the various witchcraft traditions in the Western world, and has influenced other Neopagan and occult movements.\n\nWiccan literature has been described as aiding the empowerment of young women through its lively portrayal of female protagonists. Part of the recent growth in Neo-Pagan religions has been attributed to the strong media presence of fictional works such as the Buffy the Vampire Slayer and Harry Potter series with their depictions of witchcraft. Widespread accessibility to related material through internet media such as chat rooms and forums is also thought to be driving this development. Wiccan beliefs are currently often found to be compatible with liberal ideals such as the Green movement, and particularly with feminism by providing young women with means for empowerment and for control of their own lives. This is the case particularly in North America due to the strong presence of feminist ideals. The 2002 study Enchanted Feminism: The Reclaiming Witches of San Francisco suggests that Wiccan religion represents the second wave of feminism that has also been redefined as a religious movement.\n\nStregheria is an Italian witchcraft religion popularised in the 1980s by Raven Grimassi, who claims that it evolved within the ancient Etruscan religion of Italian peasants who worked under the Catholic upper classes.\n\nModern Stregheria closely resembles Charles Leland's controversial late-19th-century account of a surviving Italian religion of witchcraft, worshipping the Goddess Diana, her brother Dianus/Lucifer, and their daughter Aradia. Leland's witches do not see Lucifer as the evil Satan that Christians see, but a benevolent god of the Sun and Moon).\n\nThe ritual format of contemporary Stregheria is roughly similar to that of other Neopagan witchcraft religions such as Wicca. The pentagram is the most common symbol of religious identity. Most followers celebrate a series of eight festivals equivalent to the Wiccan Wheel of the Year, though others follow the ancient Roman festivals. An emphasis is placed on ancestor worship.\n\nTraditional witchcraft is a term used to refer to a variety of contemporary forms of witchcraft. Pagan studies scholar Ethan Doyle White described it as \"a broad movement of aligned magico-religious groups who reject any relation to Gardnerianism and the wider Wiccan movement, claiming older, more \"traditional\" roots. Although typically united by a shared aesthetic rooted in European folklore, the Traditional Craft contains within its ranks a rich and varied array of occult groups, from those who follow a contemporary Pagan path that is suspiciously similar to Wicca to those who adhere to Luciferianism\". According to British Traditional Witch Michael Howard, the term refers to \"any non-Gardnerian, non-Alexandrian, non-Wiccan or pre-modern form of the Craft, especially if it has been inspired by historical forms of witchcraft and folk magic\". Another definition was offered by Daniel A. Schulke, the current Magister of the Cultus Sabbati, when he proclaimed that \"traditional witchcraft\" \"refers to a coterie of initiatory lineages of ritual magic, spellcraft and devotional mysticism\". Some forms of traditional witchcraft are the Feri Tradition, Cochrane's Craft and the Sabbatic craft.\n\nSatanism is a broad term referring to diverse beliefs that share a symbolic association with, or admiration for, Satan, who is seen as a liberating figure. While it is heir to the same historical period and pre-Enlightenment beliefs that gave rise to modern witchcraft, it is generally seen as completely separate from modern witchcraft and Wicca, and has little or no connection to them.\n\nModern witchcraft considers Satanism to be the \"dark side of Christianity\" rather than a branch of Wicca: – the character of Satan referenced in Satanism exists only in the theology of the three Abrahamic religions, and Satanism arose as, and occupies the role of, a rebellious counterpart to Christianity, in which all is permitted and the self is central. (Christianity can be characterized as having the diametrically opposite views to these.) Such beliefs become more visibly expressed in Europe after the Enlightenment, when works such as Milton's \"Paradise Lost\" were described anew by romantics who suggested that they presented the biblical Satan as an allegory representing crisis of faith, individualism, free will, wisdom and enlightenment; a few works from that time also begin to directly present Satan in a less negative light, such as \"Letters from the Earth\". The two major trends are theistic Satanism and atheistic Satanism; the former venerates Satan as a supernatural patriarchal deity, while the latter views Satan as merely a symbolic embodiment of certain human traits.\n\nOrganized groups began to emerge in the mid 20th century, including the Ophite Cultus Satanas (1948) and The Church of Satan (1966). After seeing Margaret Murray's book \"The God of the Witches\" the leader of Ophite Cultus Satanas, Herbert Arthur Sloane, said he realized that the horned god was Satan (\"Sathanas\"). Sloane also corresponded with his contemporary Gerald Gardner, founder of the Wicca religion, and implied that his views of Satan and the horned god were not necessarily in conflict with Gardner's approach. However, he did believe that, while \"gnosis\" referred to knowledge, and \"Wicca\" referred to wisdom, modern witches had fallen away from the true knowledge, and instead had begun worshipping a fertility god, a reflection of the creator god. He wrote that \"the largest existing body of witches who are true Satanists would be the Yezedees\". Sloane highly recommended the book \"The Gnostic Religion\", and sections of it were sometimes read at ceremonies. It was estimated that there were up to 100,000 Satanists worldwide by 2006, twice the number estimated in 1990. Satanistic beliefs have been largely permitted as a valid expression of religious belief in the West. For example, they were allowed in the British Royal Navy in 2004, and an appeal was considered in 2005 for religious status as a right of prisoners by the Supreme Court of the United States. Contemporary Satanism is mainly an American phenomenon, although it began to reach Eastern Europe in the 1990s around the time of the fall of the Soviet Union.\n\nLuciferianism, on the other hand, is a belief system and does not revere the devil figure or most characteristics typically affixed to Satan. Rather, Lucifer in this context is seen as one of many morning stars, a symbol of enlightenment, independence and human progression. Madeline Montalban was an English witch who adhered to a specific form of luciferianism which revolved around the veneration of Lucifer, or Lumiel, whom she considered to be a benevolent angelic being who had aided humanity's development. Within her Order, she emphasised that her followers discover their own personal relationship with the angelic beings, including Lumiel. Although initially seeming favourable to Gerald Gardner, by the mid-1960s she had become hostile towards him and his Gardnerian tradition, considering him to be \"a 'dirty old man' and sexual pervert.\" She also expressed hostility to another prominent Pagan Witch of the period, Charles Cardell, although in the 1960s became friends with the two Witches at the forefront of the Alexandrian Wiccan tradition, Alex Sanders and his wife, Maxine Sanders, who adopted some of her Luciferian angelic practices. In contemporary times luciferian witches exist within traditional witchcraft.\n\nThe belief in sorcery and its practice seem to have been widespread in the Ancient Near East and Nile Valley. It played a conspicuous role in the cultures of ancient Egypt and in Babylonia. The latter tradition included an Akkadian anti-witchcraft ritual, the Maqlû. A section from the Code of Hammurabi (about 2000 B.C.) prescribes:\n\nAccording to the New Advent Catholic Encyclopedia: \n\nThe King James Version uses the words \"witch\", \"witchcraft\", and \"witchcrafts\" to translate the Masoretic \"kāsháf\" () and (\"qésem\"); these same English terms are used to translate \"pharmakeia\" in the Greek New Testament. Verses such as and (\"Thou shalt not suffer a witch to live\") thus provided scriptural justification for Christian witch hunters in the early modern period (see Christian views on magic).\n\nThe precise meaning of the Hebrew , usually translated as \"witch\" or \"sorceress\", is uncertain. In the Septuagint, it was translated as \"pharmakeía\" or \"pharmakous\". In the 16th century, Reginald Scot, a prominent critic of the witch trials, translated , φαρμακεία, and the Vulgate's Latin equivalent \"veneficos\" as all meaning \"poisoner\", and on this basis, claimed that \"witch\" was an incorrect translation and poisoners were intended. His theory still holds some currency, but is not widely accepted, and in is listed alongside other magic practitioners who could interpret dreams: magicians, astrologers, and Chaldeans. Suggested derivations of include \"mutterer\" (from a single root) or \"herb user\" (as a compound word formed from the roots \"kash\", meaning \"herb\", and \"hapaleh\", meaning \"using\"). The Greek φαρμακεία literally means \"herbalist\" or one who uses or administers drugs, but it was used virtually synonymously with \"mageia\" and \"goeteia\" as a term for a sorcerer.\n\nThe Bible provides some evidence that these commandments against sorcery were enforced under the Hebrew kings:\nNote that the Hebrew word \"ob\", translated as \"familiar spirit\" in the above quotation, has a different meaning than the usual English sense of the phrase; namely, it refers to a spirit that the woman is familiar with, rather than to a spirit that physically manifests itself in the shape of an animal.\n\nThe New Testament condemns the practice as an abomination, just as the Old Testament had (Galatians 5:20, compared with Revelation 21:8; 22:15; and Acts 8:9; 13:6). The word in most New Testament translations is \"sorcerer\"/\"sorcery\" rather than \"witch\"/\"witchcraft\".\n\nJewish law views the practice of witchcraft as being laden with idolatry and/or necromancy; both being serious theological and practical offenses in Judaism. Although Maimonides vigorously denied the efficacy of all methods of witchcraft, and claimed that the Biblical prohibitions regarding it were precisely to wean the Israelites from practices related to idolatry. It is acknowledged that while magic exists, it is forbidden to practice it on the basis that it usually involves the worship of other gods. Rabbis of the Talmud also condemned magic when it produced something other than illusion, giving the example of two men who use magic to pick cucumbers (Sanhedrin 67a). The one who creates the illusion of picking cucumbers should not be condemned, only the one who actually picks the cucumbers through magic.\n\nHowever, some of the rabbis practiced \"magic\" themselves or taught the subject. For instance, Rabbah created a person and sent him to Rav Zeira, and Hanina and Hoshaiah studied every Friday together and created a small calf to eat on Shabbat (Sanhedrin 67b). In these cases, the \"magic\" was seen more as divine miracles (i.e., coming from God rather than \"unclean\" forces) than as witchcraft.\n\nJudaism does make it clear that Jews shall not try to learn about the ways of witches (Book of Deuteronomy 18: 9–10) and that witches are to be put to death (Exodus 22:17).\n\nJudaism's most famous reference to a medium is undoubtedly the Witch of Endor whom Saul consults, as recounted in 1 Samuel 28.\n\nDivination, and magic in Islam, encompass a wide range of practices, including black magic, warding off the evil eye, the production of amulets and other magical equipment, evocation, casting lots, and astrology. Muslims do commonly believe in magic (\"sihr\") and explicitly forbid its practice. \"Sihr\" translates from Arabic as sorcery or black magic. The best known reference to magic in Islam is surah al-Falaq of the Qur'an, which is known as a prayer to God to ward off black magic:\nAlso according to the Qur'an:\nIslam distinguishes between God-given gifts, which can heal sickness, and possession, and sorcery. Good supernatural powers are therefore a \"special gift from God\", whereas sorcery or black magic is achieved through help of jinn and demons. In the Qurʾānic narrative, the Prophet Sulayman had the power to speak with animals and command jinn, and he thanks God for this نعمة (i.e. gift, privilege, favour, bounty), which is only given to him with God’s permission. The Prophet Muhammad was accused of being a magician by his opponents.\n\nIt is a common belief that jinn can possess a human, thus requiring exorcism (\"ruqya\") derived from the Prophet's \"sunnah\" to cast off the jinn or devils from the body of the possessed. The practice of seeking help from the jinn is prohibited and can lead to possession. The \"ruqya\" contains verses of the Qur'an as well as prayers specifically targeted against demons. The knowledge of which verses of the Qur'an to use in what way is what is considered \"magic knowledge.\"\n\nA \"hadith\" recorded in states: \"Seventy thousand people of my followers will enter Paradise without accounts, and they are those who do not practice Ar-Ruqya and do not see an evil omen in things, and put their trust in their Lord.\" Ibn Qayyim al-Jawziyya, a scholar, commented on this \"hadith\", stating: That is because these people will enter Paradise without being called to account because of the perfection of their Tawheed, therefore he described them as people who did not ask others to perform ruqyah for them. Hence he said \"and they put their trust in their Lord.\" Because of their complete trust in their Lord, their contentment with Him, their faith in Him, their being pleased with Him and their seeking their needs from Him, they do not ask people for anything, be it ruqyah or anything else, and they are not influenced by omens and superstitions that could prevent them from doing what they want to do, because superstition detracts from and weakens Tawheed\".\n\nIbn al-Nadim hold, exorcists gain their power by their obedience to God, while sorcerers please the demons by acts of disobedience and sacrifices and they in return do him a favor. Being pious and strictly following the teachings of the Qur'an can increase the probability to perform magic or miracles, that is distinguished from witchcraft, the latter practised in aid with demons.\n\nA \"hadith\" recorded in narrates that one who has eaten seven Ajwa dates in the morning will not be adversely affected by magic in the course of that day.\n\nStudents of the history of religion have linked several magical practises in Islam with pre-Islamic Turkish and East African customs. Most notable of these customs is the Zār.\n\nIn Southern African traditions, there are three classifications of somebody who uses magic. The \"tagati\" is usually improperly translated into English as \"witch\", and is a spiteful person who operates in secret to harm others. The \"sangoma\" is a diviner, somewhere on a par with a fortune teller, and is employed in detecting illness, predicting a person's future (or advising them on which path to take), or identifying the guilty party in a crime. She also practices some degree of medicine. The \"inyanga\" is often translated as \"witch doctor\" (though many Southern Africans resent this implication, as it perpetuates the mistaken belief that a \"witch doctor\" is in some sense a \"practitioner\" of malicious magic). The \"inyanga\"s job is to heal illness and injury and provide customers with magical items for everyday use. Of these three categories the \"tagati\" is almost exclusively female, the \"sangoma\" is usually female, and the \"inyanga\" is almost exclusively male.\n\nMuch of what witchcraft represents in Africa has been susceptible to misunderstandings and confusion, thanks in no small part to a tendency among western scholars since the time of the now largely discredited Margaret Murray to approach the subject through a comparative lens vis-a-vis European witchcraft. Okeja argues that witchcraft in Africa today plays a very different social role than in Europe of the past—or present—and should be understood through an African rather than post-colonial Western lens.\n\nComplimentary remarks about witchcraft by a native Congolese initiate: \"From witchcraft ... may be developed the remedy (\"kimbuki\") that will do most to raise up our country.\" \"Witchcraft ... deserves respect ... it can embellish or redeem (\"ketula evo vuukisa\").\" \"The ancestors were equipped with the protective witchcraft of the clan (\"kindoki kiandundila kanda\"). ... They could also gather the power of animals into their hands ... whenever they needed. ... If we could make use of these kinds of witchcraft, our country would rapidly progress in knowledge of every kind.\" \"You witches (\"zindoki\") too, bring your science into the light to be written down so that ... the benefits in it ... endow our race.\"\n\nIn eastern Cameroon, the term used for witchcraft among the Maka is \"djambe\" and refers to a force inside a person; its powers may make the proprietor more vulnerable. It encompasses the occult, the transformative, killing and healing.\n\nIn some Central African areas, malicious magic users are believed by locals to be the source of terminal illness such as AIDS and cancer. In such cases, various methods are used to rid the person from the bewitching spirit, occasionally physical and psychological abuse. Children may be accused of being witches, for example a young niece may be blamed for the illness of a relative. Most of these cases of abuse go unreported since the members of the society that witness such abuse are too afraid of being accused of being accomplices. It is also believed that witchcraft can be transmitted to children by feeding. Parents discourage their children from interacting with people believed to be witches.\n\nEvery year, hundreds of people in the Central African Republic are convicted of witchcraft.\n\nChristian militias in the Central African Republic have also kidnapped, burnt and buried alive women accused of being 'witches' in public ceremonies.\n\n, between 25,000 and 50,000 children in Kinshasa, Democratic Republic of the Congo, had been accused of witchcraft and thrown out of their homes. These children have been subjected to often-violent abuse during exorcisms, sometimes supervised by self-styled religious pastors. Other pastors and Christian activists strongly oppose such accusations and try to rescue children from their unscrupulous colleagues. The usual term for these children is \"enfants sorciers\" (child witches) or \"enfants dits sorciers\" (children accused of witchcraft). In 2002, USAID funded the production of two short films on the subject, made in Kinshasa by journalists Angela Nicoara and Mike Ormsby.\n\nIn April 2008, in Kinshasa, the police arrested 14 suspected victims (of penis snatching) and sorcerers accused of using black magic or witchcraft to steal (make disappear) or shrink men's penises to extort cash for cure, amid a wave of panic.\n\nAccording to one study, the belief in magical warfare technologies (such as \"bulletproofing\") in the Eastern Democratic Republic of the Congo serves a group-level function, as it increases group efficiency in warfare, even if it is suboptimal at the individual level. The authors of the study argue that this is one reason why the belief in witchcraft persists.\n\nIn Ghana, women are often accused of witchcraft and attacked by neighbours. Because of this, there exist six witch camps in the country where women suspected of being witches can flee for safety. The witch camps, which exist solely in Ghana, are thought to house a total of around 1000 women. Some of the camps are thought to have been set up over 100 years ago. The Ghanaian government has announced that it intends to close the camps.\n\nArrests were made in an effort to avoid bloodshed seen in Ghana a decade ago, when 12 alleged penis snatchers were beaten to death by mobs. While it is easy for modern people to dismiss such reports, Uchenna Okeja argues that a belief system in which such magical practices are deemed possible offer many benefits to Africans who hold them. For example, the belief that a sorcerer has \"stolen\" a man's penis functions as an anxiety-reduction mechanism for men suffering from impotence while simultaneously providing an explanation that is consistent with African cultural beliefs rather than appealing to Western scientific notions that are tainted by the history of colonialism (at least for many Africans).\n\nIt was reported on May 21, 2008 that in Kenya, a mob had burnt to death at least 11 people accused of witchcraft.\n\nIn Malawi it is also common practice to accuse children of witchcraft and many children have been abandoned, abused and even killed as a result. As in other African countries both African traditional healers and their Christian counterparts are trying to make a living out of exorcising children and are actively involved in pointing out children as witches. Various secular and Christian organizations are combining their efforts to address this problem.\n\nAccording to William Kamkwamba, witches and wizards are afraid of money, which they consider a rival evil. Any contact with cash will snap their spell and leave the wizard naked and confused. So placing cash, such as kwacha around a room or bed mat will protect the resident from their malevolent spells.\n\nIn Nigeria, several Pentecostal pastors have mixed their evangelical brand of Christianity with African beliefs in witchcraft to benefit from the lucrative witch finding and exorcism business—which in the past was the exclusive domain of the so-called witch doctor or traditional healers. These pastors have been involved in the torturing and even killing of children accused of witchcraft. Over the past decade, around 15,000 children have been accused, and around 1,000 murdered. Churches are very numerous in Nigeria, and competition for congregations is hard. Some pastors attempt to establish a reputation for spiritual power by \"detecting\" child witches, usually following a death or loss of a job within a family, or an accusation of financial fraud against the pastor. In the course of \"exorcisms\", accused children may be starved, beaten, mutilated, set on fire, forced to consume acid or cement, or buried alive. While some church leaders and Christian activists have spoken out strongly against these abuses, many Nigerian churches are involved in the abuse, although church administrations deny knowledge of it.\n\nAmong the Mende (of Sierra Leone), trial and conviction for witchcraft has a beneficial effect for those convicted. \"The witchfinder had warned the whole village to ensure the relative prosperity of the accused and sentenced ... old people. ... Six months later all of the people ... accused, were secure, well-fed and arguably happier than at any [previous] time; they had hardly to beckon and people would come with food or whatever was needful. ... Instead of such old and widowed people being left helpless or (as in Western society) institutionalized in old people's homes, these were reintegrated into society and left secure in their old age ... . ... Old people are 'suitable' candidates for this kind of accusation in the sense that they are isolated and vulnerable, and they are 'suitable' candidates for 'social security' for precisely the same reasons.\"\n\nIn Kuranko language, the term for witchcraft is \"suwa'ye\" referring to \"extraordinary powers\".\n\nIn Tanzania in 2008, President Kikwete publicly condemned witchdoctors for killing albinos for their body parts, which are thought to bring good luck. 25 albinos have been murdered since March 2007. In Tanzania, albinos are often murdered for their body parts on the advice of witch doctors in order to produce powerful amulets that are believed to protect against witchcraft and make the owner prosper in life.\n\nBrua is an Afro-Caribbean religion and healing tradition that originates in Aruba, Bonaire, and Curaçao, in the Dutch Caribbean. A healer in this culture is called a \"kurioso\" or \"kuradó\", a man or woman who performs \"trabou chikí\" (little works) and \"trabou grandi\" (large treatments) to promote or restore health, bring fortune or misfortune, deal with unrequited love, and more serious concerns, in which sorcery is involved. Sorcery usually involves reference to the \"almasola\" or \"homber chiki\", a devil-like entity. \"Transcultural Psychiatry\" published a paper called \"Traditional healing practices originating in Aruba, Bonaire, and Curaçao: A review of the literature on psychiatry and Brua\" by Jan Dirk Blom, Igmar T. Poulina, Trevor L. van Gellecum and Hans W. Hoek of the Parnassia Psychiatric Institute.\n\nIn 1645, Springfield, Massachusetts, experienced America's first accusations of witchcraft when husband and wife Hugh and Mary Parsons accused each other of witchcraft. At America's first witch trial, Hugh was found innocent, while Mary was acquitted of witchcraft but sentenced to be hanged for the death of her child. She died in prison. From 1645–1663, about eighty people throughout England's Massachusetts Bay Colony were accused of practicing witchcraft. Thirteen women and two men were executed in a witch-hunt that lasted throughout New England from 1645–1663.\n\nThe Salem witch trials followed in 1692–93. These witch trials were the most famous in British North America and took place in the coastal settlements near Salem, Massachusetts. Prior to the witch trials, nearly 300 men and women had been suspected of partaking in witchcraft, and 19 of these people were hanged, and one was “pressed to death”. The Salem witch trials were a series of hearings before local magistrates followed by county court trials to prosecute people accused of witchcraft in Essex, Suffolk and Middlesex Counties of colonial Massachusetts, between February 1692 and May 1693. Over 150 people were arrested and imprisoned, with even more accused who were not formally pursued by the authorities. The two courts convicted 29 people of the capital felony of witchcraft. Nineteen of the accused, 14 women and 5 men, were hanged. One man who refused to enter a plea was crushed to death under heavy stones in an attempt to force him to do so. At least five more of the accused died in prison.\n\nDespite being generally known as the \"Salem\" witch trials, the preliminary hearings in 1692 were conducted in a variety of towns across the province: Salem Village (now Danvers), Salem Town, Ipswich, and Andover. The best known trials were conducted by the Court of Oyer and Terminer in 1692 in Salem Town. All 26 who went to trial before this court were convicted. The four sessions of the Superior Court of Judicature in 1693, held in Salem Town, but also in Ipswich, Boston, and Charlestown, produced only 3 convictions in the 31 witchcraft trials it conducted. Likewise, alleged witchcraft was not isolated to New England. In 1706 Grace Sherwood the \"Witch of Pungo\" was imprisoned for the crime in Princess Anne County, Virginia.\n\nIn Maryland, there is a legend of Moll Dyer, who escaped a fire set by fellow colonists only to die of exposure in December 1697. The historical record of Dyer is scant as all official records were burned in a courthouse fire, though the county courthouse has on display the rock where her frozen body was found. A letter from a colonist of the period describes her in most unfavourable terms. A local road is named after Dyer, where her homestead was said to have been. Many local families have their own version of the Moll Dyer affair, and her name is spoken with care in the rural southern counties.\n\nAccusations of witchcraft and wizardry led to the prosecution of a man in Tennessee as recently as 1833. \"The Crucible\" by Arthur Miller is a dramatized and partially fictionalized story of the Salem witch trials that took place in the Massachusetts Bay Colony during 1692–93.\n\nIn Diné culture, witches are seen as the polar opposite of ceremonial people. While spiritual leaders perform \"sings\" for healing, protection and other beneficial purposes, all practices referred to as \"witchcraft\" are intended to hurt and curse. Witches are associated with harm to the community and transgression of societal standards, especially those relating to family and the dead.\n\nThe \"yee naaldlooshii\" is the type of witch known in English as a \"skin-walker\". They are believed to take the forms of animals in order to travel in secret and do harm to the innocent. In the Navajo language, ' translates to \"with it, he goes on all fours\". While perhaps the most common variety seen in horror fiction by non-Navajo people, the ' is one of several varieties of Navajo witch, specifically a type of \"\".\n\nCorpse powder or corpse poison (, literally \"witchery\" or \"harming\") is a substance made from powdered corpses. The powder is used by witches to curse their victims. The effect of the \"\" is a curse and disease, usually indicated by an immediate action to administration of the poison, like fainting, swelling of the tongue, or lockjaw. Sometimes, however, the victims simply wastes away, as from a normal disease.\n\nTraditional Navajos usually hesitate to discuss things like witches and witchcraft with non-Navajos.\n\nWitchcraft was also an important part of the social and cultural history of late-Colonial Mexico, during the Mexican Inquisition. Spanish Inquisitors viewed witchcraft as a problem that could be cured simply through confession. Yet, as anthropologist Ruth Behar writes, witchcraft, not only in Mexico but in Latin America in general, was a \"conjecture of sexuality, witchcraft, and religion, in which Spanish, indigenous, and African cultures converged.\" Furthermore, witchcraft in Mexico generally required an interethnic and interclass network of witches. Yet, according to anthropology professor Laura Lewis, witchcraft in colonial Mexico ultimately represented an \"affirmation of hegemony\" for women, Indians, and especially Indian women over their white male counterparts as a result of the casta system.\n\nIn modern history, notoriety has been awarded to a place called Catemaco, in the state of Veracruz, which has a history of witchcraft, and where the practice of witchcraft by contemporary \"brujos\" and \"brujas\" thrives.\n\nIn Mexico City, people who practice brujería, Santería, voodoo, ocultism and magic may find items, herbs and supplies at the \"mercado de Sonora\".\n\nIn Chile there is a tradition of the Kalku in Mapuche religion; and the Warlocks of Chiloé in the folklore and Chilote mythology.\n\nThe presence of the witch is a constant in the ethnographic history of colonial Brazil, especially during the several denunciations and confessions given to the Congregation for the Doctrine of the Faith of Bahia (1591–1593), Pernambuco and Paraíba (1593–1595).\n\nBelief in the supernatural is strong in all parts of India, and lynchings for witchcraft are reported in the press from time to time. Around 750 people were killed as witches in Assam and West Bengal between 2003 and 2008. Officials in the state of Chhattisgarh reported in 2008 that at least 100 women are maltreated annually as suspected witches. A local activist stated that only a fraction of cases of abuse are reported. In Indian mythology, a common perception of a witch is a being with her feet pointed backwards.\n\nApart from other types of Violence against women in Nepal, the malpractice of abusing women in the name of witchcraft is also really prominent. According to the statistics in 2013, there was a total of 69 reported cases of abuse to women due to accusation of performing witchcraft. The perpetrators of this malpractice are usually neighbors, so-called witch doctors and family members. The main causes of these malpractices are lack of education, lack of awareness and superstition. According to the statistics by INSEC, the age group of women who fall victims to the witchcraft violence in Nepal is 20–40.\n\nIn Japanese folklore, the most common types of witch can be separated into two categories: those who employ snakes as familiars, and those who employ foxes.\n\nThe fox witch is, by far, the most commonly seen witch figure in Japan. Differing regional beliefs set those who use foxes into two separate types: the \"kitsune-mochi\", and the \"tsukimono-suji\". The first of these, the \"kitsune-mochi\", is a solitary figure who gains his fox familiar by bribing it with its favourite foods. The \"kitsune-mochi\" then strikes up a deal with the fox, typically promising food and daily care in return for the fox's magical services. The fox of Japanese folklore is a powerful trickster in and of itself, imbued with powers of shape changing, possession, and illusion. These creatures can be either nefarious; disguising themselves as women in order to trap men, or they can be benign forces as in the story of \"The Grateful foxes\". However, once a fox enters the employ of a human it almost exclusively becomes a force of evil to be feared. A fox under the employ of a human can provide many services. The fox can turn invisible and find secrets its master desires. It can apply its many powers of illusion to trick and deceive its master's enemies. The most feared power of the \"kitsune-mochi\" is the ability to command his fox to possess other humans. This process of possession is called Kitsunetsuki.\n\nBy far, the most commonly reported cases of fox witchcraft in modern Japan are enacted by \"tsukimono-suji\" families, or \"hereditary witches\". The \"Tsukimono-suji\" is traditionally a family who is reported to have foxes under their employ. These foxes serve the family and are passed down through the generations, typically through the female line. \"Tsukimono-suji\" foxes are able to supply much in the way of the same mystical aid that the foxes under the employ of a \"kitsune-mochi\" can provide its more solitary master with. In addition to these powers, if the foxes are kept happy and well taken care of, they bring great fortune and prosperity to the \"Tsukimono-suji\" house. However, the aid in which these foxes give is often overshadowed by the social and mystical implications of being a member of such a family. In many villages, the status of local families as \"tsukimono-suji\" is often common, everyday knowledge. Such families are respected and feared, but are also openly shunned. Due to its hereditary nature, the status of being \"Tsukimono-suji\" is considered contagious. Because of this, it is often impossible for members of such a family to sell land or other properties, due to fear that the possession of such items will cause foxes to inundate one's own home. In addition to this, because the foxes are believed to be passed down through the female line, it is often nearly impossible for women of such families to find a husband whose family will agree to have him married to a \"tsukimono-suji\" family. In such a union the woman's status as a \"Tsukimono-suji\" would transfer to any man who married her.\n\nWitchcraft in the Philippines is often classified as malevolent, with practitioners of black magic called \"Mangkukulam\" in Tagalog and \"Mambabarang\" in Cebuano; there are also practitioners of benevolent, white magic, in addition to some who practise both. \"Mambabarang\" in particular are noted for their ability to command insects and other invertebrates to accomplish a task, such as delivering a curse to a target.\n\nMagic and witchcraft in the Philippines varies considerably across the different ethnic groups, and is commonly a modern manifestation of pre-Colonial spirituality interwoven with Catholic religious elements such as the invocation of saints and the use of pseudo-Latin prayers (\"oración\") in spells, and \"anting-anting\" (amulets).\n\nPractitioners of traditional herbal-based medicine and divination called \"albularyo\" are not considered witches. They are perceived to be either quack doctors or a quasi-magical option when western medicine fails to identify or cure an ailment that is thus suspected to be of supernatural, often malevolent, origin. Feng shui, an influence of Filipino Chinese culture, is also not classified as witchcraft as it is considered a separate realm of belief altogether.\n\nSaudi Arabia continues to use the death penalty for sorcery and witchcraft. In 2006 Fawza Falih Muhammad Ali was condemned to death for practicing witchcraft. There is no legal definition of sorcery in Saudi, but in 2007 an Egyptian pharmacist working there was accused, convicted, and executed. Saudi authorities also pronounced the death penalty on a Lebanese television presenter, Ali Hussain Sibat, while he was performing the \"hajj\" (Islamic pilgrimage) in the country.\n\nIn 2009 the Saudi authorities set up the Anti-Witchcraft Unit of their Committee for the Promotion of Virtue and the Prevention of Vice police.\n\nIn April 2009, a Saudi woman Amina Bint Abdulhalim Nassar was arrested and later sentenced to death for practicing witchcraft and sorcery. In December 2011, she was beheaded. A Saudi man has been beheaded on charges of sorcery and witchcraft in June 2012. A beheading for sorcery occurred in 2014.\n\nIn June 2015, Yahoo reported: \"The Islamic State group has beheaded two women in Syria on accusations of \"sorcery,\" the first such executions of female civilians in Syria, the Syrian Observatory for Human Rights said Tuesday.\"\nISIS decapitated a man in Iraq over sorcery.\n\nAn expedition sent to what is now the Xinjiang region of western China by the PBS documentary series \"Nova\" found a fully clothed female Tocharian mummy wearing a black conical hat of the type now associated with witches in Europe in the storage area of a small local museum, indicative of an Indo-European priestess.\n\nWitchcraft in Europe between 500–1750 was believed to be a combination of sorcery and heresy. While sorcery attempts to produce negative supernatural effects through formulas and rituals, heresy is the Christian contribution to witchcraft in which an individual makes a pact with the Devil. In addition, heresy denies witches the recognition of important Christian values such as baptism, salvation, Christ and sacraments. The beginning of the witch accusations in Europe took place in the 14th and 15th centuries; however as the social disruptions of the 16th century took place, witchcraft trials intensified. \nIn Early Modern European tradition, witches were stereotypically, though not exclusively, women. European pagan belief in witchcraft was associated with the goddess Diana and dismissed as \"diabolical fantasies\" by medieval Christian authors. Witch-hunts first appeared in large numbers in southern France and Switzerland during the 14th and 15th centuries. The peak years of witch-hunts in southwest Germany were from 1561 to 1670.\n\nIt was commonly believed that individuals with power and prestige were involved in acts of witchcraft and even cannibalism. Because Europe had a lot of power over individuals living in West Africa, Europeans in positions of power were often accused of taking part in these practices. Though it is not likely that these individuals were actually involved in these practices, they were most likely associated due to Europe’s involvement in things like the slave trade, which negatively affected the lives of many individuals in the Atlantic World throughout the fifteenth through seventeenth centuries.\n\nThe familiar witch of folklore and popular superstition is a combination of numerous influences. The characterization of the witch as an evil magic user developed over time.\n\nEarly converts to Christianity looked to Christian clergy to work magic more effectively than the old methods under Roman paganism, and Christianity provided a methodology involving saints and relics, similar to the gods and amulets of the Pagan world. As Christianity became the dominant religion in Europe, its concern with magic lessened.\n\nThe Protestant Christian explanation for witchcraft, such as those typified in the confessions of the Pendle witches, commonly involves a diabolical pact or at least an appeal to the intervention of the spirits of evil. The witches or wizards engaged in such practices were alleged to reject Jesus and the sacraments; observe \"the witches' sabbath\" (performing infernal rites that often parodied the Mass or other sacraments of the Church); pay Divine honour to the Prince of Darkness; and, in return, receive from him preternatural powers. It was a folkloric belief that a Devil's Mark, like the brand on cattle, was placed upon a witch's skin by the devil to signify that this pact had been made. Witches were most often characterized as women. Witches disrupted the societal institutions, and more specifically, marriage. It was believed that a witch often joined a pact with the devil to gain powers to deal with infertility, immense fear for her children's well-being, or revenge against a lover. They were also depicted as lustful and perverted, and it was thought that they copulated with the devil at the Sabbath.\n\nThe Church and European society were not always so zealous in hunting witches or blaming them for misfortunes. Saint Boniface declared in the 8th century that belief in the existence of witches was un-Christian. The emperor Charlemagne decreed that the burning of supposed witches was a pagan custom that would be punished by the death penalty. In 820 the Bishop of Lyon and others repudiated the belief that witches could make bad weather, fly in the night, and change their shape. This denial was accepted into Canon law . Other rulers such as King Coloman of Hungary declared that witch-hunts should cease because witches (more specifically, strigas) do not exist.\n\nThe Church did not invent the idea of witchcraft as a potentially harmful force whose practitioners should be put to death. This idea is commonplace in pre-Christian religions. According to the scholar Max Dashu, the concept of medieval witchcraft contained many of its elements even before the emergence of Christianity. These can be found in Bacchanalias, especially in the time when they were led by priestess Paculla Annia (188BC–186BC).\n\nPowers typically attributed to European witches include turning food poisonous or inedible, flying on broomsticks or pitchforks, casting spells, cursing people, making livestock ill and crops fail, and creating fear and local chaos.\n\nHowever, even at a later date, not all witches were assumed to be harmful practicers of the craft. In England, the provision of this curative magic was the job of a witch doctor, also known as a cunning man, white witch, or wise man. The term \"witch doctor\" was in use in England before it came to be associated with Africa. Toad doctors were also credited with the ability to undo evil witchcraft. (Other folk magicians had their own purviews. Girdle-measurers specialised in diagnosing ailments caused by fairies, while magical cures for more mundane ailments, such as burns or toothache, could be had from charmers.)\n\nHistorians Keith Thomas and his student Alan Macfarlane study witchcraft by combining historical research with concepts drawn from anthropology. They argued that English witchcraft, like African witchcraft, was endemic rather than epidemic. Older women were the favorite targets because they were marginal, dependent members of the community and therefore more likely to arouse feelings of both hostility and guilt, and less likely to have defenders of importance inside the community. Witchcraft accusations were the village's reaction to the breakdown of its internal community, coupled with the emergence of a newer set of values that was generating psychic stress.\nIn Wales, fear of witchcraft mounted around the year 1500. There was a growing alarm of women's magic as a weapon aimed against the state and church. The Church made greater efforts to enforce the canon law of marriage, especially in Wales where tradition allowed a wider range of sexual partnerships. There was a political dimension as well, as accusations of witchcraft were levied against the enemies of Henry VII, who was exerting more and more control over Wales.\n\nThe records of the Courts of Great Sessions for Wales, 1536–1736 show that Welsh custom was more important than English law. Custom provided a framework of responding to witches and witchcraft in such a way that interpersonal and communal harmony was maintained, Showing to regard to the importance of honour, social place and cultural status. Even when found guilty, execution did not occur.\n\nBecoming king in 1603, James I Brought to England and Scotland continental explanations of witchcraft. His goal was to divert suspicion away from male homosociality among the elite, and focus fear on female communities and large gatherings of women. He thought they threatened his political power so he laid the foundation for witchcraft and occultism policies, especially in Scotland. The point was that a widespread belief in the conspiracy of witches and a witches' Sabbath with the devil deprived women of political influence. Occult power was supposedly a womanly trait because women were weaker and more susceptible to the devil.\n\nIn 1944 Helen Duncan was the last person in Britain to be imprisoned for fraudulently claiming to be a witch.\n\nIn the United Kingdom children believed to be witches or seen as possessed by evil spirits can be subject to severe beatings, traumatic exorcism, and/or other abuse. There have even been child murders associated with witchcraft beliefs. The problem is particularly serious among immigrant or former immigrant communities of African origin but other communities, such as those of Asian origin are also involved. Step children and children seen as different for a wide range of reasons are particularly at risk of witchcraft accusations. Children may be beaten or have chilli rubbed into their eyes during exorcisms. This type of abuse is frequently hidden and can include torture. A 2006 recommendation to record abuse cases linked to witchcraft centrally has not yet been implemented. Lack of awareness among social workers, teachers and other professionals dealing with at risk children hinders efforts to combat the problem.\n\nThere is a 'money making scam' involved. Pastors accuse a child of being a witch and later the family pays for exorcism. If a child at school says that his/her pastor called the child a witch that should become a child safeguarding issue.\n\nAs in most European countries, women in Italy were more likely suspected of witchcraft than men. Women were considered dangerous due to their supposed sexual instability, such as when being aroused, and also due to the powers of their menstrual blood.\n\nIn the 16th century, Italy had a high portion of witchcraft trials involving love magic. The country had a large number of unmarried people due to men marrying later in their lives during this time. This left many women on a desperate quest for marriage leaving them vulnerable to the accusation of witchcraft whether they took part in it or not. Trial records from the Inquisition and secular courts discovered a link between prostitutes and supernatural practices. Professional prostitutes were considered experts in love and therefore knew how to make love potions and cast love related spells. Up until 1630, the majority of women accused of witchcraft were prostitutes. A courtesan was questioned about her use of magic due to her relationship with men of power in Italy and her wealth. The majority of women accused were also considered \"outsiders\" because they were poor, had different religious practices, spoke a different language, or simply from a different city/town/region. Cassandra from Ferrara, Italy, was still considered a foreigner because not native to Rome where she was residing. She was also not seen as a model citizen because her husband was in Venice.\n\nFrom the 16th-18th centuries, the Catholic Church enforced moral discipline throughout Italy. With the help of local tribunals, such as in Venice, the two institutions investigated a woman's religious behaviors when she was accused of witchcraft.\n\nFranciscan friars from New Spain introduced Diabolism, belief in the devil, to the indigenous people after their arrival in 1524.\nBartolomé de las Casas believed that human sacrifice was not diabolic, in fact far off from it, and was a natural result of religious expression.\nMexican Indians gladly took in the belief of Diabolism and still managed to keep their belief in creator-destroyer deities.\n\nIn pre-Christian times, witchcraft was a common practice in the Cook Islands. The native name for a sorcerer was \"tangata purepure\" (a man who prays). The prayers offered by the \"ta'unga\" (priests) to the gods worshiped on national or tribal \"marae\" (temples) were termed \"karakia\"; those on minor occasions to the lesser gods were named \"pure\". All these prayers were metrical, and were handed down from generation to generation with the utmost care. There were prayers for every such phase in life; for success in battle; for a change in wind (to overwhelm an adversary at sea, or that an intended voyage be propitious); that his crops may grow; to curse a thief; or wish ill-luck and death to his foes. Few men of middle age were without a number of these prayers or charms. The succession of a sorcerer was from father to son, or from uncle to nephew. So too of sorceresses: it would be from mother to daughter, or from aunt to niece. Sorcerers and sorceresses were often slain by relatives of their supposed victims.\n\nA singular enchantment was employed to kill off a husband of a pretty woman desired by someone else. The expanded flower of a Gardenia was stuck upright—a very difficult performance—in a cup (i.e., half a large coconut shell) of water. A prayer was then offered for the husbands speedy death, the sorcerer earnestly watching the flower. Should it fall the incantation was successful. But if the flower still remained upright, he will live. The sorcerer would in that case try his skill another day, with perhaps better success.\n\nAccording to Beatrice Grimshaw, a journalist who visited the Cook Islands in 1907, the uncrowned Queen Makea was believed to have possessed the mystic power called \"mana\", giving the possessor the power to slay at will. It also included other gifts, such as second sight to a certain extent, the power to bring good or evil luck, and the ability already mentioned to deal death at will.\n\nA local newspaper informed that more than 50 people were killed in two Highlands provinces of Papua New Guinea in 2008 for allegedly practicing witchcraft. An estimated 50–150 alleged witches are killed each year in Papua New Guinea.\n\nAmong the Russian words for \"witch\", ведьма (ved'ma) literally means \"one who knows\", from Old Slavic вѣдъ \"to know\"). Another frequent term is колдунья (koldun'ya), \"sorcerer\" being колдун (koldun).\n\nPagan practices formed a part of Russian and Eastern Slavic culture; the Russian people were deeply superstitious. The witchcraft practiced consisted mostly of earth magic and herbology; it was not so significant which herbs were used in practices, but how these herbs were gathered. Ritual centered on harvest of the crops and the location of the sun was very important. One source, pagan author Judika Illes, tells that herbs picked on Midsummer's Eve were believed to be most powerful, especially if gathered on Bald Mountain near Kiev during the witches' annual revels celebration. Botanicals should be gathered, \"During the seventeenth minute of the fourteenth hour, under a dark moon, in the thirteenth field, wearing a red dress, pick the twelfth flower on the right.\"\n\nSpells also served for midwifery, shape-shifting, keeping lovers faithful, and bridal customs. Spells dealing with midwifery and childbirth focused on the spiritual wellbeing of the baby. Shape-shifting spells involved invocation of the wolf as a spirit animal. To keep men faithful, lovers would cut a ribbon the length of his erect penis and soak it in his seminal emissions after sex while he was sleeping, then tie seven knots in it; keeping this talisman of knot magic ensured loyalty. Part of an ancient pagan marriage tradition involved the bride taking a ritual bath at a bathhouse before the ceremony. Her sweat would be wiped from her body using raw fish, and the fish would be cooked and fed to the groom.\n\nDemonism, or black magic, was not prevalent. Persecution for witchcraft, mostly involved the practice of simple earth magic, founded on herbology, by solitary practitioners with a Christian influence. In one case investigators found a locked box containing something bundled in a kerchief and three paper packets, wrapped and tied, containing crushed grasses. Most rituals of witchcraft were very simple—one spell of divination consists of sitting alone outside meditating, asking the earth to show one's fate.\n\nWhile these customs were unique to Russian culture, they were not exclusive to this region. Russian pagan practices were often akin to paganism in other parts of the world. The Chinese concept of \"chi\", a form of energy that often manipulated in witchcraft, is known as bioplasma in Russian practices. The western concept of an \"evil eye\" or a \"hex\" was translated to Russia as a \"spoiler\". A spoiler was rooted in envy, jealousy and malice. Spoilers could be made by gathering bone from a cemetery, a knot of the target's hair, burned wooden splinters and several herb Paris berries (which are very poisonous). Placing these items in sachet in the victim's pillow completes a spoiler. The Sumerians, Babylonians, Assyrians, and the ancient Egyptians recognized the evil eye from as early as 3,000 BCE; in Russian practices it is seen as a sixteenth-century concept.\n\nThe dominant societal concern those practicing witchcraft was not whether paganism was effective, but whether it could cause harm. Peasants in Russian and Ukrainian societies often shunned witchcraft, unless they needed help against supernatural forces. Impotence, stomach pains, barrenness, hernias, abscesses, epileptic seizures, and convulsions were all attributed to evil (or witchcraft). This is reflected in linguistics; there are numerous words for a variety of practitioners of paganism-based healers. Russian peasants referred to a witch as a \"chernoknizhnik\" (a person who plied his trade with the aid of a black book), \"sheptun\"/\"sheptun'ia\" (a \"whisperer\" male or female), \"lekar\"/\"lekarka\" or \"znakhar\"/\"znakharka\" (a male or female healer), or \"zagovornik\" (an incanter).\n\nIronically enough, there was universal reliance on folk healers – but clients often turned them in if something went wrong. According to Russian historian Valerie A. Kivelson, witchcraft accusations were normally thrown at lower-class peasants, townspeople and Cossacks. People turned to witchcraft as a means to support themselves. The ratio of male to female accusations was 75% to 25%. Males were targeted more, because witchcraft was associated with societal deviation. Because single people with no settled home could not be taxed, males typically had more power than women in their dissent.\n\nThe history of Witchcraft had evolved around society. More of a psychological concept to the creation and usage of Witchcraft can create the assumption as to why women are more likely to follow the practices behind Witchcraft. Identifying with the soul of an individual’s self is often deemed as \"feminine\" in society. There is analyzed social and economic evidence to associate between witchcraft and women.\n\nWitchcraft trials occurred frequently in seventeenth-century Russia, although the \"great witch-hunt\" is believed to be a predominately Western European phenomenon. However, as the witchcraft-trial craze swept across Catholic and Protestant countries during this time, Orthodox Christian Europe indeed partook in this so-called \"witch hysteria.\" This involved the persecution of both males and females who were believed to be practicing paganism, herbology, the black art, or a form of sorcery within and/or outside their community. Very early on witchcraft legally fell under the jurisdiction of the ecclesiastical body, the church, in Kievan Rus' and Muscovite Russia. Sources of ecclesiastical witchcraft jurisdiction date back as early as the second half of the eleventh century, one being Vladimir the Great's first edition of his State Statute or \"Ustav\", another being multiple references in the \"Primary Chronicle\" beginning in 1024.\n\nThe sentence for an individual found guilty of witchcraft or sorcery during this time, and in previous centuries, typically included either burning at the stake or being tested with the \"ordeal of cold water\" or \"judicium aquae frigidae\". The cold-water test was primarily a Western European phenomenon, but was used as a method of truth in Russia prior to, and post, seventeenth-century witchcraft trials in Muscovy. Accused persons who submerged were considered innocent, and ecclesiastical authorities would proclaim them \"brought back,\" but those who floated were considered guilty of practicing witchcraft, and burned at the stake or executed in an unholy fashion. The thirteenth-century bishop of Vladimir, Serapion Vladimirskii, preached sermons throughout the Muscovite countryside, and in one particular sermon revealed that burning was the usual punishment for witchcraft, but more often the cold water test was used as a precursor to execution.\n\nAlthough these two methods of torture were used in the west and the east, Russia implemented a system of fines payable for the crime of witchcraft during the seventeenth century. Thus, even though torture methods in Muscovy were on a similar level of harshness as Western European methods used, a more civil method was present. In the introduction of a collection of trial records pieced together by Russian scholar Nikolai Novombergsk, he argues that Muscovite authorities used the same degree of cruelty and harshness as Western European Catholic and Protestant countries in persecuting witches. By the mid-sixteenth century the manifestations of paganism, including witchcraft, and the black arts—astrology, fortune telling, and divination—became a serious concern to the Muscovite church and state.\n\nTsar Ivan IV (reigned 1547–1584) took this matter to the ecclesiastical court and was immediately advised that individuals practicing these forms of witchcraft should be excommunicated and given the death penalty. Ivan IV, as a true believer in witchcraft, was deeply convinced that sorcery accounted for the death of his wife, Anastasiia in 1560, which completely devastated and depressed him, leaving him heartbroken. Stemming from this belief, Ivan IV became majorly concerned with the threat of witchcraft harming his family, and feared he was in danger. So, during the Oprichnina (1565–1572), Ivan IV succeeded in accusing and charging a good number of boyars with witchcraft whom he did not wish to remain as nobles. Rulers after Ivan IV, specifically during the Time of Troubles (1598–1613), increased the fear of witchcraft among themselves and entire royal families, which then led to further preoccupation with the fear of prominent Muscovite witchcraft circles.\n\nAfter the Time of Troubles, seventeenth-century Muscovite rulers held frequent investigations of witchcraft within their households, laying the ground, along with previous tsarist reforms, for widespread witchcraft trials throughout the Muscovite state. Between 1622 and 1700 ninety-one people were brought to trial in Muscovite courts for witchcraft. Although Russia did partake in the witch craze that swept across Western Europe, the Muscovite state did not persecute nearly as many people for witchcraft, let alone execute a number of individuals anywhere close to the number executed in the west during the witch hysteria.\n\nWitches have a long history of being depicted in art, although most of their earliest artistic depictions seem to originate in Early Modern Europe, particularly the Medieval and Renaissance periods. Many scholars attribute their manifestation in art as inspired by texts such as \"Canon Episcopi\", a demonology-centered work of literature, and \"Malleus Maleficarum\", a \"witch-craze\" manual published in 1487, by Heinrich Kramer and Jacob Sprenger.\n\n\"Canon Episcopi\", a ninth-century text that explored the subject of demonology, initially introduced concepts that would continuously be associated with witches, such as their ability to fly or their believed fornication and sexual relations with the devil. The text refers to two women, Diana the Huntress and Herodias, who both express the duality of female sorcerers. Diana was described as having a heavenly body and as the \"protectress of childbirth and fertility\" while Herodias symbolized \"unbridled sensuality\". They thus represent the mental powers and cunning sexuality that witches used as weapons to trick men into performing sinful acts which would result in their eternal punishment. These characteristics were distinguished as Medusa-like or Lamia-like traits when seen in any artwork (Medusa's mental trickery was associated with Diana the Huntress's psychic powers and Lamia was a rumored female figure in the Medieval ages sometimes used in place of Herodias).\nOne of the first individuals to regularly depict witches after the witch-craze of the medieval period was Albrecht Dürer, a German Renaissance artist. His famous 1497 engraving \"The Four Witches\", portrays four physically attractive and seductive nude witches. Their supernatural identities are emphasized by the skulls and bones lying at their feet as well as the devil discreetly peering at them from their left. The women's sensuous presentation speaks to the overtly sexual nature they were attached to in early modern Europe. Moreover, this attractiveness was perceived as a danger to ordinary men who they could seduce and tempt into their sinful world. Some scholars interpret this piece as utilizing the logic of the \"Canon Episcopi\", in which women used their mental powers and bodily seduction to enslave and lead men onto a path of eternal damnation, differing from the unattractive depiction of witches that would follow in later Renaissance years.\nDürer also employed other ideas from the Middle Ages that were commonly associated with witches. Specifically, his art often referred to former 12th- to 13th-century Medieval iconography addressing the nature of female sorcerers. In the Medieval period, there was a widespread fear of witches, accordingly producing an association of dark, intimidating characteristics with witches, such as cannibalism (witches described as \"[sucking] the blood of newborn infants\") or described as having the ability to fly, usually on the back of black goats. As the Renaissance period began, these concepts of witchcraft were suppressed, leading to a drastic change in the sorceress' appearances, from sexually explicit beings to the 'ordinary' typical housewives of this time period. This depiction, known as the 'Waldensian' witch became a cultural phenomenon of early Renaissance art. The term originates from the 12th-century monk Peter Waldo, who established his own religious sect which explicitly opposed the luxury and commodity-influenced lifestyle of the Christian church clergy, and whose sect was excommunicated before being persecuted as \"practitioners of witchcraft and magic\".\n\nSubsequent artwork exhibiting witches tended to consistently rely on cultural stereotypes about these women. These stereotypes were usually rooted in early Renaissance religious discourse, specifically the Christian belief that an \"earthly alliance\" had taken place between Satan's female minions who \"conspired to destroy Christendom\".\n\nAnother significant artist whose art consistently depicted witches was Dürer's apprentice, Hans Baldung Grien, a 15th-century German artist. His chiaroscuro woodcut, \"Witches\", created in 1510, visually encompassed all the characteristics that were regularly assigned to witches during the Renaissance. Social beliefs labeled witches as supernatural beings capable of doing great harm, possessing the ability to fly, and as cannibalistic. The urn in \"Witches\" seems to contain pieces of the human body, which the witches are seen consuming as a source of energy. Meanwhile, their nudity while feasting is recognized as an allusion to their sexual appetite, and some scholars read the witch riding on the back of a goat-demon as representative of their \"flight-inducing [powers]\". This connection between women's sexual nature and sins was thematic in the pieces of many Renaissance artists, especially Christian artists, due to cultural beliefs which characterized women as overtly sexual beings who were less capable (in comparison to men) of resisting sinful temptation.\n\n\n\n"}
{"id": "53954", "url": "https://en.wikipedia.org/wiki?curid=53954", "title": "Work function", "text": "Work function\n\nIn solid-state physics, the work function (sometimes spelled workfunction) is the minimum thermodynamic work (i.e. energy) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface. Here \"immediately\" means that the final electron position is far from the surface on the atomic scale, but still too close to the solid to be influenced by ambient electric fields in the vacuum.\nThe work function is not a characteristic of a bulk material, but rather a property of the surface of the material (depending on crystal face and contamination).\n\nThe work function for a given surface is defined by the difference\nwhere is the charge of an electron, is the electrostatic potential in the vacuum nearby the surface, and is the Fermi level (electrochemical potential of electrons) inside the material. The term is the energy of an electron at rest in the vacuum nearby the surface.\n\nIn practice, one directly controls by the voltage applied to the material through electrodes, and the work function is generally a fixed characteristic of the surface material. Consequently, this means that when a voltage is applied to a material, the electrostatic potential produced in the vacuum will be somewhat lower than the applied voltage, the difference depending on the work function of the material surface. Rearranging the above equation, one has\nwhere is the voltage of the material (as measured by a voltmeter, through an attached electrode), relative to an electrical ground that is defined as having zero Fermi level. The fact that depends on material surface means that the space between two dissimilar conductors will have a built-in electric field, when those conductors are in total equilibrium with each other (electrically shorted to each other, and with equal temperatures). An example of this situation is depicted in the adjacent figure. As described in the next section, these built-in vacuum electric fields can have important consequences in some cases.\n\n\nCertain physical phenomena are highly sensitive to the value of the work function.\nThe observed data from these effects can be fitted to simplified theoretical models, allowing one to extract a value of the work function.\nThese phenomenologically extracted work functions may be slightly different from the thermodynamic definition given above.\nFor inhomogeneous surfaces, the work function varies from place to place, and different methods will yield different values of the typical \"work function\" as they average or select differently among the microscopic work functions.\n\nMany techniques have been developed based on different physical effects to measure the electronic work function of a sample. One may distinguish between two groups of experimental methods for work function measurements: absolute and relative.\n\n\nThe work function is important in the theory of thermionic emission, where thermal fluctuations provide enough energy to \"evaporate\" electrons out of a hot material (called the 'emitter') into the vacuum. If these electrons are absorbed by another, cooler material (called the \"collector\") then a measurable electric current will be observed. Thermionic emission can be used to measure the work function of both the hot emitter and cold collector. Generally, these measurements involve fitting to Richardson's law, and so they must be carried out in a low temperature and low current regime where space charge effects are absent.\n\nIn order to move from the hot emitter to the vacuum, an electron's energy must exceed the emitter Fermi level by an amount\ndetermined simply by the thermionic work function of the emitter.\nIf an electric field is applied towards the surface of the emitter, then all of the escaping electrons will be accelerated away from the emitter and absorbed into whichever material is applying the electric field.\nAccording to Richardson's law the emitted current density (per unit area of emitter), \"J\" (A/m), is related to the absolute temperature \"T\" of the emitter by the equation:\nwhere \"k\" is the Boltzmann constant and the proportionality constant \"A\" is the Richardson's constant of the emitter.\nIn this case, the dependence of \"J\" on \"T\" can be fitted to yield \"W\".\n\nThe same setup can be used to instead measure the work function in the collector, simply by adjusting the applied voltage.\nIf an electric field is applied \"away from\" the emitter instead, then most of the electrons coming from the emitter will simply be reflected back to the emitter. Only the highest energy electrons will have enough energy to reach the collector, and the height of the potential barrier in this case depends on the collector's work function, rather than the emitter's.\n\nThe current is still governed by Richardson's law. However, in this case the barrier height does not depend on \"W\". The barrier height now depends on the work function of the collector, as well as any additional applied voltages:\nwhere \"W\" is the collector's thermionic work function, \"ΔV\" is the applied collector–emitter voltage, and \"ΔV\" is the Seebeck voltage in the hot emitter (the influence of \"ΔV\" is often omitted, as it is a small contribution of order 10 mV).\nThe resulting current density \"J\" through the collector (per unit of collector area) is again given by Richardson's Law, except now\nwhere \"A\" is a Richardson-type constant that depends on the collector material but may also depend on the emitter material, and the diode geometry.\nIn this case, the dependence of \"J\" on \"T\", or on \"ΔV\", can be fitted to yield \"W\".\n\nThis retarding potential method is one of the simplest and oldest methods of measuring work functions, and is advantageous since the measured material (collector) is not required to survive high temperatures.\n\nThe photoelectric work function is the minimum photon energy required to liberate an electron from a substance, in the photoelectric effect.\nIf the photon's energy is greater than the substance's work function, photoelectric emission occurs and the electron is liberated from the surface.\nSimilar to the thermionic case described above, the liberated electrons can be extracted into a collector and produce a detectable current, if an electric field is applied into the surface of the emitter.\nExcess photon energy results in a liberated electron with non-zero kinetic energy.\nIt is expected that the minimum photon energy formula_7 required to liberate an electron (and generate a current) is \nwhere \"W\" is the work function of the emitter.\n\nPhotoelectric measurements require a great deal of care, as an incorrectly designed experimental geometry can result in an erroneous measurement of work function. This may be responsible for the large variation in work function values in scientific literature.\nMoreover, the minimum energy can be misleading in materials where there are no actual electron states at the Fermi level that are available for excitation. For example, in a semiconductor the minimum photon energy would actually correspond to the valence band edge rather than work function.\n\nOf course, the photoelectric effect may be used in the retarding mode, as with the thermionic apparatus described above. In the retarding case, the dark collector's work function is measured instead.\n\nThe Kelvin probe technique relies on the detection of an electric field (gradient in \"ϕ\") between a sample material and probe material.\nThe electric field can be varied by the voltage \"ΔV\" that is applied to the probe relative to the sample.\nIf the voltage is chosen such that the electric field is eliminated (the flat vacuum condition), then\nSince the experimenter controls and knows \"ΔV\", then finding the flat vacuum condition gives directly the work function difference between the two materials.\nThe only question is, how to detect the flat vacuum condition?\nTypically, the electric field is detected by varying the distance between the sample and probe. When the distance is changed but \"ΔV\" is held constant, a current will flow due to the change in capacitance. This current is proportional to the vacuum electric field, and so when the electric field is neutralized no current will flow.\n\nAlthough the Kelvin probe technique only measures a work function difference, it is possible to obtain an absolute work function by first calibrating the probe against a reference material (with known work function) and then using the same probe to measure a desired sample.\nThe Kelvin probe technique can be used to obtain work function maps of a surface with extremely high spatial resolution, by using a sharp tip for the probe (see Kelvin probe force microscope).\n\nBelow is a table of work function values for various elements.\nNote that the work function depends on the configurations of atoms at the surface of the material. For example, on polycrystalline silver the work function is 4.26 eV, but on silver crystals it varies for different crystal faces as (100) face: 4.64 eV, (110) face: 4.52 eV, (111) face: 4.74 eV. Ranges for typical surfaces are shown in the table below.\n\nDue to the complications described in the modelling section below, it is difficult to theoretically predict the work function with accuracy. Various trends have, however, been identified. The work function tends to be smaller for metals with an open lattice, and larger for metals in which the atoms are closely packed. It is somewhat higher on dense crystal faces than open crystal faces, also depending on surface reconstructions for the given crystal face.\n\nThe work function is not simply dependent on the \"internal vacuum level\" inside the material (i.e., its average electrostatic potential), because of the formation of an atomic-scale electric double layer at the surface. This surface electric dipole gives a jump in the electrostatic potential between the material and the vacuum.\n\nA variety of factors are responsible for the surface electric dipole. Even with a completely clean surface, the electrons can spread slightly into the vacuum, leaving behind a slightly positively charged layer of material. This primarily occurs in metals, where the bound electrons do not encounter a hard wall potential at the surface but rather a gradual ramping potential due to image charge attraction. The amount of surface dipole depends on the detailed layout of the atoms at the surface of the material, leading to the variation in work function for different crystal faces.\n\nIn a semiconductor, the work function is sensitive to the doping level at the surface of the semiconductor. Since the doping near the surface can also be controlled by electric fields, the work function of a semiconductor is also sensitive to the electric field in the vacuum.\n\nThe reason for the dependence is that, typically, the vacuum level and the conduction band edge retain a fixed spacing independent of doping. This spacing is called the electron affinity (note that this has a different meaning than the electron affinity of chemistry); in silicon for example the electron affinity is 4.05 eV. If the electron affinity \"E\" and the surface's band-referenced Fermi level \"E\"-\"E\" are known, then the work function is given by\nwhere \"E\" is taken at the surface.\n\nFrom this one might expect that by doping the bulk of the semiconductor, the work function can be tuned. In reality, however, the energies of the bands near the surface are often pinned to the Fermi level, due to the influence of surface states. If there is a large density of surface states, then the work function of the semiconductor will show a very weak dependence on doping or electric field.\n\nTheoretical modeling of the work function is difficult, as an accurate model requires a careful treatment of both electronic many body effects and surface chemistry; both of these topics are already complex in their own right.\n\nOne of the earliest successful models for metal work function trends was the jellium model, which allowed for oscillations in electronic density nearby the abrupt surface (these are similar to Friedel oscillations) as well as the tail of electron density extending outside the surface. This model showed why the density of conduction electrons (as represented by the Wigner-Seitz radius \"r\") is an important parameter in determining work function.\n\nThe jellium model is only a partial explanation, as its predictions still show significant deviation from real work functions. More recent models have focussed on including more accurate forms of electron exchange and correlation effects, as well as including the crystal face dependence (this requires the inclusion of the actual atomic lattice, something that is neglected in the jellium model).\n\nThe electron behavior in metals varies with temperature and is largely reflected by the electron work function. A recent theoretical model for predicting the temperature dependence of the electron work function, developed by Reza Rahemi et. al. explains the underlying mechanism and predicts this temperature dependence for various crystal structures via calculable and measurable parameters.\n\n\nFor a quick reference to values of work function of the elements:\n\n\"*Some of the work functions listed on these sites do not agree!*\"\n"}
{"id": "1460862", "url": "https://en.wikipedia.org/wiki?curid=1460862", "title": "Yakov Perelman", "text": "Yakov Perelman\n\nYakov Isidorovich Perelman (; December 4, 1882 – March 16, 1942) was a Russian and Soviet science writer and author of many popular science books, including \"Physics Can Be Fun\" and \"Mathematics Can Be Fun\" (both translated from Russian into English).\n\nPerelman was born in 1882 in the town of Białystok, Congress Poland. He obtained the Diploma in Forestry from the Imperial Forestry Institute (Now Saint Petersburg State Forest Technical University) in Saint Petersburg, in 1909. He was influenced by Ernst Mach and probably the Russian Machist Alexander Bogdanov in his pedagogical approach to popularising science. After the success of \"Physics for Entertainment\", Perelman set out to produce other books, in which he showed himself to be an imaginative populariser of science. Especially popular were \"\"Arithmetic for entertainment\", \"Mechanics for entertainment\", \"Geometry for Entertainment\", \"Astronomy for entertainment\", \"Lively Mathematics\", \" Physics Everywhere\", and \"Tricks and Amusements\".\n\nHis famous books on physics and astronomy were translated into various languages by the erstwhile Soviet Union.\n\nThe scientist Konstantin Tsiolkovsky thought highly of Perelman's talents and creative genius, writing of him in the preface of \"Interplanetary Journeys\": \"The author has long been known by his popular, witty and quite scientific works on physics, astronomy and mathematics, which are, moreover written in a marvelous language and are very readable.\"\n\nPerelman has also authored a number of textbooks and articles in Soviet popular science magazines.\n\nIn addition to his educational and scientific writings, he also worked as an editor of science magazines, including \"Nature and People\" and \"In the Workshop of Nature\".\n\nPerelman died from starvation in 1942, during the German Siege of Leningrad. The siege started at 9 September 1941 and lasted 872 days, until \n27 January 1944. The Siege of Leningrad was one of the longest, most destructive sieges of a major city in modern history and one of the costliest in terms of casualties (1,117,000).\n\nHis older brother Yosif was a writer who published under the pseudonym Osip Dymov. He is not related to the Russian mathematician Grigori Perelman, who was born in 1966 to a different Yakov Perelman. However, Grigori Perelman told The New Yorker that his father gave him \"Physics for Entertainment\", and it inspired his interest in mathematics.\n\n\nHe has also written several books on interplanetary travel (\"Interplanetary Journeys, On a Rocket to Stars, and World Expanses\")\n\nIn 1913, Russian bookshops began carrying \"Physics for Entertainment\". The educationalist's new book attracted young readers seeking answers to scientific questions.\n\n\"Physics for Entertainment\" had a unique layout as well as an instructive style. In the preface (11th ed.) Perelman wrote: \"The main objective of \"Physics for entertainment\" is to arouse the activity of scientific imagination, to teach the reader to think in the spirit of the science of physics and to create in his mind a wide variety of associations of physical knowledge with the widely differing facts of life, with all that he normally comes into contact with.\"\n\nIn the foreword, Perelman describes the contents as “conundrums, brain-teasers, entertaining anecdotes, and unexpected comparisons,” adding, “I have quoted extensively from Jules Verne, H. G. Wells, Mark Twain and other writers, because, besides providing entertainment, the fantastic experiments these writers describe may well serve as instructive illustrations at physics classes.” The 13th edition (1936) would be the last published during the author's lifetime. Among the book's notable topics was the idea of a perpetual machine: a hypothetical machine which could run incessantly performing useful work. The author discusses perpetual motion, highlighting many attempts to build such a machine, and explains why they failed. Other topics included how to jump from a moving car, and why, “according to the law of buoyancy, we would never drown in the Dead Sea.”\n\nRandall Munroe, the creator of the web comic xkcd and author of his own popular science books, wrote: \nThe book is a series of a few hundred examples, no more than one or two pages each, asking a question that illustrates some idea in basic physics.\n\nIt’s neat to see what has and hasn’t changed in the last century or so. Many of the examples he uses seem to be straight out of a modern high school physics textbook, while others were totally new to me. And some of the answers to the questions he poses seem obvious, but others made me stop and think. [This] diagram ... shows a design for a fountain with no pump — it took me a while to get why it works... Later in the book, he explains the physics of that drinking bird toy.\nIt’s written in a fun, engaging, conversational style, as if he’s in the room chatting with you about these neat ideas.\n\n\n"}
{"id": "13873874", "url": "https://en.wikipedia.org/wiki?curid=13873874", "title": "Zeta potential titration", "text": "Zeta potential titration\n\nZeta potential titration is a titration of heterogeneous systems, for example colloids and emulsions. Solids in such systems have very high surface area. This type of titration is used to study the zeta potential of these surfaces under different conditions.\n\nThe iso-electric point is one such property. The iso-electric point is the pH value at which the zeta potential is approximately zero. At a pH near the iso-electric point (± 2 pH units), colloids are usually unstable; the particles tend to coagulate or flocculate. Such titrations use acids or bases as titration reagents. Tables of iso-electric points for different materials are available. The attached figure illustrates results of such titrations for concentrated dispersions of alumina (4% v/v) and rutile (7% v/v). It is seen that iso-electric point of alumina is around pH 9.3, whereas for rutile it is around pH 4. Alumina is unstable in the pH range from 7 to 11. Rutile is unstable in the pH range from 2 to 6.\n\nAnother purpose of this titration is determination of the optimum dose of surfactant for achieving stabilization or flocculation of a heterogeneous system. Examples can be found in the book by Dukhin and Goetz.\n\nIn a zeta-potential titration, the Zeta potential is the indicator. Measurement of the zeta potential can be performed using microelectrophoresis, or electrophoretic light scattering, or electroacoustic phenomena. The last method makes possible to perform titrations in concentrated systems, with no dilution. The book by Dukhin and Goetz provides a detailed description of such titrations.\n\n"}
