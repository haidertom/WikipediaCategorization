{"id": "4024", "url": "https://en.wikipedia.org/wiki?curid=4024", "title": "Butterfly effect", "text": "Butterfly effect\n\nIn chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.\n\nThe term, coined by Edward Lorenz, is derived from the metaphorical example of the details of a tornado (the exact time of formation, the exact path taken) being influenced by minor perturbations such as the flapping of the wings of a distant butterfly several weeks earlier. Lorenz discovered the effect when he observed that runs of his weather model with initial condition data that was rounded in a seemingly inconsequential manner would fail to reproduce the results of runs with the unrounded initial condition data. A very small change in initial conditions had created a significantly different outcome.\n\nThough Lorenz gave a name to the phenomenon, the idea that small causes may have large effects in general and in weather specifically was earlier recognized by French mathematician and engineer Henri Poincaré and American mathematician and philosopher Norbert Wiener. Edward Lorenz's work placed the concept of \"instability\" of the earth's atmosphere onto a quantitative base and linked the concept of instability to the properties of large classes of dynamic systems which are undergoing nonlinear dynamics and deterministic chaos.\n\nThe butterfly effect can also be demonstrated by very simple systems.\n\nIn \"The Vocation of Man\" (1800), Johann Gottlieb Fichte says that \"you could not remove a single grain of sand from its place without thereby ... changing something throughout all parts of the immeasurable whole\".\n\nChaos theory and the sensitive dependence on initial conditions were described in the literature in a particular case of the three-body problem by Henri Poincaré in 1890. He later proposed that such phenomena could be common, for example, in meteorology.\n\nIn 1898, Jacques Hadamard noted general divergence of trajectories in spaces of negative curvature. Pierre Duhem discussed the possible general significance of this in 1908.\n\nThe idea that one butterfly could eventually have a far-reaching ripple effect on subsequent historic events made its earliest known appearance in \"A Sound of Thunder\", a 1952 short story by Ray Bradbury about time travel.\n\nIn 1961, Lorenz was running a numerical computer model to redo a weather prediction from the middle of the previous run as a shortcut. He entered the initial condition 0.506 from the printout instead of entering the full precision 0.506127 value. The result was a completely different weather scenario.\n\nLorenz wrote:\nIn 1963 Lorenz published a theoretical study of this effect in a highly cited, seminal paper called \"Deterministic Nonperiodic Flow\" (the calculations were performed on a Royal McBee LGP-30 computer). Elsewhere he stated: Following suggestions from colleagues, in later speeches and papers Lorenz used the more poetic butterfly. According to Lorenz, when he failed to provide a title for a talk he was to present at the 139th meeting of the American Association for the Advancement of Science in 1972, Philip Merilees concocted \"Does the flap of a butterfly’s wings in Brazil set off a tornado in Texas?\" as a title. Although a butterfly flapping its wings has remained constant in the expression of this concept, the location of the butterfly, the consequences, and the location of the consequences have varied widely.\n\nThe phrase refers to the idea that a butterfly's wings might create tiny changes in the atmosphere that may ultimately alter the path of a tornado or delay, accelerate or even prevent the occurrence of a tornado in another location. The butterfly does not power or directly create the tornado, but the term is intended to imply that the flap of the butterfly's wings can \"cause\" the tornado: in the sense that the flap of the wings is a part of the initial conditions; one set of conditions leads to a tornado while the other set of conditions doesn't. The flapping wing represents a small change in the initial condition of the system, which cascades to large-scale alterations of events (compare: domino effect). Had the butterfly not flapped its wings, the trajectory of the system might have been vastly different—but it's also equally possible that the set of conditions without the butterfly flapping its wings is the set that leads to a tornado.\n\nThe butterfly effect presents an obvious challenge to prediction, since initial conditions for a system such as the weather can never be known to complete accuracy. This problem motivated the development of ensemble forecasting, in which a number of forecasts are made from perturbed initial conditions.\n\nSome scientists have since argued that the weather system is not as sensitive to initial conditions as previously believed. David Orrell argues that the major contributor to weather forecast error is model error, with sensitivity to initial conditions playing a relatively small role. Stephen Wolfram also notes that the Lorenz equations are highly simplified and do not contain terms that represent viscous effects; he believes that these terms would tend to damp out small perturbations.\n\nRecurrence, the approximate return of a system towards its initial conditions, together with sensitive dependence on initial conditions, are the two main ingredients for chaotic motion. They have the practical consequence of making complex systems, such as the weather, difficult to predict past a certain time range (approximately a week in the case of weather) since it is impossible to measure the starting atmospheric conditions completely accurately.\n\nA dynamical system displays sensitive dependence on initial conditions if points arbitrarily close together separate over time at an exponential rate. The definition is not topological, but essentially metrical.\n\nIf \"M\" is the state space for the map formula_1, then formula_1 displays sensitive dependence to initial conditions if for any x in \"M\" and any δ > 0, there are y in \"M\", with distance \"d\"(. , .) such that formula_3 and such that\n\nfor some positive parameter \"a\". The definition does not require that all points from a neighborhood separate from the base point \"x\", but it requires one positive Lyapunov exponent.\n\nThe simplest mathematical framework exhibiting sensitive dependence on initial conditions is provided by a particular parametrization of the logistic map:\n\nwhich, unlike most chaotic maps, has a closed-form solution:\n\nwhere the initial condition parameter formula_7 is given by formula_8. For rational formula_7, after a finite number of iterations formula_10 maps into a periodic sequence. But almost all formula_7 are irrational, and, for irrational formula_7, formula_10 never repeats itself – it is non-periodic. This solution equation clearly demonstrates the two key features of chaos – stretching and folding: the factor 2 shows the exponential growth of stretching, which results in sensitive dependence on initial conditions (the butterfly effect), while the squared sine function keeps formula_10 folded within the range [0, 1].\n\nThe butterfly effect is most familiar in terms of weather; it can easily be demonstrated in standard weather prediction models, for example. The climate scientists James Annan and William Connolley explain that chaos is important in the development of weather prediction methods; models are sensitive to initial conditions. They add the caveat: \"Of course the existence of an unknown butterfly flapping its wings has no direct bearing on weather forecasts, since it will take far too long for such a small perturbation to grow to a significant size, and we have many more immediate uncertainties to worry about. So the direct impact of this phenomenon on weather prediction is often somewhat overstated.\"\n\nThe potential for sensitive dependence on initial conditions (the butterfly effect) has been studied in a number of cases in semiclassical and quantum physics including atoms in strong fields and the anisotropic Kepler problem. Some authors have argued that extreme (exponential) dependence on initial conditions is not expected in pure quantum treatments; however, the sensitive dependence on initial conditions demonstrated in classical motion is included in the semiclassical treatments developed by Martin Gutzwiller and Delos and co-workers.\n\nOther authors suggest that the butterfly effect can be observed in quantum systems. Karkuszewski et al. consider the time evolution of quantum systems which have slightly different Hamiltonians. They investigate the level of sensitivity of quantum systems to small changes in their given Hamiltonians. Poulin et al. presented a quantum algorithm to measure fidelity decay, which \"measures the rate at which identical initial states diverge when subjected to slightly different dynamics\". They consider fidelity decay to be \"the closest quantum analog to the (purely classical) butterfly effect\". Whereas the classical butterfly effect considers the effect of a small change in the position and/or velocity of an object in a given Hamiltonian system, the quantum butterfly effect considers the effect of a small change in the Hamiltonian system with a given initial position and velocity. This quantum butterfly effect has been demonstrated experimentally. Quantum and semiclassical treatments of system sensitivity to initial conditions are known as quantum chaos.\n\nThe journalist Peter Dizikes, writing in \"The Boston Globe\" in 2008, notes that popular culture likes the idea of the butterfly effect, but gets it wrong. Whereas Lorenz suggested correctly with his butterfly metaphor that predictability \"is inherently limited\", popular culture supposes that each event can be explained by finding the small reasons that caused it. Dizikes explains: \"It speaks to our larger expectation that the world should be comprehensible – that everything happens for a reason, and that we can pinpoint all those reasons, however small they may be. But nature itself defies this expectation.\"\n\n\n\n"}
{"id": "34688121", "url": "https://en.wikipedia.org/wiki?curid=34688121", "title": "Coastal hazards", "text": "Coastal hazards\n\nCoastal Hazards are physical phenomena that expose a coastal area to risk of property damage, loss of life and environmental degradation. Rapid-onset hazards last over periods of minutes to several days and examples include major cyclones accompanied by high winds, waves and surges or tsunamis created by submarine earthquakes and landslides. Slow-onset hazards develop incrementally over longer time periods and examples include erosion and gradual inundation.\n\nSince early civilisation, coastal areas have been attractive settling grounds for human population as they provided abundant marine resources, fertile agricultural land and possibilities for trade and transport. This has led to high population densities and high levels of development in many coastal areas and this trend is continuing into the 21st century. At present, about 1,2 billion people live in coastal areas globally, and this number is predicted to increase to 1,8–5,2 billion by the 2080s due to a combination of population growth and coastal migration. Along with this increase follows major investments in infrastructure and the build environment.\n\nThe characteristics of coastal environments, however, pose some great challenges to human habitation. Coastlines are highly dynamic natural systems that interact with terrestrial, marine and atmospheric processes and undergo continuous change in response to these processes. Over the years, human society has often failed to recognize the hazards related to these dynamics and this has led to major disasters and societal disruption to various degrees. Even today, coastal development is often taking place with little regard to the hazards present in these environments, although climate change is likely to increase the general hazard levels. Societal activities in coastal areas can also pose a hazard to the natural balance of coastal systems, thereby disrupting e.g. sensitive ecosystems and subsequently human livelihood.\n\nCoastal hazard management has become an increasingly important aspect of coastal planning in order to improve the resilience of society to coastal hazards. Possible management options include hard engineering structures, soft protection measures, various accommodation approaches as well as a managed retreat from the coastline. For addressing coastal hazards, it is also important to have early warning systems and emergency management plans in place to be able to address sudden and potential disastrous hazards i.e. major flooding events. Events as the Hurricane Katrina affecting the southern USA in 2005 and the cyclone Nargis affecting Myanmar in 2008 provides clear examples of the importance of timely coastal hazard management.\n\nThere are many different types of environments along the coasts of the United States with very diverse features that affect, influence, and mold the near-shore processes that are involved. Understanding these ecosystems and environments can further advance the mitigating techniques and policy-making efforts against natural and man-made coastal hazards in these vulnerable areas. The five most common types of coastal zones range from the northern ice-pushing, mountainous coastline of Alaska and Maine, the barrier island coasts facing the Atlantic, the steep, cliff-back headlands along the pacific coast, the marginal-sea type coastline of the Gulf region, and the coral reef coasts bordering Southern Florida and Hawaii.\n\nIce-pushing/mountainous coastline\n\nThese coastal regions along the northernmost part of the nation were affected predominantly by, along with the rest of the Pacific Coast, continuous tectonic activity, forming a very long, irregular, ridged, steep and mostly mountainous coastline. These environments are heavily occupied with permafrost and glaciers, which are the two major conditions affecting Alaska's Coastal Development.\n\nBarrier island coastline\n\nBarrier islands are a land form system that consists of fairly narrow strips of sand running parallel to the mainland and play a significant role in mitigating storm surges and oceans swells as natural storm events occur. The morphology of the various types and sizes of barrier islands depend on the wave energy, tidal range, basement controls, and sea level trends. The islands create multiple unique environments of wetland systems including marshes, estuaries, and lagoons.\n\nSteep, cliff-backing abrasion coastline\n\nThe coastline along the western part of the nation consists of very steep, cliffed rock formations generally with vegetative slopes descending down and a fringing beach below. The various sedimentary, metamorphic, and volcanic rock formations assembled along a tectonically disturbed environment, all with altering resistances running perpendicular, cause the ridged, extensive stretch of uplifted cliffs that form the peninsulas, lagoons, and valleys.\n\nMarginal-sea type coastline\n\nThe southern banks of the United States border the Gulf of Mexico, intersecting numerous rivers, forming many inlets bays, and lagoons along its coast, consisting of vast areas of marsh and wetlands. This region of landform is prone to natural disasters yet highly and continuously developed, with man-made structures attaining to water flow and control.\n\nCoral reef coastline\n\nCoral reefs are located off the shores of the southern Florida and Hawaii consisting of rough and complex natural structures along the bottom of the ocean floor with extremely diverse ecosystems, absorbing up to ninety percent of the energy dissipated from wind-generated waves. This process is a significant buffer for the inner-lying coastlines, naturally protecting and minimizing the impact of storm surge and direct wave damage. Because of the highly diverse ecosystems, these coral reefs not only provide for the shoreline protection, but also deliver an abundant amount of services to fisheries and tourism, increasing its economic value.\n\nNatural VS Human disasters\n\nThe population that lives along or near our coastlines are an extremely vulnerable population. There are numerous issues facing our coastlines and there are two main categories that these hazards can be placed under, Natural disasters and Human disasters. Both of these issues cause great damage to our coastlines and discussion is still ongoing regarding what standards or responses need to be met to help both the individuals who want to continue living along the coastline, while keeping them safe and not eroding more coastline away. Natural disasters are disasters that are out of human control and are usually caused by the weather. Disasters that include but are not limited to; storms, tsunamis, typhoons, flooding, tides, waterspouts, nor'easters, and storm surge. Human disasters occur when humans are the main culprit behind why the disaster happened. Some human disasters are but are not limited to; pollution, trawling, and human development. Natural and human disasters continue to harm the coastlines severely and they need to be researched in order to prepare/stop the hazards if possible.\n\nThe populations that live near or along the coast experience many hazards and it affects millions of people. Around ten million people globally feel the effects of coastal problems yearly and most are due to certain natural hazards like coastal flooding with storm surges and typhoons. A major problem related to coastal regions deals with how the entire global environment is changing and in response, the coastal regions are easily affected.\n\nStorms, Flooding and Erosion\n\nStorms are one of the major hazards that are associated to coastal regions. Storms, flooding, and erosion are closely associated and can happen simultaneously. Tropical storms or Hurricanes especially can devastate coastal regions. For example, Florida during Hurricane Andrew occurred in 1992 that caused extreme damage. It was a category five hurricane that caused $26.5 billion in damages and even 23 individuals lost their lives from the storm. Hurricane Katrina also caused havoc along the coast to show the extreme force a hurricane can do in a certain region. The Chennai Floods of 2015, which affected many people, is an example of flooding due to cyclones. People across the whole state of Tamil Nadu felt its impact and even parts of Andhra Pradesh got affected. There was a loss of Rs.900 crore and 280 people died. Many cyclones like this happen across Asia but the media reports only minor hurricanes which hit the United States.\n\nAlmost all storms with high wind and water cause erosion along the coast. Erosion occurs when but not limited to; along shore currents, tides, sea level rise and fall, and high winds. Larger amounts of erosion cause the coastline to erode away at a faster rate and can leave people homeless and leave less land to develop or keep for environmental reasons. Coastal erosion has been increasing over the past few years and it is still on the rise which makes it a major coastline hazard. In the United States, 45 percent of its coast line is along the Atlantic or Gulf coast and the erosion rate per year along the Gulf coast is at six feet a year. The average rate of erosion along the Atlantic is around two to three feet a year. Even with these findings, erosion rates in specific locations vary because of various environmental factors such as major storms that can cause major erosion upwards to 100 feet or more in only one day.\n\nPollution, Trawling and Human Development\n\nPollution, trawling, and human development are major human disasters that affect coastal regions. There are two main categories related to pollution, point source pollution, and nonpoint source pollution. Point source pollution is when there is an exact location such as a pipeline or a body of water that leads into the rivers and oceans. Known dumping into the ocean is also another point source of pollution. Nonpoint source pollution would pertain more to fertilizer runoff, and industrial waste. Examples of pollution that affect the coastal regions are but are not limited to; fertilizer runoff, oil spills, and dumping of hazardous materials into the oceans. More human acts that hurt the coastline are as follows; waste discharge, fishing, dredging, mining, and drilling. Oil spills are one of the most hazardous dangers towards coastal communities. They are hard to contain, difficult to clean up, and devastate everything. The fish, animals such as birds, the water, and especially the coastline near the spill. The most recent oil spill that had everybody concerned with oil spill was the BP oil spill.\n\nTrawling hurts the normal ecosystems in the water around the coastline. It depletes all ecosystems on the ocean floor such as, flounder, shellfish, marsh etc.. It is simply a giant net that is drug across the ocean floor and destroys and catches anything in its path. Human development is one of the major problems when facing coastal hazards. The overall construction of buildings and houses on the coast line takes away the natural occurrences to handle the fluctuation in water and sea level rise. Building houses in pre-flood areas or high risk areas that are extremely vulnerable to flooding are major concerns towards human development in coastal regions. Having houses and buildings in areas that are known to have powerful storms that will create people to be in risk by living there. Also pertaining to barrier islands, where land is at risk for erosion but they still continue to build there anyway. More and more houses today are being taken by the ocean; look at picture above.\n\nCoastal hazards & climate change\n\nThe predicted climate change is adding an extra risk factor to human settlement in coastal areas. Whereas the natural dynamics that shape our coastlines have been relatively stable and predictable over the last centuries, much more rapid change is now expected in processes as sea level rise, ocean temperature and acidity, tropical storm intensity and precipitation/runoff patterns. The world's coastlines will respond to these changes in different ways and at different pace depending on their bio-geophysical characteristics, but generally society will have to recognize that past coastal trends cannot be directly projected into the future. Instead, it is necessary to consider how different coastal environments will respond to the predicted climate change and take the expected future hazards into account in the coastal planning processes.\n\nNational Flood Insurance Program\n\nThe National Flood Insurance Program or NFIP was instituted in 1968 and offers home owners in qualifying communities an opportunity to rebuild and recover after flooding events following the decision by insurance companies to discontinue providing flood insurance. This decision was made on behalf of the private insurers after continually high and widespread flood losses. The goals of this program are to not only better protect individuals from flood, but to reduce property losses, and reduce the total amount disbursed for flood loses by the government. Only communities which have adopted and implemented mitigation policies that are compliant with or exceed federal regulations. The regulatory policies reduce risk to life and property located within floodplains. The NFIP also comprehensively mapped domestic floodplains increasing public awareness of risk. The majority of structures were constructed after the mapping was completed and risk could be assessed. To reduce the cost to these owners, which constitute roughly 25% of the total policies the rates for insurance are subsidized.\n\nCoastal States Organization\n\nThe Coastal States Organization or COS was established in 1970 to represent 35 U.S. sub-federal governments on issues of coastal policies. CSO lobbies Congress on issues pertaining to Coastal Policy allowing states input on federal policy decisions. Funding, support, water quality, coastal hazards, and coastal zone management are the primary issues COS promotes. The strategic goals of COS are to provide information and assistance to members,evaluate and manage coastal needs, and secure long term funding for member states initiatives.\n\nCoastal Zone Management Act\n\nIn 1972 the Coastal Zone Management Act or CZMA works to streamline the policies which states create to a minimum federal standard for environmental protection. CZMA establishes the national policy for the development and implementation of regulatory programs for coastal land usage, which is supposed to be reflected in state legislation such as CAMA. CZMA also provides minimum building requirements to make the insurance provided through the NFIP less expensive for the government to operate by mitigating losses. Congress found that it was necessary to establish the minimum which programs should provide for. Each coastal state is required to have a program with 7 distinct parts: Identifying land uses,Identifying critical coastal areas, Management measures,Technical assistance, Public participation, Administrative coordination, State coastal zone boundary modification.\n\nThe Coastal Area Management Act\n\nThe Coastal Area Management Act or CAMA is policy that was implemented by the state of North Carolina in 1974 to work in-tandem with the CZMA. It creates a cooperative program between the state and local governments. The State government operates in an advisory capacity and reviews decisions made by local government planners. The goal of this legislation was to create a management system capable of preserving the coastal environment, insure the preservation of land and water resources, balance the use of coastal resources and establish guidelines and standards for conservations, economic development, tourism, transportation, and the protection of common law.\n\nDue to the increasing urbanization along the coastlines, planning and management are essential to protecting the ecosystems and environment from depleting. Coastal management is becoming implemented more because of the movement of people to the shore and the hazards that come with the territory. Some of the hazards include movement of barrier islands, sea level rise, hurricanes, nor'easters, earthquakes, flooding, erosion, pollution and human development along the coast. The Coastal Zone Management Act (CZMA) was created in 1972 because of the continued growth along the coast, this act introduced better management practices such as integrated coastal zone management, adaptive management and the use mitigation strategies when planning. According to the Coastal Zone Management Act, the objectives are to remain balanced to \"preserve, protect, develop, and where possible, to restore or enhance the resources of the nation's coastal zone\".\nThe development of the land can strongly affect the sea, for example the engineering of structures versus non-structures and the effects of erosion along the shore.\n\nIntegrated coastal zone management\n\nIntegrated coastal zone management means the integration of all aspects of the coastal zone; this includes environmentally, socially, culturally politically and economically to meet a sustainable balance all around. Sustainability is the goal to allow development yet protect the environment in which we develop. Coastal zones are fragile and do not do well with change so it is important to acquire sustainable development. The integration from all views will entitle a holistic view for the best implementation and management of that country, region and local scales. The five types of integration include integration among sectors, integration between land and water elements of the coastal zone, integration amount levels of government, integration between nations and integration among disciplines are all essential to meet the needs for implementation.\nManagement practices include\nThese four management practices should be based on a bottom-up approach, meaning the approach starts from a local level which is more intimate to the specific environment of that area. After assessment from the local level, the state and federal input can be implemented. The bottom-up approach is key for protecting the local environments because there is a diversity of environments that have specific needs all over the world.\n\nAdaptive management\n\nAdaptive management is another practice of development adaptation with the environment. Resources are the major factor when managing adaptively to a certain environment to accommodate all the needs of development and ecosystems. Strategies used must be flexible by either passive or active adaptive management include these key features:\nTo achieve adaptive management is testing the assumptions to achieve a desired outcome, such as trial and error, find the best known strategy then monitoring it to adapt to the environment, and learning the outcomes of success and failures of a project.\n\nMitigation\n\nThe purpose of mitigation is not only to minimize the loss of property damage, but minimize environmental damages due to development. To avoid impacts by not taking or limiting actions, to reduce or rectify impacts by rehabilitation or restoring the affected environments or instituting long-term maintenance operations and compensating for impacts by replacing or providing substitute environments for resources\nStructural mitigation is the current solution to eroding beaches and movement of sand is the use of engineered structures along the coast have been short lived and are only an illusion of safety to the public that result in long term damage of the coastline. Structural management deals with the use of the following: groins which are man-made solution to longshore current movements up and down the coast. The use of groins are efficient to some extent yet cause erosion and sand build up further down the beaches. Bulkheads are man-made structures that help protect the homes built along the coast and other bodies of water that actually induce erosion in the long run. Jetties are structures built to protect sand movement into the inlets where boats for fishing and recreation move through.\nThe use of nonstructural mitigation is the practice of using organic and soft structures for solutions to protect against coastal hazards. These include: artificial dunes, which are used to create dunes that have been either developed on or eroded. There needs to be at least two lines of dunes before any development can occur. Beach Nourishment is a major source of nonstructural mitigation to ensure that beaches are present for the communities and for the protection of the coastline. Vegetation is a key factor when protecting from erosion, specifically for to help stabilize dune erosion.\n\n\n"}
{"id": "590687", "url": "https://en.wikipedia.org/wiki?curid=590687", "title": "Cobblestone", "text": "Cobblestone\n\nCobblestone is a natural building material based on cobble-sized stones, and is used for pavement roads, streets, and buildings.\n\nIn England, it was commonplace since ancient times for flat stones with a flat narrow edge to be set on edge to provide an even paved surface. This was known as a 'pitched' surface and was common all over Britain, as it did not require rounded pebbles. Pitched surfaces predate the use of regularly-sized granite setts by more than a thousand years. Such pitched paving is quite distinct from that formed from rounded stones, although both forms are commonly referred to as 'cobbled' surfaces. Most surviving genuinely old 'cobbled' areas are in reality pitched surfaces. A cobbled area is known as a \"causey\", \"cassay\" or \"cassie\" in Scots (probably from \"causeway\").\n\nSetts are often idiomatically referred to as \"cobbles\", although a sett is distinct from a cobblestone by being quarried or shaped to a regular form, whereas cobblestone is generally of a naturally occurring form.\n\nCobblestones are typically either set in sand or similar material, or are bound together with mortar. Paving with cobblestones allows a road to be heavily used all year long. It prevents the build-up of ruts often found in dirt roads. It has the additional advantage of not getting muddy in wet weather or dusty in dry weather. Shod horses are also able to get better traction on stone cobbles, pitches or setts than tarmac/asphalt. The fact that carriage wheels, horse hooves and even modern automobiles make a lot of noise when rolling over cobblestone paving might be thought a disadvantage, but it has the advantage of warning pedestrians of their approach. In England, the custom was to strew the cobbles outside the house of a sick or dying person with straw to dampen the sound.\n\nCobblestones set in sand have the environmental advantage of being permeable paving, and of moving rather than cracking with movements in the ground.\n\nCobblestones were largely replaced by quarried granite setts (also known as Belgian block) in the nineteenth century. The word cobblestone is often wrongly used to describe such treatment. Setts were relatively even and roughly rectangular stones that were laid in regular patterns. They gave a smoother ride for carts than cobbles, although in heavily used sections, such as in yards and the like, the usual practice was to replace the setts by parallel granite slabs set apart by the standard axle length of the time.\n\nCobblestoned and \"setted\" streets gradually gave way to macadam roads, and later to tarmac, and finally to asphalt concrete at the beginning of the 20th century. However, cobble­stones are often retained in historic areas, even for streets with modern vehicular traffic. Many older villages and cities in Europe are still paved with cobblestones or pitched.\n\nPopular television soap opera Coronation Street has cobblestones and they are often referenced in relation to reports on the show.\n\nIn recent decades, cobblestones have become a popular material for paving newly pedestrianised streets in Europe. In this case, the noisy nature of the surface is an advantage as pedestrians can hear approaching vehicles. The visual cues of the cobblestones also clarify that the area is more than just a normal street. The use of cobblestones/setts is also considered to be a more \"upmarket\" roadway solution, having been described as \"unique and artistic\" compared to the normal asphalt road environment.\n\nIn older U.S. cities such as Philadelphia, Boston, Pittsburgh, New York City, Chicago, San Francisco, New Castle, Portland (Maine), Baltimore, Charleston, and New Orleans, many of the older streets are paved in cobblestones and setts (mostly setts); however, many such streets have been paved over with asphalt, which can crack and erode away due to heavy traffic, thus revealing the original stone pavement.\n\nIn some places such as Saskatoon, Saskatchewan, Canada, as late as the 1990s some busy intersections still showed cobblestones through worn down sections of pavement. In Toronto streets using setts were used by streetcar routes and disappeared by the 1980s, but are still found in the Distillery District.\n\nMany cities in Latin America, such as Buenos Aires, Argentina; Zacatecas and Guanajuato, in Mexico; Old San Juan, Puerto Rico; Philippines, Vigan; and Montevideo, Uruguay, are well known for their many cobblestone streets, which are still operational and in good condition. They are still maintained and repaired the traditional manner, by placing and arranging granite stones by hand.\n\nIn the Czech Republic, there are old cobblestone paths with colored marbles and limestones. The design with three colors (red/limestone, black/limestone, white/marble) has a long tradition in Bohemia. The cubes of the old ways are handmade.\n\nIn the Finger Lakes Region of New York State, the retreat of the glaciers during the last ice age left numerous small, rounded cobblestones available for building. Pre-Civil War architecture in the region made heavy use of cobblestones for walls. Today, the fewer than 600 remaining cobblestone buildings are prized as historic locations, most of them private homes. They are clustered south of Lake Ontario, between Buffalo and Syracuse. There is also a cluster of cobblestone buildings in the Town of Paris, Ontario. In addition to homes, cobblestones were used to build barns, stagecoach taverns, smokehouses, stores, churches, schools, factories, and cemetery markers. The history of building with cobblestones and 17 driving tours to see the remaining structures are found in \"Cobblestone Quest - Road Tours of New York's Historic Buildings\".\n\nThe only public cobblestone building is the Alexander Classical School, located in Alexander, New York.\n\nIn cycling road races, cobblestones are used as an additional difficulty for the riders. It requires a certain skill to ride cobblestones efficiently, without falling or getting a flat tire. Tour of Flanders and Paris–Roubaix are notable cobbled classics.\n\n\n"}
{"id": "1106205", "url": "https://en.wikipedia.org/wiki?curid=1106205", "title": "Cognitive closure (philosophy)", "text": "Cognitive closure (philosophy)\n\nIn philosophy of science and philosophy of mind, cognitive closure is the proposition that human minds are constitutionally incapable of solving certain perennial philosophical problems. Owen Flanagan calls this position anti-constructive naturalism or the \"new mysterianism\" and the primary advocate of the hypothesis, Colin McGinn, calls it transcendental naturalism acknowledging the possibility that solutions may be an intelligent non-human of some kind. According to McGinn, such philosophical questions include the mind-body problem, identity of the self, foundations of meaning, free will, and knowledge, both \"a priori\" and empirical.\n\nFor Friedrich Hayek, \"The whole idea of the mind explaining itself is a logical contradiction\"... and \"takes this incompleteness—the constitutional inability of mind to explain itself—to be a generalized case of Gödel's incompleteness theorem... Hayek is \"not\" a naturalistic agnostic, that is, the view that science \"currently\" cannot offer an explanation of the mind-body relationship, but in principle it could.\"\n\nNoam Chomsky argues that the cognitive capabilities of all organisms are limited by biology and that certain problems may be beyond our understanding:\nAs argued in Kant's \"Critique of Pure Reason\", human thinking is unavoidably structured by categories of the understanding:\nThese are ideas to which there is no escape, thus they pose a limit to thinking. What can be known through the categories is called phenomena and what is outside the categories is called noumena, the unthinkable \"things in themselves\".\n\nIn his (famous) essay \"What Is It Like to Be a Bat?\" Thomas Nagel mentions the possibility of cognitive closure to the subjective character of experience and the (deep) implications that it has for materialist reductionist science. Owen Flanagan noted in his 1991 book \"Science of the Mind\" that some modern thinkers have suggested that consciousness will never be completely explained. Flanagan called them \"the new mysterians\" after the rock group Question Mark and the Mysterians. According to McGinn, the solution to the mind-body problem cannot be grasped, despite the fact that the solution is \"written in our genes\".\n\nEmergent materialism is a similar but different claim that humans are not smart enough to determine \"the relationship between mind and matter.\"\n\nWhile the nature of consciousness is complex, according to some philosophers, that does not imply closure, thus, McGinn's argument is flawed.\n\n"}
{"id": "3079141", "url": "https://en.wikipedia.org/wiki?curid=3079141", "title": "Deistic evolution", "text": "Deistic evolution\n\nDeistic evolution is a position in the origins debate which involves accepting the scientific evidence for evolution and age of the universe whilst advocating the view that a deistic God created the universe but has not interfered since. The position is a counterpoint to theistic evolution and is endorsed by those who believe in both deism and the veracity of science.\n\nIn \"Christian Theology\", by Millard J. Erickson, 2013, is written:\n\nThe psychologist Steve Stewart-Williams in his book \"Darwin, God and the Meaning of Life\" (2010) states:\n\nStewart-Williams further writes that deistic evolution strips God of what most religious believers consider central. Any deistic God is not around for prayers, miracles or to intervene in people's lives and that because of this it is unpopular with monotheistic religions.\n\nDeistic Evolution adheres to the concept of some form of God, but denies any personal God. A recent defender of deistic evolution was Michael Anthony Corey, author of the book \"Back to Darwin: The Scientific Case for Deistic Evolution\" (1994).\n\nSome scholars have written that Charles Darwin was an advocate of deistic evolution.\n\nDeistic evolution is similarly the operative idea in Pandeism, which has been counted amongst the handful of spiritual beliefs which \"are compatible with modern science.\" and specifically wherein it is noted that \"\"pandeistic\" belief systems ... [present] the inclusion of God as the ever unfolding expression of a complex universe with an identifiable beginning but no teleological direction necessarily present.\"\n\nDeistic evolution is not the same as theistic evolution, yet they are sometimes confused. The difference rests on the difference between a theistic god that is interested in, if not actively involved in, the outcome of his creation and humanity specifically and a deistic god that is either disinterested in the outcome, and holds no special place for humanity, or will not intervene. Often, there is no discernible difference between the two positions—the choice of terminology has more to do with the believer and her or his need for a god, than fitting into a mostly arbitrary dictionary or academic definition.\n\nDeistic evolution has been criticised by Christian creationists as being incompatible with Christianity since it contradicts a literal reading of the Bible and more importantly, leaves no role for the \"Christian personal God\".\n\nM. J. Erickson wrote that deistic evolution is in conflict with the scriptural doctrine of providence according to which \"God is personally and intimately concerned with and involved in what is going on in the specific events within his entire creation.\"\n\nCharles P. Grannan wrote in 1894, \"Another baseless assumption of negative critics is that the general principles of Atheistic and Deistic evolution, admitted by many scientists to account for the origin of the various species of plants and animals, should also be applied to explain the origin of the Christian religion.\"\n\nCharles Wesley Rishell criticized the concept in 1899, comparing it to the notion (false, in his view), that gravity was a property of matter instead of a continued action of God:\n\nDeistic evolution does not oppose or contradict evolution or come into conflict with science as it says that a God started the process and then left it to natural processes. However deism is still a religious philosophy.\n\nStewart-Williams wrote regarding deistic evolution and science:\n\nThere is considerable room for this \"god of the gaps\" view, since scientific observation is entirely unable to shed any light on what happened during the Planck epoch, the earliest 10 seconds in the history of the universe. All development since this initial creative act merely follows laws and principles which He created:\n\n\nThe Roman Catholic Church disagrees with the doctrine of deistic evolution. In November 2005, Pope Benedict addressed a general audience of 25,000 in St. Peter's Square:\n\n"}
{"id": "44701366", "url": "https://en.wikipedia.org/wiki?curid=44701366", "title": "Directed evolution (transhumanism)", "text": "Directed evolution (transhumanism)\n\nThe term directed evolution is used within the transhumanist community to refer to the idea of applying the principles of directed evolution and experimental evolution to the control of human evolution. In this sense, it is distinct from the use of the term in biochemistry, which refers only to the evolution of proteins and RNA. Maxwell J. Melhmanh has described directed evolution of humans as the Holy Grail of transhumanism.\nOxford philosopher Julian Savulescu wrote that:\nAccording to UCLA biophysicist Gregory Stock:\nRiccardo Campa, from the Institute for Ethics and Emerging Technologies, wrote that \"self-directed evolution\" can be coupled with many different political, philosophical, and religious views.\n\nAndrew Askland, from the Sandra Day O'Connor College of Law claims that referring to transhumanism as directed evolution is problematic because evolution is ateleological and transhumanism is teleological.\n\nParticipant evolution is an alternative term that refers to the process of deliberately redesigning the human body and brain using technological means, rather than through the natural processes of mutation and natural selection, with the goal of removing \"biological limitations\" and human enhancement. The idea of participant evolution was first put forward by Manfred Clynes and Nathan S. Kline in the 1960s in their article \"Cyborgs and Space\", where they argued that the human species was already on a path of participant evolution. Science fiction writers have speculated what the next stage of such participant evolution will be.\n\nWhilst Clynes and Kline saw participant evolution as the process of creating cyborgs, the idea has been adopted and propounded by transhumanists who argue that individuals should have the choice of using human enhancement technologies on themselves and their children, to progressively become transhuman and ultimately posthuman, as part of a voluntary regimen of participant evolution.\n"}
{"id": "458540", "url": "https://en.wikipedia.org/wiki?curid=458540", "title": "Dry stone", "text": "Dry stone\n\nDry stone, sometimes called drystack or, in Scotland, drystane, is a building method by which structures are constructed from stones without any mortar to bind them together. Dry stone structures are stable because of their unique construction method, which is characterized by the presence of a load-bearing façade of carefully selected interlocking stones.\n\nDry stone construction is best known in the context of stone walls, traditionally used for the boundaries of fields and churchyards, or as retaining walls for terracing, but dry stone sculptures, buildings, bridges, and other structures also exist.\n\nThe art of dry stone walling, was inscribed in 2018 on the UNESCO representative list of the intangible cultural heritage of humanity, for dry stone walls in countries such as France, Greece, Italy, and Spain.\n\nSome dry-stone wall constructions in north-west Europe have been dated back to the Neolithic Age. Some Cornish hedges are believed by the Guild of Cornish Hedgers to date from 5000 BC, although there appears to be little dating evidence. In County Mayo, Ireland, an entire field system made from dry-stone walls, since covered in peat, have been carbon-dated to 3800 BC. The cyclopean walls of the acropolis of Mycenae, Greece, have been dated to 1350 BC and those of Tiryns slightly earlier. In Belize, the Mayan ruins at Lubaantun illustrate use of dry stone construction in architecture of the 8th and 9th centuries AD.\n\nTerminology varies regionally. When used as field boundaries, dry stone structures often are known as dykes, particularly in Scotland. Dry stone walls are characteristic of upland areas of Britain and Ireland where rock outcrops naturally or large stones exist in quantity in the soil. They are especially abundant in the West of Ireland, particularly Connemara. They may also be found throughout the Mediterranean, including retaining walls used for terracing. Such constructions are common where large stones are plentiful (for example, in The Burren) or conditions are too harsh for hedges capable of retaining livestock to be grown as reliable field boundaries. Many thousands of miles of such walls exist, most of them centuries old.\n\nIn the United States they are common in areas with rocky soils, such as New England, New York, New Jersey, and Pennsylvania and are a notable characteristic of the bluegrass region of central Kentucky as well as Virginia, where they are usually referred to as \"rock fences\" or \"stone fences\", and the Napa Valley in north central California. The technique of construction was brought to America primarily by English and Scots-Irish immigrants. The technique was also taken to Australia (principally western Victoria and some parts of Tasmania and New South Wales) and New Zealand (especially Otago).\n\nSimilar walls also are found in the Swiss-Italian border region, where they are often used to enclose the open space under large natural boulders or outcrops.\n\nThe higher-lying rock-rich fields and pastures in Bohemia's South-Western border range of Šumava (e.g. around the mountain river of Vydra) are often lined by dry stone walls built of field-stones removed from the arable or cultural land. They serve both as cattle/sheep fences and the lot's borders. Sometimes also the dry stone terracing is apparent, often combined with parts of stone masonry (house foundations and shed walls) that are held together by a clay-cum-needles \"composite\" mortar. \n\nIn Peru in the 15th century AD, the Inca made use of otherwise unusable slopes by building dry stone walls to create terraces. They also employed this mode of construction for freestanding walls. Their ashlar type construction in Machu Picchu uses the classic Inca architectural style of polished dry-stone walls of regular shape. The Incas were masters of this technique, in which blocks of stone are cut to fit together tightly without mortar. Many junctions are so perfect that not even a knife fits between the stones. The structures have persisted in the high earthquake region because of the flexibility of the walls and that in their double wall architecture, the two portions of the walls incline into each other.\n\nA wall's style and method of construction will vary, depending on the type of stone available, its intended use and local tradition. Most older walls are constructed from stones and boulders cleared from the fields during preparation for agriculture (\"field stones\") but many also from stone quarried nearby. For modern walls, quarried stone is almost always used. The type of wall built will depend on the nature of the stones available.\n\nOne type of wall is called a \"double\" wall and is constructed by placing two rows of stones along the boundary to be walled. The foundation stones are ideally set into the ground so as to rest firmly on the subsoil. The rows are composed of large flattish stones, diminishing in size as the wall rises. Smaller stones may be used as chocks in areas where the natural stone shape is more rounded. The walls are built up to the desired height layer-by-layer (\"course by course\") and, at intervals, large tie-stones or \"through stones\" are placed which span both faces of the wall and sometimes project. These have the effect of bonding what would otherwise be two thin walls leaning against each other, greatly increasing the strength of the wall. Diminishing the width of the wall as it gets higher, as traditionally done in Britain, also strengthens the wall considerably. The voids between the facing stones are carefully packed with smaller stones (\"filling\", \"hearting\").\n\nThe final layer on the top of the wall also consists of large stones, called \"capstones\", \"coping stones\" or \"copes\". As with the tie stones, the capstones span the entire width of the wall and prevent it breaking apart. In some areas, such as South Wales, there is a tradition of placing the coping stones on a final layer of flat stones slightly wider than the top of the wall proper (\"coverbands\").\n\nIn addition to gates a wall may contain smaller purposely built gaps for the passage or control of wildlife and livestock such as sheep. The smaller holes usually no more than 8  inches in height are called 'Bolt Holes' or 'Smoots'. Larger ones may be between eighteen and 24 inches in height, these are called a 'Cripple Hole'.\n\nBoulder walls are a type of single wall in which the wall consists primarily of large boulders, around which smaller stones are placed. Single walls work best with large, flatter stones. Ideally, the largest stones are being placed at the bottom and the whole wall tapers toward the top. Sometimes a row of capstones completes the top of a wall, with the long rectangular side of each capstone perpendicular to the wall alignment.\n\nGalloway dykes consist of a base of double-wall construction or larger boulders with single-wall construction above. They appear to be rickety, with many holes, which deters livestock (and people) from attempting to cross them. These dykes are principally found in locations with exceptionally high winds, where a solid wall might be at risk of being unsettled by the buffeting. The porous nature of the wall significantly reduces wind force but takes greater skill to construct. They are also found in grazing areas where they are used to maximize the utility of the available stones (where ploughing was not turning up ever more stones).\n\nAnother variation is the \"Cornish hedge\" or Welsh \"clawdd\", which is a stone-clad earth bank topped by turf, scrub, or trees and characterised by a strict inward-curved batter (the slope of the \"hedge\"). As with many other varieties of wall, the height is the same as the width of the base, and the top is half the base width.\n\nDifferent regions have made minor modifications to the general method of construction — sometimes because of limitations of building material available, but also to create a look that is distinctive for that area. Whichever method is used to build a dry stone wall, considerable skill is required. Correcting any mistakes invariably means disassembling down to the level of the error. Selection of the correct stone for every position in the wall makes an enormous difference to the lifetime of the finished product, and a skilled waller will take time making the selection.\n\nAs with many older crafts, skilled wallers, today, are few in number. With the advent of modern wire fencing, fields can be fenced with much less time and expense using wire than using stone walls; however, the initial expense of building dykes is offset by their sturdiness and consequent long, low-maintenance lifetimes. As a result of the increasing appreciation of the landscape and heritage value of dry stone walls, wallers remain in demand, as do the walls themselves. A nationally recognised certification scheme is operated in the UK by the Dry Stone Walling Association, with four grades from Initial to Master Craftsman.\n\nNotable examples include:\n\nWhile the dry-stone technique is generally used for field enclosures, it also was used for buildings. The traditional turf-roofed Highland blackhouse was constructed using the double wall dry stone method. When buildings are constructed using this method, the middle of the wall is generally filled with earth or sand in order to eliminate draughts. During the Iron Age, and perhaps earlier, the technique also was used to build fortifications such as the walls of Eketorp Castle (Öland, Sweden), Maiden Castle, North Yorkshire, Reeth, Dunlough Castle in southwest Ireland and the rampart of the Long Scar Dyke. Many of the dry-stone walls that exist today in Scotland can be dated to the 14th century or earlier when they were built to divide fields and retain livestock. Some extremely well built examples are found on the lands of Muchalls Castle.\n\nDry stone walls can be built against embankments or even vertical terraces. If they are subjected to lateral earth pressure, they are retaining walls of the type gravity wall. The weight of the stones resists the pressure from the retained soil, including any surcharges, and the friction between the stones causes most of them to act as if being a monolithic gravity wall of the same weight. Dry stone retaining walls were once built in great numbers for agricultural terracing and also to carry paths, roads and railways. Although dry stone is seldom used for these purposes today, a great many are still in use and maintained. New ones are often built in gardens and nature conservation areas. Dry stone retaining structures continue to be a subject of research.\n\nSince at least the Middle Ages some bridges capable of carrying horse or carriage traffic have been constructed using drystone techniques. An example of a well-preserved bridge of this type is a double arched limestone bridge in Alby, Sweden on the island of Öland, \"(shown at right)\".\n\nIn northeastern Somalia, on the coastal plain 20 km to Aluula's east are found ruins of an ancient monument in a platform style. The structure is formed by a rectangular dry stone wall that is low in height; the space in between is filled with rubble and manually covered with small stones. Relatively large standing stones are also positioned on the edifice's corners. Near the platform are graves, which are outlined in stones. 24 m by 17 m in dimension, the structure is the largest of a string of ancient platform and enclosed platform monuments exclusive to far northeastern Somalia. Burial sites near Burao in the northwestern part of the country likewise feature a number of old stelae.\n\nIn Great Britain, Ireland and Switzerland, it is possible to find small dry stone structures built as signs, marking mountain paths or boundaries of owned land. In many countries, cairns, as they are called in Scotland, are used as road and mountain top markers.\n\n\n\n"}
{"id": "4316600", "url": "https://en.wikipedia.org/wiki?curid=4316600", "title": "Eagle-bone whistle", "text": "Eagle-bone whistle\n\nThe eagle bone whistle is a highly sacred religious object, used by some members of Native American spiritual societies in particularly sacred ceremonies. They are made from bones of either the American bald eagle or the American golden eagle, and are considered extremely powerful spiritual objects.\n\nEagle bone whistles are only used in certain ceremonies in the Southwest and Plains cultures. The eagle bone whistle may be considered as a ceremonial or sacred object which may not be considered a musical instrument, if music is defined as entertainment: \"There is no time or need...to wallow in distinctions between a feather-and-bone raptor and a bone whistle avian mysticism; one would no doubt end in dichotomous Western readings thereof.\"\n\nThe whistle is used in some Peyote ceremonies of some sects of the Native American Church. Eagle bone whistles are used in a number of Sun Dance cultures, such as the Crow. The eagle-bone whistle is also used by the Lakota people in certain ceremonies, such as some Sun Dances.\n\nNavajo/Ute flutist R. Carlos Nakai claims to use an \"eagle-bone whistle\" (or possibly an imitation one) on multiple albums.\n\nBoth the bald and golden eagle are protected by federal law: the Migratory Bird Treaty Act of 1918 (MBTA) prohibits the taking, killing, possession, transportation, and importation of migratory birds, their eggs, parts, and nests except as authorized under a valid permit as outlined at 50 CFR 21.11 The MBTA authorizes and directs the Secretary of the Interior to determine if, and by what means, the hunting of migratory birds should be allowed, as well as to adopt and implement suitable regulations permitting and governing the hunting of any type of migratory bird (for example, hunting seasons for ducks and geese). The Eagle feather law is another name for the exemptions to this act that are sometimes granted to enrolled members of federally recognized Native American tribes. Penalties under the MBTA include a maximum of two years imprisonment and $250,000 fine for a felony conviction and six months imprisonment or $5,000 fine for a misdemeanor conviction. Fines double if the violator is an organization rather than an individual. These laws would apply to the collection and use of eagle bone whistles.\n\n"}
{"id": "58614271", "url": "https://en.wikipedia.org/wiki?curid=58614271", "title": "Earth's crustal evolution", "text": "Earth's crustal evolution\n\nCrustal evolution involves the formation, destruction and renewal of the rocky outer shell at the Earth's surface.\n\nThe variation in composition within the Earth's crust is much greater than other terrestrial planets. Mars, Venus, Mercury and other planetary bodies have relatively quasi-uniform crusts unlike that of the Earth which contains both oceanic and continental plates. This unique property reflects the complex series of crustal processes that have taken place throughout the planet's history, including the ongoing process of plate tectonics.\n\nThe proposed mechanisms, regarding Earth's crustal evolution, take a theory-orientated approach. Fragmentary geologic evidence and observations provide the basis for hypothetical solutions to problems relating to the early Earth system. Therefore a combination of these theories creates both a framework of current understanding and also a platform for future study.\n\nThe early Earth was entirely molten. This was due to high temperatures created and maintained by the following processes:\n\n\nThe mantle remained hotter than modern day temperatures throughout the Archean. Over time the Earth began to cool as planetary accretion slowed and heat stored within the magma ocean was lost to space through radiation.\n\nA theory for the initiation of magma solidification states that once cool enough, the cooler base of the magma ocean would begin to crystallise first. This is because pressure of 25GPa at the surface cause the solidus to lower. The formation of a thin 'chill-crust' at the extreme surface would provide thermal insulation to the shallow sub surface, keeping it warm enough to maintain the mechanism of crystallisation from the deep magma ocean.\n\nThe composition of the crystals produced during the crystallisation of the magma ocean varied with depth. Experiments involving the melting of peridotite magma show that deep in the ocean (>≈700m), the main mineral present would be Mg-perovskite. Whereas olivine would dominate in the shallower areas along with its high pressure polymorphs e.g. garnet and majorite.\n\nA contributing theory to the formation of the first continental crust is through intrusive plutonic volcanism. The product of these eruptions formed a hot, thick lithosphere which underwent regular cycling with the mantle. The heat released by this form of volcanism, as well as assisting mantle convection, increased the geothermal gradient of the early crust.\n\nCrustal dichotomy represents the distinct contrast in composition and nature of the oceanic and continental plates, which together form the overall crust.\n\nOceanic and continental crusts are, at the present day, produced and maintained through plate tectonic processes. However the same mechanisms are unlikely to have produced the crustal dichotomy of the early lithosphere. This is thought to be true on the basis that sections of the thin, low density continental lithosphere could not have been sub-ducted under each other.\n\nSubsequently, a proposed relative timing for crustal dichotomy is put forward stating that dichotomy took place before the commencement of global plate tectonics. This is so a difference in crustal density could be established to facilitate plate subduction.\n\nLarge and numerous impact craters can be recognised on planetary bodies across the Solar System. These craters are thought to date back to a period where there was an increased frequency and intensity of asteroid impacts with terrestrial planets, known as the Late Heavy Bombardment, which terminated approximately 4Ga. This proposal goes on, claiming the Earth would have also sustained the same relative intensity of cratering as other planetesimals in the Solar System. It is therefore only due to Earth's high erosional rates and constant plate tectonics that the craters are not visible today. By scaling up the number and size of impact craters seen on the Moon to fit the size of Earth, it is predicted that at least 50% of the Earth's initial crust was covered in impact basins. This estimate provides a lower limit of the effect impact cratering had on the Earth's surface. \nThe main effects of impact cratering on the early lithosphere were:\n\n\nThe magnitude of these impacts is interpreted, with a high level of uncertainty, to have converted roughly half of the 'continental' crust into terrestrial maria. Therefore providing a method for the formation of crustal dichotomy, as seen today.\n\nThe initial crystallisation of minerals from the magma ocean formed the primordial crust. \n\nA potential explanation of this process states the resultant solidification of the mantle edge took place approximately 4.43Ga. This would subsequently produce continents composed of komatiite, an ultramafic rock rich in magnesium with a high melting point and low dynamic viscosity. Another line of research follows up on this, proposing that differences in the densities of newly formed crystals caused separation of crustal rocks; upper crust largely composed of fractionated gabbros and lower crust composed of anorthosites. The overall result of initial crystallisation formed a primordial crust roughly 60 km in depth.\n\nThe lack of certainty regarding the formation of primordial crust is due to there being no remaining present day examples. This is due to Earth's high erosional rates and the subduction and subsequent destruction of tectonic plates throughout its 4.5 Ga history. Furthermore, during its existence the primordial crust is thought to have been regularly broken and re-formed by impacts involving other planetesimals. This continued for several hundred million years after accretion, which concluded approximately 4.4Ga. The outcome of this would be the constant alteration in the composition of the primordial crust, increasing the difficulty in determining its nature. \n\nRecycling of existing primordial crust contributes to the production of secondary crust. Partial melting of the existing crust increases the mafic content of the melt producing basaltic secondary crust. A further method of formation due to the decay of radioactive elements within the Earth releasing heat energy and eventually causing the partial melting of upper mantle, also producing basaltic lavas. As a result, most secondary crust on Earth is formed at mid ocean ridges forming the oceanic crust.\n\nThe present day continental crust is an example of a tertiary crust. Tertiary crust is the most differentiated type of crust and so has a composition vastly different to that of the bulk Earth. The tertiary crust contains over 20% of the abundance of incompatible elements, which are elements a size or charge that prevent them from being included in mineral structure. This a result of its generation from the subduction and partial melting of secondary crust where it undergoes further fractional crystallisation. Two stages of evolution produce an increased proportion of incompatible elements.\n\nThe formation and development of plumes in the early mantle contributed to triggering the lateral movement of crust across the Earth's surface. The effect of upwelling mantle plumes on the lithosphere can be seen today through local depressions around hotspots such as Hawaii. The scale of this impact is much less than that exhibited in the Archean eon where mantle temperatures were much greater. Localised areas of hot mantle rose to the surface through a central plume wedge, weakening the damaged and already thin lithosphere. Once the plume head breaks the surface, crust either side of the head is forced downwards through the conservation of mass, initiating subduction. Numerical modelling shows only strongly energetic plumes are capable of weakening the lithosphere enough to rupture it, such plumes would have been present in the hot Archean mantle.\n\nPre-tectonic subduction can also be inferred from the internal volcanism on Venus. Artemis Corona is a large plume formed by the upwelling of mantle derived magma and is on a scale potentially comparable to that in the Archean mantle. Models using its known characteristics showed that continued magmatism from conductive heat through the plume caused gravitational collapse. The weight of collapse caused the spreading of the surrounding crust outwards and subsequent subduction around the margins. The anhydrous nature of the crust on Venus prevents it from sliding past each other, whereas through the study of oxygen isotopes, the presence of water on Earth can be confirmed from 4.3Ga. Thus, this model helps provide a mechanism for how plate tectonics could have been triggered on Earth, although it does not demonstrate that subduction was initiated at the earliest confirmed presence of water on Earth. Based on these models, the onset of subduction and plate tectonics is dated at 3.6Ga.\n\nImpact cratering also had consequences for both the development of plume-induced subduction and the establishment of global plate tectonics. The steepening of geothermal gradients could have directly enhanced convective mantle transport which now beneath an increasingly fractured lithosphere could have created stresses great enough to cause rifting and the separation of crust into plates.\n\nCrustal growth rates can be used to calculate estimates for the age of the continental crust. This can be done through analysis of igneous rocks with the same isotopic composition as initial mantle rock. These igneous rocks are dated and assumed to be direct evidence of new continental crust formation. The resulting ages of isotopically juvenile igneous rocks give distinct peaks, representing an increased proportion of igneous rock and therefore increased crust growth, at 2.7, 1.9 and 1.2 Ga. The validity of these results is questioned as the peaks could represent periods of preservation rather than increased continental crust generation. This is reinforced by the fact that such peaks are not observed in recent geologic time where it is given that magmatism resulting from the plate subduction has strongly contributed to producing new crust.\n\nCrustal growth rates from igneous rocks can be compared to the rates generated from radiogenic isotope ratios in sedimentary rocks. Projections of growth rates using these techniques does not produce staggered peaks, instead smooth shallow curves presenting a more constant rate of crustal growth. Although representative of large periods of time, limitations are found where samples do not solely represent magmatic production events. Instead samples include the mixing of sediments which produces a mix of original and altered isotope ratios.\n\nZircon minerals can be both detrital grains from sedimentary rocks and crystals in igneous rocks. Therefore, a combination of zircon forms can provide a more accurate estimate of crustal growth rates. Further to this, zircon minerals can be subject to Hf and O isotope ratio analysis. This is important as Hf isotopes indicate whether a rock originates from the mantle or an existing rock. High δO values of zircons represent rock recycled at the Earth's surface and thus potentially producing mixed samples. The outcome of this combined analysis is valid zircons showing periods of increased crustal generation at at 1.9 and 3.3Ga, the latter of which representing the time period following the commencement of global plate tectonics.\n"}
{"id": "212485", "url": "https://en.wikipedia.org/wiki?curid=212485", "title": "Earth religion", "text": "Earth religion\n\nEarth religion is a term used mostly in the context of neopaganism.\n\nEarth-centered religion or nature worship is a system of religion based on the veneration of natural phenomena. It covers any religion that worships the earth, nature, or fertility deity, such as the various forms of goddess worship or matriarchal religion. Some find a connection between earth-worship and the Gaia hypothesis. Earth religions are also formulated to allow one to utilize the knowledge of preserving the earth.\n\nAccording to Marija Gimbutas, pre-Indo-European societies lived in small-scale, family-based communities that practiced matrilineal succession and goddess-centered religion where creation comes from the woman. She is the Divine Mother who can give life and take it away. In Irish mythology she is Danu, in Slavic mythology she is Mat Zemlya, and in other cultures she is Pachamama, Ninsun, Terra Mater, Nüwa, Matres or Shakti.\n\nIn the late 1800s, James Weir wrote an article describing the beginnings and aspects of early religious feeling. According to Boyer, early man was forced to locate food and shelter in order to survive, while constantly being directed by his instincts and senses. Because man's existence depended on nature, men began to form their religion and beliefs on and around nature itself. It is evident that man's first religion would have had to develop from the material world, he argues, because man relied heavily on his senses and what he could see, touch, and feel. In this sense, the worship of nature formed, allowing man to further depend on nature for survival.\n\nNeopagans have tried to make claims that religion started in ways that correspond to earth religion. In one of their published works, \"The Urantia Book\", another reason for this worship of nature came from a fear of the world around primitive man. His mind lacked the complex function of processing and sifting through complex ideas. As a result, man worshiped the very entity that surrounded him every day. That entity was nature. Man experienced the different natural phenomena around him, such as storms, vast deserts, and immense mountains. Among the very first parts of nature to be worshiped were rocks and hills, plants and trees, animals, the elements, heavenly bodies, and even man himself. As primitive man worked his way through nature worship, he eventually moved on to incorporate spirits into his worship. Although these claims may have some merit, they are nonetheless presented from a biased position that cannot be authenticated by traditional and reliable sources. Therefore, their claims can not be relied upon.\n\nThe origins of religion can be looked at through the lens of the function and processing of the human mind. Pascal Boyer suggests that, for the longest period of time, the brain was thought of as a simple organ of the body. However, he claims that the more information collected about the brain indicates that the brain is indeed not a \"blank slate.\" Humans do not just learn any information from the environment and surroundings around them. They have acquired sophisticated cognitive equipment that prepares them to analyze information in their culture and determine which information is relevant and how to apply it. Boyer states that \"having a normal human brain does not imply that you have religion. All it implies is that people can acquire it, which is very different.\" He suggests that religions started for the reasons of providing answers to humans, giving comfort, providing social order to society, and satisfying the need of the illusion-prone nature of the human mind. Ultimately, religion came into existence because of our need to answer questions and hold together our societal order.\n\nAn additional idea on the origins of religion comes not from man's cognitive development, but from the ape. Barbara J. King argues that human beings have an emotional connection with those around them, and that that desire for a connection came from their evolution from apes. The closest relative to the human species is the African ape. At birth, the ape begins negotiating with its mother about what it wants and needs in order to survive. The world the ape is born into is saturated with close family and friends. Because of this, emotions and relationships play a huge role in the ape's life. Its reactions and responses to one another are rooted and grounded in a sense of belongingness, which is derived from its dependence on the ape's mother and family. Belongingness is defined as \"mattering to someone who matters to you ... getting positive feelings from our relationships.\" This sense and desire for belongingness, which started in apes, only grew as the hominid (a human ancestor) diverged from the lineage of the ape, which occurred roughly six to seven million years ago.\n\nAs severe changes in the environment, physical evolutions in the human body (especially in the development of the human brain), and changes in social actions occurred, humans went beyond trying to simply form bonds and relationships of empathy with others. As their culture and society became more complex, they began using practices and various symbols to make sense of the natural and spiritual world around them. Instead of simply trying to find belongingness and empathy from the relationships with others, humans created and evolved God and spirits in order to fulfil that need and exploration. King argued that \"an earthly need for belonging led to human religious imagination and thus to the otherworldly realm of relating to God, gods, and spirits.\"\n\nThe term \"earth religion\" encompasses any religion that worships the earth, nature or fertility gods or goddesses. There is an array of groups and beliefs that fall under earth religion, such as paganism, which is a polytheistic, nature based religion; animism, which is the worldview that all living entities (plants, animals, and humans) possess a spirit; Wicca, who hold the concept of an earth mother goddess as well as practice ritual magic; and druidism, which equates divinity with the natural world.\n\nAnother perspective of earth religion to consider is pantheism, which takes a varied approach to the importance and purpose of the earth, and man's relationship with the planet. Several of their core statements deal with the connectivity humans share with the planet, declaring that \"all matter, energy, and life are an interconnected unity of which we are an inseparable part\" and \"we are an integral part of Nature, which we should cherish, revere and preserve in all its magnificent beauty and diversity. We should strive to live in harmony with Nature locally and globally\".\n\nThe earth also plays a vital role to many Voltaic peoples, many of whom \"consider the Earth to be Heaven’s wife\", such as the Konkomba of northern Ghana, whose economic, social and religious life is heavily influenced by the earth. It is also important to consider various Native American religions, such as Peyote Religion, Longhouse Religion, and Earth Lodge Religion.\n\nApril 22 was established as International Mother Earth Day by the United Nations in 2009, but many cultures around the world have been celebrating the Earth for thousands of years. Winter solstice and Summer solstice are celebrated with holidays like Yule and Dongzhi in the winter and Tiregān and Kupala in the summer.\n\nAnimism is practiced among the Bantu peoples of Sub-Saharan Africa. The Dahomey mythology has deities like Nana Buluku, Gleti, Mawu, Asase Yaa, Naa Nyonmo and Xevioso.\n\nIn Baltic mythology, the sun is a female deity, Saule, a mother or a bride, and Mēness is the moon, father or husband, their children being the stars. In Slavic mythology Mokosh and Mat Zemlya together with Perun head up the pantheon. Celebrations and rituals are centered on nature and harvest seasons. Dragobete is a traditional Romanian spring holiday that celebrates \"the day when the birds are betrothed.\"\n\nIn Hindu philosophy, the yoni is the creative power of nature and the origin of life. In Shaktism, the yoni is celebrated and worshipped during the Ambubachi Mela, an annual fertility festival which celebrates the Earth's menstruation.\n\nAlthough the idea of earth religion has been around for thousands of years, it did not fully show up in popular culture until the early 1990s. \"The X-Files\" was one of the first nationally broadcast television programs to air witchcraft and Wicca (types of earth religion) content. On average, Wiccans - those who practice Wicca - were more or less pleased with the way the show had portrayed their ideals and beliefs. However, they still found it to be a little \"sensationalistic\". That same year, the movie \"The Craft\" was released - also depicting the art of Wicca. Unfortunately, this cinematic feature was not as happily accepted as \"The X-Files\" had been.\n\nA few years later, programs showcasing the aforementioned religious practices - such as \"Charmed\" and \"Buffy the Vampire Slayer\" - became widely popular. Although \"Charmed\" focused mostly on witchcraft, the magic they practiced very closely resembled Wicca. Meanwhile, \"Buffy\" was one of the first shows to actually cast a Wiccan character. However, since the shows focus was primarily on vampires, the Wiccan was depicted as having supernatural powers, rather than being in-tuned with the Earth.\n\nOther movies and shows throughout the last few decades have also been placed under the genre of Earth Religion. Among them are two of director Hayao Miyazaki's most well known films - \"Princess Mononoke\" and \"My Neighbor Totoro\". Both movies present human interaction with land, animal, and other nature spirits. Speakers for Earth Religion have said that these interactions suggest overtones of Earth Religion themes.\n\nSome popular Disney movies have also been viewed as Earth Religion films. Among them are \"The Lion King\" and \"Brother Bear\". Those who practice Earth Religion view \"The Lion King\" as an Earth Religion film mainly for the \"interconnectedness\" and \"Circle of Life\" it shows between the animals, plants, and life in general. When that link is broken, viewers see chaos and despair spread throughout the once bountiful land. Congruently, \"Brother Bear\" portrays interactions and consequences when humans disobey or go against the animal and Earth spirits.\n\nOther earth religion movies include \"The 13th Warrior\", \"The Deceivers (film)\", \"Sorceress (1982 film)\", \"Anchoress (film)\", \"Eye of the Devil\", \"Agora (film)\", and \"The Wicker Man (1973 film)\". These movies all contain various aspects of earth religion and nature worship in general.\n\nMany religions have negative stereotypes of earth religion and neo-paganism in general. A common critique of the worship of nature and resources of \"Mother Earth\" is that the rights of nature and ecocide movements are inhibitors of human progress and development. This argument is fueled by the fact that those people socialized into 'western' world views believe the earth itself is not a living being. Wesley Smith believes this is “anti-humanism with the potential to do real harm to the human family.” According to Smith, earth worshipers are hindering large-scale development, and they are viewed as inhibitors of advancement.\n\nA lot of criticism of earth religion comes from the negative actions of a few people who have been chastised for their actions. One such negative representative of earth religion is Aleister Crowley. He is believed to be \"too preoccupied with awakening magical powers\" instead of putting the well-being of others in his coven. Crowley allegedly looked up to \"Old George\" Pickingill, who was another worshipper of nature who was viewed negatively. Critics regarded Pickingill as a Satanist and \"England’s most notorious Witch\".\n\nCrowley himself was \"allegedly expelled from the Craft because he was a pervert.\" He became aroused by torture and pain, and enjoyed being \"punished\" by women. This dramatically damaged Crowley’s public image, because of his lifestyle and actions. Many people regarded all followers of earth religion as perverted Satanists.\n\nFollowers of earth religion have suffered major opprobrium over the years for allegedly being Satanists. Some religious adherents can be prone to viewing religions other than their religion as being wrong sometimes because they perceive those religions as characteristic of their concept of Satan worship. To wit, Witchcraft, a common practice of Wiccans, is sometimes misinterpreted as Satan worship by members of these groups, as well as less-informed persons who may not be specifically religious but who may reside within the sphere-of-influence of pagan-critical religious adherents. From the Wiccan perspective, however, earth religion and Wicca lie outside of the phenomenological world that encompasses Satanism. An all-evil being does not exist within the religious perspective of western earth religions. Devotees worship and celebrate earth resources and earth-centric deities. Satanism and Wicca \"have entirely different beliefs about deity, different rules for ethical behavior, different expectations from their membership, different views of the universe, different seasonal days of celebration, etc.\"\n\nNeo-pagans, or earth religion followers, often claim to be unaffiliated with Satanism. Neo-pagans, Wiccans, and earth religion believers do not acknowledge the existence of a deity that conforms to the common Semitic sect religious concept of Satan. Satanism stems from Christianity, while earth religion stems from older religious concepts.\n\nSome earth religion adherents take issue with the religious harassment that is inherent in the social pressure that necessitates their having to distance themselves from the often non-uniform, Semitic sect religious concept of Satan worship. Having to define themselves as \"other\" from a religious concept that is not within their worldview implies a certain degree of outsider-facilitated, informal, but functional religious restriction that is based solely on the metaphysical and mythological religious beliefs of those outsiders. This is problematic because outsider initiated comparisons to Satanism with the intent of condemnation, even when easily refuted, can have the effect of social pressure on earth religion adherents to conform to outsider perception of acceptable customs, beliefs, and modes of religious behavior.\n\nTo illustrate, a problem could arise with the \"other\" than Satanism argument if an earth centered belief system adopted a holiday that a critic considered to be similar or identical to a holiday that Satanists celebrate. Satanists have historically been prone to adopting holidays that have origins in various pagan traditions, ostensibly because these traditional holidays are amongst the last known vestiges of traditional pre-Semitic religious practice in the west. Satanists are, perhaps irrationally, prone to interpreting non-Semitic holidays as anti-Christian and therefore as implicitly representative of their worldview. This is not surprising given the fact that this is, in fact, how many Christians interpret holidays such as Samhain. In spite of any flawed perceptions or rationale held by any other group, earth centered religion adherents do not recognize misinterpretation of their customs made by outside religious adherents or critics inclusive of Satan worshippers.\n\nOrganized Satan worship, as defined by and anchored in the Semitic worldview, is characterized by a relatively disorganized and often disparate series of movements and groups that mostly emerged in the mid-20th century. Thus, their adopted customs have varied, continue to vary, and therefore this moving target of beliefs and customs can not be justifiably nor continuously accounted for by earth centered religious adherents. Once a Satanist group adopts a holiday, social stigma may unjustifiably taint the holiday and anyone who observes it without discrimination as to whence and for what purpose it was originally celebrated. Given these facts, many earth centered religion devotees find comparisons to Satanism intrinsically oppressive in nature. This logic transfers to any and all religious customs to include prayer, magic, ceremony, and any unintentional similarity in deity characteristics (an example is the horned traditional entity Pan having similar physical characteristics to common horned depictions of Satan).\n\nThe issue is further complicated by the theory that the intra and extra-biblical mythology of Satan that is present throughout various Semitic sects may have originally evolved to figuratively demonize the heathen religions of other groups. Thus, the concept of Satan, or \"the adversary\", would have been representative of all non-Semitic religions and, by extension, the people who believed in them. Although, at times, the concept of the \"other\" as demonic has also been used to characterize competing Semitic sects. Amongst other purposes, such belief would have been extraordinarily useful during the psychological and physical process of cleansing Europe of traditional tribal beliefs in favor of Christianity. This possibility would account for the historical tendency of Christian authorities, for example, to deem most pagan customs carried out in the pagan religious context as demonic. By any modern standard, such current beliefs would violate western concepts of religious tolerance as well as be inimical to the preservation of what remains of the culture of long-persecuted religious groups.\n\nBecause of the vast diversity of religions that fall under the title of \"earth religion\" there is no consensus of beliefs. However, the ethical beliefs of most religions overlap. The most well-known ethical code is the Wiccan Rede. Many of those who practice an earth religion choose to be environmentally active. Some perform activities such as recycling or composting while others feel it to be more productive to try and support the earth spiritually. These six beliefs about ethics seem to be universal.\n\n\"An [if] it harm none, do what ye will.\" Commonly worded in modern English as \"if it doesn't harm anyone, do what you want.\" This maxim was first printed in 1964, after being spoken by the priestess Doreen Valiente in the mid-20th century, and governs most ethical belief of Wiccans and some Pagans. There is no consensus of beliefs but this rede provides a starting point for most people's interpretation of what is ethical. The rede clearly states to do no harm but what constitutes as harm and what level of self-interest is acceptable is negotiable. Many Wiccans reverse the phrase into \"Do what ye will an it harm none,\" meaning \"Do what you want if it doesn't harm anyone.\" The difference may not seem significant but it is. The first implies that it is good to do no harm but does not say that it is necessarily unethical to do so, the second implies that all forms of harm are unethical. The second phrase is nearly impossible to follow. This shift occurred when trying to better adapt the phrase into modern English as well as to stress the \"harmlessness\" of Wiccans. The true nature of the rede simply implies that there is personal responsibility for your actions. You may do as you wish but there is a karma reaction from every action. Even though this is the most well-known rede of practice, it does not mean that those that choose not to follow it are unethical. There are many other laws of practice that other groups follow.\n\nThe Threefold Law is the belief that for all actions there is always a cause and effect. For every action taken either the good or ill intention will be returned to the action taker threefold. This is why the Wiccan Rede is typically followed because of fear of the threefold return from that harmful action.\n\nThis term is what Emma Restall Orr calls reverence for the earth in her book \"Living with Honour: A Pagan Ethics\". She separates the term into three sections: courage, generosity and loyalty, or honesty, respect and responsibility. There is no evil force in Nature. Nothing exists beyond the natural, therefore it is up to the individual to choose to be ethical not because of divine judgment. All beings are connected by the earth and so all should be treated fairly. There is a responsibility toward the environment and a harmony should be found with nature.\n\nThe following was written by the Church of All Worlds in 1988 and was affirmed by the Pagan Ecumenical Conferences of Ancient Ways (California, May 27–30) and Pagan Spirit Gathering (Wisconsin, June 17). The Pagan Community Council of Ohio then presented it to the Northeast Council of W.I.C.C.A.\n\n\"We, the undersigned, as adherents of Pagan and Old and Neo-Pagan Earth Religions, including Wicca or Witchcraft, practice a variety of positive, life affirming faiths that are dedicated to healing, both of ourselves and of the Earth. As such, we do not advocate or condone any acts that victimize others, including those proscribed by law. As one of our most widely accepted precepts is the Wiccan Rede's injunction to \"harm none,\" we absolutely condemn the practices of child abuse, sexual abuse and any other form of abuse that does harm to the bodies, minds or spirits of the victims of such abuses. We recognize and revere the divinity of Nature in our Mother the Earth, and we conduct our rites of worship in a manner that is ethical, compassionate and constitutionally protected. We neither acknowledge or worship the Christian devil, \"Satan,\" who is not in our Pagan pantheons. We will not tolerate slander or libel against our Temples, clergy or Temple Assemblers and we are prepared to defend our civil rights with such legal action as we deem necessary and appropriate.\"\n"}
{"id": "5593595", "url": "https://en.wikipedia.org/wiki?curid=5593595", "title": "Earth structure", "text": "Earth structure\n\nAn earth structure is a building or other structure made largely from soil. Since soil is a widely available material, it has been used in construction since prehistoric times.\nIt may be combined with other materials, compressed and/or baked to add strength. \nSoil is still an economical material for many applications, and may have low environmental impact both during and after construction.\n\nEarth structure materials may be as simple as mud, or mud mixed with straw to make cob. Sturdy dwellings may be also built from sod or turf. Soil may be stabilized by the addition of lime or cement, and may be compacted into rammed earth. Construction is faster with pre-formed adobe or mudbricks, compressed earth blocks, earthbags or fired clay bricks.\n\nTypes of earth structure include earth shelters, where a dwelling is wholly or partly embedded in the ground or encased in soil. Native American earth lodges are examples. Wattle and daub houses use a \"wattle\" of poles interwoven with sticks to provide stability for mud walls. Sod houses were built on the northwest coast of Europe, and later by European settlers on the North American prairies. Adobe or mud-brick buildings are built around the world and include houses, apartment buildings, mosques and churches. Fujian Tulous are large fortified rammed earth buildings in southeastern China that shelter as many as 80 families. Other types of earth structure include mounds and pyramids used for religious purposes, levees, mechanically stabilized earth retaining walls, forts, trenches and embankment dams.\n\nSoil is created from rock that has been chemically or physically weathered, transported, deposited and precipitated.\nSoil particles include sand, silt and clay. Sand particles are the largest at in diameter and clay the smallest at less than in diameter.\nBoth sand and silt are mostly inert rock particles, including quartz, calcite, feldspar and mica.\n\nClays typically are phyllosilicate minerals with a sheet-like structure.\nThe very small clay particles interact with each other physically and chemically. \nEven a small proportion of clay affects the physical properties of the soil much more than might be expected.\nClays such as kaolinite do not expand or contract when wetted or dried, and are useful for brick-making. \nOthers, such as smectites, expand or contract considerably when wet or dry, and are not suitable for building.\n\nLoam is a mix of sand, silt and clay in which none predominates. \nSoils are given different names depending on the relative proportions of sand, silt and clay such as \"Silt Loam\", \"Clay Loam\" and \"Silty Clay\".\nLoam construction, the subject of this article, referred to as adobe construction when it uses unfired clay bricks, is an ancient building technology. It was used in the early civilizations of the Mediterranean, Egypt and Mesopotamia, in the Indus, Ganges and Yellow river valleys, in Central and South America. \nAs of 2005 about 1.5 billion people lived in houses built of loam.\n\nIn recent years, interest in loam construction has revived in the developed world. It is seen as a way to minimize use of fossil fuels and pollution, particularly carbon dioxide, during manufacture, and to create a comfortable living environment through the high mass and high absorption of the material.\nThe two main technologies are stamped or rammed earth, clay or loam, called \"pise de terre\" in French, and adobe, typically using sun-dried bricks made of a mud and straw mixture.\n\nEarth usually requires some sort of processing for use in construction. It may be combined with water to make mud, straw may be added, some form of stabilizing material such as lime or cement may be used to harden the earth, and the earth may be compacted to increase strength.\n\nCoursed mud construction is one of the oldest approaches to building walls. Moist mud is formed by hand to make the base of a wall, and allowed to dry. More mud is added and allowed to dry to form successive courses until the wall is complete. With puddled mud, a hand-made mud form is filled with wetter mud and allowed to dry.\nIn Iran, puddled mud walls are called \"chine\" construction. Each course is about thick, and about high. Typically the technique is used for garden walls but not for house construction, presumably because of concern about the strength of walls made in this way.\nA drawback to the approach is that a lot of time can be spent waiting for each course to dry.\nAnother technique, used in areas where wood is plentiful, is to build a wood frame house and to infill it with mud, primarily to provide insulation. In parts of England a similar technique was used with cob.\n\nCob, sometimes referred to as \"monolithic adobe\", is a natural building material made from soil that includes clay, sand or small stones and an organic material such as straw. Cob walls are usually built up in courses, have no mortar joints and need 30% or more clay in the soil. Cob can be used as in-fill in post-and-beam buildings, but is often used for load bearing walls, and can bear up to two stories. A cob wall should be at least thick, and the ratio of width to height should be no more than one to ten. It will typically be plastered inside and out with a mix of lime, soil and sand. Cob is fireproof, and its thermal mass helps stabilize indoor temperatures. \nTests have shown that cob has some resistance to seismic activity. However, building codes in the developed world may not recognize cob as an approved material.\n\nCut sod bricks, called \"terrone\" in Spanish, can be used to make tough and durable walls. The sod is cut from soil that has a heavy mat of grass roots, which may be found in river bottom lands. It is stood on edge to dry before being used in construction.\nEuropean settlers on the North American Prairies found that the sod least likely to deteriorate due to freezing or rain came from dried sloughs. Turf was once extensively used for the walls of houses in Ireland, Scotland and Iceland, where some turf houses may still be found. A turf house may last fifty years or longer if well-maintained in a cold climate.\nThe Icelanders find that the best quality turf is the \"Strengur\", the top of the grass turf.\n\nClay is usually hard and strong when dry, but becomes very soft when it absorbs water. The dry clay helps hold an earth wall together, but if the wall is directly exposed to rain, or to water leaking down from the roof, it may become saturated and collapse.\nEarth may be \"stabilized\" to make it more weather resistant. The practice of stabilizing earth by adding burnt lime is centuries old.\nPortland cement or bitumen may also be added to earth intended for construction which adds strength, although the stabilized earth is not as strong as fired clay or concrete. Mixtures of cement and lime, or pozzolana and lime, may also be used for stabilization.\n\nPreferably the sand content of the soil will be 65% – 75%. Soils with low clay content, or with no more than 15% non-expansive clay, are suitable for stabilized earth. The clay percentage may be reduced by adding sand, if available.\nIf there is more than 15% clay it may take more than 10% cement to stabilize the soil, which adds to the cost.\nIf earth contains little clay and holds 10% or more cement, it is in effect concrete.\nCement is not particularly environmentally friendly, since the manufacturing process generates large amounts of carbon dioxide.\nLow-density stabilized earth will be porous and weak. The earth must therefore be compacted either by a machine that makes blocks or within the wall using the \"rammed earth\" technique.\n\nRammed earth is a technique for building walls using natural raw materials such as earth, chalk, lime or gravel.\nA rammed earth wall is built by placing damp soil in a temporary form. The soil is manually or mechanically compacted and then the form is removed.\nRammed earth is generally made without much water, and so does not need much time to dry as the building rises. It is susceptible to moisture, so must be laid on a course that stops rising dampness, must be roofed or covered to keep out water from above, and may need protection through some sort of plaster, paint or sheathing.\n\nIn China, rammed earth walls were built by the Longshan people in 2600–1900 BC, during the period when cities first appeared in the region. Thick sloping walls made of rammed earth became a characteristic of traditional Buddhist monasteries throughout the Himalayas and became very common in northern Indian areas such as Sikkim. The technique spread to the Middle East, and to North Africa, and the city of Carthage was built of rammed earth. From there the technology was brought to Europe by the Romans.\nRammed earth structures may be long lasting. Most of the Great Wall of China was made from rammed earth, as was the Alhambra in the Kingdom of Granada. In Northern Europe there are rammed earth buildings up to seven stories high and two hundred years old.\n\nThe Romans made durable concrete strong enough for load-bearing walls. Roman concrete contains a rubble of broken bricks and rocks set in mortar. The mortar included lime and \"pozzolana\", a volcanic material that contributed significantly to its strength. Roman concrete structures such as the Colosseum, completed in 80 AD, still stand.\nTheir longevity may be explained by the fact that the builders used a relatively dry mix of mortar and aggregate and compacted it by pounding it down to eliminate air pockets. Although derived from earth products, concrete structures would not usually be considered earth structures.\n\nMudbricks or Adobe bricks are preformed modular masonry units of sun-dried mud that were invented at different times in different parts of the world as civilization developed. Construction with bricks avoids the delays while each course of puddled mud dries. Wall murals show that adobe production techniques were highly advanced in Egypt by 2500 BC. \nAdobe construction is common throughout much of Africa today. \nAdobe bricks are traditionally made from sand and clay mixed with water to a plastic consistency, with straw or grass as a binder.\nThe mud is prepared, placed in wooden forms, tamped and leveled, and then turned out of the mold to dry for several days. \nThe bricks are then stood on end to air-cure for a month or more.\n\nIn the southwest United States and Mexico adobe buildings had massive walls and were rarely more than two stories high. \nAdobe mission churches were never more than about .\nSince adobe surfaces are fragile, coatings are used to protect them. \nThese coatings, periodically renewed, have included mud plaster, lime plaster, whitewash or stucco.\nAdobe walls were historically made by laying the bricks with mud mortar, which swells and shrinks at the same rate as the bricks when wetted or dried, heated or cooled. Modern adobe may be stabilized with cement and bonded with cement mortars, but cement mortars will cause unstabilized adobe bricks to deteriorate due to the different rates of thermal expansion and contraction.\n\nCompressed earth blocks (CEB) were traditionally made by using a stick to ram soil into a wooden mold. Today they are usually made from subsoil compressed in a hand-operated or powered machine. In the developing world, manual machines can be a cost-effective solution for making uniform building blocks, while the more complex and expensive motorized machines are less likely to be appropriate. \nAlthough labor-intensive, CEB construction avoids the cost of buying and transporting materials.\nBlock-making machines may form blocks that have interlocking shapes to reduce the requirement for mortar.\nThe block may have holes or grooves so rods such as bamboo can be inserted to improve earthquake resistance.\n\nSuitable earth must be used, with enough clay to hold the block together and resist erosion, but not too much expansive clay.\nWhen the block has been made from stabilized earth, which contains cement, the concrete must be given perhaps three weeks to cure.\nDuring this time the blocks should be stacked and kept from drying out by sprinkling water over them. \nThis may be a problem in hot, dry climates where water is scarce.\nClosely stacking the blocks and covering them with a polythene sheet may help reduce water loss.\n\nEarthbag construction is a natural building technique that has evolved from historic military construction techniques for bunkers.\nLocal subsoil of almost any composition can be used, although an adobe mix would be preferable.\nThe soil is moistened so it will compact into a stable structure when packed into woven polypropylene or burlap sacks or tubes. Plastic mesh is sometimes used. Polypropylene (pp) sacks are most common, since they are durable when covered, cheap, and widely available.\nThe bags are laid in courses, with barbed wire between each course to prevent slipping. Each course is tamped after it is laid.\nThe structure in pp bags is similar to adobe but more flexible. With mesh tubing the structure is like rammed earth.\nEarthbags may be used to make dome-shaped or vertical wall buildings. With soil stabilization they may also be used for retaining walls.\n\nThe technique of firing clay bricks in a kiln dates to about 3500 BC. \nFired bricks were being used to build durable masonry across Europe, Asia and North Africa by 1200 BC and still remain an important building material. Modern fired clay bricks are formed from clays or shales, shaped and then fired in a kiln for 8–12 hours at a temperature of 900–1150 °C.\nThe result is a ceramic that is mainly composed of silica and alumina, with other ingredients such as quartz sand. The porosity of the brick depends on the materials and on the firing temperature and duration. The bricks may vary in color depending on the amount of iron and calcium carbonate in the materials used, and the amount of oxygen in the kiln.\nBricks may decay due to crystallization of salts on the brick or in its pores, from frost action and from acidic gases.\n\nBricks are laid in courses bonded with mortar, a combination of Portland cement, lime and sand.\nA wall that is two bricks thick will include stretcher bricks with their long, narrow side exposed and header bricks crossing from side to side. \nThere are various brickwork \"bonds\", or patterns of stretchers and headers, including the English, Dutch and Flemish bonds.\n\nEarth sheltering has been used for thousands of years to make energy-efficient dwellings.\nThere are various configurations. At one extreme, an earth sheltered dwelling is completely underground, with perhaps an open courtyard to provide air and light. \nAn earth house may be set into a slope, with windows or door openings in one or more of its sides, or the building may be on ground level, but with earth mounded against the walls, and perhaps with an earth roof.\n\nPit houses made by Hohokam farmers between 100 and 900 AD, in what is now the southwest of the US, were bermed structures, partially embedded in south-facing slopes. Their successful design was used for hundreds of years.\nAt Matmata, Tunisia, most of the ancient homes were built below ground level, and surrounded courtyards about square.\nThe homes were reached through tunnels. Other examples of subterranean, semi-subterranean or cliff-based dwellings in both hot and cold climates are found in Turkey, northern China and the Himalayas, and the southwest USA. A number of Buddhist monasteries built from earth and other materials into cliff sides or caves in Himalayan areas such as Tibet, Bhutan, Nepal and northern India are often perilously placed. Starting in the 1970s, interest in the technique has revived in developed countries. By setting an earth house into the ground, the house will be cooler in the warm season and warmer in the cool season.\n\nAn earth lodge is a circular building made by some of the Native Americans of North America. \nThey have wood post and beam construction and are dome-shaped.\nA typical structure would have four or more central posts planted in the ground and connected at the top by cross beams. The smoke hole would be left open in the center. Around the central structure there was a larger ring of shorter posts, also connected by cross beams. Rafters radiated from the central cross beams to the outside cross beams, and then split planks or beams formed the slanting or vertical side walls.\nThe structure was covered by sticks and brush or grass, covered in turn by a heavy layer of earth or sod.\nSome groups plastered the whole structure with mud, which dried to form a shell.\nWattle and daub is an old building technique in which vines or smaller sticks are interwoven between upright poles, and then mud mixed with straw and grass is plastered over the wall.\nThe technique is found around the world, from the Nile Delta to Japan, where bamboo was used to make the wattle.\nIn Cahokia, now in Illinois, USA, wattle and daub houses were built with the floor lowered by below the ground. \nA variant of the technique is called \"bajareque\" in Colombia.\nIn prehistoric Britain simple circular wattle and daub shelters were built wherever adequate clay was available.\nWattle and daub is still found as the panels in timber-framed buildings. Generally the walls are not structural, and in interior use the technique in the developed world was replaced by lath and plaster, and then by gypsum wallboard.\n\nEuropean pioneer farmers in the prairies of North America, where there is no wood for construction, often made their first home in a dug-out cave in the side of a hill or ravine, with a covering over the entrance. When they had time, they would build a sod house. The farmer would use a plow to cut the sod into bricks , which were then piled up to form the walls.\nThe sod strips were piled grass-side down, staggered in the same way as brickwork, in three side-by-side rows, resulting in a wall over thick. The sod wall was built around door and window frames, and the corners of the wall were secured by rods driven vertically through them. The roof was made with poles or brush, covered with prairie grass, and then sealed with a layer of sod.\nSod houses were strong and often lasted many years, but they were damp and dirty unless the interior walls were plastered.\nThe roofs tended to leak, and sometimes collapsed in a rainstorm.\n\nThere are innumerable examples of mud brick or adobe building around the world. The walled city of Shibam in Yemen, designated a World Heritage Site in 1982, is known fr its ten-story unreinforced mud-brick buildings.\nThe Djinguereber Mosque of Timbuktu, Mali, was first built at the start of the 14th century AD (8th century AH) from round mud bricks and a stone-mud misture, and was rebuilt several times afterwards, steadily growing in size.\nFurther south in Mali, the Great Mosque of Djenné, a dramatic example of Sahel mudbrick architecture. was built in 1907, based on the design of an earlier Great Mosque first built on the site in 1280. Mudbrick requires maintenance, and the fundamentalist ruler Seku Amadu had let the previous mosque collapse.\n\nThe Casa Grande Ruins, now a national monument in Arizona protected by a modern roof, is a massive four-story adobe structure built by Hohokam people between 1200 and 1450 AD.\nThe first European to record the great house was a Jesuit priest, Father Eusebio Kino, who visited the site in 1694. \nAt that time it had long been abandoned.\nBy the time a temporary roof was installed in 1903 the adobe building had been standing empty and unmaintained for hundreds of years.\n\nHuaca de la Luna in what is now northern Peru is a large adobe temple built by the Moche people. The building went through a series of construction phases, growing eventually to a height of about , with three main platforms, four plazas and many smaller rooms and enclosures. The walls were covered by striking multi-colored murals and friezes; those visible today date from about 400–610 AD.\n\nA Fujian Tulou is a type of rural dwelling of the Hakka people in the mountainous areas in southeastern Fujian, China.\nThey were mostly built between the 13th and the 20th centuries. \nA tulou is a large, enclosed and fortified earth building, rectangular or circular, with very thick load-bearing rammed earth walls between three and five stories high.\nA toulou might house up to 80 families. Smaller interior buildings are often enclosed by these huge peripheral walls which can contain halls, storehouses, wells and living areas.\nThe structure resembles a small fortified city.\nThe walls are formed by compacting earth mixed with stone, bamboo, wood and other readily available materials, and are to thick. \nThe result is a well-lit, well-ventilated, windproof and earthquake-proof building that is warm in winter and cool in summer.\n\nZiggurats were elevated temples constructed by the Sumerians between the end of the 4th millennium BC and the 2nd millennium BC, rising in a series of terraces to a temple up to above ground level. The Ziggurat of Ur contained about three million bricks, none more than in length, so construction would have been a huge project.\nThe largest ziggurat was in Babylon, and is thought by some to be the Tower of Babel mentioned in the Bible. It was destroyed by Alexander the Great and only the foundations remain, but originally it stood high on a base about square.\nSun-dried bricks were used for the interior and kiln-fired bricks for the facing. \nThe bricks were held together by clay or bitumen.\n\nMany pre-Columbian Native American societies of ancient North America built large pyramidal earth structures known as platform mounds. Among the largest and best-known of these structures is Monks Mound at the site of Cahokia in what became Illinois, completed around 1100 AD, which has a base larger than that of the Great Pyramid at Giza. Many of the mounds underwent multiple episodes of mound construction at periodic intervals, some becoming quite large. They are believed to have played a central role in the mound-building peoples' religious life and documented uses include semi-public chief's house platforms, public temple platforms, mortuary platforms, charnel house platforms, earth lodge/town house platforms, residence platforms, square ground and rotunda platforms, and dance platforms.\n\nThe Pyramid of the Sun in Teotihuacan, Mexico, was started in 100 AD. \nThe stone-faced structure contains two million tons of rammed earth.\n\nEarthworks are engineering works created through moving or processing quantities of soil or unformed rock. \nThe material may be moved to another location and formed into a desired shape for a purpose.\nLevees, embankments and dams are types of earthwork.\nA levee, floodbank or stopbank is an elongated natural ridge or artificially constructed dirt fill wall that regulates water levels. It is usually earthen and often runs parallel to the course of a river in its floodplain or along low-lying coastlines.\n\nMechanically stabilized earth (MSE) retaining walls may be used for embankments.\nMSE walls combine a concrete leveling pad, wall facing panels, coping, soil reinforcement and select backfill.\nA variety of designs of wall facing panels may be used.\nAfter the leveling pad has been laid and the first row of panels has been placed and braced, the first layer of earth backfill is brought in behind the wall and compacted.\nThe first set of reinforcements is then laid over the earth.\nThe reinforcements, which may be tensioned polymer or galvanized metal strips or grids, are attached to the facing panels.\nThis process is repeated with successive layers of panels, earth and reinforcements.\nThe panels are thus tied into the earth embankment to make a stable structure with balanced stresses.\n\nAlthough construction using the basic principles of MSE has a long history, MSE was developed in its current form in the 1960s. The reinforcing elements used can vary but include steel and geosynthetics. The term MSE is usually used in the US to distinguish it from \"Reinforced Earth\", a trade name of the Reinforced Earth Company, but elsewhere Reinforced Soil is the generally accepted term. MSE construction is relatively fast and inexpensive, and although labor-intensive, it does not demand high levels of skill. It is therefore suitable for developing as well as developed countries.\n\nEarth has been used to construct fortifications for thousands of years, including strongholds and walls, often protected by ditches.\nAerial photography in Europe has revealed traces of earth fortifications from the Roman era, and later medieval times.\nOffa's Dyke is a huge earthwork that stretches along the disputed border between England and Wales.\nLittle is known about the period or the builder, King Offa of Mercia, who died in 796 AD.\nAn early timber and earth fortification might later be succeeded by a brick or stone structure on the same site.\n\nTrenches were used by besieging forces to approach a fortification while protected from missiles.\nSappers would build \"saps\", or trenches, that zig-zagged towards the fortress being attacked.\nThey piled the excavated dirt to make a protective wall or gabion. The combined trench depth and gabion height might be . \nSometimes the sap was a tunnel, dug several feet below the surface. Sappers were highly skilled and highly paid due to the extreme danger of their work.\n\nIn the American Civil War (1861−1865) trenches were used for defensive positions throughout the struggle, but played an increasingly important role in the campaigns of the last two years.\nMilitary earthworks perhaps culminated in the vast network of trenches built during World War I (1914−1918) that stretched from Switzerland to the North Sea by the end of 1914.\nThe two lines of trenches faced each other, manned by soldiers living in appalling conditions of cold, damp and filth.\nConditions were worst in the Allied trenches. The Germans were more willing to accept the trenches as long-term positions, and used concrete blocks to build secure shelters deep underground, often with electrical lighting and heating.\n\nAn embankment dam is a massive artificial water barrier. It is typically created by the emplacement and compaction of a complex semi-plastic mound of various compositions of soil, sand, clay and/or rock. It has a semi-permanent natural waterproof covering for its surface, and a dense, waterproof core. This makes such a dam impervious to surface or seepage erosion.\nThe force of the impoundment creates a downward thrust upon the mass of the dam, greatly increasing the weight of the dam on its foundation. This added force effectively seals and makes waterproof the underlying foundation of the dam, at the interface between the dam and its stream bed. \nSuch a dam is composed of fragmented independent material particles. \nThe friction and interaction of particles binds the particles together into a stable mass rather than by the use of a cementing substance.\n\nThe Syncrude Mildred Lake Tailings Dyke in Alberta, Canada, is an embankment dam about long and from high. By volume of fill, as of 2001 it was believed to be the largest earth structure in the world.\n\nRegions with low seismic risk are safe for most earth buildings, but historic construction techniques often cannot resist even medium earthquake levels effectively because of earthen buildings' three highly undesirable qualities as a seismic building material: being relatively 'weak, heavy and brittle'. However, earthen buildings can be built to resist seismic loads.\n\nKey factors to improved seismic performance are soil strength, construction quality, robust layout and seismic reinforcement.\n\nStronger soils make stronger walls. Adobe builders can test cured blocks for strength by dropping from a specific height or by breaking them with a lever. Builders using immediate techniques like earthbag, cob, or rammed earth may prefer approximate crushing tests on smaller samples that can be oven-dried and crushed under a small lever.\n\nBuilders must understand construction processes and be able to produce consistent quality for strong buildings.\n\nRobust layout means buildings more square than elongated, and symmetrical not L-shaped, as well as no 'soft' first stories (stories with large windows, buildings on unbraced columns). New Zealand's earthen building guidelines check for enough bracing wall length in each of the two principal directions, based on wall thickness, story height, bracing wall spacing, and the roof, loft and second story weight above earthen walls.\n\nBuilding techniques that are more ductile than brittle, like the contained earth type of earthbag, or tire walls of earthships, may better avoid collapse than brittle unreinforced earth. Contained gravel base courses may add base isolation potential.\n\nWall containment can be added to techniques like adobe to resist loss of material that leads to collapse. Confined masonry is effective for adobe against quake forces of 0.3 g may be useful with earthen masonry.\n\nMany types of reinforcement can increase wall strength, such as plastic or wire mesh and reinforcing rods of steel or fiberglass or bamboo. Earth resists compression well but is weak when twisted. Tensile reinforcement must span potential damage points and be well-anchored to increase out-of-plane stability. Bond beams at wall tops are vital and must be well attached to walls.\n\nBuilders should be aware that organic reinforcements embedded in walls may be destroyed before the building is retired. Attachment details of reinforcement are critical to resist higher forces. Best adobe shear strength came from horizontal reinforcement attached directly to vertical rebar spanning from footing to bond beam.\n\nInterlaced wood in earthen walls reduces quake damage if wood is not damaged by dry rot or insects. Timberlacing includes finely webbed Dhajji, and other types.\n\n"}
{"id": "38328166", "url": "https://en.wikipedia.org/wiki?curid=38328166", "title": "Energy broker", "text": "Energy broker\n\nEnergy brokers assist clients in procuring electric or natural gas from energy wholesalers/suppliers. Since electricity and natural gas are commodities, prices change daily with the market. It is challenging for most businesses without energy managers to obtain price comparisons from a variety of suppliers since prices must be compared on exactly the same day. In addition, the terms of the particular contract offered by the supplier influences the price that is quoted. An energy broker can provide a valuable service if they work with a large number of suppliers and can actually compile the sundry prices from suppliers. An important aspect of this consulting role is to assure that the client understands the differences between the contract offers. Under some State Laws they use the term \"Suppliers\" to refer to energy suppliers, brokers, and aggregators, however there are very important differences between them all.\n\nEnergy brokers do not own or distribute energy, nor are allowed to sell energy directly to you. They simply present the rates of a wholesaler, or supplier.\n\nEnergy consultants offer a lot more than procuring energy contracts from a supplier. In the UK and Europe where there is a lot of legislation and increasing pressure for businesses and countries to do more to reduce their energy consumption a lot of services from brokers now help ensure businesses meet a lot of compliance and accreditation requirements such as the ESOS (energy saving opportunity scheme), ISO 50001, ISO 14001, Energy Performance Certificates and Display Energy Certificates.\nOther services include helping companies reduce energy consumption with the aim of meeting national and international carbon emission standards. Services include, energy health checks, energy audits, carbon zero, carbon offsetting and energy saving consulting.\n\nAdditional services such as arranging a power purchase agreement, energy export contracts can be procured as well as energy monitoring and reporting technology and solutions are also offered by energy consultants.\n\nIn the USA, energy brokers can serve residential, commercial and government entities that reside in energy deregulated states. In the UK, and some countries in Europe, the entire market is deregulated.\n\nEnergy brokers typically do not charge up front fees to provide rates. If an entity purchases energy through a broker, the broker's fee is usually included in the rate the customer pays. Some brokers will charge a fixed fee for their consulting services.\n\nNot all energy brokers are consultants; However, the energy brokers who are also consultants will perform a more detailed analysis of a consumers' usage pattern in order to provide a custom rate, which typically results in more cost savings for the consumer. Typically, they do not need any more information than that of an energy broker, because they can pull usage information from the local utility company. There are some national energy brokers that use auditing teams to verify their client's invoices.\n"}
{"id": "500948", "url": "https://en.wikipedia.org/wiki?curid=500948", "title": "Field guide", "text": "Field guide\n\nA field guide is a book designed to help the reader identify wildlife (plants or animals) or other objects of natural occurrence (e.g. minerals). It is generally designed to be brought into the 'field' or local area where such objects exist to help distinguish between similar objects. Field guides are often designed to help users distinguish animals and plants that may be similar in appearance but are not necessarily closely related.\n\nIt will typically include a description of the objects covered, together with paintings or photographs and an index. More serious and scientific field identification books, including those intended for students, will probably include identification keys to assist with identification, but the publicly accessible field guide is more often a browsable picture guide organized by family, colour, shape, location or other descriptors.\n\nPopular interests in identifying things in nature probably were strongest in bird and plant guides. Perhaps the first popular field guide to plants in the United States was the 1893 \"How to Know the Wildflowers\" by \"Mrs. William Starr Dana\" (Frances Theodora Parsons). In 1890, Florence Merriam published \"Birds Through an Opera-Glass\", describing 70 common species. Focused on living birds observed in the field, the book is considered the first in the tradition of modern, illustrated bird guides. In 1902, now writing as Florence Merriam Bailey (having married the zoologist Vernon Bailey), she published \"Handbook of Birds of the Western United States\". By contrast, the \"Handbook\" is designed as a comprehensive reference for the lab rather a portable book for the field. It was arranged by taxonomic order and had clear descriptions of species size, distribution, feeding, and nesting habits.\n\nFrom this point into the 1930s, features of field guides were introduced by Chester A. Reed and others such as changing the size of the book to fit the pocket, including colour plates, and producing guides in uniform editions that covered subjects such as garden and woodland flowers, mushrooms, insects, and dogs.\n\nIn 1934, Roger Tory Peterson, using his fine skill as an artist, changed the way modern field guides approached identification. Using color plates with paintings of similar species together – and marked with arrows showing the differences – people could use his bird guide in the field to compare species quickly to make identification easier. This technique, the \"Peterson Identification System\", was used in most of Peterson's Field Guides from animal tracks to seashells and has been widely adopted by other publishers and authors as well.\n\nToday, each field guide has its own range, focus and organization. Specialist publishers such as Croom Helm, along with organisations like the Audubon Society, the RSPB, the Field Studies Council, National Geographic, HarperCollins, and many others all produce quality field guides.\n\nIt is somewhat difficult to generalise about how field guides are intended to be used, because this varies from one guide to another, partly depending on how expert the targeted reader is expected to be.\n\nFor general public use, the main function of a field guide is to help the reader identify a bird, plant, rock, butterfly or other natural object down to at least the popular naming level. To this end some field guides employ simple keys and other techniques: the reader is usually encouraged to scan illustrations looking for a match, and to compare similar-looking choices using information on their differences. Guides are often designed to first lead readers to the appropriate section of the book, where the choices are not so overwhelming in number.\n\nGuides for students often introduce the concept of identification keys. Plant field guides such as \"Newcomb's Wildflower Guide\" (which is limited in scope to the wildflowers of northeastern North America) frequently have an abbreviated key that helps limit the search. Insect guides tend to limit identification to Order or Family levels rather than individual species, due to their diversity.\n\nMany taxa show variability and it is often difficult to capture the constant features using a small number of photographs. Illustrations by artists or post processing of photographs help in emphasising specific features needed to for reliable identification. Peterson introduced the idea of lines to point to these key features. He also noted the advantages of illustrations over photographs:\n\nField guides aid in improving the state of knowledge of various taxa. By making the knowledge of experienced museum specialists available to amateurs, they increase the gathering of information by amateurs from a wider geographic area and increasing the communication of these findings to the specialists.\n\n"}
{"id": "159472", "url": "https://en.wikipedia.org/wiki?curid=159472", "title": "Flight", "text": "Flight\n\nFlight is the process by which an object moves through an atmosphere (or beyond it, as in the case of spaceflight) without contact with the surface. This can be achieved by generating aerodynamic lift associated with propulsive thrust, aerostatically using buoyancy, or by ballistic movement.\n\nMany things can fly, from natural aviators such as birds, bats, and insects, to human inventions like aircraft, including airplanes, helicopters, balloons, and rockets which may carry spacecraft.\n\nThe engineering aspects of flight are the purview of aerospace engineering which is subdivided into aeronautics, the study of vehicles that travel through the air, and astronautics, the study of vehicles that travel through space, and in ballistics, the study of the flight of projectiles.\n\nHumans have managed to construct lighter than air vehicles that raise off the ground and fly, due to their buoyancy in air.\n\nAn aerostat is a system that remains aloft primarily through the use of buoyancy to give an aircraft the same overall density as air. Aerostats include free balloons, airships, and moored balloons. An aerostat's main structural component is its envelope, a lightweight skin that encloses a volume of lifting gas to provide buoyancy, to which other components are attached.\n\nAerostats are so named because they use \"aerostatic\" lift, a buoyant force that does not require lateral movement through the surrounding air mass to effect a lifting force. By contrast, aerodynes primarily use aerodynamic lift, which requires the lateral movement of at least some part of the aircraft through the surrounding air mass.\n\nSome things that fly do not generate propulsive thrust through the air, for example, the flying squirrel. This is termed gliding. Some other things can exploit rising air to climb such as raptors (when gliding) and man-made sailplane gliders. This is termed soaring. However most other birds and all powered aircraft need a source of propulsion to climb. This is termed powered flight.\n\nThe only groups of living things that use powered flight are birds, insects, and bats, while many groups have evolved gliding. The extinct Pterosaurs, an order of reptiles contemporaneous with the dinosaurs, were also very successful flying animals. Each of these groups' wings evolved independently. The wings of the flying vertebrate groups are all based on the forelimbs, but differ significantly in structure; those of insects are hypothesized to be highly modified versions of structures that form gills in most other groups of arthropods.\n\nBats are the only mammals capable of sustaining level flight (see \"bat flight\"). However, there are several gliding mammals which are able to glide from tree to tree using fleshy membranes between their limbs; some can travel hundreds of meters in this way with very little loss in height. Flying frogs use greatly enlarged webbed feet for a similar purpose, and there are flying lizards which fold out their mobile ribs into a pair of flat gliding surfaces. \"Flying\" snakes also use mobile ribs to flatten their body into an aerodynamic shape, with a back and forth motion much the same as they use on the ground.\n\nFlying fish can glide using enlarged wing-like fins, and have been observed soaring for hundreds of meters. It is thought that this ability was chosen by natural selection because it was an effective means of escape from underwater predators. The longest recorded flight of a flying fish was 45 seconds.\n\nMost birds fly (\"see bird flight\"), with some exceptions. The largest birds, the ostrich and the emu, are earthbound, as were the now-extinct dodos and the Phorusrhacids, which were the dominant predators of South America in the Cenozoic era. The non-flying penguins have wings adapted for use under water and use the same wing movements for swimming that most other birds use for flight. Most small flightless birds are native to small islands, and lead a lifestyle where flight would offer little advantage.\n\nAmong living animals that fly, the wandering albatross has the greatest wingspan, up to ; the great bustard has the greatest weight, topping at .\n\nMost species of insects can fly as adults. Insect flight makes use of either of two basic aerodynamic models: creating a leading edge vortex, found in most insects, and using clap and fling, found in very small insects such as thrips.\n\nMechanical flight is the use of a machine to fly. These machines include aircraft such as airplanes, gliders, helicopters, autogyros, airships, balloons, ornithopters as well as spacecraft. Gliders are capable of unpowered flight. Another form of mechanical flight is para-sailing where a parachute-like object is pulled by a boat. In an airplane, lift is created by the wings; the shape of the wings of the airplane are designed specially for the type of flight desired. There are different types of wings: tempered, semi-tempered, sweptback, rectangular and elliptical. An aircraft wing is sometimes called an airfoil, which is a device that creates lift when air flows across it.\n\nSupersonic flight is flight faster than the speed of sound. Supersonic flight is associated with the formation of shock waves that form a sonic boom that can be heard from the ground, and is frequently startling. This shockwave takes quite a lot of energy to create and this makes supersonic flight generally less efficient than subsonic flight at about 85% of the speed of sound.\n\nHypersonic flight is very high speed flight where the heat generated by the compression of the air due to the motion through the air causes chemical changes to the air. Hypersonic flight is achieved by reentering spacecraft such as the Space Shuttle and Soyuz.\nSome things generate little or no lift and move only or mostly under the action of momentum, gravity, air drag and in some cases thrust. This is termed \"ballistic flight\". Examples include balls, arrows, bullets, fireworks etc.\n\nEssentially an extreme form of ballistic flight, spaceflight is the use of space technology to achieve the flight of spacecraft into and through outer space. Examples include ballistic missiles, orbital spaceflight etc.\n\nSpaceflight is used in space exploration, and also in commercial activities like space tourism and satellite telecommunications. Additional non-commercial uses of spaceflight include space observatories, reconnaissance satellites and other earth observation satellites.\n\nA spaceflight typically begins with a rocket launch, which provides the initial thrust to overcome the force of gravity and propels the spacecraft from the surface of the Earth. Once in space, the motion of a spacecraft—both when unpropelled and when under propulsion—is covered by the area of study called astrodynamics. Some spacecraft remain in space indefinitely, some disintegrate during atmospheric reentry, and others reach a planetary or lunar surface for landing or impact.\n\nMany human cultures have built devices that fly, from the earliest projectiles such as stones and spears, the\nboomerang in Australia, the hot air Kongming lantern, and kites.\n\nGeorge Cayley studied flight scientifically in the first half of the 19th century, and in the second half of the 19th century Otto Lilienthal made over 200 gliding flights and was also one of the first to understand flight scientifically. His work was replicated and extended by the Wright brothers who made gliding flights and finally the first controlled and extended, manned powered flights.\n\nSpaceflight, particularly human spaceflight became a reality in the 20th century following theoretical and practical breakthroughs by Konstantin Tsiolkovsky and Robert H. Goddard. The first orbital spaceflight was in 1957, and Yuri Gagarin was carried aboard the first manned orbital spaceflight in 1961.\n\nThere are different approaches to flight. If an object has a lower density than air, then it is buoyant and is able to float in the air without expending energy. A heavier than air craft, known as an aerodyne, includes flighted animals and insects, fixed-wing aircraft and rotorcraft. Because the craft is heavier than air, it must generate lift to overcome its weight. The wind resistance caused by the craft moving through the air is called drag and is overcome by propulsive thrust except in the case of gliding.\n\nSome vehicles also use thrust for flight, for example rockets and Harrier Jump Jets.\n\nFinally, momentum dominates the flight of ballistic flying objects.\n\nForces relevant to flight are\n\nThese forces must be balanced for stable flight to occur.\n\nA fixed-wing aircraft generates forward thrust when air is pushed in the direction opposite to flight. This can be done in several ways including by the spinning blades of a propeller, or a rotating fan pushing air out from the back of a jet engine, or by ejecting hot gases from a rocket engine. The forward thrust is proportional to the mass of the airstream multiplied by the difference in velocity of the airstream. Reverse thrust can be generated to aid braking after landing by reversing the pitch of variable-pitch propeller blades, or using a thrust reverser on a jet engine. Rotary wing aircraft and thrust vectoring V/STOL aircraft use engine thrust to support the weight of the aircraft, and vector sum of this thrust fore and aft to control forward speed.\n\nIn the context of an air flow relative to a flying body, the lift force is the component of the aerodynamic force that is perpendicular to the flow direction. Aerodynamic lift results when the wing causes the surrounding air to be deflected - the air then causes a force on the wing in the opposite direction, in accordance with Newton's third law of motion.\n\nLift is commonly associated with the wing of an aircraft, although lift is also generated by rotors on rotorcraft (which are effectively rotating wings, performing the same function without requiring that the aircraft move forward through the air). While common meanings of the word \"lift\" suggest that lift opposes gravity, aerodynamic lift can be in any direction. When an aircraft is cruising for example, lift does oppose gravity, but lift occurs at an angle when climbing, descending or banking. On high-speed cars, the lift force is directed downwards (called \"down-force\") to keep the car stable on the road.\n\nLift can also occur in a different way if the air is not still, especially if there is an updraft due to heat (\"thermals\") or wind blowing along sloping terrain or other meteorological conditions. This form of lift permits soaring and is particularly important for gliding. It is used by birds and gliders to stay in the air for long periods with little effort.\n\nFor a solid object moving through a fluid, the drag is the component of the net aerodynamic or hydrodynamic force acting opposite to the direction of the movement. Therefore, drag opposes the motion of the object, and in a powered vehicle it must be overcome by thrust. The process which creates lift also causes some drag.\n\nAerodynamic lift is created by the motion of an aerodynamic object (wing) through the air, which due to its shape and angle deflects the air. For sustained straight and level flight, lift must be equal and opposite to weight. In general, long narrow wings are able deflect a large amount of air at a slow speed, whereas smaller wings need a higher forward speed to deflect an equivalent amount of air and thus generate an equivalent amount of lift. Large cargo aircraft tend to use longer wings with higher angles of attack, whereas supersonic aircraft tend to have short wings and rely heavily on high forward speed to generate lift.\n\nHowever, this lift (deflection) process inevitably causes a retarding force called drag. Because lift and drag are both aerodynamic forces, the ratio of lift to drag is an indication of the aerodynamic efficiency of the airplane. The lift to drag ratio is the L/D ratio, pronounced \"L over D ratio.\" An airplane has a high L/D ratio if it produces a large amount of lift or a small amount of drag. The lift/drag ratio is determined by dividing the lift coefficient by the drag coefficient, CL/CD. \n\nThe lift coefficient Cl is equal to the lift L divided by the (density r times half the velocity V squared times the wing area A). [Cl = L / (A * .5 * r * V^2)] The lift coefficient is also affected by the compressibility of the air, which is much greater at higher speeds, so velocity V is not a linear function. Compressibility is also affected by the shape of the aircraft surfaces.\nThe drag coefficient Cd is equal to the drag D divided by the (density r times half the velocity V squared times the reference area A). [Cd = D / (A * .5 * r * V^2)] \nLift-to-drag ratios for practical aircraft vary from about 4:1 for vehicles and birds with relatively short wings, up to 60:1 or more for vehicles with very long wings, such as gliders. A greater angle of attack relative to the forward movement also increases the extent of deflection, and thus generates extra lift. However a greater angle of attack also generates extra drag. \n\nLift/drag ratio also determines the glide ratio and gliding range. Since the glide ratio is based only on the relationship of the aerodynamics forces acting on the aircraft, aircraft weight will not affect it. The only effect weight has is to vary the time that the aircraft will glide for – a heavier aircraft gliding at a higher airspeed will arrive at the same touchdown point in a shorter time. \n\nAir pressure acting up against an object in air is greater than the pressure above pushing down. The buoyancy, in both cases, is equal to the weight of fluid displaced - Archimedes' principle holds for air just as it does for water.\n\nA cubic meter of air at ordinary atmospheric pressure and room temperature has a mass of about 1.2 kilograms, so its weight is about 12 newtons. Therefore, any 1-cubic-meter object in air is buoyed up with a force of 12 newtons. If the mass of the 1-cubic-meter object is greater than 1.2 kilograms (so that its weight is greater than 12 newtons), it falls to the ground when released. If an object of this size has a mass less than 1.2 kilograms, it rises in the air. Any object that has a mass that is less than the mass of an equal volume of air will rise in air - in other words, any object less dense than air will rise.\n\nThrust-to-weight ratio is, as its name suggests, the ratio of instantaneous thrust to weight (where weight means weight at the Earth's standard acceleration formula_1). It is a dimensionless parameter characteristic of rockets and other jet engines and of vehicles propelled by such engines (typically space launch vehicles and jet aircraft).\n\nIf the thrust-to-weight ratio is greater than the local gravity strength (expressed in \"g\"s), then flight can occur without any forward motion or any aerodynamic lift being required.\n\nIf the thrust-to-weight ratio times the lift-to-drag ratio is greater than local gravity then takeoff using aerodynamic lift is possible.\n\nFlight dynamics is the science of air and space vehicle orientation and control in three dimensions. The three critical flight dynamics parameters are the angles of rotation in three dimensions about the vehicle's center of mass, known as \"pitch\", \"roll\" and \"yaw\" (See Tait-Bryan rotations for an explanation).\n\nThe control of these dimensions can involve a horizontal stabilizer (i.e. \"a tail\"), ailerons and other movable aerodynamic devices which control angular stability i.e. flight attitude (which in turn affects altitude, heading). Wings are often angled slightly upwards- they have \"positive dihedral angle\" which gives inherent roll stabilization.\n\nTo create thrust so as to be able to gain height, and to push through the air to overcome the drag associated with lift all takes energy. Different objects and creatures capable of flight vary in the efficiency of their muscles, motors and how well this translates into forward thrust.\n\nPropulsive efficiency determines how much energy vehicles generate from a unit of fuel.\n\nThe range that powered flight articles can achieve is ultimately limited by their drag, as well as how much energy they can store on board and how efficiently they can turn that energy into propulsion.\n\nFor powered aircraft the useful energy is determined by their fuel fraction- what percentage of the takeoff weight is fuel, as well as the specific energy of the fuel used.\n\nAll animals and devices capable of sustained flight need relatively high power-to-weight ratios to be able to generate enough lift and/or thrust to achieve take off.\n\nVehicles that can fly can have different ways to takeoff and land. Conventional aircraft accelerate along the ground until sufficient lift is generated for takeoff, and reverse the process for landing. Some aircraft can take off at low speed; this is called a short takeoff. Some aircraft such as helicopters and Harrier jump jets can take off and land vertically. Rockets also usually take off and land vertically, but some designs can land horizontally.\n\nNavigation is the systems necessary to calculate current position (e.g. compass, GPS, LORAN, star tracker, inertial measurement unit, and altimeter).\n\nIn aircraft, successful air navigation involves piloting an aircraft from place to place without getting lost, breaking the laws applying to aircraft, or endangering the safety of those on board or on the ground.\n\nThe techniques used for navigation in the air will depend on whether the aircraft is flying under the visual flight rules (VFR) or the instrument flight rules (IFR). In the latter case, the pilot will navigate exclusively using instruments and radio navigation aids such as beacons, or as directed under radar control by air traffic control. In the VFR case, a pilot will largely navigate using dead reckoning combined with visual observations (known as pilotage), with reference to appropriate maps. This may be supplemented using radio navigation aids.\n\nA guidance system is a device or group of devices used in the navigation of a ship, aircraft, missile, rocket, satellite, or other moving object. Typically, guidance is responsible for the calculation of the vector (i.e., direction, velocity) toward an objective.\n\nA conventional fixed-wing aircraft flight control system consists of flight control surfaces, the respective cockpit controls, connecting linkages, and the necessary operating mechanisms to control an aircraft's direction in flight. Aircraft engine controls are also considered as flight controls as they change speed.\n\nIn the case of aircraft, air traffic is controlled by air traffic control systems.\n\nCollision avoidance is the process of controlling spacecraft to try to prevent collisions.\n\nAir safety is a term encompassing the theory, investigation and categorization of flight failures, and the prevention of such failures through regulation, education and training. It can also be applied in the context of campaigns that inform the public as to the safety of air travel.\n\n\n\n\n"}
{"id": "20172828", "url": "https://en.wikipedia.org/wiki?curid=20172828", "title": "Flute's Journey", "text": "Flute's Journey\n\nFlute's Journey: The Life of a Wood Thrush is a children's picture book by Lynne Cherry.\n\nThe book tells the first year of a wood thrush's life starting from it hatching in a forest that is in Maryland. Two children see the bird when it is young and calls it Flute. The children wait for Flute to return from migration and watch him and his mate build a nest and raise their young. In Flute's travels, he encounters many dangers. The eggs and nestlings are at more of a risk.\n\nA review in the book \"Gotcha!: Nonfiction Booktalks to Get Kids Excited about Reading\" says, \"Whether or not you know much about songbirds and the dangers they face, this book is great reading, and it includes ideas for things that kids can do to help them. The illustration of Flute's mother feeding her babies a worm is a good one to show.\" Books use the book for activities for kids to do. The book focused national media attention on conservation efforts to save the Belt Woods in Maryland.\n"}
{"id": "8609949", "url": "https://en.wikipedia.org/wiki?curid=8609949", "title": "Foreline", "text": "Foreline\n\nA foreline is a vacuum line between the pumps of a multistage vacuum system. No longer exclusively used by scientists in research, vacuum systems are used in numerous industries that include food production and the manufacturing of electronic components. A classic foreline is a hose or tube that connects a rotary vane pump to the outlet of an oil diffusion pump, although vacuum technology now uses a vast array of roughing pumps and high vacuum pumps.\n\nAlthough the term has been used for many decades in the high vacuum system industry and in literature concerning vacuum systems, the word foreline has not yet been added to any dictionary.\n"}
{"id": "21873573", "url": "https://en.wikipedia.org/wiki?curid=21873573", "title": "Freddy Fox", "text": "Freddy Fox\n\nFreddy Fox () is a hardcover picture book written and illustrated by Ronald J. Meyer. It tells the story of a young fox and his family, following the daily activities of the fox kit, Freddy. Freddy Fox teaches the reader about red foxes, including that they are born shades of gray, how their den is built, what they eat, and family size. Then through a simple segue, Freddy's red fox mother teaches the young fox about other animals and traits attributed to them. By learning his lessons, Freddy will grow up to be a wise fox. All of the lessons, such as the bear eats a balanced diet or the raccoon washes his hands before eating, can easily be translated into lessons for young children.\n\nThe book was first published in 2004. It won the Silver Mom's Choice Award in the \"Animal Kingdom\" category in 2008.\n\nThe author is a wildlife photographer, and he used his wildlife photographs to illustrate the book. \n\n"}
{"id": "1686779", "url": "https://en.wikipedia.org/wiki?curid=1686779", "title": "Fusion energy gain factor", "text": "Fusion energy gain factor\n\nThe fusion energy gain factor, usually expressed with the symbol Q, is the ratio of fusion power produced in a nuclear fusion reactor to the power required to maintain the plasma in steady state. The condition of \"Q\" = 1, when the power being released by the fusion reactions is equal to the required heating power, is referred to as breakeven.\n\nThe power given off by the fusion reactions may be captured within the fuel, leading to \"self-heating\". Most fusion reactions release at least some of their energy in a form that cannot be captured within the plasma, so a system at \"Q\" = 1 will cool without external heating. With typical fuels, self-heating in fusion reactors is not expected to match the external sources until at least \"Q\" = 5. If \"Q\" increases past this point, increasing self-heating eventually removes the need for external heating. At this point the reaction becomes self-sustaining, a condition called ignition. Ignition corresponds to infinite \"Q\", and is generally regarded as highly desirable for a practical reactor design.\n\nOver time, several related terms have entered the fusion lexicon. As a reactor does not cover its own heating losses until about \"Q\" = 5, the term engineering breakeven is sometimes used to describe a reactor that produces enough electricity to provide that heating. Above engineering breakeven a machine would produce more electricity than it uses, and could sell that excess. A machine that can sell enough electricity to cover its operating costs, estimated to require at least \"Q\" = 20, is sometimes known as economic breakeven.\n\n, the record for \"Q\" is held by the JET tokamak in the UK, at \"Q\" = (16 MW)/(24 MW) ≈ 0.67, first attained in 1997. ITER was originally designed to reach ignition, but is currently designed to reach \"Q\" = 10, producing 500 MW of fusion power from 50 MW of injected thermal power.\n\n\"Q\" is simply the comparison of the power being released by the fusion reactions in a reactor, \"P\", to the constant heating power being supplied, \"P\". However, there are several definitions of breakeven that consider additional power losses.\n\nIn 1955, John Lawson was the first to explore the energy balance mechanisms in detail, initially in classified works but published openly in a now-famous 1957 paper. In this paper he considered and refined work by earlier researchers, notably Hans Thirring, Peter Thonemann, and a review article by Richard Post. Expanding on all of these, Lawson's paper made detailed predictions for the amount of power that would be lost through various mechanisms, and compared that to the energy needed to sustain the reaction. This balance is today known as the Lawson criterion.\n\nIn a successful fusion reactor design, the fusion reactions generate an amount of power designated \"P\". Some amount of this energy, \"P\", is lost through a variety of mechanisms, mostly convection of the fuel to the walls of the reactor chamber and various forms of radiation that cannot be captured to generate power. In order to keep the reaction going, the system has to provide heating to make up for these losses, where \"P\" = \"P\" to maintain thermal equilibrium.\n\nThe most basic definition of breakeven is when \"Q\" = 1, that is, \"P\" = \"P\".\n\nSome works refer to this definition as scientific breakeven, to contrast it with similar terms. However, this usage is rare outside certain areas, specifically the inertial confinement fusion field, where the term is much more widely used.\n\nSince the 1950s, most commercial fusion reactor designs have been based on a mix of deuterium and tritium as their primary fuel; others fuels have been studied for a variety of reasons but are much harder to ignite. As tritium is radioactive, highly bioactive and highly mobile, it represents a significant safety concern and adds to the cost of designing and operating such a reactor.\n\nIn order to lower costs, many experimental machines are designed to run on test fuels of hydrogen or deuterium alone, leaving out the tritium. In this case, the term extrapolated breakeven is used to define the expected performance of the machine running on D-T fuel based on the performance when running on hydrogen or deuterium alone.\n\nThe records for extrapolated breakeven are slightly higher than the records for scientific breakeven. Both JET and JT-60 have reached values around 1.25 (see below for details) while running on D-D fuel. When running on D-T, only possible in JET, the maximum performance is about half the extrapolated value.\n\nAnother related term, engineering breakeven, considers the need to extract the energy from the reactor, turn that into electrical energy, and feed that back into the heating system. This closed loop is known as \"recirculation\". In this case, the basic definition changes by adding additional terms to the \"P\" side to consider the efficiencies of these processes.\n\nMost fusion reactions release energy in a variety of forms, mostly neutrons and a variety of charged particles like alpha particles. Neutrons are electrically neutral and will travel out of any magnetic confinement fusion (MFE) design, and in spite of the very high densities found in inertial confinement fusion (ICF) designs, they tend to easily escape the fuel mass in these designs as well. This means that only the charged particles from the reactions can be captured within the fuel mass and give rise to self-heating. If the fraction of the energy being released in the charged particles is \"f\", then the power in these particles is \"P\" = \"f\"\"P\". If this self-heating process is perfect, that is, all of \"P\" is captured in the fuel, that means the power available for generating electricity is the power that is not released in that form, or (1 − \"f\")\"P\".\n\nIn the case of neutrons carrying most of the practical energy, as is the case in the D-T fuel studied in most designs, this neutron energy is normally captured in a \"blanket\" of lithium that produces more tritium that is used to fuel the reactor. Due to various exothermic and endothermic reactions, the blanket may have a power gain factor a few percent higher or lower than 100%, but that will be neglected here. The blanket is then cooled and the cooling fluid used in a heat exchanger driving conventional steam turbines. These have an efficiency η which is around 35 to 40%.\n\nConsider a system that uses external heaters to heat the fusion fuel, then extracts the power from those reactions to generate electrical power. Some fraction of that power, \"f\", is needed to recirculate back into the heaters to close the loop. This is not the same as the \"P\" because the self-heating processes are providing some of the required energy. While the system as a whole requires additional power for building climate control, lighting, and the confinement system, these are generally much smaller than the plasma heating system requirements.\nConsidering all of these factors, the heating power can thus be related to the fusion power by the following equation:\n\nformula_1\n\nwhere formula_2 is the efficiency that power supplied to the heating systems is turned into heat in the fuel, as opposed to lost in the equipment itself, and formula_3 is the efficiency achieved when turning the heat into electrical power, for instance, through the Rankine cycle.\n\nThe fusion energy gain factor is then defined as:\n\nformula_4\n\nAs the temperature of the plasma increases, the rate of fusion reactions grows rapidly, and with it, the rate of self heating. In contrast, the non-capturable energy losses like x-rays do not grow at the same rate. Thus, in overall terms, the self-heating process becomes more efficient as the temperature increases, and less energy is needed from external sources to keep it hot.\n\nEventually \"P\" reaches zero, that is, all of the energy needed to keep the plasma at the operational temperature is being supplied by self-heating, and the amount of external energy that needs to be added drops to zero. This point is known as ignition.\n\nIgnition, by definition, corresponds to an infinite \"Q\", but it does not mean that \"f\" drops to zero as the other power sinks in the system, like the magnets and cooling systems, still need to be powered. Generally, however, these are much smaller than the energy in the heaters, and require a much smaller \"f\". More importantly, this number is more likely to be near constant, meaning that further improvements in plasma performance will result in more energy that can be directly used for commercial generation, as opposed to recirculation.\n\nThe final definition of breakeven is commercial breakeven, which occurs when the economic value of any net energy left over after recirculation is enough to finance the construction of the reactor. This value depends both on the reactor and the spot price of electrical power.\n\nCommercial breakeven relies on factors outside the technology of the reactor itself, and it is possible that even a reactor with a fully ignited plasma will not generate enough energy to pay for itself. Whether any of the mainline concepts like ITER can reach this goal is being debated in the field.\n\nMost fusion reactor designs being studied are based on the D-T reaction, as this is by far the easiest to ignite, and is energy dense. However, this reaction also gives off most of its energy in the form of a single highly energetic neutron, and only 20% of the energy in the form of an alpha. Thus, for the D-T reaction, \"f\" = 0.2. This means that self-heating does not become equal to the external heating until at least \"Q\" = 5. \n\nEfficiency values depend on design details but may be in the range of η = 0.7 (70%) and η = 0.4 (40%). The purpose of a fusion reactor is to produce power, not to recirculate it, so a practical reactor must have \"f\" = 0.2 approximately. Lower would be better but will be hard to achieve. Using these values we find for a practical reactor \"Q\" = 22.\n\nMany early fusion devices operated for microseconds, using some sort of pulsed power source to feed their magnetic confinement system and used the confinement as the heating source. Lawson defined breakeven in this context as the total energy released by the entire reaction cycle compared to the total energy supplied to the machine during the same cycle.\n\nOver time, as performance increased by orders of magnitude, the reaction times have extended from microseconds to seconds, and in ITER, on the order of minutes. In this case definition of \"the entire reaction cycle\" becomes blurred. In the case of an ignited plasma, for instance, P may be quite high while the system is being set up, and then drop to zero when it is fully developed, so one may be tempted to pick an instant in time when it is operating at its best to determine \"Q\". A better solution in these cases is to use the original Lawson definition averaged over the reaction to produce a similar value as the original definition.\n\nHowever, there is a complication. During the heating phase when the system is being brought up to operational conditions, some of the energy released by the fusion reactions will be used to heat the surrounding fuel, and thus not be released. This is no longer true when the plasma reaches its operational temperature and enters thermal equilibrium. Thus, if one averages over the entire cycle, this energy will be included as part of the heating term, that is, some of the energy that was captured for heating would otherwise have been released in P and is therefore not indicative of an operational \"Q\".\n\nOperators of the JET reactor argued that this input should be removed from the total:\n\nformula_5\n\nwhere:\n\nformula_6\n\nThat is, P is the amount of energy needed to raise the internal energy of the plasma. It is this definition that was used when reporting JET's record 0.67 value.\n\nSome debate over this definition continues. In 1998, the operators of the JT-60 claimed to have reached \"Q\" = 1.25 running on D-D fuel, thus reaching extrapolated breakeven. However, this measurement was based on the JET definition of Q*. Using this definition, JET had also reached extrapolated breakeven some time earlier. If one considers the energy balance in these conditions, and the analysis of previous machines, it is argued the original definition should be used, and thus both machines remain well below break-even of any sort.\n\nAlthough most fusion experiments use some form of magnetic confinement, another major branch is inertial confinement fusion (ICF) that mechanically presses together the fuel mass (the \"target\") to increase its density. This greatly increases the rate of fusion events and lowers the need to confine the fuel for long periods. This compression is accomplished by heating a lightweight capsule holding the fuel so rapidly that it explodes outwards, driving the fuel mass on the inside inward in accordance with Newton's third law. There are a variety of proposed \"drivers\" to cause the implosion process, but to date most experiments have used lasers. \n\nUsing the traditional definition of \"Q\", \"P\" / \"P\", ICF devices have extremely low \"Q\". This is because the laser is extremely inefficient; whereas formula_2 for the heaters used in magnetic systems might be on the order of 70%, lasers are on the order of 1.5%. For this reason, Lawrence Livermore National Laboratory (LLNL), the leader in ICF research, has proposed another modification of \"Q\" that defines \"P\" as the energy delivered by the driver, as opposed to the energy put into the driver. This definition produces much higher \"Q\" values, and changes the definition of breakeven to be \"P\" / \"P\" = 1. On occasion, they referred to this definition as \"scientific breakeven\". This term was not universally used, other groups adopted the redefinition of \"Q\" but continued to refer to \"P\" = \"P\" simply as breakeven. \n\nOn 7 October 2013, the BBC announced that LLNL had achieved scientific breakeven in the National Ignition Facility (NIF) on 29 September. In this experiment, \"P\" was approximately 14 kJ, while the laser output was 1.8 MJ. By their previous definition, this would be a \"Q\" of 0.0077. However, for this press release, they re-defined \"Q\" once again, this time equating \"P\" to be only the amount energy delivered to \"the hottest portion of the fuel\", calculating that only 10 kJ of the original laser energy reached the part of the fuel that was undergoing fusion reactions. This release has been heavily criticized in the field.\n\n"}
{"id": "43421", "url": "https://en.wikipedia.org/wiki?curid=43421", "title": "Henry David Thoreau", "text": "Henry David Thoreau\n\nHenry David Thoreau (see name pronunciation; July 12, 1817 – May 6, 1862) was an American essayist, poet, philosopher, abolitionist, naturalist, tax resister, development critic, surveyor, and historian. A leading transcendentalist, Thoreau is best known for his book \"Walden\", a reflection upon simple living in natural surroundings, and his essay \"Civil Disobedience\" (originally published as \"Resistance to Civil Government\"), an argument for disobedience to an unjust state.\n\nThoreau's books, articles, essays, journals, and poetry amount to more than 20 volumes. Among his lasting contributions are his writings on natural history and philosophy, in which he anticipated the methods and findings of ecology and environmental history, two sources of modern-day environmentalism. His literary style interweaves close observation of nature, personal experience, pointed rhetoric, symbolic meanings, and historical lore, while displaying a poetic sensibility, philosophical austerity, and Yankee attention to practical detail. He was also deeply interested in the idea of survival in the face of hostile elements, historical change, and natural decay; at the same time he advocated abandoning waste and illusion in order to discover life's true essential needs.\n\nHe was a lifelong abolitionist, delivering lectures that attacked the Fugitive Slave Law while praising the writings of Wendell Phillips and defending the abolitionist John Brown. Thoreau's philosophy of civil disobedience later influenced the political thoughts and actions of such notable figures as Leo Tolstoy, Mahatma Gandhi, and Martin Luther King Jr.\n\nThoreau is sometimes referred to as an anarchist. Though \"Civil Disobedience\" seems to call for improving rather than abolishing government—\"I ask for, not at once no government, but \"at once\" a better government\"—the direction of this improvement contrarily points toward anarchism: \"'That government is best which governs not at all;' and when men are prepared for it, that will be the kind of government which they will have.\"\n\nAmos Bronson Alcott and Thoreau's aunt each wrote that \"Thoreau\" is pronounced like the word \"thorough\" ( —in General American, but more precisely —in 19th-century New England). Edward Waldo Emerson wrote that the name should be pronounced \"Thó-row\", with the \"h\" sounded and stress on the first syllable. Among modern-day American speakers, it is perhaps more commonly pronounced —with stress on the second syllable.\n\nThoreau had a distinctive appearance, with a nose that he called his \"most prominent feature\". Of his appearance and disposition, Ellery Channing wrote:\n\nHis face, once seen, could not be forgotten. The features were quite marked: the nose aquiline or very Roman, like one of the portraits of Caesar (more like a beak, as was said); large overhanging brows above the deepest set blue eyes that could be seen, in certain lights, and in others gray,—eyes expressive of all shades of feeling, but never weak or near-sighted; the forehead not unusually broad or high, full of concentrated energy and purpose; the mouth with prominent lips, pursed up with meaning and thought when silent, and giving out when open with the most varied and unusual instructive sayings.\n\nHenry David Thoreau was born David Henry Thoreau in Concord, Massachusetts, into the \"modest New England family\" of John Thoreau, a pencil maker, and Cynthia Dunbar. His paternal grandfather had been born on the UK crown dependency island of Jersey. His maternal grandfather, Asa Dunbar, led Harvard's 1766 student \"Butter Rebellion\", the first recorded student protest in the American colonies. David Henry was named after his recently deceased paternal uncle, David Thoreau. He began to call himself Henry David after he finished college; he never petitioned to make a legal name change. He had two older siblings, Helen and John Jr., and a younger sister, Sophia. Thoreau's birthplace still exists on Virginia Road in Concord. The house has been restored by the Thoreau Farm Trust, a nonprofit organization, and is now open to the public.\n\nHe studied at Harvard College between 1833 and 1837. He lived in Hollis Hall and took courses in rhetoric, classics, philosophy, mathematics, and science. He was a member of the Institute of 1770 (now the Hasty Pudding Club). According to legend, Thoreau refused to pay the five-dollar fee (approximately ) for a Harvard diploma. In fact, the master's degree he declined to purchase had no academic merit: Harvard College offered it to graduates \"who proved their physical worth by being alive three years after graduating, and their saving, earning, or inheriting quality or condition by having Five Dollars to give the college.\" He commented, \"Let every sheep keep its own skin\", a reference to the tradition of using sheepskin vellum for diplomas.\n\nThe traditional professions open to college graduates—law, the church, business, medicine—did not interest Thoreau, so in 1835 he took a leave of absence from Harvard, during which he taught school in Canton, Massachusetts. After he graduated in 1837, he joined the faculty of the Concord public school, but he resigned after a few weeks rather than administer corporal punishment. He and his brother John then opened the Concord Academy, a grammar school in Concord, in 1838. They introduced several progressive concepts, including nature walks and visits to local shops and businesses. The school closed when John became fatally ill from tetanus in 1842 after cutting himself while shaving. He died in Henry's arms.\n\nUpon graduation Thoreau returned home to Concord, where he met Ralph Waldo Emerson through a mutual friend. Emerson, who was 14 years his senior, took a paternal and at times patron-like interest in Thoreau, advising the young man and introducing him to a circle of local writers and thinkers, including Ellery Channing, Margaret Fuller, Bronson Alcott, and Nathaniel Hawthorne and his son Julian Hawthorne, who was a boy at the time.\n\nEmerson urged Thoreau to contribute essays and poems to a quarterly periodical, \"The Dial\", and lobbied the editor, Margaret Fuller, to publish those writings. Thoreau's first essay published in \"The Dial\" was \"Aulus Persius Flaccus,\" an essay on the Roman playwright, in July 1840. It consisted of revised passages from his journal, which he had begun keeping at Emerson's suggestion. The first journal entry, on October 22, 1837, reads, \"'What are you doing now?' he asked. 'Do you keep a journal?' So I make my first entry to-day.\"\n\nThoreau was a philosopher of nature and its relation to the human condition. In his early years he followed Transcendentalism, a loose and eclectic idealist philosophy advocated by Emerson, Fuller, and Alcott. They held that an ideal spiritual state transcends, or goes beyond, the physical and empirical, and that one achieves that insight via personal intuition rather than religious doctrine. In their view, Nature is the outward sign of inward spirit, expressing the \"radical correspondence of visible things and human thoughts\", as Emerson wrote in \"Nature\" (1836).\n\nOn April 18, 1841, Thoreau moved into the Emerson house. There, from 1841 to 1844, he served as the children's tutor; he was also an editorial assistant, repairman and gardener. For a few months in 1843, he moved to the home of William Emerson on Staten Island, and tutored the family's sons while seeking contacts among literary men and journalists in the city who might help publish his writings, including his future literary representative Horace Greeley.\n\nThoreau returned to Concord and worked in his family's pencil factory, which he would continue to do alongside his writing and other work for most of his adult life. He rediscovered the process of making good pencils with inferior graphite by using clay as the binder. This invention allowed profitable use of a graphite source found in New Hampshire that had been purchased in 1821 by Thoreau's brother-in-law, Charles Dunbar. The process of mixing graphite and clay, known as the Conté process, had been first patented by Nicolas-Jacques Conté in 1795. The company's other source of graphite had been Tantiusques, a mine operated by Native Americans in Sturbridge, Massachusetts. Later, Thoreau converted the pencil factory to produce plumbago, a name for graphite at the time, which was used in the electrotyping process.\n\nOnce back in Concord, Thoreau went through a restless period. In April 1844 he and his friend Edward Hoar accidentally set a fire that consumed of Walden Woods.\n\nThoreau felt a need to concentrate and work more on his writing. In March 1845, Ellery Channing told Thoreau, \"Go out upon that, build yourself a hut, & there begin the grand process of devouring yourself alive. I see no other alternative, no other hope for you.\" Two months later, Thoreau embarked on a two-year experiment in simple living on July 4, 1845, when he moved to a small house he had built on land owned by Emerson in a second-growth forest around the shores of Walden Pond. The house was in \"a pretty pasture and woodlot\" of that Emerson had bought, from his family home.\nOn July 24 or July 25, 1846, Thoreau ran into the local tax collector, Sam Staples, who asked him to pay six years of delinquent poll taxes. Thoreau refused because of his opposition to the Mexican–American War and slavery, and he spent a night in jail because of this refusal. The next day Thoreau was freed when someone, likely to have been his aunt, paid the tax, against his wishes. The experience had a strong impact on Thoreau. In January and February 1848, he delivered lectures on \"The Rights and Duties of the Individual in relation to Government\", explaining his tax resistance at the Concord Lyceum. Bronson Alcott attended the lecture, writing in his journal on January 26:\n\nThoreau revised the lecture into an essay titled \"Resistance to Civil Government\" (also known as \"Civil Disobedience\"). It was published by Elizabeth Peabody in the \"Aesthetic Papers\" in May 1849. Thoreau had taken up a version of Percy Shelley's principle in the political poem \"The Mask of Anarchy\" (1819), which begins with the powerful images of the unjust forms of authority of his time and then imagines the stirrings of a radically new form of social action.\n\nAt Walden Pond, Thoreau completed a first draft of \"A Week on the Concord and Merrimack Rivers\", an elegy to his brother John, describing their trip to the White Mountains in 1839. Thoreau did not find a publisher for the book and instead printed 1,000 copies at his own expense; fewer than 300 were sold. He self-published the book on the advice of Emerson, using Emerson's publisher, Munroe, who did little to publicize the book.\n\nIn August 1846, Thoreau briefly left Walden to make a trip to Mount Katahdin in Maine, a journey later recorded in \"Ktaadn\", the first part of \"The Maine Woods\".\n\nThoreau left Walden Pond on September 6, 1847. At Emerson's request, he immediately moved back to the Emerson house to help Emerson's wife, Lidian, manage the household while her husband was on an extended trip to Europe. Over several years, as he worked to pay off his debts, he continuously revised the manuscript of what he eventually published as \"Walden, or Life in the Woods\" in 1854, recounting the two years, two months, and two days he had spent at Walden Pond. The book compresses that time into a single calendar year, using the passage of the four seasons to symbolize human development. Part memoir and part spiritual quest, \"Walden\" at first won few admirers, but later critics have regarded it as a classic American work that explores natural simplicity, harmony, and beauty as models for just social and cultural conditions.\n\nThe American poet Robert Frost wrote of Thoreau, \"In one book ... he surpasses everything we have had in America.\"\n\nThe American author John Updike said of the book, \"A century and a half after its publication, Walden has become such a totem of the back-to-nature, preservationist, anti-business, civil-disobedience mindset, and Thoreau so vivid a protester, so perfect a crank and hermit saint, that the book risks being as revered and unread as the Bible.\"\n\nThoreau moved out of Emerson's house in July 1848 and stayed at a house on nearby Belknap Street. In 1850, he and his family moved into a house at 255 Main Street, where he lived until his death.\n\nIn the summer of 1850, Thoreau and Channing journeyed from Boston to Montreal and Quebec City. These would be Thoreau's only travels outside the United States. It is as a result of this trip that he developed lectures that eventually became \"A Yankee in Canada\". He jested that all he got from this adventure \"was a cold.\" In fact, this proved an opportunity to contrast American civic spirit and democratic values with a colony apparently ruled by illegitimate religious and military power. Whereas his own country had had its revolution, in Canada history had failed to turn.\n\nIn 1851, Thoreau became increasingly fascinated with natural history and narratives of travel and expedition. He read avidly on botany and often wrote observations on this topic into his journal. He admired William Bartram and Charles Darwin's \"Voyage of the Beagle\". He kept detailed observations on Concord's nature lore, recording everything from how the fruit ripened over time to the fluctuating depths of Walden Pond and the days certain birds migrated. The point of this task was to \"anticipate\" the seasons of nature, in his word.\n\nHe became a land surveyor and continued to write increasingly detailed observations on the natural history of the town, covering an area of , in his journal, a two-million-word document he kept for 24 years. He also kept a series of notebooks, and these observations became the source of his late writings on natural history, such as \"Autumnal Tints\", \"The Succession of Trees\", and \"Wild Apples\", an essay lamenting the destruction of indigenous wild apple species.\n\nUntil the 1970s, literary critics dismissed Thoreau's late pursuits as amateur science and philosophy. With the rise of environmental history and ecocriticism as academic disciplines, several new readings of Thoreau began to emerge, showing him to have been both a philosopher and an analyst of ecological patterns in fields and woodlots. For instance, his late essay \"The Succession of Forest Trees\" shows that he used experimentation and analysis to explain how forests regenerate after fire or human destruction, through the dispersal of seeds by winds or animals.\nHe traveled to Canada East once, Cape Cod four times, and Maine three times; these landscapes inspired his \"excursion\" books, \"A Yankee in Canada\", \"Cape Cod\", and \"The Maine Woods\", in which travel itineraries frame his thoughts about geography, history and philosophy. Other travels took him southwest to Philadelphia and New York City in 1854 and west across the Great Lakes region in 1861, when he visited Niagara Falls, Detroit, Chicago, Milwaukee, St. Paul and Mackinac Island. He was provincial in his own travels, but he read widely about travel in other lands. He devoured all the first-hand travel accounts available in his day, at a time when the last unmapped regions of the earth were being explored. He read Magellan and James Cook; the arctic explorers John Franklin, Alexander Mackenzie and William Parry; David Livingstone and Richard Francis Burton on Africa; Lewis and Clark; and hundreds of lesser-known works by explorers and literate travelers. Astonishing amounts of reading fed his endless curiosity about the peoples, cultures, religions and natural history of the world and left its traces as commentaries in his voluminous journals. He processed everything he read, in the local laboratory of his Concord experience. Among his famous aphorisms is his advice to \"live at home like a traveler.\"\n\nAfter John Brown's raid on Harpers Ferry, many prominent voices in the abolitionist movement distanced themselves from Brown or damned him with faint praise. Thoreau was disgusted by this, and he composed a key speech, \"A Plea for Captain John Brown\", which was uncompromising in its defense of Brown and his actions. Thoreau's speech proved persuasive: the abolitionist movement began to accept Brown as a martyr, and by the time of the American Civil War entire armies of the North were literally singing Brown's praises. As a biographer of Brown put it, \"If, as Alfred Kazin suggests, without John Brown there would have been no Civil War, we would add that without the Concord Transcendentalists, John Brown would have had little cultural impact.\"\nThoreau contracted tuberculosis in 1835 and suffered from it sporadically afterwards. In 1860, following a late-night excursion to count the rings of tree stumps during a rainstorm, he became ill with bronchitis. His health declined, with brief periods of remission, and he eventually became bedridden. Recognizing the terminal nature of his disease, Thoreau spent his last years revising and editing his unpublished works, particularly \"The Maine Woods\" and \"Excursions\", and petitioning publishers to print revised editions of \"A Week\" and \"Walden\". He wrote letters and journal entries until he became too weak to continue. His friends were alarmed at his diminished appearance and were fascinated by his tranquil acceptance of death. When his aunt Louisa asked him in his last weeks if he had made his peace with God, Thoreau responded, \"I did not know we had ever quarreled.\"\nAware he was dying, Thoreau's last words were \"Now comes good sailing\", followed by two lone words, \"moose\" and \"Indian\". He died on May 6, 1862, at age 44. Amos Bronson Alcott planned the service and read selections from Thoreau's works, and Channing presented a hymn. Emerson wrote the eulogy spoken at the funeral. Thoreau was buried in the Dunbar family plot; his remains and those of members of his immediate family were eventually moved to Sleepy Hollow Cemetery () in Concord, Massachusetts.\n\nThoreau's friend William Ellery Channing published his first biography, \"Thoreau the Poet-Naturalist\", in 1873. Channing and another friend, Harrison Blake, edited some poems, essays, and journal entries for posthumous publication in the 1890s. Thoreau's journals, which he often mined for his published works but which remained largely unpublished at his death, were first published in 1906 and helped to build his modern reputation. A new, expanded edition of the journals is under way, published by Princeton University Press. Today, Thoreau is regarded as one of the foremost American writers, both for the modern clarity of his prose style and the prescience of his views on nature and politics. His memory is honored by the international Thoreau Society and his legacy honored by the Thoreau Institute at Walden Woods, established in 1998 in Lincoln, Massachusetts.\n\nThoreau was an early advocate of recreational hiking and canoeing, of conserving natural resources on private land, and of preserving wilderness as public land. He was himself a highly skilled canoeist; Nathaniel Hawthorne, after a ride with him, noted that \"Mr. Thoreau managed the boat so perfectly, either with two paddles or with one, that it seemed instinct with his own will, and to require no physical effort to guide it.\" \n\nHe was not a strict vegetarian, though he said he preferred that diet and advocated it as a means of self-improvement. He wrote in \"Walden\", \"The practical objection to animal food in my case was its uncleanness; and besides, when I had caught and cleaned and cooked and eaten my fish, they seemed not to have fed me essentially. It was insignificant and unnecessary, and cost more than it came to. A little bread or a few potatoes would have done as well, with less trouble and filth.\"\nThoreau neither rejected civilization nor fully embraced wilderness. Instead he sought a middle ground, the pastoral realm that integrates nature and culture. His philosophy required that he be a didactic arbitrator between the wilderness he based so much on and the spreading mass of humanity in North America. He decried the latter endlessly but felt that a teacher needs to be close to those who needed to hear what he wanted to tell them. The wildness he enjoyed was the nearby swamp or forest, and he preferred \"partially cultivated country.\" His idea of being \"far in the recesses of the wilderness\" of Maine was to \"travel the logger's path and the Indian trail\", but he also hiked on pristine land. In the essay \"Henry David Thoreau, Philosopher\" Roderick Nash wrote, \"Thoreau left Concord in 1846 for the first of three trips to northern Maine. His expectations were high because he hoped to find genuine, primeval America. But contact with real wilderness in Maine affected him far differently than had the idea of wilderness in Concord. Instead of coming out of the woods with a deepened appreciation of the wilds, Thoreau felt a greater respect for civilization and realized the necessity of balance.\"\nOf alcohol, Thoreau wrote, \"I would fain keep sober always. ... I believe that water is the only drink for a wise man; wine is not so noble a liquor. ... Of all ebriosity, who does not prefer to be intoxicated by the air he breathes?\"\n\nThoreau never married and was childless. He strove to portray himself as an ascetic puritan. However, his sexuality has long been the subject of speculation, including by his contemporaries. Critics have called him heterosexual, homosexual, or asexual. There is no evidence to suggest he had physical relations with anyone, man or woman. Some scholars have suggested that homoerotic sentiments run through his writings and concluded that he was homosexual. The elegy \"Sympathy\" was inspired by the eleven-year-old Edmund Sewell, with whom he hiked for five days in 1839. One scholar has suggested that he wrote the poem to Edmund because he could not bring himself to write it to Edmund's sister, and another that Thoreau's \"emotional experiences with women are memorialized under a camouflage of masculine pronouns\", but other scholars dismiss this. It has been argued that the long paean in \"Walden\" to the French-Canadian woodchopper Alek Therien, which includes allusions to Achilles and Patroclus, is an expression of conflicted desire. In some of Thoreau's writing there is the sense of a secret self. In 1840 he writes in his journal: \"My friend is the apology for my life. In him are the spaces which my orbit traverses\". Thoreau was strongly influenced by the moral reformers of his time, and this may have instilled anxiety and guilt over sexual desire.\n\nThoreau was fervently against slavery and actively supported the abolitionist movement. He participated in the Underground Railroad, delivered lectures that attacked the Fugitive Slave Law, and in opposition to the popular opinion of the time, supported radical abolitionist militia leader John Brown and his party. Two weeks after the ill-fated raid on Harpers Ferry and in the weeks leading up to Brown's execution, Thoreau regularly delivered a speech to the citizens of Concord, Massachusetts, in which he compared the American government to Pontius Pilate and likened Brown's execution to the crucifixion of Jesus Christ:\n\nIn \"The Last Days of John Brown\", Thoreau described the words and deeds of John Brown as noble and an example of heroism. In addition, he lamented the newspaper editors who dismissed Brown and his scheme as \"crazy\".\n\nThoreau was a proponent of limited government and individualism. Although he was hopeful that mankind could potentially have, through self-betterment, the kind of government which \"governs not at all\", he distanced himself from contemporary \"no-government men\" (anarchists), writing: \"I ask for, not at once no government, but at once a better government.\"\n\nThoreau deemed the evolution from absolute monarchy to limited monarchy to democracy as \"a progress toward true respect for the individual\" and theorized about further improvements \"towards recognizing and organizing the rights of man.\" Echoing this belief, he went on to write: \"There will never be a really free and enlightened State until the State comes to recognize the individual as a higher and independent power, from which all its power and authority are derived, and treats him accordingly.\"\n\nIt is on this basis that Thoreau could so strongly inveigh against British and Catholic power in \"A Yankee in Canada\". Despotic authority had crushed the people's sense of ingenuity and enterprise; the Canadian \"habitants\" had been reduced, in his view, to a perpetual childlike state. Ignoring the recent Rebellions, he argued that there would be no revolution in the St. Lawrence River valley.\n\nAlthough Thoreau believed resistance to unjustly exercised authority could be both violent (exemplified in his support for John Brown) and nonviolent (his own example of tax resistance displayed in \"Resistance to Civil Government\"), he regarded pacifist nonresistance as temptation to passivity, writing: \"Let not our Peace be proclaimed by the rust on our swords, or our inability to draw them from their scabbards; but let her at least have so much work on her hands as to keep those swords bright and sharp.\" Furthermore, in a formal lyceum debate in 1841, he debated the subject \"Is it ever proper to offer forcible resistance?\", arguing the affirmative.\n\nLikewise, his condemnation of the Mexican–American War did not stem from pacifism, but rather because he considered Mexico \"unjustly overrun and conquered by a foreign army\" as a means to expand the slave territory.\n\nThoreau was ambivalent towards industrialization and capitalism. On one hand he regarded commerce as \"unexpectedly confident and serene, adventurous, and unwearied\" and expressed admiration for its associated cosmopolitanism, writing:\n\nOn the other hand, he wrote disparagingly of the factory system:\n\nThoreau also favored bioregionalism, the protection of animals and wild areas, free trade, and taxation for schools and highways. He disapproved of the subjugation of Native Americans, slavery, technological utopianism, consumerism, philistinism, mass entertainment, and frivolous applications of technology.\n\nThoreau was influenced by Indian spiritual thought. In \"Walden\", there are many overt references to the sacred texts of India. For example, in the first chapter (\"Economy\"), he writes: \"How much more admirable the Bhagvat-Geeta than all the ruins of the East!\" \"American Philosophy: An Encyclopedia\" classes him as one of several figures who \"took a more pantheist or pandeist approach by rejecting views of God as separate from the world\", also a characteristic of Hinduism.\n\nFurthermore, in \"The Pond in Winter\", he equates Walden Pond with the sacred Ganges river, writing:\n\nThoreau was aware his Ganges imagery could have been factual. He wrote about ice harvesting at Walden Pond. And he knew that New England's ice merchants were shipping ice to foreign ports, including Calcutta.\n\nAdditionally, Thoreau followed various Hindu customs, including following a diet of rice (\"It was fit that I should live on rice, mainly, who loved so well the philosophy of India.\"), flute playing (reminiscent of the favorite musical pastime of Krishna), and yoga.\n\nIn an 1849 letter to his friend H.G.O. Blake, he wrote about yoga and its meaning to him:\n\nThoreau read contemporary works in the new science of biology, including the works of Alexander von Humboldt, Charles Darwin, and Asa Gray (Charles Darwin's staunchest American ally). Thoreau was deeply influenced by Humboldt, especially his work Kosmos.\n\nIn 1859, Thoreau purchased and read Darwin's \"On the Origin of Species\". Unlike many natural historians at the time, including Louis Agassiz who publicly opposed Darwinism in favor of a static view of nature, Thoreau was immediately enthusiastic about the theory of evolution by natural selection and endorsed it, stating:\n\nThoreau's political writings had little impact during his lifetime, as \"his contemporaries did not see him as a theorist or as a radical,\" viewing him instead as a naturalist. They either dismissed or ignored his political essays, including \"Civil Disobedience\". The only two complete books (as opposed to essays) published in his lifetime, \"Walden\" and \"A Week on the Concord and Merrimack Rivers\" (1849), both dealt with nature, in which he loved to wander.\" His obituary was lumped in with others rather than as a separate article in an 1862 yearbook. Nevertheless, Thoreau's writings went on to influence many public figures. Political leaders and reformers like Mohandas Gandhi, U.S. President John F. Kennedy, American civil rights activist Martin Luther King Jr., U.S. Supreme Court Justice William O. Douglas, and Russian author Leo Tolstoy all spoke of being strongly affected by Thoreau's work, particularly \"Civil Disobedience\", as did \"right-wing theorist Frank Chodorov [who] devoted an entire issue of his monthly, \"Analysis\", to an appreciation of Thoreau.\"\n\nThoreau also influenced many artists and authors including Edward Abbey, Willa Cather, Marcel Proust, William Butler Yeats, Sinclair Lewis, Ernest Hemingway, Upton Sinclair, E. B. White, Lewis Mumford, Frank Lloyd Wright, Alexander Posey, and Gustav Stickley. Thoreau also influenced naturalists like John Burroughs, John Muir, E. O. Wilson, Edwin Way Teale, Joseph Wood Krutch, B. F. Skinner, David Brower, and Loren Eiseley, whom \"Publishers Weekly\" called \"the modern Thoreau\". English writer Henry Stephens Salt wrote a biography of Thoreau in 1890, which popularized Thoreau's ideas in Britain: George Bernard Shaw, Edward Carpenter, and Robert Blatchford were among those who became Thoreau enthusiasts as a result of Salt's advocacy. Mohandas Gandhi first read \"Walden\" in 1906 while working as a civil rights activist in Johannesburg, South Africa. He first read \"Civil Disobedience\" \"while he sat in a South African prison for the crime of nonviolently protesting discrimination against the Indian population in the Transvaal. The essay galvanized Gandhi, who wrote and published a synopsis of Thoreau's argument, calling its 'incisive logic ... unanswerable' and referring to Thoreau as 'one of the greatest and most moral men America has produced'.\" He told American reporter Webb Miller, \"[Thoreau's] ideas influenced me greatly. I adopted some of them and recommended the study of Thoreau to all of my friends who were helping me in the cause of Indian Independence. Why I actually took the name of my movement from Thoreau's essay 'On the Duty of Civil Disobedience', written about 80 years ago.\"\n\nMartin Luther King, Jr. noted in his autobiography that his first encounter with the idea of nonviolent resistance was reading \"On Civil Disobedience\" in 1944 while attending Morehouse College. He wrote in his autobiography that it was,\n\nHere, in this courageous New Englander's refusal to pay his taxes and his choice of jail rather than support a war that would spread slavery's territory into Mexico, I made my first contact with the theory of nonviolent resistance. Fascinated by the idea of refusing to cooperate with an evil system, I was so deeply moved that I reread the work several times. I became convinced that noncooperation with evil is as much a moral obligation as is cooperation with good. No other person has been more eloquent and passionate in getting this idea across than Henry David Thoreau. As a result of his writings and personal witness, we are the heirs of a legacy of creative protest. The teachings of Thoreau came alive in our civil rights movement; indeed, they are more alive than ever before. Whether expressed in a sit-in at lunch counters, a freedom ride into Mississippi, a peaceful protest in Albany, Georgia, a bus boycott in Montgomery, Alabama, these are outgrowths of Thoreau's insistence that evil must be resisted and that no moral man can patiently adjust to injustice.\n\nAmerican psychologist B. F. Skinner wrote that he carried a copy of Thoreau's \"Walden\" with him in his youth. and, in 1945, wrote \"Walden Two\", a fictional utopia about 1,000 members of a community living together inspired by the life of Thoreau. Thoreau and his fellow Transcendentalists from Concord were a major inspiration of the composer Charles Ives. The 4th movement of the Concord Sonata for piano (with a part for flute, Thoreau's instrument) is a character picture and he also set Thoreau's words.\n\nActor Ron Thompson did a dramatic portrayal of Henry David Thoreau on the 1976 NBC television series \"The Rebels\".\n\nThoreau's ideas have impacted and resonated with various strains in the anarchist movement, with Emma Goldman referring to him as \"the greatest American anarchist\". Green anarchism and anarcho-primitivism in particular have both derived inspiration and ecological points-of-view from the writings of Thoreau. John Zerzan included Thoreau's text \"Excursions\" (1863) in his edited compilation of works in the anarcho-primitivist tradition titled \"Against civilization: Readings and reflections\". Additionally, Murray Rothbard, the founder of anarcho-capitalism, has opined that Thoreau was one of the \"great intellectual heroes\" of his movement. Thoreau was also an important influence on late-19th-century anarchist naturism. Globally, Thoreau's concepts also held importance within individualist anarchist circles in Spain, France, and Portugal.\n\nFor the 200th anniversary of his birth, publishers released several new editions of his work: a recreation of \"Walden\" 1902 edition with illustrations, a picture book with excerpts from \"Walden\", and an annotated collection of Thoreau's essays on slavery. The United States Postal Service issued a commemorative stamp honoring Thoreau on May 23, 2017 in Concord, MA.\n\nAlthough his writings would receive widespread acclaim, Thoreau's ideas were not universally applauded. Scottish author Robert Louis Stevenson judged Thoreau's endorsement of living alone and apart from modern society in natural simplicity to be a mark of \"unmanly\" effeminacy and \"womanish solitude\", while deeming him a self-indulgent \"skulker\".\n\nNathaniel Hawthorne had mixed feelings about Thoreau. He noted that \"He is a keen and delicate observer of nature—a genuine observer—which, I suspect, is almost as rare a character as even an original poet; and Nature, in return for his love, seems to adopt him as her especial child, and shows him secrets which few others are allowed to witness.\" On the other hand, he also wrote that Thoreau \"repudiated all regular modes of getting a living, and seems inclined to lead a sort of Indian life among civilized men\".\n\nIn a similar vein, poet John Greenleaf Whittier detested what he deemed to be the \"wicked\" and \"heathenish\" message of \"Walden\", claiming that Thoreau wanted man to \"lower himself to the level of a woodchuck and walk on four legs\".\n\nIn response to such criticisms, English novelist George Eliot, writing for the \"Westminster Review\", characterized such critics as uninspired and narrow-minded:\nThoreau himself also responded to the criticism in a paragraph of his work \"Walden\" by illustrating the irrelevance of their inquiries:\n\nRecent criticism has accused Thoreau of hypocrisy, misanthropy, and being sanctimonious, based on his writings in \"Walden\", although this criticism has been perceived as highly selective.\n\n\n\n\n"}
{"id": "48790532", "url": "https://en.wikipedia.org/wiki?curid=48790532", "title": "Hexamolybdenum", "text": "Hexamolybdenum\n\nHexamolybdenum, is a molybdenum dominant alloy discovered during a nanomineralogy investigation of the Allende meteorite. Hexamolybdenum was discovered in a small ultrarefractory inclusion within the Allende meteorite. This inclusion has been named ACM-1. Hexamolybdenum is hexagonal, with a calculated density of 11.90 g/cm. The new mineral was found along with allendeite. These minerals, are believed to demonstrate conditions during the early stages of the Solar System, as is the case with many CV3 carbonaceous chondrites such as the Allende meteorite. Hexamolybdenum lies on a continuum of high-temperature alloys that are found in meteorites and allows a link between osmium, ruthenium, and iron rich meteoritic alloys. The name hexamolybdenum refers to the crystal symmetry (primitive hexagonal) and the molybdenum rich composition. The Allende meteorite fell in 1969 near Pueblito de Allende, Chihuahua, Mexico.\n\nHexamolybdenum was found as nano-crystals in an ultrarefractory inclusion in the Allende meteorite. The Allende meteorite has shown to be full of new minerals, after nearly forty years it has produced one in ten of the now known minerals in meteorites. This CV3 carbonaceous chondrite was the largest ever recovered on earth and is referred to as the best-studied meteorite in history.\nThe inclusion has only been viewed via electron microscopy. The hexamolybdenum specimen was lost during an attempted ion probe analysis of a bordering grain. Other specimens can be found, however, in the Smithsonian Institution's National Museum of Natural History Allende section USNM 3509HC12 and in section USNM 7590 of NWA 1934, another VC3 chondrite.\n\nIt has also been reported from the NWA 1934 CV3 carbonaceous chondrite meteorite from the Erfoud region of Morocco and in the Danubian placer of Straubing, Bavaria.\n\nHexamolybdenum is an (molybdenum, ruthenium, iron, iridium, osmium) alloy.\n\nColor, streak, luster, hardness, tenacity, cleavage, fracture, density, and refractive index could not be observed because the grain size was too small and the section bearing the mineral was optically thick.\n\n"}
{"id": "164610", "url": "https://en.wikipedia.org/wiki?curid=164610", "title": "Latent heat", "text": "Latent heat\n\nLatent heat is thermal energy released or absorbed, by a body or a thermodynamic system, during a constant-temperature process — usually a first-order phase transition.\n\nLatent heat can be understood as heat energy in hidden form which is supplied or extracted to change the state of a substance without changing its temperature. Examples are latent heat of fusion and latent heat of vaporization involved in phase changes, i.e. a substance condensing or vaporizing at a specified temperature and pressure. \n\nThe term was introduced around 1762 by British chemist Joseph Black. It is derived from the Latin \"latere\" (\"to lie hidden\"). Black used the term in the context of calorimetry where a heat transfer caused a volume change in a body while its temperature was constant.\n\nIn contrast to latent heat, sensible heat is a heat transfer that results in a temperature change in a body.\n\nThe terms ″sensible heat″ and ″latent heat″ refer to types of heat transfer between a body and its surroundings; they depend on the properties of the body. ″Sensible heat″ is ″sensed″ or felt in a process as a change in the body's temperature. ″Latent heat″ is heat transferred in a process without change of the body's temperature, for example, in a phase change ( solid / liquid / gas ). \n\nBoth sensible and latent heats are observed in many processes of transfer of energy in nature. Latent heat is associated with the change of phase of atmospheric or ocean water, vaporization, condensation, freezing or melting, whereas sensible heat is energy transferred that is evident in change of the temperature of the atmosphere or ocean, or ice, without those phase changes, though it is associated with changes of pressure and volume.\n\nThe original usage of the term, as introduced by Black, was applied to systems that were intentionally held at constant temperature. Such usage referred to \"latent heat of expansion\" and several other related latent heats. These latent heats are defined independently of the conceptual framework of thermodynamics.\n\nWhen a body is heated at constant temperature by thermal radiation in a microwave field for example, it may expand by an amount described by its \"latent heat with respect to volume\" or \"latent heat of expansion\", or increase its pressure by an amount described by its \"latent heat with respect to pressure\".\nLatent heat is energy released or absorbed, by a body or a thermodynamic system, during a constant-temperature process.\nTwo common forms of latent heat are latent heat of fusion (melting) and latent heat of vaporization (boiling). These names describe the direction of energy flow when changing from one phase to the next: from solid to liquid, and liquid to gas.\n\nIn both cases the change is endothermic, meaning that the system absorbs energy.\nFor example, when water evaporates, energy is required for the water molecules to overcome the forces of attraction between them, the transition from water to vapor requires an input of energy.\n\nIf the vapor then condenses to a liquid on a surface, then the vapor's latent energy absorbed during evaporation is released as the liquid's sensible heat onto the surface.\n\nThe large value of the enthalpy of condensation of water vapor is the reason that steam is a far more effective heating medium than boiling water, and is more hazardous.\n\nIn meteorology, latent heat flux is the flux of heat from the Earth's surface to the atmosphere that is associated with evaporation or transpiration of water at the surface and subsequent condensation of water vapor in the troposphere. It is an important component of Earth's surface energy budget. Latent heat flux has been commonly measured with the Bowen ratio technique, or more recently since the mid-1900s by the Jonathan Beaver method.\n\nThe English word \"latent\" comes from Latin \"latēns\", meaning \"lying hidden\". The term \"latent heat\" was introduced into calorimetry around 1750 when Joseph Black, commissioned by producers of Scotch whisky in search of ideal quantities of fuel and water for their distilling process, to studying system changes, such as of volume and pressure, when the thermodynamic system was held at constant temperature in a thermal bath. James Prescott Joule characterised latent energy as the energy of interaction in a given configuration of particles, i.e. a form of potential energy, and the sensible heat as an energy that was indicated by the thermometer, relating the latter to thermal energy.\n\nA \"specific\" latent heat (\"L\") expresses the amount of energy in the form of heat (\"Q\") required to completely effect a phase change of a unit of mass (\"m\"), usually , of a substance as an intensive property:\nIntensive properties are material characteristics and are not dependent on the size or extent of the sample. Commonly quoted and tabulated in the literature are the specific latent heat of fusion and the specific latent heat of vaporization for many substances.\n\nFrom this definition, the latent heat for a given mass of a substance is calculated by\nwhere:\n\nThe following table shows the specific latent heats and change of phase temperatures (at standard pressure) of some common fluids and gases.\n\nThe specific latent heat of condensation of water in the temperature range from −25 °C to 40 °C is approximated by the following empirical cubic function:\nwhere the temperature formula_4 is taken to be the numerical value in °C.\n\nFor sublimation and deposition from and into ice, the specific latent heat is almost constant in the temperature range from −40 °C to 0 °C and can be approximated by the following empirical quadratic function:\n\nAs the temperature (or pressure) rises to the critical point the LHOV falls to zero :\n\n"}
{"id": "18401357", "url": "https://en.wikipedia.org/wiki?curid=18401357", "title": "Light clay", "text": "Light clay\n\nLight clay (also light straw clay, light clay straw, slipstraw) is a natural building material used to infill between a wooden frame in a timber framed building using a combination of clay and straw, woodchips or some other lighter material.\n\nA mixture of clay and straw was used as an infill material for timber framed building from at least the 12th century in Germany and elsewhere in Europe. Renewed interest in traditional building methods developed from the 1980s after which various natural building architects and builders started promoting the use of light clay.\n\nLocal clay, often local subsoil, is mixed into a slurry with water and then combined with straw or wood chip or other similar material. Wood chips can vary in size from sawdust to chip 5cm in diameter. The ratio of clay to other ingredients can be adapted to either increase thermal mass or insulation properties. The mixture is provided with additional structural strength using wattles. When used externally it can be protected with a Lime render or a clay render.\n\n"}
{"id": "4650764", "url": "https://en.wikipedia.org/wiki?curid=4650764", "title": "List of National Wildlife Refuges established for endangered species", "text": "List of National Wildlife Refuges established for endangered species\n\nThis is a list of National Wildlife Refuges (NWR) established specifically for the protection of one or more endangered species.\n\n\n"}
{"id": "49659014", "url": "https://en.wikipedia.org/wiki?curid=49659014", "title": "List of cat documentaries, television series and cartoons", "text": "List of cat documentaries, television series and cartoons\n\nList of cat documentaries, television series and cartoons includes \"serious\" documentaries, television series and cartoons, in alphabetical order, related to cats .\n\n\n\n"}
{"id": "16657861", "url": "https://en.wikipedia.org/wiki?curid=16657861", "title": "List of mire landscapes in Switzerland", "text": "List of mire landscapes in Switzerland\n\nThe List of mire landscapes in Switzerland is a list of Swiss bogs and wetlands. It is from the \"Federal Inventory of Mire Landscapes of Particular Beauty and National Importance\" in Switzerland. \n\n\n"}
{"id": "14997569", "url": "https://en.wikipedia.org/wiki?curid=14997569", "title": "Location of Earth", "text": "Location of Earth\n\nKnowledge of the location of Earth has been shaped by 400 years of telescopic observations, and has expanded radically in the last century. Initially, Earth was believed to be the center of the Universe, \nwhich consisted only of those planets visible with the naked eye and an outlying sphere of fixed stars. After the acceptance of the heliocentric model in the 17th century, observations by William Herschel and others showed that the Sun lay within a vast, disc-shaped galaxy of stars. By the 20th century, observations of spiral nebulae revealed that our galaxy was one of billions in an expanding universe, grouped into clusters and superclusters. By the end of the 20th century, the overall structure of the visible universe was becoming clearer, with superclusters forming into a vast web of filaments and voids. Superclusters, filaments and voids are the largest coherent structures in the Universe that we can observe. At still larger scales (over 1000 megaparsecs) the Universe becomes homogeneous meaning that all its parts have on average the same density, composition and structure.\n\nSince there is believed to be no \"center\" or \"edge\" of the Universe, there is no particular reference point with which to plot the overall location of the Earth in the universe. Because the observable universe is defined as that region of the Universe visible to terrestrial observers, Earth is, by definition, the center of Earth's observable universe. Reference can be made to the Earth's position with respect to specific structures, which exist at various scales. It is still undetermined whether the Universe is infinite. There have been numerous hypotheses that our universe may be only one such example within a higher multiverse; however, no direct evidence of any sort of multiverse has ever been observed, and some have argued that the hypothesis is not falsifiable.\n\n"}
{"id": "42796964", "url": "https://en.wikipedia.org/wiki?curid=42796964", "title": "Naturalism (philosophy)", "text": "Naturalism (philosophy)\n\nIn philosophy, naturalism is the \"idea or belief that only natural (as opposed to supernatural or spiritual) laws and forces operate in the world.\" Adherents of naturalism (\"i.e.\", naturalists) assert that natural laws are the rules that govern the structure and behavior of the natural universe, that the changing universe at every stage is a product of these laws.\n\n\"Naturalism can intuitively be separated into an ontological and a methodological component,\" argues David Papineau. \"Ontological\" refers to the philosophical study of the nature of reality. Some philosophers equate naturalism with materialism. For example, philosopher Paul Kurtz argues that nature is best accounted for by reference to material principles. These principles include mass, energy, and other physical and chemical properties accepted by the scientific community. Further, this sense of naturalism holds that spirits, deities, and ghosts are not real and that there is no \"purpose\" in nature. Such an absolute belief in naturalism is commonly referred to as \"metaphysical naturalism\".\n\nAssuming naturalism in working methods as the current paradigm, without the further consideration of naturalism as an absolute truth with philosophical entailment, is called \"methodological naturalism\". The subject matter here is a philosophy of acquiring knowledge based on an assumed paradigm.\n\nWith the exception of pantheists—who believe that Nature is identical with divinity while not recognizing a distinct personal anthropomorphic god—theists challenge the idea that nature contains all of reality. According to some theists, natural laws may be viewed as so-called secondary causes of God(s).\n\nIn the 20th century, Willard Van Orman Quine, George Santayana, and other philosophers argued that the success of naturalism in science meant that scientific methods should also be used in philosophy. Science and philosophy are said to form a continuum, according to this view.\n\nThe current usage of the term naturalism \"derives from debates in America in the first half of the last century. The self-proclaimed 'naturalists' from that period included John Dewey, Ernest Nagel, Sidney Hook and Roy Wood Sellars.\"\n\nCurrently, metaphysical naturalism is more widely embraced than in previous centuries, especially but not exclusively in the natural sciences and the Anglo-American, analytic philosophical communities. While the vast majority of the population of the world remains firmly committed to non-naturalistic worldviews, prominent contemporary defenders of naturalism and/or naturalistic theses and doctrines today include J. J. C. Smart, David Malet Armstrong, David Papineau, Paul Kurtz, Brian Leiter, Daniel Dennett, Michael Devitt, Fred Dretske, Paul and Patricia Churchland, Mario Bunge, Jonathan Schaffer, Hilary Kornblith, Quentin Smith, Paul Draper and Michael Martin, among many other academic philosophers.\n\nAccording to David Papineau, contemporary naturalism is a consequence of the build-up of scientific evidence during the twentieth century for the \"causal closure of the physical\", the doctrine that all physical effects can be accounted for by physical causes.\n\nThe term \"methodological naturalism\" is much more recent though. According to Ronald Numbers, it was coined in 1983 by Paul de Vries, a Wheaton College philosopher. De Vries distinguished between what he called \"methodological naturalism,\" a disciplinary method that says nothing about God's existence, and \"metaphysical naturalism,\" which \"denies the existence of a transcendent God.\" The term \"methodological naturalism\" had been used in 1937 by Edgar S. Brightman in an article in \"The Philosophical Review\" as a contrast to \"naturalism\" in general, but there the idea was not really developed to its more recent distinctions.\n\nAccording to Steven Schafersman, naturalism is a philosophy that maintains that; \n\nOr, as Carl Sagan succinctly put it: \"\"The Cosmos is all that is or ever was or ever will be\".\"\n\nIn addition Arthur C. Danto states that Naturalism, in recent usage, is a species of philosophical monism according to which whatever exists or happens is \"natural\" in the sense of being susceptible to explanation through methods which, although paradigmatically exemplified in the natural sciences, are continuous from domain to domain of objects and events. Hence, naturalism is polemically defined as repudiating the view that there exists or could exist any entities which lie, in principle, beyond the scope of scientific explanation.\n\nAccording to Kuhn, all science is based on an approved agenda of unprovable assumptions about the character of the universe, rather than merely on empirical facts. These assumptions—a paradigm—comprise a collection of beliefs, values and techniques that are held by a given scientific community, which legitimize their systems and set the limitations to their investigation. Alfred North Whitehead wrote, \"All science must start with some assumptions as to the ultimate analysis of the facts with which it deals. These assumptions are justified partly by their adherence to the types of occurrence of which we are directly conscious, and partly by their success in representing the observed facts with a certain generality, devoid of \"ad hoc\" suppositions.\" Priddy notes that all scientific study inescapably builds on at least some essential assumptions that are untested by scientific processes. For naturalists, nature is the only reality. There is no such thing as 'supernatural'. The scientific method is to be used to investigate all reality, including the human spirit: \"The great majority of contemporary philosophers would happily... reject 'supernatural' entities, and allow that science is a possible route (if not necessarily the only one) to important truths about the 'human spirit'.\"\n\nNaturalism is the implicit philosophy of working scientists, that the following basic assumptions are needed to justify the scientific method:\n\n\nMetaphysical naturalism, also called \"ontological naturalism\" and \"philosophical naturalism\", is a philosophical worldview and belief system that holds that there is nothing but natural elements, principles, and relations of the kind studied by the natural sciences, i.e., those required to understand our physical environment by mathematical modeling. Methodological naturalism, on the other hand, refers exclusively to the methodology of science, for which metaphysical naturalism provides only one possible ontological foundation.\n\nMetaphysical naturalism holds that all properties related to consciousness and the mind are reducible to, or supervene upon, nature. Broadly, the corresponding theological perspective is religious naturalism or spiritual naturalism. More specifically, metaphysical naturalism rejects the supernatural concepts and explanations that are part of many religions.\n\nMethodological naturalism concerns itself with methods of learning what nature is. These methods are useful in the evaluation of claims about existence and knowledge and in identifying causal mechanisms responsible for the emergence of physical phenomena. It attempts to explain and test scientific endeavors, hypotheses, and events with reference to natural causes and events. This second sense of the term \"naturalism\" seeks to provide a framework within which to conduct the scientific study of the laws of nature. Methodological naturalism is a way of acquiring knowledge. It is a distinct system of thought concerned with a cognitive approach to reality, and is thus a philosophy of knowledge. Studies by sociologist Elaine Ecklund suggest that religious scientists in practice apply methodological naturalism. They report that their religious beliefs affect the way they think about the implications - often moral - of their work, but not the way they practice science.\n\nIn a series of articles and books from 1996 onward, Robert T. Pennock wrote using the term \"methodological naturalism\" to clarify that the scientific method confines itself to natural explanations without assuming the existence or non-existence of the supernatural, and is not based on dogmatic metaphysical naturalism (as claimed by creationists and proponents of intelligent design, in particular by Phillip E. Johnson). Pennock's testimony as an expert witness at the Kitzmiller v. Dover Area School District trial was cited by the Judge in his \"Memorandum Opinion\" concluding that \"Methodological naturalism is a 'ground rule' of science today\":\nExpert testimony reveals that since the scientific revolution of the 16th and 17th centuries, science has been limited to the search for natural causes to explain natural phenomena... While supernatural explanations may be important and have merit, they are not part of science.\" Methodological naturalism is thus \"a paradigm of science.\" It is a \"ground rule\" that \"requires scientists to seek explanations in the world around us based upon what we can observe, test, replicate, and verify.\n\nAlvin Plantinga, Professor Emeritus of Philosophy at Notre Dame, and a Christian, has become a well-known critic of naturalism. He suggests, in his evolutionary argument against naturalism, that the probability that evolution has produced humans with reliable true beliefs, is low or inscrutable, unless the evolution of humans was guided (for example, by God). According to David Kahan of the University of Glasgow, in order to understand how beliefs are warranted, a justification must be found in the context of supernatural theism, as in Plantinga's epistemology. \"(See also supernormal stimuli).\"\n\nPlantinga argues that together, naturalism and evolution provide an insurmountable \"\"defeater\" for the belief that our cognitive faculties are reliable\", i.e., a skeptical argument along the lines of Descartes' Evil demon or Brain in a vat.\n\nRobert T. Pennock contends that as supernatural agents and powers \"are above and beyond the natural world and its agents and powers\" and \"are not constrained by natural laws\", only logical impossibilities constrain what a supernatural agent could not do. He states: \"If we could apply natural knowledge to understand supernatural powers, then, by definition, they would not be supernatural\". As the supernatural is necessarily a mystery to us, it can provide no grounds on which to judge scientific models. \"Experimentation requires observation and control of the variables... But by definition we have no control over supernatural entities or forces.\" Science does not deal with meanings; the closed system of scientific reasoning cannot be used to define itself. Allowing science to appeal to untestable supernatural powers would make the scientist's task meaningless, undermine the discipline that allows science to make progress, and \"would be as profoundly unsatisfying as the ancient Greek playwright's reliance upon the \"deus ex machina\" to extract his hero from a difficult predicament.\"\n\nNaturalism of this sort says nothing about the existence or nonexistence of the supernatural, which by this definition is beyond natural testing. As a practical consideration, the rejection of supernatural explanations would merely be pragmatic, thus it would nonetheless be possible, for an ontological supernaturalist to espouse and practice methodological naturalism. For example, scientists may believe in God while practicing methodological naturalism in their scientific work. This position does not preclude knowledge that is somehow connected to the supernatural. Generally however, anything that can be scientifically examined and explained would not be supernatural, simply by definition.\n\nW. V. O. Quine describes naturalism as the position that there is no higher tribunal for truth than natural science itself. In his view, there is no better method than the scientific method for judging the claims of science, and there is neither any need nor any place for a \"first philosophy\", such as (abstract) metaphysics or epistemology, that could stand behind and justify science or the scientific method.\n\nTherefore, philosophy should feel free to make use of the findings of scientists in its own pursuit, while also feeling free to offer criticism when those claims are ungrounded, confused, or inconsistent. In Quine's view, philosophy is \"continuous with\" science and \"both\" are empirical. Naturalism is not a dogmatic belief that the modern view of science is entirely correct. Instead, it simply holds that science is the best way to explore the processes of the universe and that those processes are what modern science is striving to understand. However, this Quinean Replacement Naturalism finds relatively few supporters among philosophers.\n\nKarl Popper equated naturalism with inductive theory of science. He rejected it based on his general critique of induction (see problem of induction), yet acknowledged its utility as means for inventing conjectures.\n\nPopper instead proposed that science should adopt a methodology based on falsifiability for demarcation, because no number of experiments can ever prove a theory, but a single experiment can contradict one. Popper holds that scientific theories are characterized by falsifiability.\n\n\n\n"}
{"id": "980435", "url": "https://en.wikipedia.org/wiki?curid=980435", "title": "Naturalistic observation", "text": "Naturalistic observation\n\nNaturalistic observation is, in contrast to analog observation, a research tool in which a subject is observed in its natural habitat without any manipulation by the observer. During naturalistic observation, researchers take great care to avoid interfering with the behavior they are observing by using unobtrusive methods. Naturalistic observation involves two main differences that set it apart from other forms of data gathering. In the context of a naturalistic observation, the environment is in no way being manipulated by the observer nor was it created by the observer.\nNaturalistic observation, as a research tool, comes with both advantages and disadvantages that impact its application. By merely observing at a given instance without any manipulation in its natural context, it makes the behaviors exhibited more credible because they are occurring in a real, typical scenario as opposed to an artificial one generated within a lab. Naturalistic observation also allows for study of events that are deemed unethical to study via experimental models, such as the impact of high school shootings on students attending the high school. Naturalistic observation is used in many techniques, from watching an animal's eating patterns in the forest to observing the behavior of students in a school setting.\n\n"}
{"id": "13360851", "url": "https://en.wikipedia.org/wiki?curid=13360851", "title": "Nature center", "text": "Nature center\n\nA nature center (or nature centre) is an organization with a visitor center or interpretive center designed to educate people about nature and the environment. Usually located within a protected open space, nature centers often have trails through their property. Some are located within a state or city park, and some have special gardens or an arboretum. Their properties can be characterized as nature preserves and wildlife sanctuaries. Nature centers generally display small live animals, such as reptiles, rodents, insects, or fish. There are often museum exhibits and displays about natural history, or preserved mounted animals or nature dioramas. Nature centers are staffed by paid or volunteer naturalists and most offer educational programs to the general public, as well as summer camp, after-school and school group programs.\n\nSome nature centers allow free admission but collect voluntary donations in order to help offset expenses. They usually rely on support from dedicated volunteers.\n\nEnvironmental education centers differ from nature centers in that their museum exhibits and education programs are available mostly by appointment, although casual visitors may be allowed to walk on their grounds.\n\nSome city, state and national parks have facilities similar to nature centers, such as museum exhibits, dioramas and trails, and some offer park nature education programs, usually presented by a park ranger.\n\n"}
{"id": "58664232", "url": "https://en.wikipedia.org/wiki?curid=58664232", "title": "Phakalane power station", "text": "Phakalane power station\n\nPhakalane Power Station is a photovoltaic pilot power plant located in Phakalane, Botswana. The power station was funded through a Japanese grant which was part of Prime Minister Hatoyama's initiative strategy called Cool Earth Partnership aimed at supporting developing countries in their efforts to combat global warming. The Cool Earth Partnership is part of the initiatives which saw Hatoyama win the Sustainable Development Leadership Award in 2010.\n\n"}
{"id": "5308780", "url": "https://en.wikipedia.org/wiki?curid=5308780", "title": "Philo (journal)", "text": "Philo (journal)\n\nPhilo was a peer-reviewed academic journal published by the Society of Humanist Philosophers from 1998 to 2014. It is published at the Center for Inquiry with assistance from Purdue University. It focused on the discussion of philosophical issues from an explicitly naturalist perspective. The journal published articles, critical discussions, review essays, and book reviews in all fields of philosophy, and particularly invited work on the philosophical credentials of both naturalism and various supernaturalist alternatives to naturalism. Electronic access to the journal is provided by the Philosophy Documentation Center.\n\n\n"}
{"id": "1413688", "url": "https://en.wikipedia.org/wiki?curid=1413688", "title": "Primary energy", "text": "Primary energy\n\nPrimary energy (PE) is an energy form found in nature that has not been subjected to any human engineered conversion process. It is energy contained in raw fuels, and other forms of energy received as input to a system. Primary energy can be non-renewable or renewable.\n\nWhere primary energy is used to describe fossil fuels, the embodied energy of the fuel is available as thermal energy and around 70% is typically lost in conversion to electrical or mechanical energy. There is a similar 60-80% conversion loss when solar and wind energy is converted to electricity, but today's UN conventions on energy statistics counts the electricity made from wind and solar as the primary energy itself for these sources. One consequence of this counting method is that the contribution of wind and solar energy is under reported compared to fossil energy sources, and there is hence an international debate on how to count primary energy from wind and solar. \n\nTotal primary energy supply (TPES) is the sum of production and imports subtracting exports and storage changes.\n\nThe concept of primary energy is used in energy statistics in the compilation of energy balances, as well as in the field of energetics. In energetics, a primary energy source (PES) refers to the energy forms required by the energy sector to generate the supply of energy carriers used by human society.\n\nSecondary energy is a carrier of energy, such as electricity. These are produced by conversion from a primary energy source.\n\nThe use of primary energy as a measure ignores conversion efficiency. Thus forms of energy with poor conversion efficiency, particularly the thermal sources, coal, gas and nuclear are overstated, whereas energy sources such as hydroelectricity which are converted efficiently, while a small fraction of primary energy are significantly more important than their total raw energy supply may seem to imply.\n\nPE and TPES are better defined in the context of worldwide energy supply.\n\nPrimary energy sources should not be confused with the energy system components (or conversion processes) through which they are converted into energy carriers.\n\nPrimary energy sources are transformed in energy conversion processes to more convenient forms of energy that can directly be used by society, such as electrical energy, refined fuels, or synthetic fuels such as hydrogen fuel. In the field of energetics, these forms are called energy carriers and correspond to the concept of \"secondary energy\" in energy statistics.\n\nEnergy carriers are energy forms which have been transformed from primary energy sources. Electricity is one of the most common energy carriers, being transformed from various primary energy sources such as coal, oil, natural gas, and wind. Electricity is particularly useful since it has low entropy (is highly ordered) and so can be converted into other forms of energy very efficiently. District heating is another example of secondary energy.\n\nAccording to the laws of thermodynamics, primary energy sources cannot be produced. They must be available to society to enable the production of energy carriers.\n\nConversion efficiency varies. For thermal energy, electricity and mechanical energy production is limited by Carnot's theorem, and generates a lot of waste heat. Other non-thermal conversions can be more efficient. For example, while wind turbines do not capture all of the wind's energy, they have a high conversion efficiency and generate very little waste heat since wind energy is low entropy. In principle solar photovoltaic conversions could be very efficient, but current conversion can only be done well for narrow ranges of wavelength, whereas solar thermal is also subject to Carnot efficiency limits. Hydroelectric power is also very ordered, and converted very efficiently. The amount of usable energy is the exergy of a system.\n\nSite energy is the term used in North America for the amount of end-use energy of all forms consumed at a specified location. This can be a mix of primary energy (such as natural gas burned at the site) and secondary energy (such as electricity). Site energy is measured at the campus, building, or sub-building level and is the basis for energy charges on utility bills.\n\nSource energy, in contrast, is the term used in North America for the amount of primary energy consumed in order to provide a facility’s site energy. It is always greater than the site energy, as it includes all site energy and adds to it the energy lost during transmission, delivery, and conversion. While source or primary energy provides a more complete picture of energy consumption, it cannot be measured directly and must be calculated using conversion factors from site energy measurements. For electricity, a typical value is three units of source energy for one unit of site energy. However, this can vary considerably depending on factors such as the primary energy source or fuel type, the type of power plant, and the transmission infrastructure. One full set of conversion factors is available as technical reference from Energy STAR.\n\nEither site or source energy can be an appropriate metric when comparing or analyzing energy use of different facilities. The U.S Energy Information Administration, for example, uses primary (source) energy for its energy overviews but site energy for its Commercial Building Energy Consumption Survey and Residential Building Energy Consumption Survey. The US Environmental Protection Agency's Energy STAR program recommends using source energy, and the US Department of Energy uses site energy in its definition of a zero net energy building.\n\nEnergy accidents are accidents that occur in systems that provide energy or power. These can result in fatalities, as can the normal running of many systems, for example those deaths due to pollution.\n\nGlobally, coal is responsible for 100,000 deaths per trillion kWh.\n\n\n\n"}
{"id": "513965", "url": "https://en.wikipedia.org/wiki?curid=513965", "title": "Rammed earth", "text": "Rammed earth\n\nRammed earth, also known as taipa in Portuguese, tapial or tapia in Spanish, pisé (de terre) in French, and hangtu (), is a technique for constructing foundations, floors, and walls using natural raw materials such as earth, chalk, lime, or gravel. It is an ancient method that has been revived recently as a sustainable building material used in a technique of natural building.\n\nRammed earth is simple to manufacture, non-combustible, thermally massive, strong, and durable. However, structures such as walls can be laborious to construct of rammed earth without machinery, e. g., powered tampers, and they are susceptible to water damage if inadequately protected or maintained.\n\nEdifices formed of rammed earth are on every continent except Antarctica, in a range of environments including temperate, wet, semiarid desert, montane, and tropical regions. The availability of suitable soil and a building design appropriate for local climatic conditions are the factors that favour its use.\n\nManufacturing rammed earth involves compressing a damp mixture of earth that has suitable proportions of sand, gravel, clay, and/or an added stabilizer into an externally supported frame or mold, forming either a solid wall or individual blocks. Historically, additives such as lime or animal blood were used to stabilize it, while modern construction adds lime, cement, or asphalt emulsions. To add variety, some modern builders also add coloured oxides or other materials, e.g. bottles, tires, or pieces of timber.\n\nThe construction of an entire wall begins with a temporary frame, denominated the \"formwork\", which is usually made of wood or plywood, as a mold for the desired shape and dimensions of each section of wall. The form must be durable and well braced, and the two opposing faces must be clamped together to prevent bulging or deformation caused by the large compressing forces. Damp material is poured into the formwork to a depth of and then compacted to approximately 50% of its original height. The material is compressed iteratively, in batches or courses, so as to gradually erect the wall up to the top of the formwork. Tamping was historically manual with a long ramming pole, and was very laborious, but modern construction can be made less so by employing pneumatically powered tampers.\n\nAfter a wall is complete, it is sufficiently strong to immediately remove the formwork. This is necessary if a surface texture is to be applied, e.g., by wire brushing, carving, or mold impression, because the walls become too hard to work after approximately one hour. Construction is optimally done in warm weather so that the walls can dry and harden. The compression strength of the rammed earth increases as it cures; some time is necessary for it to dry and as long as two years can be necessary for complete curing. Exposed walls must be sealed to prevent water damage.\n\nIn modern variations of the technique, rammed-earth walls are constructed on top of conventional footings or a reinforced concrete slab base.\n\nWhere blocks made of rammed earth are used, they are generally stacked like regular blocks and are bonded together with a thin mud slurry instead of cement. Special machines, usually powered by small engines and often portable, are used to compress the material into blocks.\n\nPresently more than 30% of the world's population uses earth as a building material. Rammed earth has been used globally in a wide range of climatic conditions. Rammed-earth housing may resolve homelessness caused by otherwise expensive construction techniques.\n\nThe compressive strength of rammed earth is a maximum of . This is less than that of concrete but more than sufficiently strong for domestic edifices. Indeed, properly constructed rammed earth endures for thousands of years, as many ancient structures that are still standing around the world demonstrate. Rammed earth reinforced with rebar, wood, or bamboo can prevent collapse caused by earthquakes or heavy storms, because unreinforced edifices of rammed earth resist earthquake damage extremely poorly. See 1960 Agadir earthquake for an example of the total destruction which may be inflicted on such structures by an earthquake. Adding cement to soil mixtures poor in clay can also increase the load-bearing capacity of rammed-earth edifices. The United States Department of Agriculture observed in 1925 that rammed-earth structures endure indefinitely and can be constructed for less than two-thirds of the cost of standard frame houses.\n\nSoil is a widely available, inexpensive, and sustainable resource. Therefore, construction with rammed earth is very viable. Unskilled labour can do most of the necessary work. While the cost of rammed earth is low, rammed-earth construction without mechanical tools is very time-consuming and laborious; however, with a mechanical tamper and prefabricated formwork it can require only two or three days to construct the walls of a house.\n\nOne significant benefit of rammed earth is its high thermal mass: like brick or concrete, it can absorb heat during daytime and nocturnally release it. This action moderates daily temperature variations and reduces the need for air conditioning and heating. In colder climates, rammed-earth walls can be insulated with Styrofoam or a similar insert. It must also be protected from heavy rain and insulated with vapour barriers.\n\nRammed earth can effectively regulate humidity if unclad walls containing clay are exposed to an internal space. Humidity is regulated between 40% and 60%, which is the ideal range for asthma sufferers and for the storage of susceptible objects such as books. The material mass and clay content of rammed earth allows an edifice to breathe more than concrete edifices, which avoids problems of condensation but prevents significant loss of heat.\n\nUntouched, rammed-earth walls have the colour and texture of natural earth. Moisture-impermeable finishes, such as cement render, are avoided because they impair the ability of a wall to desorb moisture, which quality is necessary to preserve its strength. Well-cured walls accept nails and screws easily, and can be effectively patched with the same material used to build them. Blemishes can be repaired using the soil mixture as a plaster and sanded smooth.\n\nThe thickness, typically , and density of rammed-earth walls make them suitable for soundproofing. They are also inherently fireproof, resistant to termite damage, and non-toxic.\n\nEdifices of rammed earth are thought to be more sustainable and environmentally friendly than popular techniques of construction. Because rammed-earth edifices use locally available materials, they usually have low embodied energy and generate very little waste. The soils used are typically subsoils low in clay, between 5% and 15%, which conserve the topsoil for agriculture. When the soil excavated in preparation for a foundation can be used, the cost and energy consumption of transportation are minimal. Rammed earth is probably the least environmentally detrimental construction material and technique that is readily and commercially available today to construct solid masonry edifices. Rammed earth has potentially low manufacturing impact, contingent on the amount of cement and the amount that is locally sourced; it is often quarried aggregates rather than \"earth\".\n\nFormwork is removable and can be reused, reducing the need for lumber.\nMixing cement with the soil can counteract sustainable benefits such as low embodied energy and humidity regulation because manufacture of the cement itself adds to the global carbon dioxide burden at a rate of 1.25 tonnes per tonne of cement produced. Partial substitution of cement with alternatives such as ground granulated blast furnace slag has not been demonstrated to be effective, and implicates other questions of sustainability.\n\nRammed earth can contribute to the overall energy efficiency of edifices: the density, thickness, and thermal conductivity of rammed earth render it an especially suitable material for passive solar heating. Warmth requires almost 12 hours to be conducted through a wall thick.\n\nRammed-earth construction may also reduce the ecological impacts of deforestation and the toxicity of artificial materials associated with conventional construction techniques.\n\nAlthough it has low greenhouse emissions in theory, transportation and the production of cement can add significantly to the overall emissions of modern rammed earth construction. The most basic kind of traditional rammed earth has very low greenhouse gas emissions but the more engineered and processed variant of rammed earth has the potential for significant emissions.\n\nEvidence of ancient use of rammed earth has been found in Neolithic archaeological sites of the Yangshao and Longshan cultures along the Yellow River in China, dating to 5000 BCE. By 2000 BCE, rammed-earth architectural techniques (夯土 \"Hāng tǔ\") were commonly used for walls and foundations in China.\n\nIn the 1800s, rammed earth was popularized in the United States by the book \"Rural Economy\" by S. W. Johnson. The technique was used to construct the Borough House Plantation and the Church of the Holy Cross in Stateburg, South Carolina, both being National Historic Landmarks.\nAn outstanding example of a rammed-earth edifice in Canada is St. Thomas Anglican Church in Shanty Bay, Ontario, erected between 1838 and 1841.\n\nFrom the 1920s through the 1940s rammed-earth construction in the US was studied. South Dakota State College extensively researched and constructed almost one hundred weathering walls of rammed earth. For over 30 years the college investigated the use of paints and plasters in relation to colloids in soil. In 1945, Clemson Agricultural College of South Carolina published the results of their research of rammed earth in a pamphlet titled \"Rammed Earth Building Construction\". In 1936, on a homestead near Gardendale, Alabama, the United States Department of Agriculture constructed an experimental community of rammed-earth edifices with architect Thomas Hibben. The houses were inexpensively constructed and were sold to the public along with sufficient land for gardens and small plots for livestock. The project successfully provided valuable homes to low-income families.\n\nThe US Agency for International Development is working with undeveloped countries to improve the engineering of rammed-earth houses. It also financed the authorship of the \"Handbook of Rammed Earth\" by Texas A&M University and the Texas Transportation Institute. The \"Handbook\" was unavailable for purchase by the public until the Rammed Earth Institute International gained permission to reprint it.\n\nInterest in rammed earth declined after World War II when the cost of modern construction materials decreased. Rammed earth was considered substandard, and still is opposed by many contractors, engineers, and tradesmen who are unfamiliar with earthen construction techniques. The prevailing perception that such materials and techniques perform poorly in regions prone to earthquakes has prevented their use in much of the world. In Chile, for example, rammed earth edifices normally cannot be conventionally insured against damage or even be approved by the government.\n\nA notable example of 21st century use of rammed earth is the façade of the Nk'Mip Desert Cultural Centre in southern British Columbia, Canada. As of 2014 it is the largest rammed earth wall in North America.\n\n\nRammed earth wall construction at Central Arizona College\n"}
{"id": "44120129", "url": "https://en.wikipedia.org/wiki?curid=44120129", "title": "Revolving rivers", "text": "Revolving rivers\n\nRevolving rivers are a surprising, uncommon way of sand pile growth that can be found in a few sands around the world, but has been studied in detail only for one Cuban sand from a place called Santa Teresa (Pinar del Rio province).\n\nWhen pouring \"revolving\" sand on a flat surface from a fixed position, the growth of a conical pile does not occur by the common avalanche mechanism, where sand slides down the pile in a more or less random fashion. What happens in that a relatively thin \"river\" of flowing sand travels from the pouring point at the apex of the pile to its base, while the rest of the sand at the surface is static. In addition, the river \"revolves\" around the pile either in clockwise or counter-clockwise directions (looking from top) depending on the initial conditions of the experiment. Actually the river constitutes the \"cutting edge\" of a layer of sand that deposits as a helix on the conical pile, and makes it grow.\nFor small sandpiles, rivers are continuous, but they become intermittent\nfor larger piles.\n\nThe phenomenon was observed first by E. Altshuler at the University of Havana in 1995, but at the time he assumed that it was well known, and temporarily forgot about it. In 2000, being at the University of Houston, he told K. E. Bassler, who showed a vivid interest in the matter. Embarrassingly enough, Altshuler was unable to demonstrate it before Bassler using a random sand from Houston, so he had to send him a video from Cuba after his return to the island.\n\nOnce the existence of the strange phenomenon was confirmed for everyone, E. Altshuler and a number of collaborators performed a systematic study in Havana, which was then jointly published with Bassler.\nFurther work has been done to understand in more detail the\nphenomenon, and it has been found in other sands from different parts of the world. \nHowever, the connection between the physical, chemical (and possibly biological) properties of the grains in a specific sand, the nature of the inter-grain interactions, and the emergence of the revolving rivers is still an open question.\n\nSand from Santa Teresa is made of almost pure silicon dioxide grains with an average grain size of 0.2 mm approximately and no visible special features regarding grain shape. But in spite of its apparent simplicity, many puzzles still remain. For example, after many experiments one batch of sand may stop showing revolving rivers (just as singing sand eventually stops singing), which suggests that the decay is connected to certain properties of the surface of the grains that degrade by continued friction.\n\nVideos of the effect are available on YouTube.\n"}
{"id": "55613190", "url": "https://en.wikipedia.org/wiki?curid=55613190", "title": "Scotlandite", "text": "Scotlandite\n\nScotlandite is a sulfite mineral first discovered in a mine at Leadhills in South Lanarkshire, Scotland, an area known to mineralogists and geologists for its wide range of different mineral species found in the veins that lie deep in the mine shafts. This specific mineral is found in the Susanna vein of Leadhills, where the crystals are formed as chisel-shaped or bladed. Scotlandite was actually the first naturally occurring sulfite, which has the ideal chemical formula of PbSO. The mineral has been approved by the Commission on New Minerals and Mineral Names, IMA, to be named scotlandite for Scotland.\n\nScotlandite is found in association with pyromorphite, anglesite, lanarkite, leadhillite, susannite, and barite. It occurs in cavities in massive barite and anglesite, and is closely associated with lanarkite and susannite. Scotlandite represents the latest phase in the crystallization sequence of the associated lead secondary minerals. It can often be found in the vuggy anglesite as yellowish single crystals up to 1 millimeter in length that sometimes arrange in a fan-shaped aggregates. Anglesite can usually be recognized in a very thin coating on scotlandite which is used to protect the sulfite from further oxidation. A second variety of scotlandite can also occur in discontinuously distributed cavities between the anglesite mass containing the first variety and the barite matrix. This variety is characterized by tiny, whitish to water-clear crystals, and crystal clusters less than one millimeter in size, which encrust large portions of the interior of the cavities. Scotlandite is a uniquely rare mineral, as it occurs in small amounts in few locations around the world.\n\nScotlandite is a pale yellow, greyish-white, colorless, transparent mineral with an adamantine or pearly luster. It exhibits a hardness of 2 on the Mohs hardness scale. Scotlandite occurs as chisel-shaped or bladed crystals elongated along the c-axis, with a tendency to form radiating clusters. Its crystals are characterized by the {100}, {010}, {011}, {021}, {031}, and {032}. faces. Scotlandite shows perfect cleavage along the {100} plane and a less good one along the {010} plane. The measured density is 6.37 g/cm.\n\nScotlandite is biaxial positive, which means it will refract light along two axes. The mineral is optically biaxial positive, 2V 35° 24'(Na). The refractive indices are: α ~ 2.035, β ~ 2.040, and γ ~ 2.085 (Na). Dispersion is strong, v » r. The extinction is β//b, and α [001] = 20° (γ [100] = 4° in the obtuse angle β. H(Mohs) < 2. D = 6.37 and calculated D = 6.40 g cm. The infrared spectrum of scotlandite shows conclusively that it is an anhydrous sulfite, with no OH groups or other polyatomic anions being present. It is also proven by electron microprobe analysis and infrared spectroscopy that scotlandite must be a polymorph of lead sulfite.\n\nScotlandite is a sulfite compared with chemically related compounds, it is very close to the value of anglesite (6.38 g cm), but distinctly different from that of lanarkite (6.92 g cm). Orthorhombic lead sulfite is of higher density (D = 6.54, calculated D = 6.56 g cm), and has the same chemical properties as well. The empirical chemical formula for scotlandite calculated on the basis of Pb+S = 2, is PbSO or more ideally PbSO.\n\nA small crystal of scotlandite, showing some cleavage faces, was examined using Weissenberg and precession techniques. Scotlandite is in the monoclinic crystal system. The only systematic extinctions observed from the single crystal patterns were 0k0 where k was odd. Thus the possible space group is either P2 or P2/m. The unit cell parameters obtained from the single crystal study were used to index the X-ray powder pattern and were then refined with the indexed powder data. The results are: a = 4.505 Å, b = 5.333 Å, c = 6.405 Å; β= 106.24°; Z = 2. If the present a and c axes are interchanged, the unit cell of scotlandite is very similar, isotypic, to that of molybdomenite, PbSeO. Lead is coordinated to nine oxygen atoms with Pb-O=2.75 Å, and possibly further to one sulfur atom with Pb−S=3.46 Å. The average S−O distance in the pyramidal SO group is 1.52 Å.\n\nList of Minerals\n"}
{"id": "46324244", "url": "https://en.wikipedia.org/wiki?curid=46324244", "title": "Skeletal changes of organisms transitioning from water to land", "text": "Skeletal changes of organisms transitioning from water to land\n\nInnovations conventionally associated with terrestrially first appeared in aquatic elpistostegalians such as \"Panderichthys rhombolepis\", \"Elpistostege watsoni\", and \"Tiktaalik roseae\". Phylogenetic analyses distribute the features that developed along the tetrapod stem and display a stepwise process of character acquisition, rather than abrupt. The complete transition occurred over a period of 25 million years beginning with the tetrapodomorph diversification in the Middle Devonian (380 myr).\n\nBy the Upper Devonian period, the fin-limb transition as well as other skeletal changes such as gill arch reduction, opercular series loss, mid-line fin loss, and scale reduction were already completed in many aquatic organisms. As aquatic tetrapods began their transition to land, several skeletal changes are thought to have occurred to allow for movement and respiration on land. Some adaptations required to adjust to non-aquatic life include the movement and use of alternating limbs, the use of pelvic appendages as sturdy propulsors, and the use of a solid surface at the organism’s base to generate propulsive force required for walking.\n\nThe Osteolepiformes and Elpistostegalia are two crown groups of rhipidistians with respect to the tetrapods. The development of skull roof and cheekbone patterns in these organisms match those found in the first tetrapods. Palatal and nasal skeletal features like choanae are present in these groups and are also observed in modern amphibians. This indicates that incipient air breathing was developed, as well as modification of the hyoid arch towards stapes development. These characteristics account for why osteichthyans are accepted as the sister group of tetrapods.\n\nThe elpistostegalid fish are considered the most apomorphic of fish in comparison to tetrapods. From well-preserved fossils, it is observed that they share a paltybasic skull with eye ridges, and external nares situated on the margin of the mouth. Development of eye ridges and flatting of the skull are also observed in primitive fossil amphibians and reptiles. The most likely reason for the traits to be adaptive was for their use in aerial vision above the waterline. The traits enabled animals to check area on land for safe spots if being chased by a predator in water, as well as being useful for searching for prey items above the water. The water-based lateral line system was used substantially by these aquatic tetrapods to detect danger from predators. Within the Osteichthyan diversification, there were no changes related to respiration in the transition as can be seen by the nasal region and palatal morphology in elpistostegalid fishes. The primary change from basic ostelepiform ancestors to the first elpistostegalid in the middle Devonian was to the pre-existing roof skulls.\n\nIn \"Elginerpeton pancheni\", a prototetrapod from the late Frasnian, basic tetrapod characteristics in the lower jaw and the cranium are observed. The taxon is believed to fill the gap between elpistostegalid fishes and well-preserved Devonian tetrapods. The \"Elginerpeton\" is considered more derived than the elpistostegalid fishes due to presence of paired fangs on the parasymphysial toothplate, a slender shaped anterior coronoid, and in the loss of the intracranial joint and coronoid fossa. The loss of the intercranial joint was a direct functional necessity to strengthen the broad and long platybasic skull when the animal was out of the water. The tubular lower jaw of the \"Elginerpeton\", compared to the flat-lamina jaw shape of fishes gave it superior cross-sectional force, required when not supported in an aquatic setting – allowing for opening of the mouth outside of water. The adaptation may also be interpreted as a specialization for buccopharyngeal breathing. It is speculated to be the first step towards aerial respiration in the transition from fish to tetrapod.\n\nIn the tetrapod and higher clades from the lower-middle Famennian there are several defining changes on the basis of anatomy of \"Ichthyostega\", \"Tulerpeton\", and \"Acanthostega\". In the cranium, there is a stapes derived from the hyomandibular of fishes; a single bilateral pair of nasal bones, and a fenestra ovalis in the otic capsule of the braincase. The opening of the otic wall of the braincase can be considered a paedomorphic feature for tetrapods and is linked to the stapes functionally. The stapes was thought to be just a structural support between the palate and the stapedial plate of the braincase. In the \"Acanthostega\", it is likely that due to the otic capsule of the brain case being mesial to the stapedial plate, sound was picked up from the palate or the otic notch to allow for rudimentary hearing. It was able to perceive vibrations by opening its mouth by way of the palate. Other factors that caused aquatic tetrapods to spend more time on land caused the development of terrestrial hearing with the development of a tympanum within an otic notch and developed by convergent evolution at least three times.\nThere was also a change in the dermal bones of the skull in the aquatic tetrapods. It involved the enlargement of the jugal, ceasing the contact of the maxilla with the squamosal and the single bilateral pair of nasal bones. The feature allows for a stronger bite as well as increasing the strength of the skull.\n\nFeeding on land is a completely different task than feeding in water. Water is much more dense and viscous compared to air, causing hunting techniques adapted in water to be less successful when applied on land. The main technique used in water is suction feeding and is used by most aquatic vertebrates. This technique does not function in air so animals use methods of overtaking prey with jaws followed by biting down. Transitional forms prior to fully developed terrestrial tetrapods such as \"Acanthostega\", are thought to have captured prey in the water. Large coronoid fangs are present in the fishes \"Eusthenopteron\", \"Panderichthys\", and \"Tiktaalik\", and the early tetrapod, \"Ventasega\". In \"Acanthostega\", which is more derived, the large teeth are absent. In \"Eusthenopetron\" and \"Panderichthys\", an ossified operculum is exhibited unlike in the \"Tiktaalik\", \"Ventastega\", and \"Acanthostega\". These differences as well as reductions of the gill chamber and changes in the nature of the lower jaw are hypothesized to indicate a reduced reliance on suction feeding in early tetrapods in comparison to osteolepiform fish. This morphological data is not enough however to prove that suction feeding was less used as the morphological changes have been found in fish that use the suction feeding mechanism.\n\nCranial sutures are indicators of skull function and morphologies can be linked to specific feeding modes. Transitional feeding changes can be observed by examining cross sectional morphology of a suture in taxa of the fish-tetrapod transition. Comparing positionally comparable sutures in extant fish allows for the creation of a sutural morphospace. The main cause of sutural deformation is caused by strain during feeding activity, most prominent with feeding mechanisms involving sucking a prey into the mouth. There is a tension anteriorly, and compression posteriorly strain patterns are observed in \"Polypterus\", a prey-sucking predator. In terrestrial tetrapod \"Phonerpeton\", there is compression between the frontals and parietals and a complex loading between the post parietals. There is no evidence of tensile strain in any sutures. \"Acanthostega\" fossil records demonstrate that no strain pattern was exhibited that relate to prey capture by means of suction. The load compression is similar to extant tetrapods. It is most likely that the organism captured prey by biting in the water or near the edge of the water. This finding indicates that the terrestrial mode of feeding first emerged in an aquatic environment.\n\nThe cranial endoskeleton of \"T. roseae\" shares derived features with tetrapods. There was a loss of opercular and extrascapular elements, enhancing head mobility in \"T. roseae\" compared to other tetrapodomorph fish. The formation of the neck allowed for locomotion in shallow waters. This environment allows for less motility compared to the three-dimensional space that fish are able to orient themselves in. The body of the organism in these environments would be fixed in the shallow pools with appendages planted on a substrate.\n\nIn the \"Acanthostega\" and \"Ichthyostega\", which are considered to be more derived than other basal aquatic tetrapods, the pectoral girdle is decoupled from the skull. There is also a loss of the dorsal pectoral girdle bones, which permits a large degree of movement for the shoulder. This allowed for a greater degree of movement, and is a necessity for improving aquatic maneuveurs and terrestrial locomotion. This could have been driven by the need to lift the head to aid aerial respiration by using nostrils and choanae.\n\nLimbs in vertebrates are occasionally organized into stylopod (relating to the humerus and femur), zeugopod (relating to the radius and tibia, along with associated structures) and autopod (relating to digits) categories, although anatomically, the evolutionary differences between these groups in early tetrapods tends to be vague.\nThe transition from fins to limbs occurred once an endoskeleton entered the base of the fin, as seen in today's lungfish. This is thought to have originated in the group Sarcopterygians, including osteolipiforms like \"Eusthenopteron\", due to the homology of the tetrapod forelimb and the osteolepiform fin endoskeleton.\n\n\"Acanthostega\" is a partially aquatic tetrapod with developed limbs that shares features common with the earlier tetrapods, \"Panderichthys\" and \"Eusthenopteron\". Like \"Panderichthys\", the humerus of \"Acanthostega\" is flattened dorso-ventrally, the intermedium terminates level with the radius, and the endoskeleton can be divided into stylopodium, zeugopodium and autopodium segments. Similar to \"Eusthenopteron\", the radials do not articulate with the radius on the distal end. \"Acanthostega\" also has a 1:2 ratio of humerus to radius and ulna, a feature seen in all tetrapods higher than \"Acanthostega\" on the phylogeny.\n\nUnlike \"Panderichthys\", \"Acanthostega\" hind limbs are at least the size of its fore limbs, if not larger. This development of larger limbs is required to physically support the organism during emergence from an aquatic setting to land. The humerus and femur of \"Acanthostega\" also contain evidence of greater development of the appendicular muscles compared to more aquatic tetrapods, hinting at the presence of digits.\n\nSimilarly, \"Ossinodus\" has two hindlimbs located bilaterally and proximodistally aymmetrical. Due to the presence of a small femur during juvenile development, this Carboniferous- period tetrapod is thought to be aquatic during juvenile development; only emerging onto land once it reaches adulthood. \"Ossinodus\" also has a broad, flat tibia, akin to \"Acanthostega\", and is thought to be only partially terrestrial.\n\nThe development of the pelvic region was crucial for the adaptation from water to land, yet some features of tetrapod locomotion are thought to have arose before the origin of digited limbs or the transition from water to land. The fossil record of early tetrapods shows evidence of distinct pelvic development occurring in osteolepiforms, further supporting osteolepiform ancestry of terrestrial tetrapods.\n\n\"Acanthostega\" has a large pelvis, with the iliac region articulating with the axial skeleton and a broad ischial plate. It has a sacrum; a fundamental skeletal feature that allows the organism to transfer force produced in its hindlimbs to its axial skeleton, and move in a terrestrial environment. A pubo-ischiadic symphysis is also observed, uniting the two pelvic halves.\n\nIn contrast, \"Protopterus annectens\" (a member of lungfish, thought to be a sister group to tetrapods) has a small, anatomically simpler pelvis, a derived limb endoskeleton and a lack of digits. Yet, it shares the ability to lift itself using a solid surface as a base with its pelvic region with \"Acanthostega\" and is also observed to move with tetrapod-like locomotion in an aquatic environment. This illustrates that a fundamental innovation in tetrapods is also found in a lower, sister taxon, in which members lack a sacrum.\n\n\"Acanthostega\" is the earliest example of a digitized tetrapod. The humerus and femur of \"Acanthostega\" contain evidence of greater development of the appendicular muscles compared to more aquatic tetrapods. \"Acanthostega\" has a total lack of dermal fin rays and displays the presence of two or more spool-shaped bones or cartilages articulating individually in antero-posterial sets on the distal end of its limbs. This feature can now be distinguished as digits instead of the endoskeletal radials seen in earlier tetrapods.\n\n\"Pederpes\", a tetrapod from the Early Carboniferous period, also has hindlimbs containing 5 digits that are rotated to face anteriorly. Unlike previous tetrapods, who have been only partially adapted to land, \"Pederpes\" has the novel ability to bend its limbs and propel itself forwards in a terrestrial setting. This is attributed to the symmetry of the digits and limbs in \"Pederpes\", allowing it to rotate its hindlimbs to an anteriorly facing position and propel itself from the edge of the foot when moving forward. This morphological development of bendable wrists and ankles can distinguish \"Pederpes\" the first true terrestrial tetrapod.\n"}
{"id": "43868891", "url": "https://en.wikipedia.org/wiki?curid=43868891", "title": "Social Bonding and Nurture Kinship", "text": "Social Bonding and Nurture Kinship\n\nSocial Bonding and Nurture Kinship: Compatibility between Cultural and Biological Approaches is a book on human kinship and social behavior by Maximilian Holland, published in 2012. The work synthesizes the perspectives of evolutionary biology, psychology and sociocultural anthropology towards understanding human social bonding and cooperative behavior. It presents a theoretical treatment that many consider to have resolved longstanding questions about the proper place of genetic (or 'blood') connections in human kinship and social relations, and a synthesis that \"should inspire more nuanced ventures in applying Darwinian approaches to sociocultural anthropology\". The book has been called \"A landmark in the field of evolutionary biology\" which \"gets to the heart of the matter concerning the contentious relationship between kinship categories, genetic relatedness and the prediction of behavior\", \"places genetic determinism in the correct perspective\" and serves as \"a shining example of what can be achieved when excellent scholars engage fully across disciplinary boundaries.\"\n\nThe aim of the book is to show that \"properly interpreted, cultural anthropology approaches (and ethnographic data) and biological approaches are perfectly compatible regarding processes of social bonding in humans.\" Holland's position is based on demonstrating that the dominant biological theory of social behavior (inclusive fitness theory) is typically misunderstood to predict that genetic ties are necessary for the \"expression\" of social behaviors, whereas in fact the theory only implicates genetic associations as necessary for the \"evolution\" of social behaviors. Whilst rigorous evolutionary biologists have long understood the distinction between these levels of analysis (see Tinbergen's four questions), past attempts to apply inclusive fitness theory to humans have often overlooked the distinction between \"evolution\" and \"expression\".\n\nBeyond its central argument, the broader philosophical implications of Holland's work are considered by commentators to be that it both \"helps to untangle a long-standing disciplinary muddle\" and \"clarifies the relationship between biological and sociocultural approaches to human kinship.\" It is claimed that the book \"demonstrates that an alternative non-deterministic interpretation of evolutionary biology is more compatible with actual human social behavior and with the frameworks that sociocultural anthropology employs\" and as a consequence, delivers \"a convincing, solid and informed blow to the residual genetic determinism that still influences the interpretation of social behaviour.\"\n\nThe book's form consists of a cumulative argument (using a wide range of supporting evidence) made over nine chapters, with each chapter ending in a brief retrospective summary, and the final chapter containing a recapitulation and summary of the whole, and drawing some wider conclusions.\n\nHolland begins by tracing transitions in the history of anthropological theories of social behavior and kinship, noting the varying importance with which 'blood ties' have been understood to be a necessary element of human kinship and social relations. He suggests that whilst the mounting ethnographic evidence has led to a move away from the 'blood kinship' concept in recent decades, many sociocultural anthropologists still query the connection between kinship and blood, reproduction or some other apparently biological functions. Meanwhile, many biologists, biological anthropologists and evolutionary psychologists have persisted in viewing human kinship and cooperative behavior as necessarily associated with genetic relationships and 'blood ties'. The current situation has been characterized as \"a clash between incommensurate paradigms, holding as they may, completely incompatible ideas about human nature.\" Holland argues that a clear resolution to these questions is still outstanding, and would therefore be of value. In closing the introduction, Holland writes; \"The approach is not reductive. The claim is rather that a thorough investigation of the ‘biological facts’ can be useful mainly though allowing a change in focus... away from confusion about the place of genealogy in social ties, and onto a reformulated baseline, built around varied \"processual aspects of social bonding.\"\"\n\nThe book reviews the background and key elements of Hamilton's inclusive fitness theory from the 1960s onwards, setting out its significant conceptual and heuristic value. Holland notes that Hamilton acknowledged that his earliest and most widely known account (1964) contained technical inaccuracies. He also notes Hamilton's early speculations about possible proximate mechanisms of the expression of social behavior (\"supergenes\" as a possible alternative to \"behaviour-evoking-situations\") contained errors that have nevertheless remained very influential in popular accounts. Specifically, the supergenes notion (sometimes called the \"Green-beard effect\") - that organisms may evolve genes that are able to identify identical copies in others and preferentially direct social behaviours towards them - was theoretically clarified and withdrawn by Hamilton in 1987. However, in the intervening years, the notion that supergenes (or more often, simply individual organisms) have evolved \"to identify genetic relatives and preferentially cooperate with them\" took hold, and became the way many biologists came to understand the theory. This persisted, despite Hamilton's 1987 correction. In Holland's view it is the pervasiveness of this longstanding but erroneous perspective, and the suppression of the alternative 'behaviour-evoking-situations' perspective regarding social expression mechanisms, that is largely responsible for the ongoing clash between biological and sociocultural approaches to human kinship.\n\nHolland shows that, in the 1970s and 80s, the first wave of attempts (known as \"human sociobiology\" or \"Darwinian anthropology\") to apply inclusive fitness theory to human social behavior relied on, and further reinforced, this same misinterpretation (above section) about the theory's predictions and the proximate mechanisms of social behavior. Holland also shows that this period of research was burdened with many misplaced assumptions about \"universal\" attributes of the human sexes, sexuality and gender roles, apparently projected from the \"specific cultural values\" of the researchers themselves. Holland also shows that, following the perceived failures of this early wave, and particularly its methodological agnosticism regarding proximate mechanisms of social behavior, the evolutionary psychology school grew up in its place. Although this latter school typically avoided engaging with the ethnographic data on human kinship, Holland argues that in the few cases where it did so, it repeated the misinterpretation of inclusive fitness theory that characterized the first wave. Holland also notes that Kitcher, in his critique of the sociobiological position, suggested that perhaps the expression of social behaviors in humans might quite simply be based on cues of context and familiarity, rather than genetic relatedness \"per se\".\n\nChapters four and five investigate further the theory and evidence surrounding the \"proximate mechanisms\" of social behavior; specifically the question of whether social behaviors are expressed by organisms via \"behaviour-evoking-situations\" or via direct detection of actual genetic relatedness. Related questions have been the domain of kin recognition theory. Holland notes that the name 'kin recognition' itself suggests some expectation that a positive identification of genetic relatedness is a prediction of inclusive fitness theory, and is thus expected. Similar points have been made by others; \"many behavioural ecologists seem to implicitly assume that specialised mechanisms allowing individuals to distinguish their kin from non-kin must have evolved.\" Again, the possibility that \"behaviour-evoking-situations\" might be the more parsimonious mechanism of the expression of social behavior, and fully compatible with inclusive fitness theory, has often been underemphasized. However, Holland's review of the evidence notes that field studies in this area quickly established that \"behaviour-evoking-situations\" do in fact overwhelmingly mediate social behaviours in those species studied, and that, particularly in mammal species, social bonding and familiarity formed in early developmental contexts (e.g. in burrows or nesting sites) are a common mediating mechanism for social behaviors, independently of genetic relatedness \"per se\". On the basis of the preceding theoretical analysis and review of evidence, at the end of chapter five, Holland argues that;\nIt is entirely erroneous, both in reference to theory and in reference to the evidence, to claim or suggest that 'the facts of biology' support the claim that organisms have evolved to cooperate with genetic relatives \"per se\".\n\nHaving argued for the above position on the lack of necessity for genetic relatedness \"per se\" to mediate social bonding and behavior, Holland suggests that \"The further question then is; can we uncover in any greater detail how familiarity and other context-dependent cues operate?\". To discover the extent to which the variety of human kinship behaviors may nevertheless be compatible with this (less deterministic) interpretation of biological theory of social behavior, Holland suggests that a survey of primates' most fundamental social patterns may give clues, especially those of species most closely connected with humans. The variety of primate mating systems, group-membership ('philopatry') patterns, and life-cycle patterns are reviewed. Holland finds that;\n\nLike other mammals, Catarrhini primate demographics are strongly influenced by ecological conditions, particularly density and distribution of food sources... Cohesive social groups and delayed natal dispersal mean that maternally related individuals, including maternal siblings, face a statistically reliable context of interaction in all Catarrhini primates. This reliable context of interaction with maternally related individuals is extended amongst those species with female philopatry (especially Cercopithecinae).\n\nAs with other social mammals, evidence suggest that the reliability of 'behaviour-evoking-situations' this social context provides has shaped the mechanisms of proximate expression of social bonding and behavior;\n\nAdoption of infants by females (and sometimes males) demonstrates that care-giving and bonding to infants is not mediated by positive powers of discrimination. From the infant's perspective, it will bond with any responsive carer. If not necessarily the actual mother, in natural conditions this will often be a maternal relative (particularly an older sibling), but the context is primary, not the actual relatedness. Similarly, social bonding and social behaviours between maternal siblings (and occasionally between other maternal relatives) is context-driven in primates, and mediated via the care-giver.\n\nHolland also notes how Bowlby and colleagues' attachment theory was strongly informed by primate bonding patterns and mechanisms, and that in Bowlby's later writing the then emerging inclusive fitness theory was explicitly linked to. \n[Bowlby's] work demonstrated that social attachments form on the basis of provision of care, and responsiveness to elicitations for care. The social context of living together and the familiarity this brings, provides the circumstance within which social bonds can form...\n\nOn the basis of combining more recent primate research with the findings of attachment theory, Holland proposes that \"In attempting to define more specific forms of \"the giving of care and nurture\" which may mediate social bonding we [find] that provision of food is likely to play a part, as well as the more intangible provision of warmth and comfort, and a safe base for sleeping.\"\n\nHolland claims that, while biological theory of social behavior is not deterministic in respect of genetic relatedness vis-a-vis the formation of social bonds and expression of social behaviors, evidence does point to compatibility between a non-reductive interpretation of the theory and how such bonds and behaviors operate in social mammals, primates and in humans. In the final part of the book, Holland explores the extent to which this perspective is also compatible with sociocultural anthropology's ethnographic accounts of human kinship and social behavior, both occasional accounts from the past, as well as more contemporary accounts that have explicitly eschewed the earlier 'blood ties' assumption. Holland finds that;\n\nMany contemporary accounts focus on social bonds formed in childhood and the importance of the performance of acts of care, including food provision, in mediating these bonds. In all cases it is this performance of care which is considered the overriding factor in\nmediating social bonds, notwithstanding 'blood ties'. In short, there is strong compatibility between the perspectives on social bonding that emerge from a proper account of biological theory and those documented by ethnographers.\n\nHolland's concluding chapter gives a summary of his fundamental position;\n\nA crucial implication of this argument taken as a whole is that \"the expression of the kinds of social behaviours treated by inclusive fitness theory does not require genetic relatedness.\" Sociobiology and evolutionary psychology's claims that biological science predicts that organisms \"will\" direct social behaviour towards relatives are thus both \"theoretically\" and \"empirically\" erroneous. Such claims and their supporting arguments also give a highly misleading and reductive account of basic biological theory. Properly interpreted, \"cultural anthropology approaches (and ethnographic data) and biological approaches are perfectly compatible\" regarding processes of social bonding in humans. Most of all, this requires a focus on the circumstances and processes which lead to social bonding.\n\nThe book notes that, as an outcome of the analysis, Schneider's sociocultural perspective on human kinship is vindicated;\n\nDo the biological facts have some priority or are they but one of the conditions, like ecology, economy, demography, etc., to which kinship systems must adapt? Take note: if the latter is the case, then kinship must be as much rooted in these other conditions as in the biological facts.\n\nThe author supplies several examples of the insight that Schneider's broad approach can provide. The book closes with an example of a clash of cultural perspectives on kinship and family norms, and makes the suggestion that;\n\nConstructing from narrow cultural particulars (Euro-American or otherwise) an essentialised model of 'human nature' does not constitute science; it is closer to cultural colonialism. In any analysis intended to shed light on proposed universals of the human condition, reflexivity is essential, and cultural and biological approaches both surely necessary.\n\nKinship theorist and member of the national academy of science, Robin Fox wrote of the work:\n\nAn excellent and constructive discussion of matters in kinship and its cultural and biological components, handsomely reconciling what have been held to be incompatible positions.Max Holland gets to the heart of the matter concerning the contentious relationship between kinship categories, genetic relatedness and the prediction of behavior. If he had been in the debate in the 1980s then a lot of subsequent confusion could have been avoided\"\n\nIrwin Bernstein, distinguished research professor in the university of Georgia's \"Behavioral and Brain Sciences Program\" made the following comment on Holland's book:\n\nMax Holland has demonstrated extraordinarily thorough scholarship in his exhaustive review of the often contentious discussions of kinship. He has produced a balanced synthesis melding the two approaches exemplified in the biological and sociocultural behavioral positions. His work in reconciling opposing views clearly demonstrates the value of interdisciplinary approaches. This should be the definitive word on the subject.\n\nPhilip Kitcher, John Dewey Professor of Philosophy, and James R. Barker Professorship of Contemporary Civilization at Columbia University, past president of the American Philosophical Association and inaugural winner of the Prometheus Prize, stated of the book:\n\nMax Holland has provided a wide-ranging and deeply-probing analysis of the influence of genetic relatedness and social context on human kinship. He argues that while genetic relatedness may play a role in the evolution of social behavior, it does not determine the forms of such behavior. His discussion is exemplary for its thoroughness, and should inspire more nuanced ventures in applying Darwinian approaches to sociocultural anthropology.\n\nKirk Endicott, professor emeritus of anthropology at the university of Dartmouth, wrote that Holland's book was:\n\nA brilliant discussion of the relationship between kinship and social bonding as understood in evolutionary biology and in sociocultural anthropology. Among other contributions, it debunks the common misconception that biological evolution involves individual organisms actively pursuing the goal of increasing the numbers of their genes in successive generations, the measure of their so-called ‘individual inclusive fitness’. Holland demonstrates that an alternative non-deterministic interpretation of evolutionary biology is more compatible with actual human social behavior and with the frameworks that sociocultural anthropology employs.\n\nJanet Carsten, kinship theorist and professor of anthropology at the university of Edinburgh stated that:\n\nThis book is a scholarly attempt to get beyond the often sterile oppositions between evolutionary and culturalist approaches to kinship. In bringing together two sides of the debate, it constitutes a valuable contribution to kinship studies.\n\nIn a review for the journal Critique of Anthropology, Nicholas Malone concluded that:\n\nLucid and effective... Holland has produced a significant work of scholarship that will be of interest to a wide swath of the anthropological community.\n\nCommenting on the book for the journal Social Analysis, Anni Kajanus found that:\n\nHolland has done an excellent and thorough job in reviewing the disciplinary and interdisciplinary histories of approaches to kinship and social bonds in anthropology, biology, and psychology. Most importantly, he clarifies the different levels of analysis when looking at human behavior in real time and in the evolutionary time frame. This makes the book essential reading for anyone who acknowledges that human relatedness and social bonds are shaped by the\nevolved dispositions of our species, their development through the life-course of an individual, and our specific cultural-historical environments... Holland’s book goes a long way toward clarifying and therefore advancing these theoretical debates\n\nAn in-depth review of the book by primatologist Augusto Vitale, in the journal \"Folia Primatologica\", found that:\n\nStuart Semple, evolutionary anthropologist, reviewing the book in the journal \"Acta Ethologica\" stated that:\n\nAs someone who teaches behavioural ecology to biologists, and primate biology to social and biological anthropologists, I will be strongly recommending this book to all of my advanced undergraduates, masters and PhD students, as well as to my colleagues. Not only does it help to resolve debates that have run for many years, but it is also an outstanding example of what can be achieved by immersing oneself in literature from different fields, while retaining an intellectual openness and exercising incisive analysis. Many of us talk enthusiastically about inter- and multi-disciplinarity, but often this is not much more than lip service. This book is a shining example of what can be achieved when excellent scholars engage fully across disciplinary boundaries. There should be more texts like this.\n\nIn addition to praise for the book's significance, the Folia Primatologica review noted that the book is at times too dense and requires close reading;\nThe argument here and there becomes too detailed and tortuous, but it is absolutely captivating... [Colleagues] who are less used to extremely detailed theoretical reasoning, will find it difficult at the beginning...\n\n"}
{"id": "37738", "url": "https://en.wikipedia.org/wiki?curid=37738", "title": "Soil", "text": "Soil\n\nSoil is a mixture of organic matter, minerals, gases, liquids, and organisms that together support life. Earth's body of soil is the pedosphere, which has four important functions: it is a medium for plant growth; it is a means of water storage, supply and purification; it is a modifier of Earth's atmosphere; it is a habitat for organisms; all of which, in turn, modify the soil.\n\nThe pedosphere interfaces with the lithosphere, the hydrosphere, the atmosphere, and the biosphere. The term \"pedolith\", used commonly to refer to the soil, translates to \"ground stone\". Soil consists of a solid phase of minerals and organic matter (the soil matrix), as well as a porous phase that holds gases (the soil atmosphere) and water (the soil solution). Accordingly, soils are often treated as a three-state system of solids, liquids, and gases.\n\nSoil is a product of the influence of climate, relief (elevation, orientation, and slope of terrain), organisms, and its parent materials (original minerals) interacting over time. It continually undergoes development by way of numerous physical, chemical and biological processes, which include weathering with associated erosion. Given its complexity and strong internal connectedness, it is considered an ecosystem by soil ecologists.\n\nMost soils have a dry bulk density (density of soil taking into account voids when dry) between 1.1 and 1.6 g/cm, while the soil particle density is much higher, in the range of 2.6 to 2.7 g/cm. Little of the soil of planet Earth is older than the Pleistocene and none is older than the Cenozoic, although fossilized soils are preserved from as far back as the Archean.\n\nSoil science has two basic branches of study: edaphology and pedology. Edaphology is concerned with the influence of soils on living things. Pedology is focused on the formation, description (morphology), and classification of soils in their natural environment. In engineering terms, soil is included in the broader concept of regolith, which also includes other loose material that lies above the bedrock, as can be found on the Moon and other celestial objects, as well. Soil is also commonly referred to as earth or dirt; some scientific definitions distinguish \"dirt\" from \"soil\" by restricting the former term specifically to the displaced soil.\n\nSoil is a major component of the Earth's ecosystem. The world's ecosystems are impacted in far-reaching ways by the processes carried out in the soil, from ozone depletion and global warming to rainforest destruction and water pollution. With respect to Earth's carbon cycle, soil is an important carbon reservoir, and it is potentially one of the most reactive to human disturbance and climate change. As the planet warms, it has been predicted that soils will add carbon dioxide to the atmosphere due to increased biological activity at higher temperatures, a positive feedback (amplification). This prediction has, however, been questioned on consideration of more recent knowledge on soil carbon turnover.\n\nSoil acts as an engineering medium, a habitat for soil organisms, a recycling system for nutrients and organic wastes, a regulator of water quality, a modifier of atmospheric composition, and a medium for plant growth, making it a critically important provider of ecosystem services. Since soil has a tremendous range of available niches and habitats, it contains most of the Earth's genetic diversity. A gram of soil can contain billions of organisms, belonging to thousands of species, mostly microbial and in the main still unexplored. Soil has a mean prokaryotic density of roughly 10 organisms per gram, whereas the ocean has no more than 10 procaryotic organisms per milliliter (gram) of seawater. Organic carbon held in soil is eventually returned to the atmosphere through the process of respiration carried out by heterotrophic organisms, but a substantial part is retained in the soil in the form of soil organic matter; tillage usually increases the rate of soil respiration, leading to the depletion of soil organic matter. Since plant roots need oxygen, ventilation is an important characteristic of soil. This ventilation can be accomplished via networks of interconnected soil pores, which also absorb and hold rainwater making it readily available for uptake by plants. Since plants require a nearly continuous supply of water, but most regions receive sporadic rainfall, the water-holding capacity of soils is vital for plant survival.\n\nSoils can effectively remove impurities, kill disease agents, and degrade contaminants, this latter property being called natural attenuation. Typically, soils maintain a net absorption of oxygen and methane and undergo a net release of carbon dioxide and nitrous oxide. Soils offer plants physical support, air, water, temperature moderation, nutrients, and protection from toxins. Soils provide readily available nutrients to plants and animals by converting dead organic matter into various nutrient forms.\n\nA typical soil is about 50% solids (45% mineral and 5% organic matter), and 50% voids (or pores) of which half is occupied by water and half by gas. The percent soil mineral and organic content can be treated as a constant (in the short term), while the percent soil water and gas content is considered highly variable whereby a rise in one is simultaneously balanced by a reduction in the other. The pore space allows for the infiltration and movement of air and water, both of which are critical for life existing in soil. Compaction, a common problem with soils, reduces this space, preventing air and water from reaching plant roots and soil organisms.\n\nGiven sufficient time, an undifferentiated soil will evolve a soil profile which consists of two or more layers, referred to as soil horizons, that differ in one or more properties such as in their texture, structure, density, porosity, consistency, temperature, color, and reactivity. The horizons differ greatly in thickness and generally lack sharp boundaries; their development is dependent on the type of parent material, the processes that modify those parent materials, and the soil-forming factors that influence those processes. The biological influences on soil properties are strongest near the surface, while the geochemical influences on soil properties increase with depth. Mature soil profiles typically include three basic master horizons: A, B, and C. The solum normally includes the A and B horizons. The living component of the soil is largely confined to the solum, and is generally more prominent in the A horizon.\n\nThe soil texture is determined by the relative proportions of the individual particles of sand, silt, and clay that make up the soil. The interaction of the individual mineral particles with organic matter, water, gases via biotic and abiotic processes causes those particles to flocculate (stick together) to form aggregates or peds. Where these aggregates can be identified, a soil can be said to be developed, and can be described further in terms of color, porosity, consistency, reaction (acidity), etc.\n\nWater is a critical agent in soil development due to its involvement in the dissolution, precipitation, erosion, transport, and deposition of the materials of which a soil is composed. The mixture of water and dissolved or suspended materials that occupy the soil pore space is called the soil solution. Since soil water is never pure water, but contains hundreds of dissolved organic and mineral substances, it may be more accurately called the soil solution. Water is central to the dissolution, precipitation and leaching of minerals from the soil profile. Finally, water affects the type of vegetation that grows in a soil, which in turn affects the development of the soil, a complex feedback which is exemplified in the dynamics of banded vegetation patterns in semi-arid regions.\n\nSoils supply plants with nutrients, most of which are held in place by particles of clay and organic matter (colloids) The nutrients may be adsorbed on clay mineral surfaces, bound within clay minerals (absorbed), or bound within organic compounds as part of the living organisms or dead soil organic matter. These bound nutrients interact with soil water to buffer the soil solution composition (attenuate changes in the soil solution) as soils wet up or dry out, as plants take up nutrients, as salts are leached, or as acids or alkalis are added.\n\nPlant nutrient availability is affected by soil pH, which is a measure of the hydrogen ion activity in the soil solution. Soil pH is a function of many soil forming factors, and is generally lower (more acid) where weathering is more advanced.\n\nMost plant nutrients, with the exception of nitrogen, originate from the minerals that make up the soil parent material. Some nitrogen originates from rain as dilute nitric acid and ammonia, but most of the nitrogen is available in soils as a result of nitrogen fixation by bacteria. Once in the soil-plant system, most nutrients are recycled through living organisms, plant and microbial residues (soil organic matter), mineral-bound forms, and the soil solution. Both living microorganisms and soil organic matter are of critical importance to this recycling, and thereby to soil formation and soil fertility. Microbial activity in soils may release nutrients from minerals or organic matter for use by plants and other microorganisms, sequester (incorporate) them into living cells, or cause their loss from the soil by volatilisation (loss to the atmosphere as gases) or leaching.\n\nThe history of the study of soil is intimately tied to humans' urgent need to provide food for themselves and forage for our animals. Throughout history, civilizations have prospered or declined as a function of the availability and productivity of their soils.\n\nThe Greek historian Xenophon (450–355 BCE) is credited with being the first to expound upon the merits of green-manuring crops: \"But then whatever weeds are upon the ground, being turned into earth, enrich the soil as much as dung.\"\n\nColumella's \"Husbandry,\" circa 60 CE, advocated the use of lime and that clover and alfalfa (green manure) should be turned under, and was used by 15 generations (450 years) under the Roman Empire until its collapse. From the fall of Rome to the French Revolution, knowledge of soil and agriculture was passed on from parent to child and as a result, crop yields were low. During the European Dark Ages, Yahya Ibn al-'Awwam's handbook, with its emphasis on irrigation, guided the people of North Africa, Spain and the Middle East; a translation of this work was finally carried to the southwest of the United States when under Spanish influence. Olivier de Serres, considered as the father of French agronomy, was the first to suggest the abandonment of fallowing and its replacement by hay meadows within crop rotations, and he highlighted the importance of soil (the French terroir) in the management of vineyards. His famous book \"Le Théâtre d’Agriculture et mesnage des champs\" contributed to the rise of modern, sustainable agriculture and to the collapse of old agricultural practices such as the lifting of forest litter for the amendment of crops (the French \"soutrage\") and assarting, which ruined the soils of western Europe during Middle Ages and even later on according to regions.\n\nExperiments into what made plants grow first led to the idea that the ash left behind when plant matter was burned was the essential element but overlooked the role of nitrogen, which is not left on the ground after combustion, a belief which prevailed until the 19th century. In about 1635, the Flemish chemist Jan Baptist van Helmont thought he had proved water to be the essential element from his famous five years' experiment with a willow tree grown with only the addition of rainwater. His conclusion came from the fact that the increase in the plant's weight had apparently been produced only by the addition of water, with no reduction in the soil's weight. John Woodward (d. 1728) experimented with various types of water ranging from clean to muddy and found muddy water the best, and so he concluded that earthy matter was the essential element. Others concluded it was humus in the soil that passed some essence to the growing plant. Still others held that the vital growth principal was something passed from dead plants or animals to the new plants. At the start of the 18th century, Jethro Tull demonstrated that it was beneficial to cultivate (stir) the soil, but his opinion that the stirring made the fine parts of soil available for plant absorption was erroneous.\n\nAs chemistry developed, it was applied to the investigation of soil fertility. The French chemist Antoine Lavoisier showed in about 1778 that plants and animals must [combust] oxygen internally to live and was able to deduce that most of the 165-pound weight of van Helmont's willow tree derived from air. It was the French agriculturalist Jean-Baptiste Boussingault who by means of experimentation obtained evidence showing that the main sources of carbon, hydrogen and oxygen for plants were air and water, while nitrogen was taken from soil. Justus von Liebig in his book \"Organic chemistry in its applications to agriculture and physiology\" (published 1840), asserted that the chemicals in plants must have come from the soil and air and that to maintain soil fertility, the used minerals must be replaced. Liebig nevertheless believed the nitrogen was supplied from the air. The enrichment of soil with guano by the Incas was rediscovered in 1802, by Alexander von Humboldt. This led to its mining and that of Chilean nitrate and to its application to soil in the United States and Europe after 1840.\n\nThe work of Liebig was a revolution for agriculture, and so other investigators started experimentation based on it. In England John Bennet Lawes and Joseph Henry Gilbert worked in the Rothamsted Experimental Station, founded by the former, and (re)discovered that plants took nitrogen from the soil, and that salts needed to be in an available state to be absorbed by plants. Their investigations also produced the \"superphosphate\", consisting in the acid treatment of phosphate rock. This led to the invention and use of salts of potassium (K) and nitrogen (N) as fertilizers. Ammonia generated by the production of coke was recovered and used as fertiliser. Finally, the chemical basis of nutrients delivered to the soil in manure was understood and in the mid-19th century chemical fertilisers were applied. However, the dynamic interaction of soil and its life forms still awaited discovery.\n\nIn 1856 J. Thomas Way discovered that ammonia contained in fertilisers was transformed into nitrates, and twenty years later Robert Warington proved that this transformation was done by living organisms. In 1890 Sergei Winogradsky announced he had found the bacteria responsible for this transformation.\n\nIt was known that certain legumes could take up nitrogen from the air and fix it to the soil but it took the development of bacteriology towards the end of the 19th century to lead to an understanding of the role played in nitrogen fixation by bacteria. The symbiosis of bacteria and leguminous roots, and the fixation of nitrogen by the bacteria, were simultaneously discovered by the German agronomist Hermann Hellriegel and the Dutch microbiologist Martinus Beijerinck.\n\nCrop rotation, mechanisation, chemical and natural fertilisers led to a doubling of wheat yields in western Europe between 1800 and 1900.\n\nThe scientists who studied the soil in connection with agricultural practices had considered it mainly as a static substrate. However, soil is the result of evolution from more ancient geological materials, under the action of biotic and abiotic (not associated with life) processes. After studies of the improvement of the soil commenced, others began to study soil genesis and as a result also soil types and classifications.\n\nIn 1860, in Mississippi, Eugene W. Hilgard studied the relationship among rock material, climate, and vegetation, and the type of soils that were developed. He realised that the soils were dynamic, and considered soil types classification. Unfortunately his work was not continued. At the same time Vasily Dokuchaev (about 1870) was leading a team of soil scientists in Russia who conducted an extensive survey of soils, finding that similar basic rocks, climate and vegetation types lead to similar soil layering and types, and established the concepts for soil classifications. Due to language barriers, the work of this team was not communicated to western Europe until 1914 through a publication in German by Konstantin Dmitrievich Glinka, a member of the Russian team.\n\nCurtis F. Marbut was influenced by the work of the Russian team, translated Glinka's publication into English, and as he was placed in charge of the U. S. National Cooperative Soil Survey, applied it to a national soil classification system.\n\nSoil formation, or pedogenesis, is the combined effect of physical, chemical, biological and anthropogenic processes working on soil parent material. Soil is said to be formed when organic matter has accumulated and colloids are washed downward, leaving deposits of clay, humus, iron oxide, carbonate, and gypsum, producing a distinct layer called the B horizon. This is a somewhat arbitrary definition as mixtures of sand, silt, clay and humus will support biological and agricultural activity before that time. These constituents are moved from one level to another by water and animal activity. As a result, layers (horizons) form in the soil profile. The alteration and movement of materials within a soil causes the formation of distinctive soil horizons. However, more recent definitions of soil embrace soils without any organic matter, such as those regoliths that formed on Mars and analogous conditions in planet Earth deserts.\n\nAn example of the development of a soil would begin with the weathering of lava flow bedrock, which would produce the purely mineral-based parent material from which the soil texture forms. Soil development would proceed most rapidly from bare rock of recent flows in a warm climate, under heavy and frequent rainfall. Under such conditions, plants (in a first stage nitrogen-fixing lichens and cyanobacteria then epilithic higher plants) become established very quickly on basaltic lava, even though there is very little organic material. The plants are supported by the porous rock as it is filled with nutrient-bearing water that carries minerals dissolved from the rocks. Crevasses and pockets, local topography of the rocks, would hold fine materials and harbour plant roots. The developing plant roots are associated with mineral-weathering mycorrhizal fungi that assist in breaking up the porous lava, and by these means organic matter and a finer mineral soil accumulate with time. Such initial stages of soil development have been described on volcanoes, inselbergs, and glacial moraines.\n\nHow soil formation proceeds is influenced by at least five classic factors that are intertwined in the evolution of a soil. They are: parent material, climate, topography (relief), organisms, and time. When reordered to climate, relief, organisms, parent material, and time, they form the acronym CROPT.\n\nThe mineral material from which a soil forms is called parent material. Rock, whether its origin is igneous, sedimentary, or metamorphic, is the source of all soil mineral materials and the origin of all plant nutrients with the exceptions of nitrogen, hydrogen and carbon. As the parent material is chemically and physically weathered, transported, deposited and precipitated, it is transformed into a soil.\n\nTypical soil parent mineral materials are:\nParent materials are classified according to how they came to be deposited. Residual materials are mineral materials that have weathered in place from primary bedrock. Transported materials are those that have been deposited by water, wind, ice or gravity. Cumulose material is organic matter that has grown and accumulates in place.\n\nResidual soils are soils that develop from their underlying parent rocks and have the same general chemistry as those rocks. The soils found on mesas, plateaux, and plains are residual soils. In the United States as little as three percent of the soils are residual.\n\nMost soils derive from transported materials that have been moved many miles by wind, water, ice and gravity.\n\nCumulose parent material is not moved but originates from deposited organic material. This includes peat and muck soils and results from preservation of plant residues by the low oxygen content of a high water table. While peat may form sterile soils, muck soils may be very fertile.\n\nThe weathering of parent material takes the form of physical weathering (disintegration), chemical weathering (decomposition) and chemical transformation. Generally, minerals that are formed under high temperatures and pressures at great depths within the Earth's mantle are less resistant to weathering, while minerals formed at low temperature and pressure environment of the surface are more resistant to weathering. Weathering is usually confined to the top few meters of geologic material, because physical, chemical, and biological stresses and fluctuations generally decrease with depth. Physical disintegration begins as rocks that have solidified deep in the Earth are exposed to lower pressure near the surface and swell and become mechanically unstable. Chemical decomposition is a function of mineral solubility, the rate of which doubles with each 10 °C rise in temperature, but is strongly dependent on water to effect chemical changes. Rocks that will decompose in a few years in tropical climates will remain unaltered for millennia in deserts. Structural changes are the result of hydration, oxidation, and reduction. Chemical weathering mainly results from the excretion of organic acids and chelating compounds by bacteria and fungi, thought to increase under present-day greenhouse effect.\n\n\nOf the above, hydrolysis and carbonation are the most effective, in particular in regions of high rainfall, temperature and physical erosion. Chemical weathering becomes more effective as the surface area of the rock increases, thus is favoured by physical disintegration. This stems in latitudinal and altitudinal climate gradients in regolith formation.\n\nSaprolite is a particular example of a residual soil formed from the transformation of granite, metamorphic and other types of bedrock into clay minerals. Often called [weathered granite], saprolite is the result of weathering processes that include: hydrolysis, chelation from organic compounds, hydration (the solution of minerals in water with resulting cation and anion pairs) and physical processes that include freezing and thawing. The mineralogical and chemical composition of the primary bedrock material, its physical features, including grain size and degree of consolidation, and the rate and type of weathering transforms the parent material into a different mineral. The texture, pH and mineral constituents of saprolite are inherited from its parent material. This process is also called \"arenization\", resulting in the formation of sandy soils (granitic arenas), thanks to the much higher resistance of quartz compared to other mineral components of granite (micas, amphiboles, feldspars).\n\nThe principal climatic variables influencing soil formation are effective precipitation (i.e., precipitation minus evapotranspiration) and temperature, both of which affect the rates of chemical, physical, and biological processes. Temperature and moisture both influence the organic matter content of soil through their effects on the balance between primary production and decomposition: the colder or drier the climate the lesser atmospheric carbon is fixed as organic matter while the lesser organic matter is decomposed.\n\nClimate is the dominant factor in soil formation, and soils show the distinctive characteristics of the climate zones in which they form, with a feedback to climate through transfer of carbon stocked in soil horizons back to the atmosphere. If warm temperatures and abundant water are present in the profile at the same time, the processes of weathering, leaching, and plant growth will be maximized. According to the climatic determination of biomes, humid climates favor the growth of trees. In contrast, grasses are the dominant native vegetation in subhumid and semiarid regions, while shrubs and brush of various kinds dominate in arid areas.\n\nWater is essential for all the major chemical weathering reactions. To be effective in soil formation, water must penetrate the regolith. The seasonal rainfall distribution, evaporative losses, site topography, and soil permeability interact to determine how effectively precipitation can influence soil formation. The greater the depth of water penetration, the greater the depth of weathering of the soil and its development. Surplus water percolating through the soil profile transports soluble and suspended materials from the upper layers (eluviation) to the lower layers (illuviation), including clay particles and dissolved organic matter. It may also carry away soluble materials in the surface drainage waters. Thus, percolating water stimulates weathering reactions and helps differentiate soil horizons. Likewise, a deficiency of water is a major factor in determining the characteristics of soils of dry regions. Soluble salts are not leached from these soils, and in some cases they build up to levels that curtail plant and microbial growth. Soil profiles in arid and semi-arid regions are also apt to accumulate carbonates and certain types of expansive clays (calcrete or caliche horizons). In tropical soils, when the soil has been deprived of vegetation (e.g. by deforestation) and thereby is submitted to intense evaporation, the upward capillary movement of water, which has dissolved iron and aluminum salts, is responsible for the formation of a superficial hard pan of laterite or bauxite, respectively, which is improper for cutivation, a known case of irreversible soil degradation (lateritization, bauxitization).\n\nThe direct influences of climate include:\n\nClimate directly affects the rate of weathering and leaching. Wind moves sand and smaller particles (dust), especially in arid regions where there is little plant cover, depositing it close or far from the entrainment source. The type and amount of precipitation influence soil formation by affecting the movement of ions and particles through the soil, and aid in the development of different soil profiles. Soil profiles are more distinct in wet and cool climates, where organic materials may accumulate, than in wet and warm climates, where organic materials are rapidly consumed. The effectiveness of water in weathering parent rock material depends on seasonal and daily temperature fluctuations, which favour tensile stresses in rock minerals, and thus their mechanical disaggregation, a process called \"thermal fatigue\". By the same process freeze-thaw cycles are an effective mechanism which breaks up rocks and other consolidated materials.\n\nClimate also indirectly influences soil formation through the effects of vegetation cover and biological activity, which modify the rates of chemical reactions in the soil.\n\nThe topography, or relief, is characterized by the inclination (slope), elevation, and orientation of the terrain. Topography determines the rate of precipitation or runoff and rate of formation or erosion of the surface soil profile. The topographical setting may either hasten or retard the work of climatic forces.\n\nSteep slopes encourage rapid soil loss by erosion and allow less rainfall to enter the soil before running off and hence, little mineral deposition in lower profiles. In semiarid regions, the lower effective rainfall on steeper slopes also results in less complete vegetative cover, so there is less plant contribution to soil formation. For all of these reasons, steep slopes prevent the formation of soil from getting very far ahead of soil destruction. Therefore, soils on steep terrain tend to have rather shallow, poorly developed profiles in comparison to soils on nearby, more level sites.\n\nIn swales and depressions where runoff water tends to concentrate, the regolith is usually more deeply weathered and soil profile development is more advanced. However, in the lowest landscape positions, water may saturate the regolith to such a degree that drainage and aeration are restricted. Here, the weathering of some minerals and the decomposition of organic matter are retarded, while the loss of iron and manganese is accelerated. In such low-lying topography, special profile features characteristic of wetland soils may develop. Depressions allow the accumulation of water, minerals and organic matter and in the extreme, the resulting soils will be saline marshes or peat bogs. Intermediate topography affords the best conditions for the formation of an agriculturally productive soil.\n\nSoil is the most abundant ecosystem on Earth, but the vast majority of organisms in soil are microbes, a great many of which have not been described. There may be a population limit of around one billion cells per gram of soil, but estimates of the number of species vary widely from 50,000 per gram to over a million per gram of soil. The total number of organisms and species can vary widely according to soil type, location, and depth.\n\nPlants, animals, fungi, bacteria and humans affect soil formation (see soil biomantle and stonelayer). Soil animals, including soil macrofauna and soil mesofauna, mix soils as they form burrows and pores, allowing moisture and gases to move about, a process called bioturbation. In the same way, plant roots penetrate soil horizons and open channels upon decomposition. Plants with deep taproots can penetrate many metres through the different soil layers to bring up nutrients from deeper in the profile. Plants have fine roots that excrete organic compounds (sugars, organic acids, mucigel), slough off cells (in particular at their tip) and are easily decomposed, adding organic matter to soil, a process called \"rhizodeposition\". Micro-organisms, including fungi and bacteria, effect chemical exchanges between roots and soil and act as a reserve of nutrients in a soil biological \"hotspot\" called rhizosphere. The growth of roots through the soil stimulates microbial populations, stimulating in turn the activity of their predators (notably amoeba), thereby increasing the mineralization rate, and in last turn root growth, a positive feedback called the soil microbial loop. Out of root influence, in the bulk soil, most bacteria are in a quiescent stage, forming microaggregates, i.e. mucilaginous colonies to which clay particles are glued, offering them a protection against desiccation and predation by soil microfauna (bacteriophagous protozoa and nematodes). Microaggregates (20-250 µm) are ingested by soil mesofauna and macrofauna, and bacterial bodies are partly or totally digested in their guts.\n\nHumans impact soil formation by removing vegetation cover with erosion, waterlogging, lateritization or podzolization (according to climate and topography) as the result. Their tillage also mixes the different soil layers, restarting the soil formation process as less weathered material is mixed with the more developed upper layers, resulting in net increased rate of mineral weathering.\n\nEarthworms, ants, termites, moles, gophers, as well as some millipedes and tenebrionid beetles mix the soil as they burrow, significantly affecting soil formation. Earthworms ingest soil particles and organic residues, enhancing the availability of plant nutrients in the material that passes through their bodies. They aerate and stir the soil and create stable soil aggregates, after having disrupted links between soil particles during the intestinal transit of ingested soil, thereby assuring ready infiltration of water. In addition, as ants and termites build mounds, they transport soil materials from one horizon to another. Other important functions are fulfilled by earthworms in the soil ecosystem, in particular their intense mucus production, both within the intestine and as a lining in their galleries, exert a priming effect on soil microflora, giving them the status of ecosystem engineers, which they share with ants and termites.\n\nIn general, the mixing of the soil by the activities of animals, sometimes called pedoturbation, tends to undo or counteract the tendency of other soil-forming processes that create distinct horizons. Termites and ants may also retard soil profile development by denuding large areas of soil around their nests, leading to increased loss of soil by erosion. Large animals such as gophers, moles, and prairie dogs bore into the lower soil horizons, bringing materials to the surface. Their tunnels are often open to the surface, encouraging the movement of water and air into the subsurface layers. In localized areas, they enhance mixing of the lower and upper horizons by creating, and later refilling, underground tunnels. Old animal burrows in the lower horizons often become filled with soil material from the overlying A horizon, creating profile features known as crotovinas.\n\nVegetation impacts soils in numerous ways. It can prevent erosion caused by excessive rain that might result from surface runoff. Plants shade soils, keeping them cooler and slow evaporation of soil moisture, or conversely, by way of transpiration, plants can cause soils to lose moisture, resulting in complex and highly variable relationships between leaf area index (measuring light interception) and moisture loss: more generally plants prevent soil from desiccation during driest months while they dry it during moister months, thereby acting as a buffer against strong moisture variation. Plants can form new chemicals that can break down minerals, both directly and indirectly through mycorrhizal fungi and rhizosphere bacteria, and improve the soil structure. The type and amount of vegetation depends on climate, topography, soil characteristics and biological factors, mediated or not by human activities. Soil factors such as density, depth, chemistry, pH, temperature and moisture greatly affect the type of plants that can grow in a given location. Dead plants and fallen leaves and stems begin their decomposition on the surface. There, organisms feed on them and mix the organic material with the upper soil layers; these added organic compounds become part of the soil formation process.\n\nHuman activities widely influence soil formation. For example, it is believed that Native Americans regularly set fires to maintain several large areas of prairie grasslands in Indiana and Michigan, although climate and mammalian grazers (e.g. bisons) are also advocated to explain the maintenance of the Great Plains of North America. In more recent times, human destruction of natural vegetation and subsequent tillage of the soil for crop production has abruptly modified soil formation. Likewise, irrigating soil in an arid region drastically influences soil-forming factors, as does adding fertilizer and lime to soils of low fertility.\n\nTime is a factor in the interactions of all the above. While a mixture of sand, silt and clay constitute the texture of a soil and the aggregation of those components produces peds, the development of a distinct B horizon marks the development of a soil or pedogenesis. With time, soils will evolve features that depend on the interplay of the prior listed soil-forming factors. It takes decades to several thousand years for a soil to develop a profile, although the notion of soil development has been criticized, soil being in a constant state-of-change under the influence of fluctuating soil-forming factors. That time period depends strongly on climate, parent material, relief, and biotic activity. For example, recently deposited material from a flood exhibits no soil development as there has not been enough time for the material to form a structure that further defines soil. The original soil surface is buried, and the formation process must begin anew for this deposit. Over time the soil will develop a profile that depends on the intensities of biota and climate. While a soil can achieve relative stability of its properties for extended periods, the soil life cycle ultimately ends in soil conditions that leave it vulnerable to erosion. Despite the inevitability of soil retrogression and degradation, most soil cycles are long.\n\nSoil-forming factors continue to affect soils during their existence, even on \"stable\" landscapes that are long-enduring, some for millions of years. Materials are deposited on top or are blown or washed from the surface. With additions, removals and alterations, soils are always subject to new conditions. Whether these are slow or rapid changes depends on climate, topography and biological activity.\n\nThe physical properties of soils, in order of decreasing importance for ecosystem services such as crop production, are texture, structure, bulk density, porosity, consistency, temperature, colour and resistivity. Soil texture is determined by the relative proportion of the three kinds of soil mineral particles, called soil separates: sand, silt, and clay. At the next larger scale, soil structures called peds or more commonly \"soil aggregates\" are created from the soil separates when iron oxides, carbonates, clay, silica and humus, coat particles and cause them to adhere into larger, relatively stable secondary structures. Soil bulk density, when determined at standardized moisture conditions, is an estimate of soil compaction. Soil porosity consists of the void part of the soil volume and is occupied by gases or water. Soil consistency is the ability of soil materials to stick together. Soil temperature and colour are self-defining. Resistivity refers to the resistance to conduction of electric currents and affects the rate of corrosion of metal and concrete structures which are buried in soil. These properties vary through the depth of a soil profile, i.e. through soil horizons. Most of these properties determine the aeration of the soil and the ability of water to infiltrate and to be held within the soil.\n\nThe mineral components of soil are sand, silt and clay, and their relative proportions determine a soil's texture. Properties that are influenced by soil texture include porosity, permeability, infiltration, shrink-swell rate, water-holding capacity, and susceptibility to erosion. In the illustrated USDA textural classification triangle, the only soil in which neither sand, silt nor clay predominates is called loam. While even pure sand, silt or clay may be considered a soil, from the perspective of conventional agriculture a loam soil with a small amount of organic material is considered \"ideal\", inasmuch as fertilizers or manure are currently used to mitigate nutrient losses due to crop yields in the long term. The mineral constituents of a loam soil might be 40% sand, 40% silt and the balance 20% clay by weight. Soil texture affects soil behaviour, in particular, its retention capacity for nutrients (e.g., cation exchange capacity) and water.\n\nSand and silt are the products of physical and chemical weathering of the parent rock; clay, on the other hand, is most often the product of the precipitation of the dissolved parent rock as a secondary mineral, except when derived from the weathering of mica. It is the surface area to volume ratio (specific surface area) of soil particles and the unbalanced ionic electric charges within those that determine their role in the fertility of soil, as measured by its cation exchange capacity. Sand is least active, having the least specific surface area, followed by silt; clay is the most active. Sand's greatest benefit to soil is that it resists compaction and increases soil porosity, although this property stands only for pure sand, not for sand mixed with smaller minerals which fill the voids among sand grains. Silt is mineralogically like sand but with its higher specific surface area it is more chemically and physically active than sand. But it is the clay content of soil, with its very high specific surface area and generally large number of negative charges, that gives a soil its high retention capacity for water and nutrients. Clay soils also resist wind and water erosion better than silty and sandy soils, as the particles bond tightly to each other,\nand that with a strong mitigation effect of organic matter.\n\nSand is the most stable of the mineral components of soil; it consists of rock fragments, primarily quartz particles, ranging in size from in diameter. Silt ranges in size from . Clay cannot be resolved by optical microscopes as its particles are or less in diameter and a thickness of only 10 angstroms (10 m). In medium-textured soils, clay is often washed downward through the soil profile (a process called eluviation) and accumulates in the subsoil (a process called illuviation). There is no clear relationship between the size of soil mineral components and their mineralogical nature: sand and silt particles can be calcareous as well as siliceous, while textural clay () can be made of very fine quartz particles as well as of multi-layered secondary minerals. Soil mineral components belonging to a given textural class may thus share properties linked to their specific surface area (e.g. moisture retention) but not those linked to their chemical composition (e.g. cation exchange capacity).\n\nSoil components larger than are classed as rock and gravel and are removed before determining the percentages of the remaining components and the textural class of the soil, but are included in the name. For example, a sandy loam soil with 20% gravel would be called gravelly sandy loam.\n\nWhen the organic component of a soil is substantial, the soil is called organic soil rather than mineral soil. A soil is called organic if:\n\n\nThe clumping of the soil textural components of sand, silt and clay causes aggregates to form and the further association of those aggregates into larger units creates soil structures called peds (a contraction of the word pedolith). The adhesion of the soil textural components by organic substances, iron oxides, carbonates, clays, and silica, the breakage of those aggregates from expansion-contraction caused by freezing-thawing and wetting-drying cycles, and the build-up of aggregates by soil animals, microbial colonies and root tips shape soil into distinct geometric forms. The peds evolve into units which have various shapes, sizes and degrees of development. A soil clod, however, is not a ped but rather a mass of soil that results from mechanical disturbance of the soil such as cultivation. Soil structure affects aeration, water movement, conduction of heat, plant root growth and resistance to erosion. Water, in turn, has a strong effect on soil structure, directly via the dissolution and precipitation of minerals, the mechanical destruction of aggregates (slaking) and indirectly by promoting plant, animal and microbial growth.\n\nSoil structure often gives clues to its texture, organic matter content, biological activity, past soil evolution, human use, and the chemical and mineralogical conditions under which the soil formed. While texture is defined by the mineral component of a soil and is an innate property of the soil that does not change with agricultural activities, soil structure can be improved or destroyed by the choice and timing of farming practices.\n\nSoil structural classes:\n\n\nAt the largest scale, the forces that shape a soil's structure result from swelling and shrinkage that initially tend to act horizontally, causing vertically oriented prismatic peds. This mechanical process is mainly exemplified in the development of vertisols. Clayey soil, due to its differential drying rate with respect to the surface, will induce horizontal cracks, reducing columns to blocky peds. Roots, rodents, worms, and freezing-thawing cycles further break the peds into smaller peds of a more or less spherical shape.\n\nAt a smaller scale, plant roots extend into voids (macropores) and remove water causing macroporosity to increase and microporosity to decrease, thereby decreasing aggregate size. At the same time, root hairs and fungal hyphae create microscopic tunnels that break up peds.\n\nAt an even smaller scale, soil aggregation continues as bacteria and fungi exude sticky polysaccharides which bind soil into smaller peds. The addition of the raw organic matter that bacteria and fungi feed upon encourages the formation of this desirable soil structure.\n\nAt the lowest scale, the soil chemistry affects the aggregation or dispersal of soil particles. The clay particles contain polyvalent cations which give the faces of clay layers localized negative charges. At the same time, the edges of the clay plates have a slight positive charge, thereby allowing the edges to adhere to the negative charges on the faces of other clay particles or to flocculate (form clumps). On the other hand, when monovalent ions, such as sodium, invade and displace the polyvalent cations, they weaken the positive charges on the edges, while the negative surface charges are relatively strengthened. This leaves negative charge on the clay faces that repel other clay, causing the particles to push apart, and by doing so deflocculate clay suspensions. As a result, the clay disperses and settles into voids between peds, causing those to close. In this way the open structure of the soil is destroyed and the soil is made impenetrable to air and water. Such sodic soil (also called haline soil) tends to form columnar peds near the surface.\n\nSoil particle density is typically 2.60 to 2.75 grams per cm and is usually unchanging for a given soil. Soil particle density is lower for soils with high organic matter content, and is higher for soils with high iron-oxides content. Soil bulk density is equal to the dry mass of the soil divided by the volume of the soil; i.e., it includes air space and organic materials of the soil volume. Thereby soil bulk density is always less than soil particle density and is a good indicator of soil compaction. The soil bulk density of cultivated loam is about 1.1 to 1.4 g/cm (for comparison water is 1.0 g/cm). Contrary to particle density, soil bulk density is highly variable for a given soil, with a strong causal relationship with soil biological activity and management strategies. However, it has been shown that, depending on species and the size of their aggregates (faeces), earthworms may either increase or decrease soil bulk density. A lower bulk density by itself does not indicate suitability for plant growth due to the confounding influence of soil texture and structure. A high bulk density is indicative of either soil compaction or a mixture of soil textural classes in which small particles fill the voids among coarser particles. Hence the positive correlation between the fractal dimension of soil, considered as a porous medium, and its bulk density, that explains the poor hydraulic conductivity of silty clay loam in the absence of a faunal structure.\n\nPore space is that part of the bulk volume of soil that is not occupied by either mineral or organic matter but is open space occupied by either gases or water. In a productive, medium-textured soil the total pore space is typically about 50% of the soil volume. Pore size varies considerably; the smallest pores (cryptopores; <0.1 µm) hold water too tightly for use by plant roots; plant-available water is held in ultramicropores, micropores and mesopores (0.1–75 µm); and macropores (>75 µm) are generally air-filled when the soil is at field capacity.\n\nSoil texture determines total volume of the smallest pores; clay soils have smaller pores, but more total pore space than sands, despite of a much lower permeability. Soil structure has a strong influence on the larger pores that affect soil aeration, water infiltration and drainage. Tillage has the short-term benefit of temporarily increasing the number of pores of largest size, but these can be rapidly degraded by the destruction of soil aggregation.\n\nThe pore size distribution affects the ability of plants and other organisms to access water and oxygen; large, continuous pores allow rapid transmission of air, water and dissolved nutrients through soil, and small pores store water between rainfall or irrigation events. Pore size variation also compartmentalizes the soil pore space such that many microbial and faunal organisms are not in direct competition with one another, which may explain not only the large number of species present, but the fact that functionally redundant organisms (organisms with the same ecological niche) can co-exist within the same soil.\n\nConsistency is the ability of soil to stick to itself or to other objects (cohesion and adhesion, respectively) and its ability to resist deformation and rupture. It is of approximate use in predicting cultivation problems and the engineering of foundations. Consistency is measured at three moisture conditions: air-dry, moist, and wet. In those conditions the consistency quality depends upon the clay content. In the wet state, the two qualities of stickiness and plasticity are assessed. A soil's resistance to fragmentation and crumbling is assessed in the dry state by rubbing the sample. Its resistance to shearing forces is assessed in the moist state by thumb and finger pressure. Additionally, the cemented consistency depends on cementation by substances other than clay, such as calcium carbonate, silica, oxides and salts; moisture content has little effect on its assessment. The measures of consistency border on subjective compared to other measures such as pH, since they employ the apparent feel of the soil in those states.\n\nThe terms used to describe the soil consistency in three moisture states and a last not affected by the amount of moisture are as follows:\n\n\nSoil consistency is useful in estimating the ability of soil to support buildings and roads. More precise measures of soil strength are often made prior to construction.\n\nSoil temperature depends on the ratio of the energy absorbed to that lost. Soil has a temperature range between -20 to 60 °C, with a mean annual temperature from -10 to 26 °C according to biomes. Soil temperature regulates seed germination, breaking of seed dormancy, plant and root growth and the availability of nutrients. Soil temperature has important seasonal, monthly and daily variations, fluctuations in soil temperature being much lower with increasing soil depth. Heavy mulching (a type of soil cover) can slow the warming of soil in summer, and, at the same time, reduce fluctuations in surface temperature.\n\nMost often, agricultural activities must adapt to soil temperatures by:\n\n\nSoil temperatures can be raised by drying soils or the use of clear plastic mulches. Organic mulches slow the warming of the soil.\n\nThere are various factors that affect soil temperature, such as water content, soil color, and relief (slope, orientation, and elevation), and soil cover (shading and insulation), in addition to air temperature. The color of the ground cover and its insulating properties have a strong influence on soil temperature. Whiter soil tends to have a higher albedo than blacker soil cover, which encourages whiter soils to have lower soil temperatures. The specific heat of soil is the energy required to raise the temperature of soil by 1 °C. The specific heat of soil increases as water content increases, since the heat capacity of water is greater than that of dry soil. The specific heat of pure water is ~ 1 calorie per gram, the specific heat of dry soil is ~ 0.2 calories per gram, hence, the specific heat of wet soil is ~ 0.2 to 1 calories per gram (0.8 to 4.2 kJ per kilogram). Also, a tremendous energy (~540 cal/g or 2260 kJ/kg) is required to evaporate water (known as the heat of vaporization). As such, wet soil usually warms more slowly than dry soil – wet surface soil is typically 3 to 6 °C colder than dry surface soil.\n\nSoil heat flux refers to the rate at which heat energy moves through the soil in response to a temperature difference between two points in the soil. The heat flux density is the amount of energy that flows through soil per unit area per unit time and has both magnitude and direction. For the simple case of conduction into or out of the soil in the vertical direction, which is most often applicable the heat flux density is:\n\nIn SI units\n\nHeat flux is in the direction opposite the temperature gradient, hence the minus sign. That is to say, if the temperature of the surface is higher than at depth x the negative sign will result in a positive value for the heat flux q, and which is interpreted as the heat being conducted into the soil.\n\nSoil temperature is important for the survival and early growth of seedlings. Soil temperatures affect the anatomical and morphological character of root systems. All physical, chemical, and biological processes in soil and roots are affected in particular because of the increased viscosities of water and protoplasm at low temperatures. In general, climates that do not preclude survival and growth of white spruce above ground are sufficiently benign to provide soil temperatures able to maintain white spruce root systems. In some northwestern parts of the range, white spruce occurs on permafrost sites and although young unlignified roots of conifers may have little resistance to freezing, the root system of containerized white spruce was not affected by exposure to a temperature of less than 30 °C.\n\nOptimum temperatures for tree root growth range between 10 °C and 25 °C in general and for spruce in particular. In 2-week-old white spruce seedlings that were then grown for 6 weeks in soil at temperatures of 15 °C, 19 °C, 23 °C, 27 °C, and 31 °C; shoot height, shoot dry weight, stem diameter, root penetration, root volume, and root dry weight all reached maxima at 19 °C.\n\nHowever, whereas strong positive relationships between soil temperature (5 °C to 25 °C) and growth have been found in trembling aspen and balsam poplar, white and other spruce species have shown little or no changes in growth with increasing soil temperature. Such insensitivity to soil low temperature may be common among a number of western and boreal conifers.\n\nSoil temperatures are increasing worldwide under the influence of present-day global climate warming, with opposing views about expected effects on carbon capture and storage and feedback loops to climate change Most threats are about permafrost thawing and attended effects on carbon destocking and ecosystem collapse.\n\nSoil colour is often the first impression one has when viewing soil. Striking colours and contrasting patterns are especially noticeable. The Red River of the South carries sediment eroded from extensive reddish soils like Port Silt Loam in Oklahoma. The Yellow River in China carries yellow sediment from eroding loess soils. Mollisols in the Great Plains of North America are darkened and enriched by organic matter. Podsols in boreal forests have highly contrasting layers due to acidity and leaching.\n\nIn general, color is determined by the organic matter content, drainage conditions, and degree of oxidation. Soil color, while easily discerned, has little use in predicting soil characteristics. It is of use in distinguishing boundaries of horizons within a soil profile, determining the origin of a soil's parent material, as an indication of wetness and waterlogged conditions, and as a qualitative means of measuring organic, iron oxide and clay contents of soils. Color is recorded in the Munsell color system as for instance 10YR3/4 \"Dusky Red\", with 10YR as \"hue\", 3 as \"value\" and 4 as \"chroma\". Munsell color dimensions (hue, value and chroma) can be averaged among samples and treated as quantitative parameters, displaying significant correlations with various soil and vegetation properties.\n\nSoil color is primarily influenced by soil mineralogy. Many soil colours are due to various iron minerals. The development and distribution of colour in a soil profile result from chemical and biological weathering, especially redox reactions. As the primary minerals in soil parent material weather, the elements combine into new and colourful compounds. Iron forms secondary minerals of a yellow or red colour, organic matter decomposes into black and brown humic compounds, and manganese and sulfur can form black mineral deposits. These pigments can produce various colour patterns within a soil. Aerobic conditions produce uniform or gradual colour changes, while reducing environments (anaerobic) result in rapid colour flow with complex, mottled patterns and points of colour concentration.\n\nSoil resistivity is a measure of a soil's ability to retard the conduction of an electric current. The electrical resistivity of soil can affect the rate of galvanic corrosion of metallic structures in contact with the soil. Higher moisture content or increased electrolyte concentration can lower resistivity and increase conductivity, thereby increasing the rate of corrosion. Soil resistivity values typically range from about 1 to 100000 Ω·m, extreme values being for saline soils and dry soils overlaying cristalline rocks, respectively.\n\nWater that enters a field is removed from a field by runoff, drainage, evaporation or transpiration. Runoff is the water that flows on the surface to the edge of the field; drainage is the water that flows through the soil downward or toward the edge of the field underground; evaporative water loss from a field is that part of the water that evaporates into the atmosphere directly from the field's surface; transpiration is the loss of water from the field by its evaporation from the plant itself.\n\nWater affects soil formation, structure, stability and erosion but is of primary concern with respect to plant growth. Water is essential to plants for four reasons:\n\n\nIn addition, water alters the soil profile by dissolving and re-depositing minerals, often at lower levels, and possibly leaving the soil sterile in the case of extreme rainfall and drainage. In a loam soil, solids constitute half the volume, gas one-quarter of the volume, and water one-quarter of the volume of which only half will be available to most plants, with a strong variation according to matric potential.\n\nA flooded field will drain the gravitational water under the influence of gravity until water's adhesive and cohesive forces resist further drainage at which point it is said to have reached field capacity. At that point, plants must apply suction to draw water from a soil. The water that plants may draw from the soil is called the available water. Once the available water is used up the remaining moisture is called unavailable water as the plant cannot produce sufficient suction to draw that water in. A plant must produce suction that increases from zero for a flooded field to 1/3 bar at field dry condition (one bar is a little less than one atmosphere pressure). At 15 bar suction, wilting point, seeds will not germinate, plants begin to wilt and then die. Water moves in soil under the influence of gravity, osmosis and capillarity. When water enters the soil, it displaces air from interconnected macropores by buoyancy, and breaks aggregates into which air is entrapped, a process called slaking.\n\nThe rate at which a soil can absorb water depends on the soil and its other conditions. As a plant grows, its roots remove water from the largest pores (macropores) first. Soon the larger pores hold only air, and the remaining water is found only in the intermediate- and smallest-sized pores (micropores). The water in the smallest pores is so strongly held to particle surfaces that plant roots cannot pull it away. Consequently, not all soil water is available to plants, with a strong dependence on texture. When saturated, the soil may lose nutrients as the water drains. Water moves in a draining field under the influence of pressure where the soil is locally saturated and by capillarity pull to drier parts of the soil. Most plant water needs are supplied from the suction caused by evaporation from plant leaves (transpiration) and a lower fraction is supplied by suction created by osmotic pressure differences between the plant interior and the soil solution. Plant roots must seek out water and grow preferentially in moister soil microsites, but some parts of the root system are also able to remoisten dry parts of the soil. Insufficient water will damage the yield of a crop. Most of the available water is used in transpiration to pull nutrients into the plant.\n\nWater is retained in a soil when the adhesive force of attraction that water's hydrogen atoms have for the oxygen of soil particles is stronger than the cohesive forces that water's hydrogen feels for other water oxygen atoms. When a field is flooded, the soil pore space is completely filled by water. The field will drain under the force of gravity until it reaches what is called field capacity, at which point the smallest pores are filled with water and the largest with water and gases. The total amount of water held when field capacity is reached is a function of the specific surface area of the soil particles. As a result, high clay and high organic soils have higher field capacities. The total force required to pull or push water out of soil is termed suction and usually expressed in units of bars (10 pascal) which is just a little less than one-atmosphere pressure. Alternatively, the terms \"soil moisture tension\" or water potential may be used.\n\nThe forces with which water is held in soils determine its availability to plants. Forces of adhesion hold water strongly to mineral and humus surfaces and less strongly to itself by cohesive forces. A plant's root may penetrate a very small volume of water that is adhering to soil and be initially able to draw in water that is only lightly held by the cohesive forces. But as the droplet is drawn down, the forces of adhesion of the water for the soil particles produce increasingly higher suction, finally up to 15 bar. At 15 bar suction, the soil water amount is called wilting point. At that suction the plant cannot sustain its water needs as water is still being lost from the plant by transpiration, the plant's turgidity is lost, and it wilts, although stomatal closure may decrease transpiration and thus may retard wilting below the wilting point, in particular under adaptation or acclimatization to drought. The next level, called air-dry, occurs at 1000 bar suction. Finally the oven dry condition is reached at 10,000 bar suction. All water below wilting percentage is called unavailable water.\n\nWhen the soil moisture content is optimal for plant growth, the water in the large and intermediate size pores can move about in the soil and be easily used by plants. The amount of water remaining in a soil drained to field capacity and the amount that is available are functions of the soil type. Sandy soil will retain very little water, while clay will hold the maximum amount. The time required to drain a field from flooded condition for a clay loam that begins at 43% water by weight to a field capacity of 22% is six days, whereas a sand loam that is flooded to its maximum of 22% water will take two days to reach field capacity of 11% water. The available water for the clay loam might be 11% whereas for the sand loam it might be only 8% by weight.\n\nThe above are average values for the soil textures as the percentages of sand, silt and clay vary.\n\nWater moves through soil due to the force of gravity, osmosis and capillarity. At zero to one-third bar suction, water is pushed through soil from the point of its application under the force of gravity and the pressure gradient created by the pressure of the water; this is called saturated flow. At higher suction, water movement is pulled by capillarity from wetter toward drier soil. This is caused by water's adhesion to soil solids, and is called unsaturated flow.\n\nWater infiltration and movement in soil is controlled by six factors:\n\n\nWater infiltration rates range from per hour for high clay soils to per hour for sand and well stabilised and aggregated soil structures. Water flows through the ground unevenly, in the form of so-called \"gravity fingers\", because of the surface tension between water particles.\n\nTree roots, whether living or dead, create preferential channels for rainwater flow through soil, magnifying infiltration rates of water up to 27 times.\n\nFlooding temporarily increases soil permeability in river beds, helping to recharge aquifers.\n\nWater applied to a soil is pushed by pressure gradients from the point of its application where it is saturated locally, to less saturated areas, such as the vadose zone. Once soil is completely wetted, any more water will move downward, or percolate out of the range of plant roots, carrying with it clay, humus, nutrients, primarily cations, and various contaminants, including pesticides, pollutants, viruses and bacteria, potentially causing groundwater contamination. In order of decreasing solubility, the leached nutrients are:\n\nIn the United States percolation water due to rainfall ranges from zero inches just east of the Rocky Mountains to twenty or more inches in the Appalachian Mountains and the north coast of the Gulf of Mexico.\n\nSoil physics (Darcy-type model) predicts that at suctions less than one-third bar, water moves theoretically in all directions via unsaturated flow at a rate that is dependent on the square of the diameter of the water-filled pores, but there is still not an adequate physical theory linking all types of waterflow in soil. Preferential flow occurs along interconnected macropores, crevices, root and worm channels, which drain water under gravity. Water is also pulled by capillary action due to the adhesion force of water to the soil solids, producing a suction gradient from wet towards drier soil and from macropores to micropores. Water flow (also called hydraulic conductivity) is primarily from coarse-textured soil into fine-textured soil horizons and is slowest in fine-textured soils such as clay.\n\nOf equal importance to the storage and movement of water in soil is the means by which plants acquire it and their nutrients. Most soil water is taken up by plants as passive absorption caused by the pulling force of water evaporating (transpiring) from the long column of water (xylem sap flow) that leads from the plant's roots to its leaves, according to the cohesion-tension theory. The upward movement of water and solutes (hydraulic lift) is regulated in the roots by the endodermis and in the plant foliage by stomatal conductance, and can be interrupted in root and shoot xylem vessels by cavitation, also called \"xylem embolism\". In addition, the high concentration of salts within plant roots creates an osmotic pressure gradient that pushes soil water into the roots. Osmotic absorption becomes more important during times of low water transpiration caused by lower temperatures (for example at night) or high humidity, and the reverse occurs under high temperature or low humidity. It is these process that cause guttation and wilting, respectively.\n\nRoot extension is vital for plant survival. A study of a single winter rye plant grown for four months in one cubic foot of loam soil showed that the plant developed 13,800,000 roots, a total of 385 miles in length with 2,550 square feet in surface area; and 14 billion hair roots of 6,600 miles total length and 4,320 square feet total area; for a total surface area of 6,870 square feet (83 ft squared). The total surface area of the loam soil was estimated to be 560,000 square feet. In other words, the roots were in contact with only 1.2% of the soil. However, root extension should be viewed as a dynamic process, allowing new roots to explore a new volume of soil each day, increasing dramatically the total volume of soil explored over a given growth period, and thus the volume of water taken up by the root system over this period. Root architecture, i.e. the spatial configuration of the root system, plays a prominent role in the adaptation of plants to soil water and nutrient availabiity, and thus in plant productivity.\n\nRoots must seek out water as the unsaturated flow of water in soil can move only at a rate of up to 2.5 cm (one inch) per day; as a result they are constantly dying and growing as they seek out high concentrations of soil moisture. Insufficient soil moisture, to the point of causing wilting, will cause permanent damage and crop yields will suffer. When grain sorghum was exposed to soil suction as low as 13.0 bar during the seed head emergence through bloom and seed set stages of growth, its production was reduced by 34%.\n\nOnly a small fraction (0.1% to 1%) of the water used by a plant is held within the plant. The majority is ultimately lost via transpiration, while evaporation from the soil surface is also substantial, the transpiration:evaporation ratio varying according to vegetation type and climate, peaking in tropical rainforests and dipping in steppes and deserts. Transpiration plus evaporative soil moisture loss is called evapotranspiration. Evapotranspiration plus water held in the plant totals to consumptive use, which is nearly identical to evapotranspiration.\n\nThe total water used in an agricultural field includes surface runoff, drainage and consumptive use. The use of loose mulches will reduce evaporative losses for a period after a field is irrigated, but in the end the total evaporative loss (plant plus soil) will approach that of an uncovered soil, while more water is immediately available for plant growth. Water use efficiency is measured by the transpiration ratio, which is the ratio of the total water transpired by a plant to the dry weight of the harvested plant. Transpiration ratios for crops range from 300 to 700. For example, alfalfa may have a transpiration ratio of 500 and as a result 500 kilograms of water will produce one kilogram of dry alfalfa.\n\nThe atmosphere of soil, or soil gas, is radically different from the atmosphere above. The consumption of oxygen by microbes and plant roots, and their release of carbon dioxide, decrease oxygen and increase carbon dioxide concentration. Atmospheric CO concentration is 0.04%, but in the soil pore space it may range from 10 to 100 times that level, thus potentially contributing to the inhibition of root respiration. Calcareous soils regulate CO concentration thanks to carbonate buffering, contrary to acid soils in which all CO respired accumulates in the soil pore system. At extreme levels CO is toxic. This suggests a possible negative feedback control of soil CO concentration through its inhibitory effects on root and microbial respiration (also called 'soil respiration'). In addition, the soil voids are saturated with water vapour, at least until the point of maximal hygroscopicity, beyond which a vapour-pressure deficit occurs in the soil pore space. Adequate porosity is necessary, not just to allow the penetration of water, but also to allow gases to diffuse in and out. Movement of gases is by diffusion from high concentrations to lower, the diffusion coefficient decreasing with soil compaction. Oxygen from above atmosphere diffuses in the soil where it is consumed and levels of carbon dioxide in excess of above atmosphere diffuse out with other gases (including greenhouse gases) as well as water. Soil texture and structure strongly affect soil porosity and gas diffusion. It is the total pore space (porosity) of soil, not the pore size, and the degree of pore interconnection (or conversely pore sealing), together with water content, air turbulence and temperature, that determine the rate of diffusion of gases into and out of soil. Platy soil structure and soil compaction (low porosity) impede gas flow, and a deficiency of oxygen may encourage anaerobic bacteria to reduce (strip oxygen) from nitrate NO to the gases N, NO, and NO, which are then lost to the atmosphere, thereby depleting the soil of nitrogen. Aerated soil is also a net sink of methane CH but a net producer of methane (a strong heat-absorbing greenhouse gas) when soils are depleted of oxygen and subject to elevated temperatures.\n\nSoil atmosphere is also the seat of emissions of volatiles other than carbon and nitrogen oxides from various soil organisms, e.g. roots, bacteria, fungi, animals. These volatiles are used as chemical cues, making soil atmosphere the seat of interaction networks playing a decisive role in the stability, dynamics and evolution of soil ecosystems. Biogenic soil volatile organic compounds are exchanged with the aboveground atmosphere, in which they are just 1–2 orders of magnitude lower than those from aboveground vegetation.\n\nWe humans can get some idea of the soil atmosphere through the well-known 'after-the-rain' scent, when infiltering rainwater flushes out the whole soil atmosphere after a drought period, or when soil is excavated, a bulk property attributed in a reductionist manner to particular biochemical compounds such as petrichor or geosmin.\n\nSoil particles can be classified by their chemical composition (mineralogy) as well as their size. The particle size distribution of a soil, its texture, determines many of the properties of that soil, in particular hydraulic conductivity and water potential but the mineralogy of those particles can strongly modify those properties. The mineralogy of the finest soil particles, clay, is especially important.\n\nGravel, sand and silt are the larger soil particles, and their mineralogy is often inherited from the parent material of the soil, but may include products of weathering (such as concretions of calcium carbonate or iron oxide), or residues of plant and animal life (such as silica phytoliths). Quartz is the most common mineral in the sand or silt fraction as it is resistant to chemical weathering, except under hot climate; other common minerals are feldspars, micas and ferromagnesian minerals such as pyroxenes, amphiboles and olivines, which are dissolved or transformed in clay under the combined influence of physico-chemical and biological processes.\n\nDue to its high specific surface area and its unbalanced negative electric charges, clay is the most active mineral component of soil. It is a colloidal and most often a crystalline material. In soils, clay is a soil textural class and is defined in a physical sense as any mineral particle less than in effective diameter. Many soil minerals, such as gypsum, carbonates, or quartz, are small enough to be classified as clay based on their physical size, but chemically they do not afford the same utility as do mineralogically-defined clay minerals. Chemically, clay minerals are a range of phyllosilicate minerals with certain reactive properties.\n\nBefore the advent of X-ray diffraction clay was thought to be very small particles of quartz, feldspar, mica, hornblende or augite, but it is now known to be (with the exception of mica-based clays) a precipitate with a mineralogical composition that is dependent on but different from its parent materials and is classed as a secondary mineral. The type of clay that is formed is a function of the parent material and the composition of the minerals in solution. Clay minerals continue to be formed as long as the soil exists. Mica-based clays result from a modification of the primary mica mineral in such a way that it behaves and is classed as a clay. Most clays are crystalline, but some clays or some parts of clay minerals are amorphous. The clays of a soil are a mixture of the various types of clay, but one type predominates.\n\nTypically there are four main groups of clay minerals: kaolinite, montmorillonite-smectite, illite, and chlorite. Most clays are crystalline and most are made up of three or four planes of oxygen held together by planes of aluminium and silicon by way of ionic bonds that together form a single layer of clay. The spatial arrangement of the oxygen atoms determines clay's structure. Half of the weight of clay is oxygen, but on a volume basis oxygen is ninety percent. The layers of clay are sometimes held together through hydrogen bonds, sodium or potassium bridges and as a result will swell less in the presence of water. Clays such as montmorillonite have layers that are loosely attached and will swell greatly when water intervenes between the layers.\n\nIn a wider sense clays can be classified as:\n\n\nAlumino-silica clays or aluminosilicate clays are characterised by their regular crystalline or quasi-crystalline structure. Oxygen in ionic bonds with silicon forms a tetrahedral coordination (silicon at the center) which in turn forms sheets of silica. Two sheets of silica are bonded together by a plane of aluminium which forms an octahedral coordination, called alumina, with the oxygens of the silica sheet above and that below it. Hydroxyl ions (OH) sometimes substitute for oxygen. During the clay formation process, Al may substitute for Si in the silica layer, and as much as one fourth of the aluminium Al may be substituted by Zn, Mg or Fe in the alumina layer. The substitution of lower-valence cations for higher-valence cations (isomorphous substitution) gives clay a local negative charge on an oxygen atom that attracts and holds water and positively charged soil cations, some of which are of value for plant growth. Isomorphous substitution occurs during the clay's formation and does not change with time.\n\nThe carbonate and sulfate minerals are much more soluble and hence are found primarily in desert soils where leaching is less active.\n\nAmorphous clays are young, and commonly found in volcanic ash. They are mixtures of alumina and silica which have not formed the ordered crystal shape of alumino-silica clays which time would provide. The majority of their negative charges originates from hydroxyl ions, which can gain or lose a hydrogen ion (H) in response to soil pH, in such way was as to buffer the soil pH. They may have either a negative charge provided by the attached hydroxyl ion (OH), which can attract a cation, or lose the hydrogen of the hydroxyl to solution and display a positive charge which can attract anions. As a result, they may display either high CEC in an acid soil solution, or high anion exchange capacity in a basic soil solution.\n\nSesquioxide clays are a product of heavy rainfall that has leached most of the silica from alumino-silica clay, leaving the less soluble oxides iron hematite (FeO), iron hydroxide (Fe(OH)), aluminium hydroxide gibbsite (Al(OH)), hydrated manganese birnessite (MnO). It takes hundreds of thousands of years of leaching to create sesquioxide clays. \"Sesqui\" is Latin for \"one and one-half\": there are three parts oxygen to two parts iron or aluminium; hence the ratio is one and one-half (not true for all). They are hydrated and act as either amorphous or crystalline. They are not sticky and do not swell, and soils high in them behave much like sand and can rapidly pass water. They are able to hold large quantities of phosphates. Sesquioxides have low CEC but are able to hold anions as well as cations. Such soils range from yellow to red in colour. Such clays tend to hold phosphorus so tightly that it is unavailable for absorption by plants.\n\nHumus is the final state of decomposition of organic matter. While it may linger for a thousand years, on the larger scale of the age of the mineral soil components, it is temporary. It is composed of the very stable lignins (30%) and complex sugars (polyuronides, 30%), proteins (30%), waxes, and fats that are resistant to breakdown by microbes. Its chemical assay is 60% carbon, 5% nitrogen, some oxygen and the remainder hydrogen, sulfur, and phosphorus. On a dry weight basis, the CEC of humus is many times greater than that of clay.\n\nIn the extreme environment of high temperatures and the leaching caused by the heavy rain of tropical rain forests, the clay and organic colloids are largely destroyed. The heavy rains wash the alumino-silicate clays from the soil leaving only sesquioxide clays of low CEC. The high temperatures and humidity allow bacteria and fungi to virtually dissolve any organic matter on the rain-forest floor overnight and much of the nutrients are volatilized or leached from the soil and lost. However, carbon in the form of charcoal is far more stable than soil colloids and is capable of performing many of the functions of the soil colloids of sub-tropical soils. Soil containing substantial quantities of charcoal, of an anthropogenic origin, is called terra preta. Research into terra preta is still young but is promising. Fallow periods \"on the Amazonian Dark Earths can be as short as 6 months, whereas fallow periods on oxisols are usually 8 to 10 years long\"\n\nThe chemistry of a soil determines its ability to supply available plant nutrients and affects its physical properties and the health of its microbial population. In addition, a soil's chemistry also determines its corrosivity, stability, and ability to absorb pollutants and to filter water. It is the surface chemistry of mineral and organic colloids that determines soil's chemical properties. \"A colloid is a small, insoluble, nondiffusible particle larger than a molecule but small enough to remain suspended in a fluid medium without settling. Most soils contain organic colloidal particles called humus as well as the inorganic colloidal particles of clays.\" The very high specific surface area of colloids and their net charges, gives soil its ability to hold and release ions. Negatively charged sites on colloids attract and release cations in what is referred to as cation exchange. Cation-exchange capacity (CEC) is the amount of exchangeable cations per unit weight of dry soil and is expressed in terms of milliequivalents of positively charged ions per 100 grams of soil (or centimoles of positive charge per kilogram of soil; cmol/kg). Similarly, positively charged sites on colloids can attract and release anions in the soil giving the soil anion exchange capacity (AEC).\n\nThe cation exchange, that takes place between colloids and soil water, buffers (moderates) soil pH, alters soil structure, and purifies percolating water by adsorbing cations of all types, both useful and harmful.\n\nThe negative or positive charges on colloid particles make them able to hold cations or anions, respectively, to their surfaces. The charges result from four sources.\n\n\nCations held to the negatively charged colloids resist being washed downward by water and out of reach of plants' roots, thereby preserving the fertility of soils in areas of moderate rainfall and low temperatures.\n\nThere is a hierarchy in the process of cation exchange on colloids, as they differ in the strength of adsorption by the colloid and hence their ability to replace one another. If present in equal amounts in the soil water solution:\n\nAl replaces H replaces Ca replaces Mg replaces K same as NH replaces Na\n\nIf one cation is added in large amounts, it may replace the others by the sheer force of its numbers. This is called mass action. This is largely what occurs with the addition of fertiliser.\n\nAs the soil solution becomes more acidic (low pH, and an abundance of H), the other cations more weakly bound to colloids are pushed into solution as hydrogen ions occupy those sites. A low pH may cause hydrogen of hydroxyl groups to be pulled into solution, leaving charged sites on the colloid available to be occupied by other cations. This ionisation of hydroxyl groups on the surface of soil colloids creates what is described as pH-dependent charges. Unlike permanent charges developed by isomorphous substitution, pH-dependent charges are variable and increase with increasing pH. Freed cations can be made available to plants but are also prone to be leached from the soil, possibly making the soil less fertile. Plants are able to excrete H into the soil and by that means, change the pH of the soil near the root and push cations off the colloids, thus making those available to the plant.\n\nCation exchange capacity should be thought of as the soil's ability to remove cations from the soil water solution and sequester those to be exchanged later as the plant roots release hydrogen ions to the solution. CEC is the amount of exchangeable hydrogen cation (H) that will combine with 100 grams dry weight of soil and whose measure is one milliequivalents per 100 grams of soil (1 meq/100 g). Hydrogen ions have a single charge and one-thousandth of a gram of hydrogen ions per 100 grams dry soil gives a measure of one milliequivalent of hydrogen ion. Calcium, with an atomic weight 40 times that of hydrogen and with a valence of two, converts to (40/2) x 1 milliequivalent = 20 milliequivalents of hydrogen ion per 100 grams of dry soil or 20 meq/100 g. The modern measure of CEC is expressed as centimoles of positive charge per kilogram (cmol/kg) of oven-dry soil.\n\nMost of the soil's CEC occurs on clay and humus colloids, and the lack of those in hot, humid, wet climates, due to leaching and decomposition respectively, explains the relative sterility of tropical soils. Live plant roots also have some CEC.\n\nAnion exchange capacity should be thought of as the soil's ability to remove anions from the soil water solution and sequester those for later exchange as the plant roots release carbonate anions to the soil water solution. Those colloids which have low CEC tend to have some AEC. Amorphous and sesquioxide clays have the highest AEC, followed by the iron oxides. Levels of AEC are much lower than for CEC. Phosphates tend to be held at anion exchange sites.\n\nIron and aluminum hydroxide clays are able to exchange their hydroxide anions (OH) for other anions. The order reflecting the strength of anion adhesion is as follows:\n\nThe amount of exchangeable anions is of a magnitude of tenths to a few milliequivalents per 100 g dry soil. As pH rises, there are relatively more hydroxyls, which will displace anions from the colloids and force them into solution and out of storage; hence AEC decreases with increasing pH (alkalinity).\n\nSoil reactivity is expressed in terms of pH and is a measure of the acidity or alkalinity of the soil. More precisely, it is a measure of hydrogen ion concentration in an aqueous solution and ranges in values from 0 to 14 (acidic to basic) but practically speaking for soils, pH ranges from 3.5 to 9.5, as pH values beyond those extremes are toxic to life forms.\n\nAt 25 °C an aqueous solution that has a pH of 3.5 has 10 moles H (hydrogen ions) per litre of solution (and also 10 mole/litre OH). A pH of 7, defined as neutral, has 10 moles hydrogen ions per litre of solution and also 10 moles of OH per litre; since the two concentrations are equal, they are said to neutralise each other. A pH of 9.5 has 10 moles hydrogen ions per litre of solution (and also 10 mole per litre OH). A pH of 3.5 has one million times more hydrogen ions per litre than a solution with pH of 9.5 (9.5 - 3.5 = 6 or 10) and is more acidic.\n\nThe effect of pH on a soil is to remove from the soil or to make available certain ions. Soils with high acidity tend to have toxic amounts of aluminium and manganese. Plants which need calcium need moderate alkalinity, but most minerals are more soluble in acid soils. Soil organisms are hindered by high acidity, and most agricultural crops do best with mineral soils of pH 6.5 and organic soils of pH 5.5.\n\nIn high rainfall areas, soils tend to acidity as the basic cations are forced off the soil colloids by the mass action of hydrogen ions from the rain as those attach to the colloids. High rainfall rates can then wash the nutrients out, leaving the soil sterile. Once the colloids are saturated with H, the addition of any more hydrogen ions or aluminum hydroxyl cations drives the pH even lower (more acidic) as the soil has been left with no buffering capacity. In areas of extreme rainfall and high temperatures, the clay and humus may be washed out, further reducing the buffering capacity of the soil. In low rainfall areas, unleached calcium pushes pH to 8.5 and with the addition of exchangeable sodium, soils may reach pH 10. Beyond a pH of 9, plant growth is reduced. High pH results in low micro-nutrient mobility, but water-soluble chelates of those nutrients can correct the deficit. Sodium can be reduced by the addition of gypsum (calcium sulphate) as calcium adheres to clay more tightly than does sodium causing sodium to be pushed into the soil water solution where it can be washed out by an abundance of water.\n\nThere are acid-forming cations (hydrogen and aluminium) and there are base-forming cations. The fraction of the base-forming cations that occupy positions on the soil colloids is called the base saturation percentage. If a soil has a CEC of 20 meq and 5 meq are aluminium and hydrogen cations (acid-forming), the remainder of positions on the colloids (20-5 = 15 meq) are assumed occupied by base-forming cations, so that the percentage base saturation is 15/20 x 100% = 75% (the compliment 25% is assumed acid-forming cations). When the soil pH is 7 (neutral), base saturation is 100 percent and there are no hydrogen ions stored on the colloids. Base saturation is almost in direct proportion to pH (increases with increasing pH). It is of use in calculating the amount of lime needed to neutralise an acid soil. The amount of lime needed to neutralize a soil must take account of the amount of acid forming ions on the colloids not just those in the soil water solution. The addition of enough lime to neutralize the soil water solution will be insufficient to change the pH, as the acid forming cations stored on the soil colloids will tend to restore the original pH condition as they are pushed off those colloids by the calcium of the added lime.\n\nThe resistance of soil to change in pH, as a result of the addition of acid or basic material, is a measure of the buffering capacity of a soil and (for a particular soil type) increases as the CEC increases. Hence, pure sand has almost no buffering ability, while soils high in colloids have high buffering capacity. Buffering occurs by cation exchange and neutralisation.\n\nThe addition of a small amount highly basic aqueous ammonia to a soil will cause the ammonium to displace hydrogen ions from the colloids, and the end product is water and colloidally fixed ammonium, but little permanent change overall in soil pH.\n\nThe addition of a small amount of lime, Ca(OH), will displace hydrogen ions from the soil colloids, causing the fixation of calcium to colloids and the evolution of CO and water, with little permanent change in soil pH.\n\nThe above are examples of the buffering of soil pH. The general principal is that an increase in a particular cation in the soil water solution will cause that cation to be fixed to colloids (buffered) and a decrease in solution of that cation will cause it to be withdrawn from the colloid and moved into solution (buffered). The degree of buffering is often related to the CEC of the soil; the greater the CEC, the greater the buffering capacity of the soil.\n\nSixteen elements or nutrients are essential for plant growth and reproduction. They are carbon C, hydrogen H, oxygen O, nitrogen N, phosphorus P, potassium K, sulfur S, calcium Ca, magnesium Mg, iron Fe, boron B, manganese Mn, copper Cu, zinc Zn, molybdenum Mo, nickel Ni and chlorine Cl. Nutrients required for plants to complete their life cycle are considered essential nutrients. Nutrients that enhance the growth of plants but are not necessary to complete the plant's life cycle are considered non-essential. With the exception of carbon, hydrogen and oxygen, which are supplied by carbon dioxide and water, and nitrogen, provided through nitrogen fixation, the nutrients derive originally from the mineral component of the soil.\n\nPlant uptake of nutrients can only proceed when they are present in a plant-available form. In most situations, nutrients are absorbed in an ionic form from (or together with) soil water. Although minerals are the origin of most nutrients, and the bulk of most nutrient elements in the soil is held in crystalline form within primary and secondary minerals, they weather too slowly to support rapid plant growth. For example, The application of finely ground minerals, feldspar and apatite, to soil seldom provides the necessary amounts of potassium and phosphorus at a rate sufficient for good plant growth, as most of the nutrients remain bound in the crystals of those minerals.\n\nThe nutrients adsorbed onto the surfaces of clay colloids and soil organic matter provide a more accessible reservoir of many plant nutrients (e.g. K, Ca, Mg, P, Zn). As plants absorb the nutrients from the soil water, the soluble pool is replenished from the surface-bound pool. The decomposition of soil organic matter by microorganisms is another mechanism whereby the soluble pool of nutrients is replenished – this is important for the supply of plant-available N, S, P, and B from soil.\n\nGram for gram, the capacity of humus to hold nutrients and water is far greater than that of clay minerals. All in all, small amounts of humus may remarkably increase the soil's capacity to promote plant growth.\n\nNutrients in the soil are taken up by the plant through its roots. To be taken up by a plant, a nutrient element must be located near the root surface; however, the supply of nutrients in contact with the root is rapidly depleted. There are three basic mechanisms whereby nutrient ions dissolved in the soil solution are brought into contact with plant roots:\n\n\nAll three mechanisms operate simultaneously, but one mechanism or another may be most important for a particular nutrient. For example, in the case of calcium, which is generally plentiful in the soil solution, mass flow alone can usually bring sufficient amounts to the root surface. However, in the case of phosphorus, diffusion is needed to supplement mass flow. For the most part, nutrient ions must travel some distance in the soil solution to reach the root surface. This movement can take place by mass flow, as when dissolved nutrients are carried along with the soil water flowing toward a root that is actively drawing water from the soil. In this type of movement, the nutrient ions are somewhat analogous to leaves floating down a stream. In addition, nutrient ions continually move by diffusion from areas of greater concentration toward the nutrient-depleted areas of lower concentration around the root surface. That process is due to random motion of molecules. By this means, plants can continue to take up nutrients even at night, when water is only slowly absorbed into the roots as transpiration has almost stopped. Finally, root interception comes into play as roots continually grow into new, undepleted soil.\n\nIn the above table, phosphorus and potassium nutrients move more by diffusion than they do by mass flow in the soil water solution, as they are rapidly taken up by the roots creating a concentration of almost zero near the roots (the plants cannot transpire enough water to draw more of those nutrients near the roots). The very steep concentration gradient is of greater influence in the movement of those ions than is the movement of those by mass flow. The movement by mass flow requires the transpiration of water from the plant causing water and solution ions to also move toward the roots. Movement by root interception is slowest as the plants must extend their roots.\n\nPlants move ions out of their roots in an effort to move nutrients in from the soil. Hydrogen H is exchanged for other cations, and carbonate (HCO) and hydroxide (OH) anions are exchanged for nutrient anions. As plant roots remove nutrients from the soil water solution, they are replenished as other ions move off of clay and humus (by ion exchange or desorption), are added from the weathering of soil minerals, and are released by the decomposition of soil organic matter. Plants derive a large proportion of their anion nutrients from decomposing organic matter, which typically holds about 95 percent of the soil nitrogen, 5 to 60 percent of the soil phosphorus and about 80 percent of the soil sulfur. Where crops are produced, the replenishment of nutrients in the soil must usually be augmented by the addition of fertilizer or organic matter.\n\nBecause nutrient uptake is an active metabolic process, conditions that inhibit root metabolism may also inhibit nutrient uptake. Examples of such conditions include waterlogging or soil compaction resulting in poor soil aeration, excessively high or low soil temperatures, and above-ground conditions that result in low translocation of sugars to plant roots.\n\nPlants obtain their carbon from atmospheric carbon dioxide. About 45% of a plant's dry mass is carbon; plant residues typically have a carbon to nitrogen ratio (C/N) of between 13:1 and 100:1. As the soil organic material is digested by arthropods and micro-organisms, the C/N decreases as the carbonaceous material is metabolized and carbon dioxide (CO) is released as a byproduct which then finds its way out of the soil and into the atmosphere. The nitrogen is sequestered in the bodies of the living matter of those decomposing organisms and so it builds up in the soil. Normal CO concentration in the atmosphere is 0.03%, this can be the factor limiting plant growth. In a field of maize on a still day during high light conditions in the growing season, the CO concentration drops very low, but under such conditions the crop could use up to 20 times the normal concentration. The respiration of CO by soil micro-organisms decomposing soil organic matter contributes an important amount of CO to the photosynthesising plants. Within the soil, CO concentration is 10 to 100 times that of atmospheric levels but may rise to toxic levels if the soil porosity is low or if diffusion is impeded by flooding.\n\nNitrogen is the most critical element obtained by plants from the soil and nitrogen deficiency often limits plant growth. Plants can use the nitrogen as either the ammonium cation (NH) or the anion nitrate (NO). Usually, most of the nitrogen in soil is bound within organic compounds that make up the soil organic matter, and must be mineralized to the ammonium or nitrate form before it can be taken up by most plants. The total nitrogen content depends largely on the soil organic matter content, which in turn depends on the climate, vegetation, topography, age and soil management. Soil nitrogen typically decreases by 0.2 to 0.3% for every temperature increase by 10 °C. Usually, grassland soils contain more soil nitrogen than forest soils. Cultivation decreases soil nitrogen by exposing soil organic matter to decomposition by microorganisms, and soils under no-tillage maintain more soil nitrogen than tilled soils.\n\nSome micro-organisms are able to metabolise organic matter and release ammonium in a process called \"mineralisation\". Others take free ammonium and oxidise it to nitrate. Nitrogen-fixing bacteria are capable of metabolising N into the form of ammonia in a process called nitrogen fixation. Both ammonium and nitrate can be \"immobilized\" by their incorporation into the microbes' living cells, where it is temporarily sequestered in the form of amino acids and protein. Nitrate may also be lost from the soil when bacteria metabolise it to the gases N and NO. The loss of gaseous forms of nitrogen to the atmosphere due to microbial action is called \"denitrification\". Nitrogen may also be \"leached\" from the soil if it is in the form of nitrate or lost to the atmosphere as ammonia due to a chemical reaction of ammonium with alkaline soil by way of a process called \"volatilisation\". Ammonium may also be sequestered in clay by \"fixation\". A small amount of nitrogen is added to soil by rainfall.\n\nIn the process of mineralisation, microbes feed on organic matter, releasing ammonia (NH), ammonium (NH) and other nutrients. As long as the carbon to nitrogen ratio (C/N) of fresh residues in the soil is above 30:1, nitrogen will be in short supply and other bacteria will feed on the ammonium and incorporate its nitrogen into their cells in the immobilization process. In that form the nitrogen is said to be \"immobilised\". Later, when such bacteria die, they too are \"mineralised\" and some of the nitrogen is released as ammonium and nitrate. If the C/N is less than 15, ammonia is freed to the soil, where it may be used by bacteria which oxidise it to nitrate (nitrification). Bacteria may on average add nitrogen per acre, and in an unfertilised field, this is the most important source of usable nitrogen. In a soil with 5% organic matter perhaps 2 to 5% of that is released to the soil by such decomposition. It occurs fastest in warm, moist, well aerated soil. The mineralisation of 3% of the organic material of a soil that is 4% organic matter overall, would release of nitrogen as ammonium per acre.\n\nIn nitrogen fixation, rhizobium bacteria convert N to ammonia (NH). Rhizobia share a symbiotic relationship with host plants, since rhizobia supply the host with nitrogen and the host provides rhizobia with nutrients and a safe environment. It is estimated that such symbiotic bacteria in the root nodules of legumes add 45 to 250 pounds of nitrogen per acre per year, which may be sufficient for the crop. Other, free-living nitrogen-fixing bacteria and blue-green algae live independently in the soil and release nitrate when their dead bodies are converted by way of mineralisation.\n\nSome amount of usable nitrogen is fixed by lightning as nitric oxide (NO) and nitrogen dioxide (NO). Nitrogen dioxide is soluble in water to form nitric acid (HNO) solution of H and NO. Ammonia, NH, previously released from the soil or from combustion, may fall with precipitation as nitric acid at a rate of about five pounds nitrogen per acre per year.\n\nWhen bacteria feed on soluble forms of nitrogen (ammonium and nitrate), they temporarily sequester that nitrogen in their bodies in a process called \"immobilisation\". At a later time when those bacteria die, their nitrogen may be released as ammonium by the processes of mineralisation.\n\nProtein material is easily broken down, but the rate of its decomposition is slowed by its attachment to the crystalline structure of clay and when trapped between the clay layers. The layers are small enough that bacteria cannot enter. Some organisms can exude extracellular enzymes that can act on the sequestered proteins. However, those enzymes too may be trapped on the clay crystals.\n\nAmmonium fixation occurs when ammonium pushes potassium ions from between the layers of clay such as illite or montmorillonite. Only a small fraction of soil nitrogen is held this way.\n\nUsable nitrogen may be lost from soils when it is in the form of nitrate, as it is easily leached. Further losses of nitrogen occur by denitrification, the process whereby soil bacteria convert nitrate (NO) to nitrogen gas, N or NO. This occurs when poor soil aeration limits free oxygen, forcing bacteria to use the oxygen in nitrate for their respiratory process. Denitrification increases when oxidisable organic material is available and when soils are warm and slightly acidic. Denitrification may vary throughout a soil as the aeration varies from place to place. Denitrification may cause the loss of 10 to 20 percent of the available nitrates within a day and when conditions are favourable to that process, losses of up to 60 percent of nitrate applied as fertiliser may occur.\n\n\"Ammonium volatilisation\" occurs when ammonium reacts chemically with an alkaline soil, converting NH to NH. The application of ammonium fertiliser to such a field can result in volatilisation losses of as much as 30 percent.\n\nAfter nitrogen, phosphorus is probably the element most likely to be deficient in soils. The soil mineral apatite is the most common mineral source of phosphorus. While there is on average 1000 lb of phosphorus per acre in the soil, it is generally in the form of phosphates with low solubility. Total phosphorus is about 0.1 percent by weight of the soil, but only one percent of that is available. Of the part available, more than half comes from the mineralisation of organic matter. Agricultural fields may need to be fertilised to make up for the phosphorus that has been removed in the crop.\n\nWhen phosphorus does form solubilised ions of HPO, they rapidly form insoluble phosphates of calcium or hydrous oxides of iron and aluminum. Phosphorus is largely immobile in the soil and is not leached but actually builds up in the surface layer if not cropped. The application of soluble fertilisers to soils may result in zinc deficiencies as zinc phosphates form. Conversely, the application of zinc to soils may immobilise phosphorus again as zinc phosphate. Lack of phosphorus may interfere with the normal opening of the plant leaf stomata, resulting in plant temperatures 10 percent higher than normal. Phosphorus is most available when soil pH is 6.5 in mineral soils and 5.5 in organic soils.\n\nThe amount of potassium in a soil may be as much as 80,000 lb per acre-foot, of which only 150 lb is available for plant growth. Common mineral sources of potassium are the mica biotite and potassium feldspar, KAlSiO. When solubilised, half will be held as exchangeable cations on clay while the other half is in the soil water solution. Potassium fixation often occurs when soils dry and the potassium is bonded between layers of illite clay. Under certain conditions, dependent on the soil texture, intensity of drying, and initial amount of exchangeable potassium, the fixed percentage may be as much as 90 percent within ten minutes. Potassium may be leached from soils low in clay.\n\nCalcium is one percent by weight of soils and is generally available but may be low as it is soluble and can be leached. It is thus low in sandy and heavily leached soil or strongly acidic mineral soil. Calcium is supplied to the plant in the form of exchangeable ions and moderately soluble minerals. Calcium is more available on the soil colloids than is potassium because the common mineral calcite, CaCO, is more soluble than potassium-bearing minerals.\n\nMagnesium is one of the dominant exchangeable cations in most soils (as are calcium and potassium). Primary minerals that weather to release magnesium include hornblende, biotite and vermiculite. Soil magnesium concentrations are generally sufficient for optimal plant growth, but highly weathered and sandy soils may be magnesium deficient due to leaching by heavy precipitation.\n\nMost sulfur is made available to plants, like phosphorus, by its release from decomposing organic matter. Deficiencies may exist in some soils (especially sandy soils) and if cropped, sulfur needs to be added. The application of large quantities of nitrogen to fields that have marginal amounts of sulfur may cause sulfur deficiency in the rapidly growing plants by the plant's growth outpacing the supply of sulfur. A 15-ton crop of onions uses up to 19 lb of sulfur and 4 tons of alfalfa uses 15 lb per acre. Sulfur abundance varies with depth. In a sample of soils in Ohio, United States, the sulfur abundance varied with depths, 0-6 inches, 6-12 inches, 12-18 inches, 18-24 inches in the amounts: 1056, 830, 686, 528 lb per acre respectively.\n\nThe micronutrients essential in plant life, in their order of importance, include iron, manganese, zinc, copper, boron, chlorine and molybdenum. The term refers to plants' needs, not to their abundance in soil. They are required in very small amounts but are essential to plant health in that most are required parts of some enzyme system which speeds up plants' metabolisms. They are generally available in the mineral component of the soil, but the heavy application of phosphates can cause a deficiency in zinc and iron by the formation of insoluble zinc and iron phosphates. Iron deficiency may also result from excessive amounts of heavy metals or calcium minerals (lime) in the soil. Excess amounts of soluble boron, molybdenum and chloride are toxic.\n\nNutrients which enhance the health but whose deficiency does not stop the life cycle of plants include: cobalt, strontium, vanadium, silicon and nickel. As their importance are evaluated they may be added to the list of essential plant nutrients.\n\nSoil organic matter is made up of organic compounds and includes plant, animal and microbial material, both living and dead. A typical soil has a biomass composition of 70% microorganisms, 22% macrofauna, and 8% roots. The living component of an acre of soil may include 900 lb of earthworms, 2400 lb of fungi, 1500 lb of bacteria, 133 lb of protozoa and 890 lb of arthropods and algae.\n\nA small part of the organic matter consists of the living cells such as bacteria, molds, and actinomycetes that work to break down the dead organic matter. Were it not for the action of these micro-organisms, the entire carbon dioxide part of the atmosphere would be sequestered as organic matter in the soil.\n\nChemically, organic matter is classed as follows:\n\n\nMost living things in soils, including plants, insects, bacteria, and fungi, are dependent on organic matter for nutrients and/or energy. Soils have organic compounds in varying degrees of decomposition which rate is dependent on the temperature, soil moisture, and aeration. Bacteria and fungi feed on the raw organic matter, which are fed upon by amoebas, which in turn are fed upon by nematodes and arthropods. Organic matter holds soils open, allowing the infiltration of air and water, and may hold as much as twice its weight in water. Many soils, including desert and rocky-gravel soils, have little or no organic matter. Soils that are all organic matter, such as peat (histosols), are infertile. In its earliest stage of decomposition, the original organic material is often called raw organic matter. The final stage of decomposition is called humus.\n\nIn grassland, much of the organic matter added to the soil is from the deep, fibrous, grass root systems. By contrast, tree leaves falling on the forest floor are the principal source of soil organic matter in the forest. Another difference is the frequent occurrence in the grasslands of fires that destroy large amounts of aboveground material but stimulate even greater contributions from roots. Also, the much greater acidity under any forests inhibits the action of certain soil organisms that otherwise would mix much of the surface litter into the mineral soil. As a result, the soils under grasslands generally develop a thicker A horizon with a deeper distribution of organic matter than in comparable soils under forests, which characteristically store most of their organic matter in the forest floor (O horizon) and thin A horizon.\n\nHumus refers to organic matter that has been decomposed by soil flora and fauna to the point where it is resistant to further breakdown. Humus usually constitutes only five percent of the soil or less by volume, but it is an essential source of nutrients and adds important textural qualities crucial to soil health and plant growth. Humus also hold bits of undecomposed organic matter which feed arthropods and worms which further improve the soil. The end product, humus, is soluble in water and forms a weak acid that can attack silicate minerals. Humus is a colloid with a high cation and anion exchange capacity that on a dry weight basis is many times greater than that of clay colloids. It also acts as a buffer, like clay, against changes in pH and soil moisture.\n\nHumic acids and fulvic acids, which begin as raw organic matter, are important constituents of humus. After the death of plants and animals, microbes begin to feed on the residues, resulting finally in the formation of humus. With decomposition, there is a reduction of water-soluble constituents, cellulose and hemicellulose, and nutrients such as nitrogen, phosphorus, and sulfur. As the residues break down, only stable molecules made of aromatic carbon rings, oxygen and hydrogen remain in the form of humin, lignin and lignin complexes collectively called humus. While the structure of humus has few nutrients, it is able to attract and hold cation and anion nutrients by weak bonds that can be released into the soil solution in response to changes in soil pH.\n\nLignin is resistant to breakdown and accumulates within the soil. It also reacts with amino acids, which further increases its resistance to decomposition, including enzymatic decomposition by microbes. Fats and waxes from plant matter have some resistance to decomposition and persist in soils for a while. Clay soils often have higher organic contents that persist longer than soils without clay as the organic molecules adhere to and are stabilised by the clay. Proteins normally decompose readily, but when bound to clay particles, they become more resistant to decomposition. Clay particles also absorb the enzymes exuded by microbes which would normally break down proteins. The addition of organic matter to clay soils can render that organic matter and any added nutrients inaccessible to plants and microbes for many years. High soil tannin (polyphenol) content can cause nitrogen to be sequestered in proteins or cause nitrogen immobilisation.\n\nHumus formation is a process dependent on the amount of plant material added each year and the type of base soil. Both are affected by climate and the type of organisms present. Soils with humus can vary in nitrogen content but typically have 3 to 6 percent nitrogen. Raw organic matter, as a reserve of nitrogen and phosphorus, is a vital component affecting soil fertility. Humus also absorbs water, and expands and shrinks between dry and wet states, increasing soil porosity. Humus is less stable than the soil's mineral constituents, as it is reduced by microbial decomposition, and over time its concentration diminshes without the addition of new organic matter. However, humus may persist over centuries if not millennia.\n\nThe production, accumulation and degradation of organic matter are greatly dependent on climate. Temperature, soil moisture and topography are the major factors affecting the accumulation of organic matter in soils. Organic matter tends to accumulate under wet or cold conditions where decomposer activity is impeded by low temperature or excess moisture which results in anaerobic conditions. Conversely, excessive rain and high temperatures of tropical climates enables rapid decomposition of organic matter and leaching of plant nutrients; forest ecosystems on these soils rely on efficient recycling of nutrients and plant matter to maintain their productivity. Excessive slope may encourage the erosion of the top layer of soil which holds most of the raw organic material that would otherwise eventually become humus.\n\nCellulose and hemicellulose undergo fast decomposition by fungi and bacteria, with a half-life of 12–18 days in a temperate climate. Brown rot fungi can decompose the cellulose and hemicellulose, leaving the lignin and phenolic compounds behind. Starch, which is an energy storage system for plants, undergoes fast decomposition by bacteria and fungi. Lignin consists of polymers composed of 500 to 600 units with a highly branched, amorphous structure. Lignin undergoes very slow decomposition, mainly by white rot fungi and actinomycetes; its half-life under temperate conditions is about six months.\n\nA horizontal layer of the soil, whose physical features, composition and age are distinct from those above and beneath, is referred to as a soil horizon. The naming of a horizon is based on the type of material of which it is composed. Those materials reflect the duration of specific processes of soil formation. They are labelled using a shorthand notation of letters and numbers which describe the horizon in terms of its colour, size, texture, structure, consistency, root quantity, pH, voids, boundary characteristics and presence of nodules or concretions. No soil profile has all the major horizons. Some may have only one horizon.\n\nThe exposure of parent material to favourable conditions produces mineral soils that are marginally suitable for plant growth. That growth often results in the accumulation of organic residues. The accumulated organic layer called the O horizon produces a more active soil due to the effect of the organisms that live within it. Organisms colonise and break down organic materials, making available nutrients upon which other plants and animals can live. After sufficient time, humus moves downward and is deposited in a distinctive organic surface layer called the A horizon.\n\nSoil is classified into categories in order to understand relationships between different soils and to determine the suitability of a soil for a particular use. One of the first classification systems was developed by Russian scientist Dokuchaev around 1880. It was modified a number of times by American and European researchers, and developed into the system commonly used until the 1960s. It was based on the idea that soils have a particular morphology based on the materials and factors that form them. In the 1960s, a different classification system began to emerge which focused on soil morphology instead of parental materials and soil-forming factors. Since then it has undergone further modifications. The World Reference Base for Soil Resources (WRB) aims to establish an international reference base for soil classification.\n\nThere are fourteen soil orders at the top level of the Australian Soil Classification. They are: Anthroposols, Organosols, Podosols, Vertosols, Hydrosols, Kurosols, Sodosols, Chromosols, Calcarosols, Ferrosols, Dermosols, Kandosols, Rudosols and Tenosols.\n\nThe EU's soil taxonomy is based on a new standard soil classification in the World Reference Base for Soil Resources produced by the UN's Food and Agriculture Organization. According to this, the major soils in the European Union are:\n\nA taxonomy is an arrangement in a systematic manner; the USDA soil taxonomy has six levels of classification. They are, from most general to specific: order, suborder, great group, subgroup, family and series. Soil properties that can be measured quantitatively are used in this classification system – they include: depth, moisture, temperature, texture, structure, cation exchange capacity, base saturation, clay mineralogy, organic matter content and salt content. There are 12 soil orders (the top hierarchical level) in soil taxonomy. The names of the orders end with the suffix \"-sol\". The criteria for the different soil orders include properties that reflect major differences in the genesis of soils. The orders are:\n\nThe percentages listed above are for land area free of ice. \"Soils of Mountains\", which constitute the balance (11.6%), have a mixture of those listed above, or are classified as \"Rugged Mountains\" which have no soil.\n\nThe above soil orders in sequence of increasing degree of development are Entisols, Inceptisols, Aridisols, Mollisols, Alfisols, Spodosols, Ultisols, and Oxisols. Histosols and Vertisols may appear in any of the above at any time during their development.\n\nThe soil suborders within an order are differentiated on the basis of soil properties and horizons which depend on soil moisture and temperature. Forty-seven suborders are recognized in the United States.\n\nThe soil great group category is a subdivision of a suborder in which the kind and sequence of soil horizons distinguish one soil from another. About 185 great groups are recognized in the United States. Horizons marked by clay, iron, humus and hard pans and soil features such as the expansion-contraction of clays (that produce self-mixing provided by clay), temperature, and marked quantities of various salts are used as distinguishing features.\n\nThe great group categories are divided into three kinds of soil subgroups: typic, intergrade and extragrade. A typic subgroup represents the basic or 'typical' concept of the great group to which the described subgroup belongs. An intergrade subgroup describes the properties that suggest how it grades towards (is similar to) soils of other soil great groups, suborders or orders. These properties are not developed or expressed well enough to cause the soil to be included within the great group towards which they grade, but suggest similarities. Extragrade features are aberrant properties which prevent that soil from being included in another soil classification. About 1,000 soil subgroups are defined in the United States.\n\nA soil family category is a group of soils within a subgroup and describes the physical and chemical properties which affect the response of soil to agricultural management and engineering applications. The principal characteristics used to differentiate soil families include texture, mineralogy, pH, permeability, structure, consistency, the locale's precipitation pattern, and soil temperature. For some soils the criteria also specify the percentage of silt, sand and coarse fragments such as gravel, cobbles and rocks. About 4,500 soil families are recognised in the United States.\n\nA family may contain several soil series which describe the physical location using the name of a prominent physical feature such as a river or town near where the soil sample was taken. An example would be Merrimac for the Merrimack River in New Hampshire. More than 14,000 soil series are recognised in the United States. This permits very specific descriptions of soils.\n\nA soil phase of series, originally called 'soil type' describes the soil surface texture, slope, stoniness, saltiness, erosion, and other conditions.\n\nSoil is used in agriculture, where it serves as the anchor and primary nutrient base for plants; however, as demonstrated by hydroponics, it is not essential to plant growth if the soil-contained nutrients can be dissolved in a solution. The types of soil and available moisture determine the species of plants that can be cultivated.\n\nSoil material is also a critical component in the mining, construction and landscape development industries. Soil serves as a foundation for most construction projects. The movement of massive volumes of soil can be involved in surface mining, road building and dam construction. Earth sheltering is the architectural practice of using soil for external thermal mass against building walls. Many building materials are soil based.\n\nSoil resources are critical to the environment, as well as to food and fibre production. Soil provides minerals and water to plants. Soil absorbs rainwater and releases it later, thus preventing floods and drought. Soil cleans water as it percolates through it. Soil is the habitat for many organisms: the major part of known and unknown biodiversity is in the soil, in the form of invertebrates (earthworms, woodlice, millipedes, centipedes, snails, slugs, mites, springtails, enchytraeids, nematodes, protists), bacteria, archaea, fungi and algae; and most organisms living above ground have part of them (plants) or spend part of their life cycle (insects) below-ground. Above-ground and below-ground biodiversities are tightly interconnected, making soil protection of paramount importance for any restoration or conservation plan.\n\nThe biological component of soil is an extremely important carbon sink since about 57% of the biotic content is carbon. Even on desert crusts, cyanobacteria, lichens and mosses capture and sequester a significant amount of carbon by photosynthesis. Poor farming and grazing methods have degraded soils and released much of this sequestered carbon to the atmosphere. Restoring the world's soils could offset the effect of increases in greenhouse gas emissions and slow global warming, while improving crop yields and reducing water needs.\n\nWaste management often has a soil component. Septic drain fields treat septic tank effluent using aerobic soil processes. Landfills use soil for daily cover. Land application of waste water relies on soil biology to aerobically treat BOD.\n\nOrganic soils, especially peat, serve as a significant fuel resource; but wide areas of peat production, such as sphagnum bogs, are now protected because of patrimonial interest.\n\nGeophagy is the practice of eating soil-like substances. Both animals and human cultures occasionally consume soil for medicinal, recreational, or religious purposes. It has been shown that some monkeys consume soil, together with their preferred food (tree foliage and fruits), in order to alleviate tannin toxicity.\n\nSoils filter and purify water and affect its chemistry. Rain water and pooled water from ponds, lakes and rivers percolate through the soil horizons and the upper rock strata, thus becoming groundwater. Pests (viruses) and pollutants, such as persistent organic pollutants (chlorinated pesticides, polychlorinated biphenyls), oils (hydrocarbons), heavy metals (lead, zinc, cadmium), and excess nutrients (nitrates, sulfates, phosphates) are filtered out by the soil. Soil organisms metabolise them or immobilise them in their biomass and necromass, thereby incorporating them into stable humus. The physical integrity of soil is also a prerequisite for avoiding landslides in rugged landscapes.\n\nLand degradation refers to a human-induced or natural process which impairs the capacity of land to function. Soils degradation involves the acidification, contamination, desertification, erosion or salination.\n\nSoil acidification is beneficial in the case of alkaline soils, but it degrades land when it lowers crop productivity and increases soil vulnerability to contamination and erosion. Soils are often initially acid because their parent materials were acid and initially low in the basic cations (calcium, magnesium, potassium and sodium). Acidification occurs when these elements are leached from the soil profile by rainfall or by the harvesting of forest or agricultural crops. Soil acidification is accelerated by the use of acid-forming nitrogenous fertilizers and by the effects of acid precipitation.\n\nSoil contamination at low levels is often within a soil's capacity to treat and assimilate waste material. Soil biota can treat waste by transforming it; soil colloids can adsorb the waste material. Many waste treatment processes rely on this treatment capacity. Exceeding treatment capacity can damage soil biota and limit soil function. Derelict soils occur where industrial contamination or other development activity damages the soil to such a degree that the land cannot be used safely or productively. Remediation of derelict soil uses principles of geology, physics, chemistry and biology to degrade, attenuate, isolate or remove soil contaminants to restore soil functions and values. Techniques include leaching, air sparging, chemical amendments, phytoremediation, bioremediation and natural degradation.\n\nDesertification is an environmental process of ecosystem degradation in arid and semi-arid regions, often caused by human activity. It is a common misconception that droughts cause desertification. Droughts are common in arid and semiarid lands. Well-managed lands can recover from drought when the rains return. Soil management tools include maintaining soil nutrient and organic matter levels, reduced tillage and increased cover. These practices help to control erosion and maintain productivity during periods when moisture is available. Continued land abuse during droughts, however, increases land degradation. Increased population and livestock pressure on marginal lands accelerates desertification.\n\nErosion of soil is caused by water, wind, ice, and movement in response to gravity. More than one kind of erosion can occur simultaneously. Erosion is distinguished from weathering, since erosion also transports eroded soil away from its place of origin (soil in transit may be described as sediment). Erosion is an intrinsic natural process, but in many places it is greatly increased by human activity, especially poor land use practices. These include agricultural activities which leave the soil bare during times of heavy rain or strong winds, overgrazing, deforestation, and improper construction activity. Improved management can limit erosion. Soil conservation techniques which are employed include changes of land use (such as replacing erosion-prone crops with grass or other soil-binding plants), changes to the timing or type of agricultural operations, terrace building, use of erosion-suppressing cover materials (including cover crops and other plants), limiting disturbance during construction, and avoiding construction during erosion-prone periods.\n\nA serious and long-running water erosion problem occurs in China, on the middle reaches of the Yellow River and the upper reaches of the Yangtze River. From the Yellow River, over 1.6 billion tons of sediment flow each year into the ocean. The sediment originates primarily from water erosion (gully erosion) in the Loess Plateau region of northwest China.\n\nSoil piping is a particular form of soil erosion that occurs below the soil surface. It causes levee and dam failure, as well as sink hole formation. Turbulent flow removes soil starting at the mouth of the seep flow and the subsoil erosion advances up-gradient. The term sand boil is used to describe the appearance of the discharging end of an active soil pipe.\n\nSoil salination is the accumulation of free salts to such an extent that it leads to degradation of the agricultural value of soils and vegetation. Consequences include corrosion damage, reduced plant growth, erosion due to loss of plant cover and soil structure, and water quality problems due to sedimentation. Salination occurs due to a combination of natural and human-caused processes. Arid conditions favour salt accumulation. This is especially apparent when soil parent material is saline. Irrigation of arid lands is especially problematic. All irrigation water has some level of salinity. Irrigation, especially when it involves leakage from canals and overirrigation in the field, often raises the underlying water table. Rapid salination occurs when the land surface is within the capillary fringe of saline groundwater. Soil salinity control involves watertable control and flushing with higher levels of applied water in combination with tile drainage or another form of subsurface drainage.\n\nSoils which contain high levels of particular clays, such as smectites, are often very fertile. For example, the smectite-rich clays of Thailand's Central Plains are among the most productive in the world.\n\nMany farmers in tropical areas, however, struggle to retain organic matter in the soils they work. In recent years, for example, productivity has declined in the low-clay soils of northern Thailand. Farmers initially responded by adding organic matter from termite mounds, but this was unsustainable in the long-term. Scientists experimented with adding bentonite, one of the smectite family of clays, to the soil. In field trials, conducted by scientists from the International Water Management Institute in cooperation with Khon Kaen University and local farmers, this had the effect of helping retain water and nutrients. Supplementing the farmer's usual practice with a single application of 200 kg bentonite per rai (6.26 rai = 1 hectare) resulted in an average yield increase of 73%. More work showed that applying bentonite to degraded sandy soils reduced the risk of crop failure during drought years.\n\nIn 2008, three years after the initial trials, IWMI scientists conducted a survey among 250 farmers in northeast Thailand, half of whom had applied bentonite to their fields. The average improvement for those using the clay addition was 18% higher than for non-clay users. Using the clay had enabled some farmers to switch to growing vegetables, which need more fertile soil. This helped to increase their income. The researchers estimated that 200 farmers in northeast Thailand and 400 in Cambodia had adopted the use of clays, and that a further 20,000 farmers were introduced to the new technique.\n\nIf the soil is too high in clay, adding gypsum, washed river sand and organic matter will balance the composition. Adding organic matter (like ramial chipped wood for instance) to soil which is depleted in nutrients and too high in sand will boost its quality.\n\n"}
{"id": "1335297", "url": "https://en.wikipedia.org/wiki?curid=1335297", "title": "Spaceship Earth", "text": "Spaceship Earth\n\nSpaceship Earth or Spacecraft Earth is a world view encouraging everyone on Earth to act as a harmonious crew working toward the greater good.\n\nThe earliest known use is a passage in Henry George's best known work, \"Progress and Poverty\" (1879).\nFrom book IV, chapter 2: \nIt is a well-provisioned ship, this on which we sail through space. If the bread and beef above decks seem to grow scarce, we but open a hatch and there is a new supply, of which before we never dreamed. And very great command over the services of others comes to those who as the hatches are opened are permitted to say, \"This is mine!\"\n\nGeorge Orwell later paraphrases Henry George in \"The Road to Wigan Pier\":\n\nThe world is a raft sailing through space with, potentially, plenty of provisions for everybody; the idea that we must all cooperate and see to it that everyone does his fair share of the work and gets his fair share of the provisions seems so blatantly obvious that one would say that no one could possibly fail to accept it unless he had some corrupt motive for clinging to the present system.\n\nIn 1965 Adlai Stevenson made a famous speech to the UN in which he said:\n\nWe travel together, passengers on a little space ship, dependent on its vulnerable reserves of air and soil; all committed for our safety to its security and peace; preserved from annihilation only by the care, the work, and, I will say, the love we give our fragile craft. We cannot maintain it half fortunate, half miserable, half confident, half despairing, half slave—to the ancient enemies of man—half free in a liberation of resources undreamed of until this day. No craft, no crew can travel safely with such vast contradictions. On their resolution depends the survival of us all.\n\nThe following year, \"Spaceship Earth\" became the title of a book by a friend of Stevenson's, the internationally influential economist Barbara Ward.\n\nAlso in 1966, Kenneth E. Boulding, who was influenced by reading Henry George, used the phrase in the title of an essay, \"The Economics of the Coming Spaceship Earth\". Boulding described the past open economy of apparently illimitable resources, which he said he was tempted to call the \"cowboy economy\", and continued: \"The closed economy of the future might similarly be called the 'spaceman' economy, in which the earth has become a single spaceship, without unlimited reservoirs of anything, either for extraction or for pollution, and in which, therefore, man must find his place in a cyclical ecological system\".\n\nThe phrase was also popularized by Buckminster Fuller, who published a book in 1968 under the title of \"Operating Manual for Spaceship Earth\". This quotation, referring to fossil fuels, reflects his approach: \n…we can make all of humanity successful through science's world-engulfing industrial evolution provided that we are not so foolish as to continue to exhaust in a split second of astronomical history the orderly energy savings of billions of years' energy conservation aboard our Spaceship Earth. These energy savings have been put into our Spaceship's life-regeneration-guaranteeing bank account for use only in self-starter functions.\n\nUnited Nations Secretary-General U Thant spoke of Spaceship Earth on Earth Day March 21, 1971 at the ceremony of the ringing of the Japanese Peace Bell: \"May there only be peaceful and cheerful Earth Days to come for our beautiful Spaceship Earth as it continues to spin and circle in frigid space with its warm and fragile cargo of animate life.\"\n\nSpaceship Earth is the name given to the 50 m diameter geodesic sphere that greets visitors at the entrance of Walt Disney World's Epcot theme park. Housed within the sphere is a dark ride that serves to explore the history of communications and promote Epcot's founding principles, \"[a] belief and pride in man's ability to shape a world that offers hope to people everywhere.\" A previous incarnation of the ride, narrated by actor Jeremy Irons and revised in 2008, was explicit in its message:\n\nLike a grand and miraculous spaceship, our planet has sailed through the universe of time, and for a brief moment, we have been among its many passengers….We now have the ability and the responsibility to build new bridges of acceptance and co-operation between us, to create a better world for ourselves and our children as we continue our amazing journey aboard Spaceship Earth.\n\nDavid Deutsch has pointed out that the picture of Earth as a friendly \"spaceship\" habitat is difficult to defend even in metaphorical sense. The Earth environment is harsh and survival is constant struggle for life, including whole species extinction. Humans wouldn't be able to live in most of the areas where they are living now without knowledge necessary to build life-support systems such as houses, heating, water supply, etc.\n\nThe term \"Spaceship Earth\" is frequently used on the labels of Emanuel Bronner's products to refer to the Earth.\n\n"}
{"id": "10854000", "url": "https://en.wikipedia.org/wiki?curid=10854000", "title": "Statistical study of energy data", "text": "Statistical study of energy data\n\nEnergy statistics refers to collecting, compiling, analyzing and disseminating data on commodities such as coal, crude oil, natural gas, electricity, or renewable energy sources (biomass, geothermal, wind or solar energy), when they are used for the energy they contain. Energy is the capability of some substances, resulting from their physico-chemical properties, to do work or produce heat. Some energy commodities, called fuels, release their energy content as heat when they burn. This heat could be used to run an internal or external combustion engine.\n\nThe need to have statistics on energy commodities became obvious during the 1973 oil crisis that brought tenfold increase in petroleum prices. Before the crisis, to have accurate data on global energy supply and demand was not deemed critical. Another concern of energy statistics today is a huge gap in energy use between developed and developing countries. As the gap narrows (\"see picture\"), the pressure on energy supply increases tremendously. \n\nThe data on energy and electricity come from three principal sources:\nThe flows of and trade in energy commodities are measured both in physical units (e.g., metric tons), and, when energy balances are calculated, in energy units (e.g., terajoules or tons of oil equivalent). What makes energy statistics specific and different from other fields of economic statistics is the fact that energy commodities undergo greater number of transformations (flows) than other commodities. In these transformations energy is conserved, as defined by and within the limitations of the first and second laws of thermodynamics. \n\n\n\n"}
{"id": "4354345", "url": "https://en.wikipedia.org/wiki?curid=4354345", "title": "Stone wall", "text": "Stone wall\n\nStone walls are a kind of masonry construction that has been used for thousands of years. The first stone walls were constructed by farmers and primitive people by piling loose field stones into a dry stone wall. Later, mortar and plaster were used, especially in the construction of city walls, castles, and other fortifications before and during the Middle Ages. These stone walls are spread throughout the world in different forms. One of the best example is the Cyclopean Wall in Rajgir, India.\n\nStone walls are usually made of local materials varying from limestone and flint to granite and sandstone. However, the quality of building stone varies greatly, both in its endurance to weathering, resistance to water penetration and in its ability to be worked into regular shapes before construction. Worked stone is usually known as ashlar, and is often used for corners in stone buildings. Granite is very resistant to weathering, while some limestones are very weak. Other limestones, such as Portland stone, are more weather-resistant.\n\nLarge structures are usually made of very thick walls, so that castles and cathedrals possess walls which may be up to 12 feet thick. They normally consist of a layered stone exterior and rubble infill.\n\n"}
{"id": "31880880", "url": "https://en.wikipedia.org/wiki?curid=31880880", "title": "Theoretical foundations of evolutionary psychology", "text": "Theoretical foundations of evolutionary psychology\n\nThe theoretical foundations of evolutionary psychology are the general and specific scientific theories that explain the ultimate origins of psychological traits in terms of evolution. These theories originated with Charles Darwin's work, including his speculations about the evolutionary origins of social instincts in humans. Modern evolutionary psychology, however, is possible only because of advances in evolutionary theory in the 20th century.\n\nEvolutionary psychologists say that natural selection has provided humans with many psychological adaptations, in much the same way that it generated humans' anatomical and physiological adaptations. As with adaptations in general, psychological adaptations are said to be specialized for the environment in which an organism evolved, the environment of evolutionary adaptedness, or EEA. Sexual selection provides organisms with adaptations related to mating. For male mammals, which have a relatively fast reproduction rate, sexual selection leads to adaptations that help them compete for females. For female mammals, with a relatively slow reproduction rate, sexual selection leads to choosiness, which helps females select higher quality mates. Charles Darwin described both natural selection and sexual selection, but he relied on group selection to explain the evolution of self-sacrificing behavior. Group selection is a weak explanation because in any group the less self-sacrificing animals will be more likely to survive and the group will become less self-sacrificing.\n\nIn 1964, William D. Hamilton proposed inclusive fitness theory, emphasizing a \"gene's-eye\" view of evolution. Hamilton noted that individuals can increase the replication of their genes into the next generation by helping close relatives with whom they share genes survive and reproduce. According to \"Hamilton's rule\", a self-sacrificing behavior can evolve if it helps close relatives so much that it more than compensates for the individual animal's sacrifice. Inclusive fitness theory resolved the issue of how \"altruism\" evolved. Other theories also help explain the evolution of altruistic behavior, including evolutionary game theory, tit-for-tat reciprocity, and generalized reciprocity. These theories not only help explain the development of altruistic behavior but also account for hostility toward cheaters (individuals that take advantage of others' altruism).\n\nSeveral mid-level evolutionary theories inform evolutionary psychology. The r/K selection theory proposes that some species prosper by having many offspring while others follow the strategy of having fewer offspring but investing much more in each one. Humans follow the second strategy. Parental investment theory explains how parents invest more or less in individual offspring based on how successful those offspring are likely to be, and thus how much they might improve the parents' inclusive fitness. According to the Trivers-Willard hypothesis, parents in good conditions tend to invest more in sons (who are best able to take advantage of good conditions), while parents in poor conditions tend to invest more in daughters (who are best able to have successful offspring even in poor conditions). According to life history theory, animals evolve life histories to match their environments, determining details such as age at first reproduction and number of offspring. Dual inheritance theory posits that genes and human culture have interacted, with genes affecting the development of culture and culture, in turn, affecting human evolution on a genetic level (see also the Baldwin effect).\n\nCritics of evolutionary psychology have sometimes challenged its theoretical underpinnings, saying that humans never developed powerful social instincts through natural selection and that the hypotheses of evolutionary psychologists are merely just-so-stories.\n\nEvolutionary psychology primarily uses the theories of natural selection, sexual selection, and inclusive fitness to explain the evolution of psychological adaptations.\n\nEvolutionary psychology is sometimes seen not simply as a subdiscipline of psychology but as a metatheoretical framework in which \"the entire field of psychology can be examined.\"\n\nEvolutionary psychologists consider Charles Darwin's theory of natural selection to be important to an understanding of psychology. Natural selection occurs because individual organisms who are genetically better suited to the current environment leave more descendants, and their genes spread through the population, thus explaining why organisms fit their environments so closely. This process is slow and cumulative, with new traits layered over older traits. The advantages created by natural selection are known as adaptations. Evolutionary psychologists say that animals, just as they evolve physical adaptations, evolve psychological adaptations.\n\nEvolutionary psychologists emphasize that natural selection mostly generates specialized adaptations, which are more efficient than general adaptations. They point out that natural selection operates slowly, and that adaptations are sometimes out of date when the environment changes rapidly. In the case of humans, evolutionary psychologists say that much of human nature was shaped during the stone age and may not match the contemporary environment.\n\nSexual selection favors traits that provide mating advantages, such as the peacock's tail, even if these same traits are usually hindrances. Evolutionary psychologists point out that, unlike natural selection, sexual selection typically leads to the evolution of sex differences. Sex differences typically make reproduction faster for one sex and slower for the other, in which case mates are relatively scarce for the faster sex. Sexual selection favors traits that increase the number of mates for the fast sex and the quality of mates for the slow sex. For mammals, the female has the slower reproduction rate. Males typically evolve either traits to help them fight other males or traits to impress females. Females typically evolve greater abilities to discern the qualities of males, such as choosiness in mating.\n\nInclusive fitness theory, proposed by William D. Hamilton, emphasized a \"gene's-eye\" view of evolution. Hamilton noted that what evolution ultimately selects are genes, not groups or species. From this perspective, individuals can increase the replication of their genes into the next generation not only directly via reproduction, by also indirectly helping close relatives with whom they share genes survive and reproduce. General evolutionary theory, in its modern form, \"is\" essentially inclusive fitness theory.\n\nInclusive fitness theory resolved the issue of how \"altruism\" evolved. The dominant, pre-Hamiltonian view was that altruism evolved via group selection: the notion that altruism evolved for the benefit of the group. The problem with this was that if one organism in a group incurred any fitness costs on itself for the benefit of others in the group, (i.e. acted \"altruistically\"), then that organism would reduce its own ability to survive and/or reproduce, therefore reducing its chances of passing on its altruistic traits.\n\nFurthermore, the organism that benefited from that altruistic act and only acted on behalf of its own fitness would increase its own chance of survival and/or reproduction, thus increasing its chances of passing on its \"selfish\" traits.\nInclusive fitness resolved \"the problem of altruism\" by demonstrating that altruism can evolve via kin selection as expressed in Hamilton's rule:\ncost < relatedness × benefit\nIn other words, altruism can evolve as long as the fitness \"cost\" of the altruistic act on the part of the actor is less than the \"degree of genetic relatedness\" of the recipient times the fitness \"benefit\" to that recipient.\nThis perspective reflects what is referred to as the gene-centered view of evolution and demonstrates that group selection is a very weak selective force.\n\nMiddle-level evolutionary theories are consistent with general evolutionary theory, but focus on certain domains of functioning (Buss, 2011) Specific evolutionary psychology hypotheses may be derivative from a mid-level theory (Buss, 2011). Three very important middle-level evolutionary theories were contributed by Robert Trivers as well as Robert MacArthur and E. O. Wilson\n\n"}
{"id": "1103359", "url": "https://en.wikipedia.org/wiki?curid=1103359", "title": "Ultra-high vacuum", "text": "Ultra-high vacuum\n\nUltra-high vacuum (UHV) is the vacuum regime characterised by pressures lower than about 10 pascal or 100 nanopascals (10 mbar, ~10 torr). UHV conditions are created by pumping the gas out of a UHV chamber. At these low pressures the mean free path of a gas molecule is greater than approximately 40 km, so the gas is in free molecular flow, and gas molecules will collide with the chamber walls many times before colliding with each other. Almost all molecular interactions therefore take place on various surfaces in the chamber.\n\nUHV conditions are integral to scientific research. Surface science experiments often require a chemically clean sample surface with the absence of any unwanted adsorbates. Surface analysis tools such as X-ray photoelectron spectroscopy and low energy ion scattering require UHV conditions for the transmission of electron or ion beams. For the same reason, beam pipes in particle accelerators such as the Large Hadron Collider are kept at UHV.\n\n\nMaintaining UHV conditions requires the use of unusual materials for equipment. Heating of the entire system above 100 °C for many hours (\"baking\") to remove water and other trace gases which adsorb on the surfaces of the chamber is required upon \"cycling\" the equipment to atmosphere. To save time, energy, and integrity of the UHV volume an \"interlock\" is often used. The interlock volume has one door or valve facing the UHV side of the volume, and another door against atmospheric pressure through which samples or workpieces are initially introduced. After sample introduction and assuring that the door against atmosphere is closed, the interlock volume is typically pumped down to a medium-high vacuum. In some cases the workpiece itself is baked out or otherwise pre-cleaned under this medium-high vacuum. The gateway to the UHV chamber is then opened, the workpiece transferred to the UHV by robotic means or by other contrivance if necessary, and the UHV valve re-closed. While the initial workpiece is being processed under UHV, a subsequent sample can be introduced into the interlock volume, pre-cleaned, and so-on and so-forth, saving much time. Although a \"puff\" of gas is generally released into the UHV system when the valve to the interlock volume is opened, the UHV system pumps can generally snatch this gas away before it has time to adsorb onto the UHV surfaces. In a system well designed with suitable interlocks, the UHV components seldom need bakeout and the UHV may improve over time even as workpieces are introduced and removed.\n\nMany common materials are used sparingly if at all due to high vapor pressure, high adsorptivity or absorptivity resulting in subsequent troublesome outgassing, or high permeability in the face of differential pressure (i.e.: \"through-gassing\"):\n\nTechnical limitations: \n\nUltra-high vacuum is necessary for many surface analytic techniques such as:\n\nUHV is necessary for these applications to reduce surface contamination, by reducing the number of molecules reaching the sample over a given time period. At 0.1 mPa (10 Torr), it only takes 1 second to cover a surface with a contaminant, so much lower pressures are needed for long experiments.\n\nUHV is also required for:\nand, while not compulsory, can prove beneficial in applications such as:\n\nTypically, UHV requires:\n\nOutgassing is a problem for UHV systems. Outgassing can occur from two sources: surfaces and bulk materials. Outgassing from bulk materials is minimized by selection of materials with low vapor pressures (such as glass, stainless steel, and ceramics) for everything inside the system. Materials which are not generally considered absorbent can outgas, including most plastics and some metals. For example, vessels lined with a highly gas-permeable material such as palladium (which is a high-capacity hydrogen sponge) create special outgassing problems.\n\nOutgassing from surfaces is a subtler problem. At extremely low pressures, more gas molecules are adsorbed on the walls than are floating in the chamber, so the total surface area inside a chamber is more important than its volume for reaching UHV. Water is a significant source of outgassing because a thin layer of water vapor rapidly adsorbs to everything whenever the chamber is opened to air. Water evaporates from surfaces too slowly to be fully removed at room temperature, but just fast enough to present a continuous level of background contamination. Removal of water and similar gases generally requires baking the UHV system at 200 to 400 °C while vacuum pumps are running. During chamber use, the walls of the chamber may be chilled using liquid nitrogen to reduce outgassing further.\n\nHydrogen and carbon monoxide are the most common background gases in a well-designed, well-baked UHV system. Both Hydrogen and CO diffuse out from the grain boundaries in stainless steel. Helium could diffuse through the steel and glass from the outside air, but this effect is usually negligible due to the low abundance of He in the atmosphere.\n\nThere is no single vacuum pump that can operate all the way from atmospheric pressure to ultra-high vacuum. Instead, a series of different pumps is used, according to the appropriate pressure range for each pump. Pumps commonly used to achieve UHV include:\n\nUHV pressures are measured with an ion gauge, either a hot filament or an inverted magnetron type.\n\nMetal seals, with knife edges on both sides cutting into a soft, copper gasket. This all-metal seal can maintain pressures down to 100 pPa (~10 Torr).\n\nMeasurement of high vacuum is done using a \"nonabsolute gauge\" that measures a pressure-related property of the vacuum, for example, its thermal conductivity. See, for example, Pacey. These gauges must be calibrated. The gauges capable of measuring the lowest pressures are magnetic gauges based upon the pressure dependence of the current in a spontaneous gas discharge in intersecting electric and magnetic fields.\n\nA UHV manipulator allows an object which is inside a vacuum chamber and under vacuum to be mechanically positioned. It may provide rotary\nmotion, linear motion, or a combination of both. The most complex devices give motion in three axes and rotations around two of those axes. To generate the mechanical movement inside the chamber, two basic mechanisms are commonly employed: a mechanical coupling through the vacuum wall (using a vacuum-tight seal around the coupling), or a magnetic coupling that transfers motion from air-side to vacuum-side. Various forms of motion control are available for manipulators, such as knobs, handwheels, motors, stepping motors, piezoelectric motors, and pneumatics.\n\nThe manipulator or sample holder may include features that allow additional control and testing of a sample, such as the ability to apply heat, cooling, voltage, or a magnetic field. Sample heating can be accomplished by electron bombardment or thermal radiation. For electron bombardment, the sample holder is equipped with a filament which emits electrons when biased at a high negative potential. The impact of the\nelectrons bombarding the sample at high energy causes it to heat. For thermal radiation, a filament is mounted close to the sample and resistively heated to high temperature. The infrared energy from the filament heats the sample.\n\n\n"}
{"id": "31596828", "url": "https://en.wikipedia.org/wiki?curid=31596828", "title": "Unequal crossing over", "text": "Unequal crossing over\n\nUnequal crossing over is a type of gene duplication or deletion event that deletes a sequence in one strand and replaces it with a duplication from its sister chromatid in mitosis or from its homologous chromosome during meiosis. It is a type of chromosomal crossover between homologous sequences that are not paired precisely. Normally genes are responsible for occurrence of crossing over. It exchanges sequences of different links between chromosomes. Along with gene conversion, it is believed to be the main driver for the generation of gene duplications and is a source of mutation in the genome.\n\nDuring meiosis, the duplicated chromosomes (chromatids) in eukaryotic organisms are attached to each other in the centromere region and are thus paired. The maternal and paternal chromosomes then align alongside each other. During this time, recombination can take place via crossing over of sections of the paternal and maternal chromatids and leads to reciprocal recombination or non-reciprocal recombination. Unequal crossing over requires a measure of similarity between the sequences for misalignment to occur. The more similarity within the sequences, the more likely unequal crossing over will occur. One of the sequences is thus lost and replaced with the duplication of another sequence.\n\nWhen two sequences are misaligned, unequal crossing over may create a tandem repeat on one chromosome and a deletion on the other. The rate of unequal crossing over will increase with the number of repeated sequences around the duplication. This is because these repeated sequences will pair together, allowing for the mismatch in the cross over point to occur.\n\nUnequal crossing over is the process most responsible for creating regional gene duplications in the genome. Repeated rounds of unequal crossing over cause the homogenization of the two sequences. With the increase in the duplicates, unequal crossing over can lead to dosage imbalance in the genome and can be highly deleterious.\n\nIn unequal crossing over, there can be large sequence exchanges between the chromosomes. Compared with gene conversion, which can only transfer a maximum of 1,500 base pairs, unequal crossing over in yeast rDNA genes has been found to transfer about 20,000 base pairs in a single crossover event Unequal crossover can be followed by the concerted evolution of duplicated sequences.\n\nIt has been suggested that longer intron found between two beta-globin genes are a response to deleterious selection from unequal crossing over in the beta-globin genes. Comparisons between alpha-globin, which does not have long introns, and beta-globin genes show that alpha-globin have 50 times higher concerted evolution.\n\nWhen unequal crossing over creates a gene duplication, the duplicate has 4 evolutionary fates. This is due to the fact that purifying selection acting on a duplicated copy is not very strong. Now that there is a redundant copy, neutral mutations can act on the duplicate. Most commonly the neutral mutations will continue until the duplicate becomes a pseudogene. If the duplicate copy increases the dosage effect of the gene product, then the duplicate may be retained as a redundant copy. Neofunctionalization is also a possibility: the duplicated copy acquires a mutation that gives it a different function than its ancestor. If both copies acquire mutations, it is possible that a subfunctional event occurs. This happens when both of the duplicated sequences have a more specialized function than the ancestral copy\n\nGene duplications are the main reason for the increase of genome size, and as unequal crossing over is the main mechanism for gene duplication, unequal crossing over contributes to genome size evolution is the most common regional duplication event that increases the size of the genome.\n\nWhen viewing the genome of a eukaryote, a striking observation is the large amount of tandem, repetitive DNA sequences that make up a large portion of the genome. For example, over 50% of the \"Dipodmys ordii\" genome is made up of three specific repeats. \"Drosophila virilis\" has three sequences that make up 40% of the genome, and 35% of the \"Absidia glauca\" is repetitive DNA sequences. These short sequences have no selection pressure acting on them and the frequency of the repeats can be changed by unequal crossing over.\n"}
{"id": "5547312", "url": "https://en.wikipedia.org/wiki?curid=5547312", "title": "Vacuum deposition", "text": "Vacuum deposition\n\nVacuum deposition is a family of processes used to deposit layers of material atom-by-atom or molecule-by-molecule on a solid surface. These processes operate at pressures well below atmospheric pressure (i.e., vacuum). The deposited layers can range from a thickness of one atom up to millimeters, forming freestanding structures. Multiple layers of different materials can be used, for example to form optical coatings. The process can be qualified based on the vapor source; physical vapor deposition uses a liquid or solid source and chemical vapor deposition uses a chemical vapor.\n\nThe vacuum environment may serve one or more purposes:\n\nCondensing particles can be generated in various ways:\n\nIn reactive deposition, the depositing material reacts either with a component of the gaseous environment (Ti + N → TiN) or with a co-depositing species (Ti + C → TiC). A plasma environment aids in activating gaseous species (N → 2N) and in decomposition of chemical vapor precursors (SiH → Si + 4H). The plasma may also be used to provide ions for vaporization by sputtering or for bombardment of the substrate for sputter cleaning and for bombardment of the depositing material to densify the structure and tailor properties (ion plating).\n\nWhen the vapor source is a liquid or solid the process is called physical vapor deposition (PVD). When the source is a chemical vapor precursor, the process is called chemical vapor deposition (CVD). The latter has several variants: \"low-pressure chemical vapor deposition\" (LPCVD), Plasma-enhanced chemical vapor deposition (PECVD), and \"plasma-assisted CVD\" (PACVD). Often a combination of PVD and CVD processes are used in the same or connected processing chambers.\n\n\nA thickness of less than one micrometre is generally called a thin film while a thickness greater than one micrometre is called a coating.\n\n\n"}
{"id": "469990", "url": "https://en.wikipedia.org/wiki?curid=469990", "title": "Whirlpool", "text": "Whirlpool\n\nA whirlpool is a body of rotating water produced by the meeting of opposing currents. The vast majority of whirlpools are not very powerful and very small whirlpools can be easily seen when a bath or a sink is draining. More powerful ones in seas or oceans may be termed maelstroms. \"Vortex\" is the proper term for any whirlpool that has a downdraft.\n\nIn oceans, in narrow straits with fast flowing water, whirlpools are normally caused by tides; there are few stories of large ships ever being sucked into such a maelstrom, although smaller craft are in danger. Smaller whirlpools also appear at the base of many waterfalls and can also be observed downstream from manmade structures such as weirs and dams. In the case of powerful waterfalls, like Niagara Falls, these whirlpools can be quite strong.\n\nThe Maelstrom of Saltstraumen is the Earth's strongest maelstrom, and is located close to the Arctic Circle, round the bay on the Highway 17, south-east of the city of Bodø, Norway. The strait at its narrowest is in width and water \"funnels\" through the channel four times a day. It is estimated that of water passes the narrow strait during this event. The water is creamy in colour and most turbulent during high tide, which is witnessed by thousands of tourists. It reaches speeds of , with mean speed of about . As navigation is dangerous in this strait only a small slot of time is available for large ships to pass through. Its impressive strength is caused by the world's strongest tide occurring in the same location during the new and full moon. A narrow channel of length connects the outer Saltfjord with its extension, the large Skjerstadfjord, causing a colossal tide which in turn produces the Saltstraumen maelstrom.\n\nMoskstraumen is an unusual system of whirlpools in the open seas in the Lofoten Islands off the Norwegian coast. It is the second strongest whirlpool in the world with flow currents reaching speeds as high as . It finds mention in several books and movies.\n\nThe Moskstraumen is formed by the combination of powerful semi-diurnal tides and the unusual shape of the seabed, with a shallow ridge between the Moskenesøya and Værøy islands which amplifies and whirls the tidal currents.\n\nThe fictional depictions of the Maelstrom by Edgar Allan Poe, Jules Verne, and Cixin Liu describe it as a gigantic circular vortex that reaches the bottom of the ocean, when in fact it is a set of currents and crosscurrents with a rate of . Poe described this phenomenon in his short story \"A Descent into the Maelstrom,\" which in 1841 was the first to use the word \"maelstrom\" in the English language; in this story related to the Lofoten Maelstrom, two fishermen are swallowed by the maelstrom while one survives miraculously.\n\nThe Corryvreckan is a narrow strait between the islands of Jura and Scarba, in Argyll and Bute, on the northern side of the Gulf of Corryvreckan, Scotland. It is the third-largest whirlpool in the world. Flood tides and inflow from the Firth of Lorne to the west can drive the waters of Corryvreckan to waves of over , and the roar of the resulting maelstrom, which reaches speeds of , can be heard away. Though it was initially classified as non-navigable by the British navy it was later categorized as \"extremely dangerous\".\n\nA documentary team from Scottish independent producers Northlight Productions once threw a mannequin into the Corryvreckan (\"the Hag\") with a life jacket and depth gauge. The mannequin was swallowed and spat up far down current with a depth gauge reading of with evidence of being dragged along the bottom for a great distance.\n\nOld Sow whirlpool is located between Deer Island, New Brunswick, Canada, and Moose Island, Eastport, Maine, USA. It is given the epithet \"pig-like\" as it makes a screeching noise when the vortex is at its full fury and reaches speeds of up to . The smaller whirlpools around this Old Sow are known as \"Piglets.\n\nThe Naruto whirlpools are located in the Naruto Strait near Awaji Island in Japan, which have speeds of .\n\nSkookumchuck Narrows is a tidal rapids that develops whirlpools, on the Sunshine Coast, Canada with current speeds exceeding .\n\nFrench Pass () is a narrow and treacherous stretch of water that separates D'Urville Island from the north end of the South Island of New Zealand. In 2000 a whirlpool there caught student divers, resulting in fatalities.\n\nThere was a short-lived whirlpool that sucked in a portion of the 1300 acre (~530 hectares) Lake Peigneur in Louisiana, United States after a drilling mishap in November 1980. This was not a naturally occurring whirlpool, but a man-made disaster caused by underwater drillers breaking through the roof of a salt mine. The lake then drained into the mine until the mine filled and the water levels equalized but the ten-foot deep lake was now 1,300 feet deep. This mishap resulted in destruction of five houses, loss of nineteen barges and eight tug boats, oil rigs, a mobile home, and most of a botanical garden. The adjacent settlement of Jefferson Island was reduced in area by 10%. A crater 0.5-mile (~1km) across was left behind. Nine of the barges which had sunk floated back.\n\nA more recent example of a man-made whirlpool that received significant media coverage was in early June 2015, when an intake vortex formed in Lake Texoma, on the Oklahoma–Texas border, near the floodgates of the dam that forms the lake. At the time of the whirlpool's formation, the lake was being drained after reaching its highest level ever. The Army Corps of Engineers, which operates the dam and lake, expected that the whirlpool would last until the lake reached normal seasonal levels by late July.\n\nPowerful whirlpools have killed unlucky seafarers, but their power tends to be exaggerated by laymen. There are virtually no stories of large ships ever being sucked into a whirlpool. Tales like those by Paul the Deacon, Edgar Allan Poe, and Jules Verne are entirely fictional.\n\nHowever, temporary whirlpools caused by major engineering disasters are capable of submerging large ships. A prominent example is the drilling disaster that occurred on November 20, 1980, in Lake Peigneur. A drilling platform, eleven barges, several trees, and multiple acres of the surrounding terrain were submerged by the resulting whirlpool. Days after the disaster, once the water pressure equalized, nine of the eleven sunken barges popped out of the whirlpool and refloated on the lake's surface.\n\nApart from Poe and Verne other literary source is of the 1500s, of Olaus Magnus, a Swedish Bishop, who had stated that the maelstrom which was more powerful than \"The Odyssey\" destroyed ships which sank to the bottom of the sea, and even whales were sucked in. Pytheas, the Greek historian, also mentioned that maelstroms swallowed ships and threw them up again.\n\nCharybdis in Greek mythology was later rationalized as a whirlpool, which sucked entire ships into its fold in the narrow coast of Sicily, a disaster faced by navigators.\n\nIn the 8th century, Paul the Deacon, who had lived among the Belgii, described tidal bores and the maelstrom for a Mediterranean audience unused to such violent tidal surges:\n\nThree of the most notable literary references to the Lofoten Maelstrom date from the nineteenth century. The first is the Edgar Allan Poe short story \"A Descent into the Maelström\" (1841). The second is \"20,000 Leagues Under the Sea\" (1870), the famous novel by Jules Verne. At the end of this novel, Captain Nemo seems to commit suicide, sending his \"Nautilus\" submarine into the Maelstrom (although in Verne's sequel Nemo and the Nautilus were seen to have survived). The \"Norway maelstrom\" is also mentioned in Herman Melville's \"Moby-Dick\".\n\nIn the 'Life of St Columba', the author, Adomnan of Iona', attributes to the saint miraculous knowledge of a particular bishop who ran into a whirlpool off the coast of Ireland. In Adomnan's narrative, he quotes Columba saying\n\nOne of the earliest uses in English of the Scandinavian word (\"malström\" or \"malstrøm\") was by Edgar Allan Poe in his short story \"A Descent into the Maelström\" (1841). In turn, the Nordic word is derived from the Dutch \"maelstrom\", modern spelling \"maalstroom\", from \"malen\" (\"to grind\") and \"stroom\" (\"stream\"), to form the meaning \"grinding current\" or literally \"mill-stream\", in the sense of milling (grinding) grain.\n\n\n\n"}
