{"id": "41401861", "url": "https://en.wikipedia.org/wiki?curid=41401861", "title": "AMSilk", "text": "AMSilk\n\nAMSilk is an industrial supplier of synthetic silk biopolymers. The polymers are biocompatible and breathable. The company was founded in 2008 and has its headquarters at the IZB in Planegg near Munich. AMSilk is an industrial biotechnology company with a proprietary production process for their silk materials.\n\nAMSilk is known for creating a biodegradable running shoe for Adidas made of recombinant spider silk. Jens Klein, CEO of AMSilk, said during an interview that the biodegradable material can help reduce the amount of waste that has to be burned or pollutes the environment.\n\nAMSilk is also developing breast implants made of biodegradable spider silk in collaboration with the German company Polytech.\n\n"}
{"id": "30744522", "url": "https://en.wikipedia.org/wiki?curid=30744522", "title": "Applications of evolution", "text": "Applications of evolution\n\nEvolutionary biology, in particular the understanding of how organisms evolve through natural selection, is an area of science with many practical applications. Creationists often claim that the theory of evolution lacks any practical applications; however, this claim has been refuted by scientists.\n\nThe evolutionary approach is key to much current research in biology that does not set out to study evolution per se, especially in organismal biology and ecology. For example, evolutionary thinking is key to life history theory. Annotation of genes and their function relies heavily on comparative, that is evolutionary, approaches. The field of evolutionary developmental biology investigates how developmental processes work by using the comparative method to determine how they evolved.\n\nA major technological application of evolution is artificial selection, which is the intentional selection of certain traits in a population of organisms. Humans have used artificial selection for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA in molecular biology. It is also possible to use repeated rounds of mutation and selection to evolve proteins with particular properties, such as modified enzymes or new antibodies, in a process called directed evolution.\n\nAntibiotic resistance can be a result of point mutations in the pathogen genome at a rate of about 1 in 10 per chromosomal replication. The antibiotic action against the pathogen can be seen as an environmental pressure; those bacteria which have a mutation allowing them to survive will live on to reproduce. They will then pass this trait to their offspring, which will result in a fully resistant colony.\n\nUnderstanding the changes that have occurred during organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation, such as crystallins and the melanocortin 1 receptor. Similarly, comparing the genome of the Antarctic icefish, which lacks red blood cells, to close relatives such as the Antarctic rockcod revealed genes needed to make these blood cells.\n\nAs evolution can produce highly optimised processes and networks, it has many applications in computer science. Here, simulations of evolution using evolutionary algorithms and artificial life started with the work of Nils Aall Barricelli in the 1960s, and was extended by Alex Fraser, who published a series of papers on simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s and early 1970s, who used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimise the design of systems.\n"}
{"id": "36136905", "url": "https://en.wikipedia.org/wiki?curid=36136905", "title": "Avalon explosion", "text": "Avalon explosion\n\nThe Avalon explosion, named from the Precambrian fauna of the Avalon Peninsula, is a proposed evolutionary radiation in the history of the Animalia, about 575 million years ago, some 33 million years earlier than the Cambrian explosion.\n\nTrace fossils of these Avalon organisms have been found worldwide, and represent the earliest known complex multicellular organisms. The Avalon explosion produced the Ediacaran biota. The biota largely disappeared contemporaneously with the rapid increase in biodiversity known as the Cambrian explosion.\n\nThe Avalon explosion appears similar to the Cambrian explosion in the rapid increase in diversity of morphologies in a relatively small time frame, followed by diversification within the established body plans, a pattern similar to that observed in other evolutionary events.\n\nThe Avalon explosion was proposed in 2008 by Virginia Tech paleontologists through the analysis of the morphological space change in several Ediacaran assemblages. The discovery suggests that in the early evolution of animals, there may have been more than one explosive event. The original analysis has been the subject of dispute in the literature.\n"}
{"id": "48114438", "url": "https://en.wikipedia.org/wiki?curid=48114438", "title": "Bobdownsite", "text": "Bobdownsite\n\nBobdownsite is the fluorine bearing mineral of the whitlockite group of phosphate minerals whose formula is Ca(Mg)(PO)(POF). It is isotypic with whitlockite and was misidentified as such until proper chemical analysis. Whitlockites structure and relationships with other phosphate compounds has been extensively studied. Bobdownsite was first recovered from Big Fish River, Yukon, Canada from a Lower Cretaceous outcrop of bedded ironstones and shales. Bobdownsite is named after Robert Terrace Downs a professor of mineralogy in the Department of Geosciences at the University of Arizona, who lived and worked in the Yukon Territory in the 1970s. Bobdownsite is unique because it is the first known naturally forming phosphate to contain a P-F bond.\n\nBobdownsite is closely related to whitlockite and merrillite.\n\nMember of the whitlockite group:\n\nOther members of group:\n\nBobdownsite is originally found from Big Fish River, Yukon, Canada (about Latitude 68°28'N, Longitude 136°30'W). Bobdownsite occurs in a vein from an east- west- trending faulted vein. Samples recovered from this exposure are found with Lower Cretaceous bedded ironstones and shales. Bobdownsite is a phosphate mineral with its exposure downstream from phosphate nodule slopes. The vein at the exposure is composed of anhedral and euhedral crystals and is 0 to 4 cm wide. It occurs associated with siderite, lazulite, kulanite, gormanite, quartz and collinsite. Bobdownsite has also been reported from the Tip Top mine of Custer County, South Dakota. Furthermore, it has been found with other witlockite group minerals in Martian meteorites.\n\nBobdownsite, whitlockite, and merrillite are all important phosphate minerals found closely related in lunar rocks. Merrillite crystals with considerable amounts of fluorine and traces of chlorine were found in several Martian meteorites. But more research conducted on bobdownsite suggests that an F content of greater that about 0.9 wt%, including F- bearing merrillite, is bobdownsite.\n\nIn Yukon, Canada bobdownsite is found composed of anhedral and euhedral crystals, tabular, colorless in transmitted light, transparent, with a white streak and a vitreous luster. The Tip Top mine sample has druses of pale purple, yellow, or transparent colorless rhombohedral crystals, brittle, with a hardness about 5, no cleavage, parting or twinning, and an uneven and subconchoidal fracture. Bobdownsite has a density of 3.14 and 3.16 g/cm and is insoluble in water, acetone, or hydrochloric acid.\n\nBobdownsite is isostructural with whitlockite having a trigonal crystal system and R3c space group. The point group is 3m with unit cell parameters \"ɑ\" 1.3224(3) Å, c 37.70(2) Å, \"V\" 3420.7(6) Å. Bobdownsite has trigonal - ditrigonal pyramid tabular, euhedral crystals. The mineral is characterized by [M(PO)] ligands held together by intralayer Ca cations to form layers parallel to (001). The layers parallel to (001) are further linked by intralayer Ca, Na, and Sr cations along [001]. These ligands can be referred to as the \"Mg pinwheel\". The pinwheels are formed in closest packed planar arrangement using the term cubic eutaxy. For this arrangement to happen there are tetrahedral and octahedral sites. The octahedral sites contain POF tetrahedral groups which are approximately in the midplane between the two pinwheel layers and are each coordinated to the closest two interlayer Ca cations by the oxygen. The P position in the POF is located either just above or just below the midplane between the two pinwheel layers, leading to the F to point either along +c or -c. The metal of the whitlockite- type compounds is slightly offset from the origin along c and is octahedrally coordinated residing at the center of the M pinwheel. The layers stack in an ABAB fashion along the c, there is no obvious eutaxy, they can be described as chains of corner-sharing LiO tetrahedra cross connected by POF tetrahedra.\n"}
{"id": "25646039", "url": "https://en.wikipedia.org/wiki?curid=25646039", "title": "Boston Journal of Natural History", "text": "Boston Journal of Natural History\n\nThe Boston Journal of Natural History (1834-1863) was a scholarly journal published by the Boston Society of Natural History in mid-19th century Massachusetts. Contributors included Charles T. Jackson, Augustus A. Gould, and others. Each volume featured lithographic illustrations, some in color, drawn/engraved by E.W. Bouvé, B.F. Nutting, A. Sonrel, \"et al.\" and printed by Pendleton's Lithography and other firms.\n\nThe journal was continued by \"Memoirs Read Before the Boston Society of Natural History\" in 1863.\n\n"}
{"id": "37970127", "url": "https://en.wikipedia.org/wiki?curid=37970127", "title": "Buttered toast phenomenon", "text": "Buttered toast phenomenon\n\nThe buttered toast phenomenon is an observation that buttered toast tends to land butter-side down after it falls. It is used an idiom representing pessimistic outlooks. Various people have attempted to determine whether there is an actual tendency for bread to fall in this fashion, with varying results.\n\nThe phenomenon is said to be an old proverb from \"the north country.\" Written accounts can be traced to the mid-19th century. The phenomenon is often attributed to a parodic poem of James Payn from 1884:\n\nIn the past, this has often been considered just a pessimistic belief. A study by the BBC's television series \"Q.E.D.\" found that when toast is thrown in the air, it lands butter-side down just one-half of the time (as would be predicted by chance). However, several scientific studies have found that when toast is dropped from a table (as opposed to being thrown in the air), it does fall butter-side down. A study by Robert A J Matthews won the Ig Nobel Prize in 1996.\n\nWhen toast falls out of one's hand, it does so at an angle, by nature of it having slipped from its previous position, then the toast rotates. Given that tables are usually between two and six feet (0.7 to 1.83 meters), there is enough time for the toast to rotate about one-half of a turn, and thus lands upside down relative to its original position. Since the original position is usually butter-side up, the toast lands butter-side down. However, if the table is over 10 feet (3 meters) tall, the toast will rotate a full 360 degrees, and land butter-side up. Also, if the toast travels horizontally at over 3.6 miles per hour (1.6 m/s), the toast will not rotate enough to land butter-side down. In fact, the phenomenon is caused by fundamental physical constants.\n\nThe added weight of the butter has no effect on the falling process, since the butter spreads throughout the slice.\n\nThe following findings are from Mythbusters:\n\nIn the 2010 M. Night Shyamalan film \"Devil\", a group of adults in an elevator become trapped by unknown forces. Slowly over the course of the film, people begin to die in the elevator as the power blinks on and off, pinning the people against each other and creating a false narrative that the others are murderers (However, it is common opinion that the name of the film spoiled this false narrative). In the film, supporting cast member and fictional security guard Ramirez, in his suspicion that there may be unholy forces at play, dropped a slice of toast with jelly on one side, and dropped it on the floor. The toast landed on the jelly-side, after which Ramirez coined the phrase \"Jelly Side Down,\" and subsequently 'proved' that the devil was nearby. However, there is some dispute in the film about whether or not this did actually prove the devil was near.\n\nThis phenomenon was also demonstrated and parodied in the October 29, 2013 \"Nostalgia Critic\" review of \"Devil\", (Reuploaded on July 23, 2016 by Channel Awesome on YouTube) in which actor Doug Walker plays a priest who uses multiple food items to determine whether or not the Devil is near. After the other items show no sign of the Devil, the priest tosses a piece of toast as the last test. The toast lands jelly side down which means the Devil is near and the end of the world is imminent. Everybody in the church screams and panics as the priest repeatedly shouts, \"Jelly side down!\"\n\n"}
{"id": "45682373", "url": "https://en.wikipedia.org/wiki?curid=45682373", "title": "Character evolution", "text": "Character evolution\n\nCharacter evolution is the process by which a character or trait (a certain body part or property of an organism) evolves along the branches of an evolutionary tree. Character evolution usually refers to single changes within a lineage that make this lineage unique from others. These changes are called character state changes and they are often used in the study of evolution to provide a record of common ancestry. Character state changes can be phenotypic changes, nucleotide substitutions, or amino acid substitutions. These small changes in a species can be identifying features of when exactly a new lineage diverged from an old one. \nIn the study of phylogenetics or cladistics, researchers can look at the characters shared by a collection of species and then group them into what is called a clade. The term clade was coined in 1957 by the biologist Julian Huxley to refer to the result of cladogenesis, a concept Huxley borrowed from Bernhard Rensch. A clade is by definition monophyletic, meaning it contains one ancestor (which can be an organism, a population, or a species) and all its descendants.\n\nNatural Selection is the process by which organisms that are better adapted to their environment are selected to survive and reproduce more offspring. Natural selection selects for the phenotype or the characteristics of an organism that gives the organism a reproductive advantage in which it becomes the gene pool of a population. In addition, mutations also arise in the genome of an individual organism and offspring(s) can inherit such mutations. This genetic variation allows more organisms to adapt to a changing environment.\n\nIt is often the case in the study of phylogenies that the vast majority of organisms of interest are long extinct. It is therefore a matter of speculation to reconstruct what ancestral organisms existed long before the present time, and how the evolutionary process led from one organism to another, and which present-day organisms are most closely related. Character evolution and the character state changes that drive this type of evolution are what help researchers construct these trees in a fashion referred to as maximum parsimony. When talking about phylogenetics, maximum parsimony refers to a method of inferring a phylogenetic tree in a way that minimizes the number of implied character state transformations in the observed data (hence \"maximally parsimonious\"). The basic ideas were presented by James S. Farris in 1970.\n\nAlthough fairly effective, maximum parsimony (like any method of phylogenetic inference) may not recover the true course of evolution for a given feature. For a number of reasons, two organisms can possess a trait not present in their last common ancestor. The phenomena of convergent evolution, parallel evolution, and evolutionary reversals (collectively termed \"homoplasy\") are evolutionary forces that may disrupt the effectiveness of the maximum parsimony method of inferring phylogenetic relationships. However, Rindal and Brower showed that the vast majority of the time, parsimony and model-based phylogenetic analyses of the same data sets gave results that were not significantly different from one another, implying that if parsimony is producing false hypotheses of relationships due to homoplasy, then the Maximum Likelihood or Bayesian methods are doing so as well.\n\nLamarck is best known for his \"Theory of Inheritance of Acquired Characteristics\" in 1801. His theory states that the characteristics an organism acquires throughout its life in order to adapt to its environment are passed down to its offspring. For example, Lamarck believed that the long necks of giraffes evolved as generations of giraffes reached for ever higher leaves of a tree. Their offspring and later generations inherited the resulting long necks.\n\n"}
{"id": "44262036", "url": "https://en.wikipedia.org/wiki?curid=44262036", "title": "Coleridge's theory of life", "text": "Coleridge's theory of life\n\nRomanticism grew largely out of an attempt to understand not just inert nature, but also vital nature. Romantic works in the realm of art and Romantic medicine were a response to the general failure of the application of method of inertial science to reveal the foundational laws and operant principles of vital nature. German romantic science and medicine sought to understand the nature of the life principle identified by John Hunter as distinct from matter itself via Johan Friedrich Blumenbach's \"Bildungstrieb\" and Romantic medicine's \"Lebenskraft\", as well as Röschlaub's development of the Brunonian system of medicine system of John Brown, in his \"excitation theory\" of life (German:\"Erregbarkeit theorie\"), working also with Schelling's \"Naturphilosophie\", the work of Goethe regarding morphology, and the first dynamic conception of physiology of Richard Saumarez. But it is in Samuel Taylor Coleridge that we find the question of life and vital nature most intensely and comprehensively examined, particularly in his Hints towards the Formation of a more Comprehensive Theory of Life (1818), providing the foundation for Romantic philosophy, science and medicine. The work is key to understanding the relationship of Romantic literature and science.\n\nThe Enlightenment had developed a philosophy and science supported by formidable twin pillars: the first the Cartesian split of mind and matter, the second Newtonian physics, with its conquest of inert nature, both of which focused the mind's gaze on things or objects. For Cartesian philosophy, life existed on the side of matter, not mind; and for the physical sciences, the method that had been so productive for revealing the secrets of inert nature, should be equally productive in examining vital nature. The initial attempt to seek the cause and principle of life in matter was challenged by John Hunter, who held that the principle of life was not to be found nor confined within matter, but existed independently of matter itself, and informed or animated it, that is, he implied, it was the unifying or antecedent cause of the things or what Aristotelean philosophy termed \"natura naturata\" (the outer appearances of nature).\nThis reduction of the question of life to matter, and the corollary, that the method of the inertial sciences was the way to understand the very phenomenon of life, that is, its very nature and essence as a power (\"natura naturans\"), not as manifestations through sense-perceptible appearances (\"natura naturata\"), also reduced the individual to a material-mechanical 'thing' and seemed to render human freedom an untenable concept. It was this that Romanticism challenged, seeking instead to find an approach to the essence of nature as being also vital not simply inert, through a systematic method involving not just physics, but physiology (living functions). For Coleridge, quantitative analysis was anti-realist and needed to be grounded in qualitative analysis ('-ologies') (as was the case with Goethe's approach).\nAt the same time, the Romantics had to deal with the idealistic view that life was a 'somewhat' outside of things, such that the things themselves lost any real existence, a stream coming through Hume and Kant, and also infusing the German natural philosophical stream, German idealism, and in particular \"naturphilosophie\", eventuating scientifically in the doctrine of 'vitalism'. For the Romantics, life is independent of and antecedent to nature, but also infused and suspended in nature, not apart from it, As David Bohm expresses it in more modern terms \"In nature nothing remains constant…everything comes from other things and gives rise to other things. This principle is…at the foundation of the possibility of our understanding nature in a rational way.\"\n\nAnd as Coleridge explained, 'this antecedent unity, or cause and principle of each union, it has since the time of Bacon and Kepler been customary to call a law.\" And as law, \"we derive from it a progressive insight into the necessity and generation of the phenomena of which it is the law.\"\n\nColeridge's was the dominant mind on many issues involving the philosophy and science in his time, as John Stuart Mill acknowledged, along with others since who have studied the history of Romanticism.\n\nFor Coleridge, as for many of his romantic contemporaries, the idea that matter itself can begat life only dealt with the various changes in the arrangement of particles and did not explain life itself as a principle or power that lay behind the material manifestations, \"natura naturans\" or \"the productive power suspended and, as it were, quenched in the product\" Until this were addressed, according to Coleridge, \"we have not yet attained to a science of nature.\"\n\nThis productive power is above sense experience, but not above nature herself, that is, supersensible, but not supernatural, and, thus, not 'occult' as was the case with vitalism. Vitalism failed to distinguish between spirit and nature, and then within nature, between the visible appearances and the invisible, yet very real and not simply hypothesized notion, essence or motivating principle (\"natura naturans\"). Even Newton spoke of things invisible in themselves (though not in their manifestations), such as force, though Comte, the thorough materialist, complained of the use of such terms as the 'force of gravity' as being relics of animism.\n\nMatter was not a 'datum' or thing in and of itself, but rather a product or effect, and for Coleridge, looking at life in its broadest sense, it was the product of a polarity of forces and energies, but derived from a unity which is itself a power, not an abstract or nominal concept, that is Life, and this polar nature of forces within the power of Life is the very law or 'Idea' (in the Platonic sense) of Creation.\n\nFor Coleridge the essence of the universe is motion and motion is driven by a dynamic polarity of forces that is both inherent in the world as potential and acting inherently in all manifestations. This polarity is the very dynamic that acts throughout all of nature, including into the more particular form of 'life biological', as well as of mind and consciousness.\n\nAnd this polarity is dynamic, that is real, though not visible, and not simply logical or abstract. Thus, the polarity results in manifestations that are real, as the opposite powers are not contradictory, but counteracting and inter-penetrating.\n\nThus, then, Life itself is not a thing—a self-subsistent hypostasis—but an act and process...\n\nAnd in that sense Coleridge re-phrases the question \"What is Life?\" to \"What is not Life that really is?\"\n\nThis dynamic polar essence of nature in all its functions and manifestations is a universal law in the order of the law of gravity and other physical laws of inert nature. And, critically, this dynamic polarity of constituent powers of life at all levels is not outside or above nature, but is within nature (\"natura naturans\"), not as a part of the visible product, but as the ulterior natural functions that produce such products or things.\n\nIt is these functions that provided the bridge being sought by Romantic science and medicine, in particular by Andreas Röschlaub and the Brunonian system of medicine, between the inertial science of inert nature (physics) and the vital science of vital nature (physiology) and its therapeutic application or physic (the domain of the physician).\n\nColeridge was influenced by German philosophy, in particular Kant, Fichte and Schelling (Naturphilosophie), as well as the physiology of Blumenbach and the dynamic excitation theory of life of the Brunonian system. He sought a path that was neither the mystical tendency of the earlier vitalists nor the materialistic reductionist approach to natural science, but a dynamic one.\n\nColeridge's challenge was to describe something that was dynamic neither in mystical terms not materialistic ones, but via analogy, drawing from the examples of inertial science. As one writer explains, he uses the examples of electricity, magnetism and gravity not because they are like life, but because they offer a way of understanding powers, forces and energies, which lie at the heart of life. And using these analogies, Coleridge seeks to demonstrate that life is not a material force, but a product of relations amongst forces. Life is not linear and static, but dynamic process of self-regulation and Emergent evolution that results in increasing complexity and individuation. This spiral, upward movement (cf. Goethe's ideas) creates a force for organization that unifies, and is most intense and powerful in that which is most complex and most individual - the self-regulating, enlightened, developed individual mind. But at the same time, this process of life increases interdependence (like the law of comparative advantage in economics) and associational powers of the mind. Thus, he is not talking about an isolated, individual subjective mind, but about the evolution of a higher level of consciousness and thought at the core of the process of life.\n\nAnd the direction of this motion is towards increasing individuation, that is the creation of specific, individual units of things. At the same time, given the dynamic polarity of the world, there must always be an equal and opposite tendency, in this case, that of connection. So, a given of our experience is that man is both an individual, tending in each life and in history generally to greater and greater individualization, and a social creature seeking interaction and connection. It is the dynamic interplay between the individuation and connecting forces that leads to higher and higher individuation.\n\nColeridge makes a further distinction between mathematics and life, the latter being productive or creative, that is, living, and the former ideal. Thus, the mathematical approach that works so well with inert nature, is not suitable for vital nature.\n\nThe counteraction then of the two assumed forces does not depend on their meeting from opposite directions; the power which acts in them is indestructible; it is therefore inexhaustibly re-ebullient; and as something must be the result of these two forces, both alike infinite, and both alike indestructible; and as rest or neutralization cannot be this result; no other conception is possible, but that the product must be a tertium aliquid, [a third thing] or finite generation. Consequently this conception is necessary. Now this tertium aliquid can be no other than an inter-penetration of the counteracting powers, partaking of both… Consequently the 'constituent powers', that have given rise to a body, may then reappear in it as its function: \"a Power, acting in and by its Product or Representative to a predetermined purpose is a Function...the first product of its energy is the thing itself: \"ipsa se posuit et iam facta est ens positum\". Still, however, its productive energy is not exhausted in this product, but overflows, or is effluent, as the specific forces, properties, faculties, of the product. It reappears, in short, as the function of the body...The vital functions are consequents of the \"Vis Vitae Principium Vitale\", and presuppose the Organs, as the Functionaries.\n\nLife, that is, the essential polarity in unity (multeity in unity) in Coleridge’s sense also has a four beat cycle, different from the arid dialectics of abstraction - namely the tension of the polar forces themselves, the charge of their synthesis, the discharge of their product (indifference) and the resting state of this new form (predominance). The product is not a neutralization, but a new form of the essential forces, these forces remaining within, though now as the functions of the form.\n\nTo make it adequate, we must substitute the idea of positive production for that of rest, or mere neutralization. To the fancy alone it is the null-point, or zero, but to the reason it is the punctum saliens, and the power itself in its eminence.\n\nThis dynamic polarity that is Life is expressed at different levels. At its most basic it is Space-Time, with its product - motion. The interplay of both gives us either a line or a circle, and then there are different degrees possible within a given form or “predominance” of forces. Geometry is not conceivable except as the dynamic interplay of space (periphery) and time (point). Space, time and motion are also geometrically represented by width, length (breadth) and depth. And this correspondence is repeated throughout the scale of Life.\n\nMatter, then, is the product of the dynamic forces - repulsion (centrifugal), and attraction (centripetal); it is not itself a productive power. It is also the mass of a given body.\n\nColeridge’s understanding of life is contrasted with the materialist view which is essentially reduced to defining life as that which is the opposite of not-life, or that which resists death, that is, that which is life.\n\nThe problem for Coleridge and the Romantics was that the intellect, 'left to itself' as Bacon stated, was capable of apprehending only the outer forms of nature (natura naturata) and not the inmost, living functions (natura naturans) giving rise to these forms. Thus, effects can only be 'explained' in terms of other effects, not causes. It takes a different capacity to 'see' these living functions, which is an imaginative activity. For Coleridge, there is an innate, primitive or 'primary' imagination that configures invisibly sense-experience into perception, but a rational perception, that is, one raised into consciousness and awareness and then rationally presentable, requires a higher level, what he termed 'secondary imagination', which is able to connect with the thing being experienced, penetrate to its essence in terms of the living dynamics upholding its outer form, and then present the phenomena as and within its natural law, and further, using reason, develop the various principles of its operation.\n\nThis cognitive capacity involved what Coleridge termed the 'inmost sense' or what Goethe termed the Gemüt. It also involved the reactivation of the old Greek noetic capacity, and the ability to 'see' or produce the theory (Greek \"theoria\" from the verb 'to see') dynamic polarities, or natural Laws, the dynamic transcendent (foundational) entities that Plato termed 'Ideas' (\"eidos\").\n\nSince \"natura naturata\" is sustained by \"natura naturans\", and the creative power of \"natura naturans\" is one of a kind with the human mind, itself creative, then there must be a correspondence or connection between the mind and the things we perceive, such that we can overcome the apparent separation between the object and the representation in the mind of the object that came to bedevil Enlightenment thought (Hume, Kant). As one commentator noted \"to speak at all of the unity of intelligence and nature is of course flatly to contradict Descartes.\"\n\nFor Coleridge the power of life lies in every seed as a potential to be unfolded as a result of interaction with the environment (heat, light, air, moisture, etc.), an insight which allowed him to see in the Brunonian system a dynamic polarity in excitation theory.\nColeridge also saw that there was a progressive movement through time and space of life or the law of polarity, from the level of physics (space and time) and the mineral or inert nature (law of gravity, operating through forces of attraction and repulsion), up to man, with his law of resonance in terms of his innate desire to be himself (force of individuation) and to also connect with like-minded (force of connection), as Goethe expressed in his novel \"Elective Affinities\" (\"Wahlverwandschaften\") as well as in his own life's experience.\nEvolution occurred because the original polarity of creation, the very 'Law of Creation', itself gives birth to subsequent polarities, as each pole is itself a unity that can be further polarized (what Wilhelm Reich later termed 'orgonomic functionalism' and what at the biological level constitutes physiology), an insight that would later be taken up by the concept of emergent evolution, including the emergence of mind and consciousness.\n\nAnd that this is so, is also an intimate and shared experience of all humans, as is set out in Reid's Common Sense philosophy. As Coleridge states\n\nThat nature evolves towards a purpose, and that is the unfolding of the human mind and consciousness in all its levels and degrees, is not teleological but a function of the very nature of the law of polarity or creation itself, namely that of increasing individuation of an original unity, what Coleridge termed 'multeity in unity'. As he states, \"without assigning to nature as nature, a conscious purpose\" we must still \"distinguish her agency from a blind and lifeless mechanism.\"\n\nWhile man contains and is subject to the various laws of nature, man as a self-conscious being is also the summa of a process of creation leading to greater mind and consciousness, that is, a creative capacity of imagination. Instead of being a creature of circumstance, man is the creator of them, or at least has that potential.\n\n"}
{"id": "53867278", "url": "https://en.wikipedia.org/wiki?curid=53867278", "title": "Coloration evidence for natural selection", "text": "Coloration evidence for natural selection\n\nAnimal coloration provided important early evidence for evolution by natural selection, at a time when little direct evidence was available. Three major functions of coloration were discovered in the second half of the 19th century, and subsequently used as evidence of selection: camouflage (protective coloration); mimicry, both Batesian and Müllerian; and aposematism.\n\nCharles Darwin's \"On the Origin of Species\" was published in 1859, arguing from circumstantial evidence that selection by human breeders could produce change, and that since there was clearly a struggle for existence, that natural selection must be taking place. But he lacked an explanation either for genetic variation or for heredity, both essential to the theory. Many alternative theories were accordingly considered by biologists, threatening to undermine Darwinian evolution.\n\nSome of the first evidence was provided by Darwin's contemporaries, the naturalists Henry Walter Bates and Fritz Müller. They described forms of mimicry that now carry their names, based on their observations of tropical butterflies. These highly specific patterns of coloration are readily explained by natural selection, since predators such as birds which hunt by sight will more often catch and kill insects that are less good mimics of distasteful models than those that are better mimics; but the patterns are otherwise hard to explain. \nDarwinists such as Alfred Russel Wallace and Edward Bagnall Poulton, and in the 20th century Hugh Cott and Bernard Kettlewell, sought evidence that natural selection was taking place. Wallace noted that snow camouflage, especially plumage and pelage that changed with the seasons, suggested an obvious explanation as an adaptation for concealment. Poulton's 1890 book, \"The Colours of Animals\", written during Darwinism's lowest ebb, used all the forms of coloration to argue the case for natural selection. Cott described many kinds of camouflage, and in particular his drawings of coincident disruptive coloration in frogs convinced other biologists that these deceptive markings were products of natural selection. Kettlewell experimented on peppered moth evolution, showing that the species had adapted as pollution changed the environment; this provided compelling evidence of Darwinian evolution.\n\nCharles Darwin published \"On the Origin of Species\" in 1859, arguing that evolution in nature must be driven by natural selection, just as breeds of domestic animals and cultivars of crop plants were driven by artificial selection.\nDarwin's theory radically altered popular and scientific opinion about the development of life. However, he lacked evidence and explanations for some critical components of the evolutionary process. He could not explain the source of variation in traits within a species, and did not have a mechanism of heredity that could pass traits faithfully from one generation to the next. This made his theory vulnerable; alternative theories were being explored during the eclipse of Darwinism; and so Darwinian field naturalists like Wallace, Bates and Müller looked for clear evidence that natural selection actually occurred. Animal coloration, readily observable, soon provided strong and independent lines of evidence, from camouflage, mimicry and aposematism, that natural selection was indeed at work. The historian of science Peter J. Bowler wrote that Darwin's theory\n\nIn his 1889 book \"Darwinism\", the naturalist Alfred Russel Wallace considered the white coloration of Arctic animals. He recorded that the Arctic fox, Arctic hare, ermine and ptarmigan change their colour seasonally, and gave \"the obvious explanation\", that it was for concealment. The modern ornithologist W. L. N. Tickell, reviewing proposed explanations of white plumage in birds, writes that in the ptarmigan \"it is difficult to escape the conclusion that cryptic brown summer plumage becomes a liability in snow, and white plumage is therefore another cryptic adaptation.\" All the same, he notes, \"in spite of winter plumage, many Ptarmigan in NE Iceland are killed by Gyrfalcons throughout the winter.\"\n\nMore recently, decreasing snow cover in Poland, caused by global warming, is reflected in a reduced percentage of white-coated weasels that become white in winter. Days with snow cover halved between 1997 and 2007, and as few as 20 percent of the weasels had white winter coats. This was shown to be a result of natural selection by predators making use of camouflage mismatch.\n\nIn the words of camouflage researchers Innes Cuthill and A. Székely, the English zoologist and camouflage expert Hugh Cott's 1940 book \"Adaptive Coloration in Animals\" provided \"persuasive arguments for the survival value of coloration, and for adaptation in general, at a time when natural selection was far from universally accepted within evolutionary biology.\" In particular, they argue, \"Coincident Disruptive Coloration\" (one of Cott's categories) \"made Cott's drawings the most compelling evidence for natural selection enhancing survival through disruptive camouflage.\" Cott explained, while discussing \"a little frog known as \"Megalixalus fornasinii\"\" in his chapter on coincident disruptive coloration, that \"it is only when the pattern is considered in relation to the frog's normal attitude of rest that its remarkable nature becomes apparent... The attitude and very striking colour-scheme thus combine to produce an extraordinary effect, whose deceptive appearance depends upon the breaking up of the entire form into two strongly contrasted areas of brown and white. Considered separately, neither part resembles part of a frog. Together in nature the white configuration alone is conspicuous. This stands out and distracts the observer's attention from the true form and contour of the body and appendages on which it is superimposed\". Cott concluded that the effect was concealment \"so long as the false configuration is recognized in preference to the real one\". Such patterns embody, as Cott stressed, considerable precision as the markings must line up accurately for the disguise to work. Cott's description and in particular his drawings convinced biologists that the markings, and hence the camouflage, must have survival value (rather than occurring by chance); and further, as Cuthill and Székely indicate, that the bodies of animals that have such patterns must indeed have been shaped by natural selection.\n\nBetween 1953 and 1956, the geneticist Bernard Kettlewell experimented on peppered moth evolution. He presented results showing that in a polluted urban wood with dark tree trunks, dark moths survived better than pale ones, causing industrial melanism, whereas in a clean rural wood with paler trunks, pale moths survived better than dark ones. The implication was that survival was caused by camouflage against suitable backgrounds, where predators hunting by sight (insect-eating birds, such as the great tits used in the experiment) selectively caught and killed the less well-camouflaged moths. The results were intensely controversial, and from 2001 Michael Majerus carefully repeated the experiment. The results were published posthumously in 2012, vindicating Kettlewell's work as \"the most direct evidence\", and \"one of the clearest and most easily understood examples of Darwinian evolution in action\".\n\nBatesian mimicry, named for the 19th century naturalist Henry Walter Bates who first noted the effect in 1861, \"provides numerous excellent examples of natural selection\" at work. The evolutionary entomologist James Mallet noted that mimicry was \"arguably the oldest Darwinian theory not attributable to Darwin.\" Inspired by \"On the Origin of Species\", Bates realized that unrelated Amazonian butterflies resembled each other when they lived in the same areas, but had different coloration in different locations in the Amazon, something that could only have been caused by adaptation.\n\nMüllerian mimicry, too, in which two or more distasteful species that share one or more predators have come to mimic each other's warning signals, was clearly adaptive; Fritz Müller described the effect in 1879, in an account notable for being the first use of a mathematical argument in evolutionary ecology to show how powerful the effect of natural selection would be.\n\nIn 1867, in a letter to Darwin, Wallace described warning coloration. The evolutionary zoologist James Mallet notes that this discovery \"rather illogically\" followed rather than preceded the accounts of Batesian and Müllerian mimicry, which both rely on the existence and effectiveness of warning coloration. The conspicuous colours and patterns of animals with strong defences such as toxins are advertised to predators, signalling honestly that the animal is not worth attacking. This directly increases the reproductive fitness of the potential prey, providing a strong selective advantage. The existence of unequivocal warning coloration is therefore clear evidence of natural selection at work.\n\nEdward Bagnall Poulton's 1890 book, \"The Colours of Animals\", renamed Wallace's concept of warning colours \"aposematic\" coloration, as well as supporting Darwin's then unpopular theories of natural selection and sexual selection. Poulton's explanations of coloration are emphatically Darwinian. For example, on aposematic colouration he wrote that\n\nPoulton summed up his allegiance to Darwinism as an explanation of Batesian mimicry in one sentence: \"Every step in the gradually increasing change of the mimicking in the direction of specially protected form, would have been an advantage in the struggle for existence\".\n\nThe historian of science Peter J. Bowler commented that Poulton used his book to complain about experimentalists' lack of attention to what field naturalists (like Wallace, Bates, and Poulton) could readily see were adaptive features. Bowler added that \"The fact that the adaptive significance of coloration \"was\" (sic) widely challenged indicates just how far anti-Darwinian feeling had developed. Only field naturalists such as Poulton refused to give in, convinced that their observations showed the validity of selection, whatever the theoretical problems.\"\n"}
{"id": "48048662", "url": "https://en.wikipedia.org/wiki?curid=48048662", "title": "Dessauite-(Y)", "text": "Dessauite-(Y)\n\nDessauite-(Y) is a mineral member of the crichtonite group with the formula (Sr,Pb)(Y,U)(Ti,Fe)O. It is associated with derbylite, hematite, rutile, karelianite, siderite, and calcite. Founded in the Buca della Vena Mine, Tuscany, Italy, the mineral was called dessauite in honor of professor Gabor Dessau (1907–1983). \n\nDessauite occurs as small, flattened rhombohedral crystals, tabular {001} with hexagonal outline. Members of the Crichtonite group may be confused with ilmenite or hematite. The difference between dessauite and other minerals in the crichonite group is the occurrence of three additional octahedral sites and of a site in square pyramidal coordination, all with low occupancies. The mineral is black and opaque, presents a metallic luster, and it is brittle. Dessauite presents dimensions of diameter up to 1mm and thickness up to 0.2mm. In reflected plane-polarized light the color is ash-grey with pale bluish tones. The calculated density is 4.68 g/cm. The habit is tabular, forming thin dimensions in one direction and hardness of 6.5 and 7. Dessauite differs from other elements of the crichtonite group because of the quantity of cations and X-ray diffraction pattern.\n\nDessauite was found in the Buca della Vena Mine, Apuan Alps, northern Tuscany, Italy, with many other minerals, coming from hydrothermal fluids circulating through a small hematite-barite ore deposit within dolomite, during an alpine metamorphic event. It occurs in calcite veins hosted within dolomite and is associated with calcite, rutile, hematite, siderite, and derbylite.\n\n"}
{"id": "55895027", "url": "https://en.wikipedia.org/wiki?curid=55895027", "title": "Disposable soma theory of aging", "text": "Disposable soma theory of aging\n\nThe disposable soma theory of aging states that organisms age due to an evolutionary trade-off between growth, reproduction, and DNA repair maintenance. Formulated by Thomas Kirkwood, the disposable soma theory explains that an organism only has a limited amount of resources or \"soma\" that it can allocate to its various cellular processes. Therefore, a greater investment in growth and reproduction would result in reduced investment in DNA repair maintenance, leading to increased cellular damage, shortened telomeres, accumulation of mutations, compromised stem cells, and ultimately, senescence. Although many models, both animal and human, have appeared to support this theory, parts of it are still controversial. \nSpecifically, while the evolutionary trade-off between growth and aging has been well established, \nthe relationship between reproduction and aging is still without scientific consensus, and the cellular mechanisms largely undiscovered.\n\nBritish biologist Thomas Kirkwood first proposed the disposable soma theory of aging in a 1977 \"Nature\" review article. The theory was inspired by Leslie Orgel's Error Catastrophe Theory of Aging, which was published fourteen years earlier, in 1963. Orgel believed that the process of aging arose due to mutations acquired during the replication process, and Kirkwood developed the disposable soma theory in order to mediate Orgel's work with evolutionary genetics.\n\nThe disposable soma theory of aging acts on the premise that there is a tradeoff in resource allocation between somatic maintenance and reproductive investment. Too low an investment in self-repair would be evolutionarily unsound, as the organism would likely die before reproductive age. However, too high an investment in self-repair would also be evolutionarily unsound due to the fact that one's offspring would likely die before reproductive age. Therefore, there is a compromise and resources are partitioned accordingly. However, this compromise is thought to damage somatic repair systems, which can lead to progressive cellular damage and senescence. Repair costs can be categorized into three groups: (1) the costs of increased durability of nonrenewable parts; (2) the costs of maintenance involving cell renewal, and (3) the costs of intracellular maintenance. In a nutshell, aging and decline is essentially a tradeoff for increased reproductive robustness in youth.\n\nMuch research has been done on the antagonistic effects of increased growth on lifespan. Specifically, the hormone insulin-like growth factor 1 (IGF-1), binds to a cell receptor, leading to a phosphorylation cascade. This cascade results in kinases phosphorylating forkhead transcription factor (FOXO), deactivating it. Deactivation of FOXO results in an inability to express genes involved in responding to oxidative stress response, such as antioxidants, chaperones, and heat-shock proteins. \nAdditionally, uptake of IGF-1 stimulates the mTOR pathway, which activates protein synthesis (and therefore growth) through upregulation of the translation-promoting S6K1, and also inhibits autophagy, a process necessary for recycling of damaged cellular products. Decline of autophagy causes neurodegeneration, protein aggregation and premature aging. Lastly, studies have also indicated that the mTOR pathway also alters immune responses and stimulates cyclin-dependent kinase (CDK) inhibitors such as p16 and p21. This leads to alteration of the stem-cell niche and results in stem cell exhaustion, another theorized mechanism of aging.\n\nThe mechanism of why reproduction inhibits lifespan with regards to multicellular organisms is still unclear. Although many models do illustrate an inverse relationship, and the theory makes sense from an evolutionary perspective, the cellular mechanisms have yet to be explored. However, with regards to cellular replication, the progressive shortening of telomeres is a mechanism which limits the amount of generations of a single cell may undergo. Furthermore, in unicellular organisms like \"Saccharomyces cerevisiae\", the formation of extrachromosomal rDNA circles (ERCs) in mother cells (but not daughter cells) upon every subsequent division is an identifiable type of DNA damage that is associated with replication. These ERCs accumulate over time and eventually trigger replicative senescence and death of the mother cell.\n\nThere is a large body of evidence indicating the negative effects of growth on longevity across many species. As a general rule, individuals of a smaller size generally live longer than larger individuals of the same species.\n\nIn dwarf models of mice, such Snell or Ames mice, mutations have arisen, either rendering them incapable of producing IGF-1 or unable to have adequate receptors for IGF-1 uptake. Furthermore, mice injected with growth hormone have been shown to have progressive weight loss, roughing of the coat, curvature of the spine, enlargement of the organs, kidney lesions and increased cancer risk. This effect is also seen in different breeds of dogs, where smaller breeds of dogs typically live significantly longer compared to their larger counterparts. Selectively bred for their small size, smaller dog breeds like the Chihuahua (average lifespan of 15–20 years) have the B/B genotype for the IGF-1 haplotype, reducing the amount of IGF-1 produced. Conversely, large dogs like the Great Dane (average lifespan of 6–8 years) are homozygous for the IGF-1 I allele, which increases the amount of IGF-1 production.\n\nInitially, it was believed that growth hormone actually prolonged lifespan due to a 1990 study that indicated that injection of growth hormone to men over 60 years of age appeared to reverse various biomarkers implicated in aging, such as decreased muscle mass, bone density, skin thickness, and increased adipose tissue. However, a 1999 study found that administering growth hormone also significantly increased mortality rate. Recent genomic studies have confirmed that the genes involved in growth hormone uptake and signaling are largely conserved across a plethora of species, such as yeast, nematodes, fruit flies, mice and humans. These studies have also shown that individuals with Laron syndrome, an autosomal recessive disorder resulting in dwarfism due to defects in growth hormone receptors, have increased lifespan. Additionally, these individuals have much lower incidences of age-related diseases such as type 2 diabetes and cancer. Lastly, human centenarians around the world are disproportionately of short stature, and have low levels of IGF-1.\n\nNumerous studies have found that lifespan is inversely correlated with both the total amount of offspring birthed, as well as the age at which females first gives birth, also known as primiparity. Additionally, it has been found that reproduction is a costly mechanism that alters the metabolism of fat. Lipids invested in reproduction would be unable to be allocated to support mechanisms involved in somatic maintenance.\n\nThe disposable soma theory has been consistent with the majority of animal models. It was found in numerous animal studies that castration or genetic deformities of reproduction organs was correlated with increased lifespan. Moreover, in red squirrels, it was found that females with an early primiparity achieved the highest immediate and lifetime reproductive success. However, it was also found that these same individuals had a decreased median and maximum lifespan. Specifically squirrels who mated earlier had a 22.4% rate of mortality until two years of age compared to a 16.5% rate of mortality in late breeders. In addition, these squirrels had an average maximum lifespan of 1035 days compared to an average maximum lifespan of 1245 days for squirrels that bred later.\n\nIn another study, researchers selectively bred fruit flies over three years to develop two different strains, an early-reproducing strain and a late-reproducing strain. The late-reproducing line had a significantly longer lifespan than the early-reproducing line. Even more telling was that when the researchers introduced a mutation in the ovarian-associated gene \"ovoD1\", resulting in defective oogenesis, the differences in lifespan between the two lines disappeared. The researchers in this case concluded that \"aging has evolved primarily because of the damaging effects of reproduction earlier in life\".\n\nProminent aging researcher Steven Austad also performed a large-scale ecological study on the coast of Georgia in 1993. Austad isolated two opossum populations, one from the predator-infested mainland and one from the predator-absent nearby island of Sapelo. According to the disposable soma theory, a genetically isolated population subject to low environmentally-induced mortality would evolve delayed reproduction and aging. This is because without the pressure of predation, it would be evolutionarily advantageous to allocate more resources to somatic maintenance than reproduction, as early offspring mortality would be low. As predicted, even after controlling for predation, the isolated population had a longer lifespan, delayed primiparity, and reduced aging biomarkers such as tail collagen cross-linking.\n\nIn general, only a few studies exist in human models. It was found that castrated men live longer than their fertile counterparts. Further studies found that in British women, primiparity was earliest in women who died early and latest in women who died at the oldest ages. Furthermore, increased number of children birthed was associated with a decreased lifespan. A final study found that female centenarians were more likely to have children in later life compared average, especially past the age of 40. The researchers discovered that 19.2% of female centenarians had their first child after the age of 40, compared to 5.5% of the rest of the female population.\n\nThere are numerous studies that support cellular damage, often due to a lack of somatic maintenance mechanisms, as a primary determinant for aging, and these studies have given rise to the free radical theory of aging and the DNA damage theory of aging. One study found that the cells of short-living rodents \"in vitro\" show much greater mutation rates and a general lack of genome surveillance compared to human cells and are far more susceptible to oxidative stress. \nOther studies have been conducted on the naked mole rat, a rodent species with remarkable longevity (30 years), capable of outliving the brown rat (3 years) by ten-fold. Additionally, almost no incidence cancer has ever been detected in naked mole rats. Nearly all of the differences found between these two organisms, which are otherwise rather genetically similar, was in somatic maintenance. Naked mole rats were found to have higher levels of superoxide dismutase, a reactive oxygen species clearing antioxidant. In addition, naked mole rats had higher levels of base excision repair, DNA damage response signaling, homologous recombination repair, mismatch repair, nucleotide excision repair, and non-homologous end joining. In fact, many of these processes were near or exceeded human levels. Proteins from naked mole rats were also more resistant to oxidation, misfolding, ubiquitination, and had increased translational fidelity.\n\nFurther studies have been conducted on patients with Hutchinson-Gilford Progeria Syndrome (HGPS), a condition that leads to premature aging. Patients with HGPS typically age about seven times faster than average and usually succumb to the disease in their early teens. Patients with HGPS have cellular defects, specifically in the lamin proteins, which regulate the organization of the lamina and nuclear envelope for mitosis. \nLastly, as mentioned previously, it has been found that the suppression of autophagy is associated with reduced lifespan, while stimulation is associated with extended lifespan. Activated in times of caloric restriction, autophagy is a process that prevents cellular damage through clearance and recycling of damaged proteins and organelles.\n\nOne of the main weaknesses of the disposable soma theory is that it does not postulate any specific cellular mechanisms to which an organism shifts energy to somatic repair over reproduction. Instead, it only offers an evolutionary perspective on why aging may occur due to reproduction. Therefore, parts of it are rather limited outside of the field of evolutionary biology.\n\nCritics have pointed out the supposed inconsistencies of the disposable soma theory due to the observed effects of caloric restriction, which is correlated with increased lifespan. Although it activates autophagy, according to classical disposable soma principles, with less caloric intake, there would less total energy to be distributed towards somatic maintenance, and decreased lifespan would be observed (or at least the positive autophagic effects would be balanced out). However, Kirkwood, alongside his collaborator Darryl P. Shanley, assert that caloric restriction triggers an adaptive mechanism which causes the organism to shift a higher proportion of resources to somatic maintenance, away from reproduction. This theory is supported by multiple studies, which show that caloric restriction typically results in impaired fertility, but leave an otherwise healthy organism. Evolutionarily, an organism would want to delay reproduction to when resources were more plentiful. During a resource-barren period, it would evolutionarily unwise to invest resources in progeny that would be unlikely to survive in famine. Mechanistically, the NAD-dependent deacetylase Sirtuin 1 (SIRT-1) is upregulated during low-nutrient periods. SIRT-1 increases insulin sensitivity, decreases the amount of inflammatory cytokines, stimulates autophagy, and activates FOXO, the aforementioned protein involved in activating stress response genes. SIRT-1 is also found to result in decreased fertility.\n\nIn additional to differential partitioning of energy allocation during caloric restriction, less caloric intake would result in less metabolic waste in the forms of free radicals like hydrogen peroxide, superoxide and hydroxyl radicals, which damage important cellular components, particularly mitochondria. Elevated levels of free radicals in mice has been correlated with neurodegeneration, myocardial injury, severe anemia, and premature death.\n\nAnother primary criticism of the disposable soma theory is that it fails to account for why women tend to live longer than their male counterparts. Afterall, females invest significantly more resources into reproduction and according to the classical disposable soma principles, this would compromise energy diverted to somatic maintenance. However, this can be reconciled with the grandmother hypothesis. The Grandmother Hypothesis states that menopause comes about into older women in order to restrict the time of reproduction as a protective mechanism. This would allow women to live longer and increase the amount of care they could provide to their grandchildren, increasing their evolutionary fitness. And so, although women do invest a greater proportion of resources into reproduction during their fertile years, their overall reproductive period is significantly shorter than men, who are able of reproduction during and even beyond middle age. Additionally, males invest more resources into growth, and have significantly higher levels of IGF-1 compared to females, which is correlated with decreased lifespan. Other variables such as increased testosterone levels in males are not accounted for. Increased testosterone is often associated with reckless behaviour, which may lead to a high accidental death rate.\n\nA few contradicting animal models weaken the validity of the disposable soma theory. This includes studies done on the aforementioned naked mole rats. In these studies, it was found that reproductive naked mole rats actually show significantly increased lifespans compared to non-reproductive individuals, which contradicts the principles of diposable soma. However, although these naked mole rats are mammalian, they are highly atypical in terms of aging studies and may not serve as the best model for humans. For example, naked mole rats have a disproportionately high longevity quotient and live in eusocial societies, where breeding is usually designated to a queen.\n\nThe disposable soma theory is tested disproportionately on female organisms for the relationship between reproduction and aging, as females carry a greater burden in reproduction. Additionally, for the relationship between growth and aging, studies are disproportionately conducted on males, to minimize the hormonal fluctuations that occur with menstrual cycling. Lastly, genetic and environmental factors, rather than reproductive patterns, may explain the variations in human lifespan. For example, studies have shown that poorer individuals, to whom nutritious food and medical care is less accessible, typically have higher birth rates and earlier primiparity.\n\n\n"}
{"id": "2262154", "url": "https://en.wikipedia.org/wiki?curid=2262154", "title": "Doppler broadening", "text": "Doppler broadening\n\nIn atomic physics, Doppler broadening is the broadening of spectral lines due to the Doppler effect caused by a distribution of velocities of atoms or molecules. Different velocities of the emitting particles result in different Doppler shifts, the cumulative effect of which is the line broadening.\nThis resulting line profile is known as a Doppler profile. A particular case is the thermal Doppler broadening due to the thermal motion of the particles. Then, the broadening depends only on the frequency of the spectral line, the mass of the emitting particles, and their temperature, and therefore can be used for inferring the temperature of an emitting body.\n\nSaturated absorption spectroscopy, also known as Doppler-free spectroscopy, can be used to find the true frequency of an atomic transition without cooling a sample down to temperatures at which the Doppler broadening is minimal.\n\nWhen thermal motion causes a particle to move towards the observer, the emitted radiation will be shifted to a higher frequency. Likewise, when the emitter moves away, the frequency will be lowered. For non-relativistic thermal velocities, the Doppler shift in frequency will be:\n\nwhere formula_2 is the observed frequency, formula_3 is the rest frequency, formula_4 is the velocity of the emitter towards the observer, and formula_5 is the speed of light.\n\nSince there is a distribution of speeds both toward and away from the observer in any volume element of the radiating body, the net effect will be to broaden the observed line. If formula_6 is the fraction of particles with velocity component formula_7 to formula_8 along a line of sight, then the corresponding distribution of the frequencies is\n\nwhere formula_10 is the velocity towards the observer corresponding to the shift of the rest frequency formula_11 to formula_12. Therefore,\n\nWe can also express the broadening in terms of the wavelength formula_13. Recalling that in the non-relativistic limit formula_14, we obtain\n\nIn the case of the thermal Doppler broadening, the velocity distribution is given by the Maxwell distribution\n\nwhere formula_16 is the mass of the emitting particle, formula_17 is the temperature and formula_18 is the Boltzmann constant. \n\nThen,\n\nWe can simplify this expression as\n\nwhich we immediately recognize as a Gaussian profile with the standard deviation\n\nand full width at half maximum (FWHM)\n\nIn astronomy and plasma physics, the thermal Doppler broadening is one of the explanations for the broadening of spectral lines, and as such gives an indication for the temperature of observed material. Other causes of velocity distributions may exist, though, for example due to turbulent motion. For a fully developed turbulence, the resulting line profile is generally very difficult to distinguish from the thermal one.\nAnother cause could be a large range of \"macroscopic\" velocities resulting, e.g., from the receding and approaching portions of a rapidly spinning accretion disk. Finally, there are many other factors which can also broaden the lines. For example, a sufficiently high particle number density may lead to significant Stark broadening.\n\nDoppler broadening can also be used to determine the velocity distribution of a gas given its absorption spectrum. In particular, this has been used to determine the velocity distribution of interstellar gas clouds. \n\nDoppler broadening has also been used as a design consideration in high temperature nuclear reactors. In principle, as the reactor fuel heats up, the neutron absorption spectrum will broaden due to the relative thermal motion of the fuel atoms with respect to the neutrons. Given the shape of the neutron absorption spectrum, this has the result of reducing neutron absorption cross section, reducing the likelihood of absorption and fission. The end result is that reactors designed to take advantage of doppler broadening will decrease their reactivity as temperature increases, creating a passive safety measure. This tends to be more relevant to gas cooled reactors as other mechanisms are dominant in water cooled reactors.\n\n"}
{"id": "41077022", "url": "https://en.wikipedia.org/wiki?curid=41077022", "title": "Earth's internal heat budget", "text": "Earth's internal heat budget\n\nEarth's internal heat budget is fundamental to the thermal history of the Earth. The flow of heat from Earth's interior to the surface is estimated at formula_1 terawatts (TW) and comes from two main sources in roughly equal amounts: the \"radiogenic heat\" produced by the radioactive decay of isotopes in the mantle and crust, and the \"primordial heat\" left over from the formation of the Earth.\n\nEarth's internal heat powers most geological processes and drives plate tectonics. Despite its geological significance, this heat energy coming from Earth's interior is actually only 0.03% of Earth's total energy budget at the surface, which is dominated by 173,000 TW of incoming solar radiation. The insolation that eventually, after reflection, reaches the surface penetrates only several tens of centimeters on the daily cycle and only several tens of meters on the annual cycle. This renders solar radiation minimally relevant for internal processes.\n\nBased on calculations of Earth's cooling rate, which assumed constant conductivity in the Earth's interior, in 1862 William Thomson (later made Lord Kelvin) estimated the age of the Earth at 98 million years, which contrasts with the age of 4.5 billion years obtained in the 20th century by radiometric dating. As pointed out by John Perry in 1895 a variable conductivity in the Earth's interior could expand the computed age of the Earth to billions of years, as later confirmed by radiometric dating. Contrary to the usual representation of Kelvin's argument, the observed thermal gradient of the Earth's crust would not be explained by the addition of radioactivity as a heat source. More significantly, mantle convection alters how heat is transported within the Earth, invalidating Kelvin's assumption of purely conductive cooling.\n\nEstimates of the total heat flow from Earth’s interior to surface span a range of 43 to 49 terawatts (TW) (a terawatt is 10 watts). One recent estimate is 47 TW, equivalent to an average heat flux of 91.6 mW/m, and is based on more than 38,000 measurements. The respective mean heat flows of continental and oceanic crust are 70.9 and 105.4 mW/m.\n\nWhile the total internal Earth heat flow to the surface is well constrained, the relative contribution of the two main sources of Earth's heat, radiogenic and primordial heat, are highly uncertain because their direct measurement is difficult. Chemical and physical models give estimated ranges of 15–41 TW and 12–30 TW for radiogenic heat and primordial heat, respectively. \n\nThe structure of the Earth is a rigid outer crust that is composed of thicker continental crust and thinner oceanic crust, solid but plastically flowing mantle, a liquid outer core, and a solid inner core. The fluidity of a material is proportional to temperature; thus, the solid mantle can still flow on long time scales, as a function of its temperature and therefore as a function of the flow of Earth's internal heat. The mantle convects in response to heat escaping from Earth's interior, with hotter and more buoyant mantle rising and cooler, and therefore denser, mantle sinking. This convective flow of the mantle drives the movement of Earth's lithospheric plates; thus, an additional reservoir of heat in the lower mantle is critical for the operation of plate tectonics and one possible source is an enrichment of radioactive elements in the lower mantle.\n\nEarth heat transport occurs by conduction, mantle convection, hydrothermal convection, and volcanic advection. Earth's internal heat flow to the surface is thought to be 80% due to mantle convection, with the remaining heat mostly originating in the Earth's crust, with about 1% due to volcanic activity, earthquakes, and mountain building. Thus, about 99% of Earth's internal heat loss at the surface is by conduction through the crust, and mantle convection is the dominant control on heat transport from deep within the Earth. Most of the heat flow from the thicker continental crust is attributed to internal radiogenic sources, in contrast the thinner oceanic crust has only 2% internal radiogenic heat. The remaining heat flow at the surface would be due to basal heating of the crust from mantle convection. Heat fluxes are negatively correlated with rock age, with the highest heat fluxes from the youngest rock at mid-ocean ridge spreading centers (zones of mantle upwelling), as observed in the .\n\nThe radioactive decay of elements in the Earth's mantle and crust results in production of daughter isotopes and release of geoneutrinos and heat energy, or radiogenic heat. Four radioactive isotopes are responsible for the majority of radiogenic heat because of their enrichment relative to other radioactive isotopes: uranium-238 (U), uranium-235 (U), thorium-232 (Th), and potassium-40 (K). Due to a lack of rock samples from below 200 km depth, it is difficult to determine precisely the radiogenic heat throughout the whole mantle, although some estimates are available. For the Earth's core, geochemical studies indicate that it is unlikely to be a significant source of radiogenic heat due to an expected low concentration of radioactive elements partitioning into iron. Radiogenic heat production in the mantle is linked to the structure of mantle convection, a topic of much debate, and it is thought that the mantle may either have a layered structure with a higher concentration of radioactive heat-producing elements in the lower mantle, or small reservoirs enriched in radioactive elements dispersed throughout the whole mantle.\n\nGeoneutrino detectors can detect the decay of U and Th and thus allow estimation of their contribution to the present radiogenic heat budget, while U and K is not detectable. Regardless, K is estimated to contribute 4 TW of heating. However, due to the short half-lives the decay of U and K contributed a large fraction of radiogenic heat flux to the early Earth, which was also much hotter than at present. Initial results from measuring the geoneutrino products of radioactive decay from within the Earth, a proxy for radiogenic heat, yielded a new estimate of half of the total Earth internal heat source being radiogenic, and this is consistent with previous estimates.\n\nPrimordial heat is the heat lost by the Earth as it continues to cool from its original formation, and this is in contrast to its still actively-produced radiogenic heat. The Earth core's heat flow—heat leaving the core and flowing into the overlying mantle—is thought to be due to primordial heat, and is estimated at 5–15 TW. Estimates of mantle primordial heat loss range between 7 and 15 TW, which is calculated as the remainder of heat after removal of core heat flow and bulk-Earth radiogenic heat production from the observed surface heat flow.\n\nThe early formation of the Earth's dense core could have caused superheating and rapid heat loss, and the heat loss rate would slow once the mantle solidified. Heat flow from the core is necessary for maintaining the convecting outer core and the geodynamo and Earth's magnetic field, therefore primordial heat from the core enabled Earth's atmosphere and thus helped retain Earth's liquid water.\n\nControversy over the exact nature of mantle convection makes the linked evolution of Earth's heat budget and the dynamics and structure of the mantle difficult to unravel. There is evidence that the processes of plate tectonics were not active in the Earth before 3.2 billion years ago, and that early Earth's internal heat loss could have been dominated by advection via heat-pipe volcanism. Terrestrial bodies with lower heat flows, such as the Moon and Mars, conduct their internal heat through a single lithospheric plate, and higher heat flows, such as on Jupiter's moon Io, result in advective heat transport via enhanced volcanism, while the active plate tectonics of Earth occur with an intermediate heat flow and a convecting mantle.\n\n"}
{"id": "226741", "url": "https://en.wikipedia.org/wiki?curid=226741", "title": "Earth in science fiction", "text": "Earth in science fiction\n\nAn overwhelming majority of fiction is set on or features the Earth. However, authors of speculative fiction novels and writers and directors of science fiction film deal with Earth quite differently from authors of conventional fiction. Unbound from the same ties that bind authors of traditional fiction to the Earth, they can either completely ignore the Earth or use it as but one of many settings in a more complicated universe, exploring a number of common themes through examining outsiders' perceptions of and interactions with Earth.\n\n\n\n\n\nThe overarching plot in both the original and re-imagined \"Battlestar Galactica\" is the quest to find Earth, which is thought to be the location of the lost thirteenth colony of Kobol. Colonial history dictates that Kobol is the homeworld of all humanity, and that the Thirteen Tribes of Kobol fled that world thousands of years earlier, with twelve tribes founding the Twelve Colonies and the thirteenth heading to Earth. Both shows are similar in that the location of Earth is initially unknown, but clues to its location are gradually discovered by the refugee fleet from the Twelve Colonies. In both series, the exodus of the Thirteen Tribes took place so far in the past that most modern Colonials have come to assume that the stories of Earth are simply religious myths.\n\nIn the original series, several clues indicate that the existence of Earth is real. On the prison planet of Proteus, Starbuck encounters drawings of star systems on the wall of a cell once occupied by a mysterious prisoner. The star charts turn out to be that of the Solar System. Additionally, when the \"Galactica\" later reaches a planet called Terra, it is inhabited by humans who use Earth units of measurement (hours, minutes, etc.) rather than Colonial units of measurement, suggesting that it was settled by members of the lost Thirteenth Tribe thousands of years earlier on their way to Earth.\n\nIn \"Galactica 1980\", a continuation of the original series, the fleet did eventually discover Earth as it was in 1980.\n\nIn the Season Three finale of the re-imagined series, Kara Thrace returns to \"Galactica\" after her apparent death, claiming to have been to Earth and intending to lead the fleet there. The camera then pans out from the fleet to view the Milky Way galaxy, and then zooms back in to show Earth, confirming the existence of the planet. In the Season Four mid-season finale episode \"Revelations\", the fleet finally reaches Earth, only to discover that it is a lifeless, radioactive wasteland.\n\nIn \"Sometimes a Great Notion\", it is revealed that the Thirteenth Tribe consisted of humanoid and mechanical Cylons of a type previously unknown. It is also revealed that the final five Cylons had previously lived on Earth 2000 years in the past, when a nuclear war devastated the planet.\n\nIn the final episode, a twist ending shows \"Galactica\" reaching our Earth, 150,000 years ago. The Colonials and the Cylons they've made peace with decide to call their new world \"Earth\" due to the hope associated with the name of the now devastated planet the Thirteenth Tribe once inhabited. They then abandon their technology and live among the new Earth's native Hominini. 150,000 years later, in the present day, the remains of Mitochondrial Eve – a Colonial human/Cylon hybrid (named Hera Agathon) whose birth and destiny had been a major plot element of the series – are discovered.\n\nIn most variations on the \"Buck Rogers\" mythos (comic strip, TV series, feature film), Earth of the 25th century (where the action takes place) is recovering from various atomic wars, usually variations on World War III. In the original comic, Mongols have taken over the Earth; in the TV series, the Draconian Empire fills this role (although the Draconians are obviously based on Mongols). Most of Earth's cities lie in ruins, although rebuilding is in progress (Earth's capital is New Chicago; other cities include New Paris, New London etc.). The second season of the TV series revealed that much of Earth's population fled the planet in the wake of the atomic war and founded colonies in deep space; the Earth ship \"Searcher\" is dispatched to investigate.\n\nIn Joss Whedon's Buffyverse, established by \"Buffy the Vampire Slayer\" and \"Angel\", Earth is one of several dimensions; the term \"Earth\" is used both to refer to the specific planet and to the dimension the planet exists in as a whole. Born from the Seed of Wonder, the source of all magic, Earth originated as a world of Demons, and was ruled over by the Old Ones during a time known as the Primordium Age. Eventually, however, the human race rose up and fought back against the Old Ones, banishing them to other dimensions.\n\nIn Jerry Pournelle's \"CoDominium\" series (now largely alternate history) the Earth comes under the control of the CoDominium, an alliance between the United States and Soviet Union, in the year 1990. The CoDominium imposes its control over all other nations of the Earth, halting scientific development and warfare. The CoDominium is ruled by a Grand Senate located on the Moon, and eventually constructs interstellar colonies for the joint goal of economic gain and a means of exiling troublesome elements of society. Eventually in 2103, the CoDominium dissolves, with the US and USSR engaging in the nuclear \"Great Patriotic Wars\" which destroy almost all of Earth (it is mentioned that Jamaica and the Tyrolean Alps are untouched).\n\nThe CD Space Navy escapes to the planet Sparta, which eventually becomes the nucleus of the \"Empire of Man\". During the Empire's Formation Wars the Earth is once more hit hard, but is eventually incorporated into the Imperium as the \"honorary capital.\" When the Empire dissolves in the Secession Wars in the 27th century, Earth is once more subjected to nuclear attacks, but by the early 31st century has been reclaimed by the Second Empire. By that time, the Earth city of \"New Annapolis\" is a training center for the Imperial Space Navy. To inhabitants of planets newly contacted, such as Prince Samual's World in \"King David's Spaceship\", the condition of the still largely desolate Earth is presented as an object lesson for the prohibitive price of war and a justification for Empire's claim to universal rule.\n\nIn Akira Toriyama's \"Dragon Ball\" series, Earth is the primary setting and one of many planets in the North Galaxy. The planet is inhabited by humans, anthropomorphic animals, and demons, among others. \"Dragon Ball\"s Earth features heavy science fiction themes, such as humanoid robots and flying cars, as well as heavy magical influence.\n\nIn Frank Herbert's \"Dune\" series of novels, Earth is referred to as \"Old Earth / Old Terra\" by the time of the original novel \"Dune\" (at least 21,500 years in the future). The Sun is called Al-Lat, and humanity had populated many planets (among them Caladan, Giedi Prime and Salusa Secundus). In the time of Paul Atreides, the Earth is an uninhabited and largely forgotten land, shrouded in legend. In \"Dune Messiah\", Paul refers to Hitler and Genghis Khan, in comparing the destructiveness of his Jihad to their wars. It is a wilderness and recovering an ecosystem of its own as humans have abandoned it. The artifacts of \"Homo sapiens\" have for the most part crumbled back into the planet, though a more than casual observer can find many traces of the old civilizations.\n\nPaul's son, the God Emperor Leto II, refers to the Earth many times in his journals. The God Emperor seemed particularly fond of the ancestors he had from the Western sections of Eurasia. He makes references to Israel, Urartu, (also called Armenia), Edom, Damascus, Media, Babylon, Arpad, Umlias, the plains of Central Asia, and the Greeks; the family name refers to their descent from Atreus. He seems to have had ancestors among the Turks or the Mongols as he says that one of his memories involves a horse plain with felt yurts. Leto also has the memories of a famous politician from the United States whose name was Jacob Broom. In the book Children of Dune, Leto II mentions an ancestor named Agamemnon, and makes reference to Geoffrey Chaucer and the \"Canterbury Tales\".\n\nIn \"Heretics of Dune\", it is noted that the Bene Gesserit Mother Superior Taraza has the preserved Vincent van Gogh painting \"Cottages at Cordeville\" hanging in her room.\n\nIn the \"Legends of Dune\" series by Brian Herbert and Kevin J. Anderson, set in the Dune universe, it is revealed that at the beginning of mankind's war with the Machines, called the Butlerian Jihad, Earth had been devastated by humans themselves using atomics in an attack on the Machines. In the \"Prelude to Dune\" prequel series, also by Herbert and Anderson, it is mentioned that certain Monet and Gauguin paintings are owned by House Vernius, and hang in the Grand Palais at Ix.\n\nIn the Joss Whedon series \"Firefly\", Earth is long since abandoned. It is referred to with awe as \"Earth-that-Was\", having been abandoned centuries ago due to overpopulation and depletion of the planet's natural resources. After fleeing the planet, the remnants of humanity traveled in generation ships for decades (many humans lived their entire lives within a spaceship's walls) until finding a new star system. Collection of Earth-that-Was artifacts is a hobby for the rich, and ancient Earth artifacts are known to be very valuable.\n\nIt is unknown whether Earth has actually been destroyed, or if the planet still physically exists; in the feature film \"Serenity\", ancient starships are shown leaving a sickly brown Earth with gray oceans, but the fate of the planet is never fully revealed. A puppet show in the episode \"Heart of Gold\" implies that Earth has in fact been obliterated, but this was never actually confirmed on screen.\n\nIn much of Isaac Asimov's fiction, the future Earth is an underprivileged planet — impoverished, overcrowded and disease-ridden — which is regarded with disdain by the arrogant Spacers of the \"Outer Planets\" (at this stage, there are about fifty of them).\n\nIn the \"Robot\" series, the inhabitants of these planets are still aware that their ancestors came from Earth, but this does not make them fond of the place. Rather, they develop a racist theory by which \"the best strains\" had left Earth to colonize the other planets and left \"the inferior strains\" behind. However, they have no choice but to ask the help of the protagonist, a detective from the despised Earth, to solve murder mysteries which baffle their own police. Afterwards, Earth embarks on a major new campaign of space colonization, with the hope that the new colonists will prove more faithful to the Mother Planet than the earlier ones. However, in the end of the series, the Earth is doomed to a slow radioactive process that will leave the planet uninhabitable, causing a more rapid expansion of colonization from Earth.\n\nIn the \"Galactic Empire\" series, taking place thousands of years later (originally conceived as completely separate but made by Asimov in his later career into the direct sequel of the Robot Period), Earth and settlements from it are still clearly remembered in \"The Stars, Like Dust\". By the time of \"The Currents of Space\", Earth is ruled by Trantor, not yet a Galactic Empire. Its status as the original homeworld of humanity is now disputed.\n\nIn \"Pebble in the Sky\", we see Earth in the early days of the Empire of Trantor. Earth has a largely radioactive crust with only patches of habitable land in between, and its people have to undergo compulsory euthanasia at the age of 60. It is a backwater province, and among inhabitants of other planets there is a prevalent prejudice known as \"Anti-Terrestrialism\", (obviously modeled on antisemitism), with the main negative stereotype having to do with the radiation-induced diseases prevalent on Earth.\n\nBy this time, Earth people still believe themselves to be the original home of Humanity, but hardly anyone else shares this belief. Fanatical priests, based in a mysterious Temple erected on the ruins of Washington, D.C., cultivate the mystique of Earth's ancient glories and conceive a plot to spread a Terrestrial disease throughout the Galaxy and in this way take over the Empire (and incidentally, act out the stereotype). The plot is foiled by a middle-aged tailor from the Twentieth Century, who possess powerful psychic abilities as a result of experiments performed upon him when he arrived in the future. Schwartz, the tailor, is often described as being Jewish, though this is never stated within the novel.\n\nBy the time of the Galactic Empire's decline, Earth is vaguely remembered as 'Sol' in \"Foundation\", and only one candidate for being the Original World. In \"Foundation and Earth\", records of Earth are missing, so two citizens of the mature Foundation go looking for it, and eventually find that it is desolated by nuclear radiation. The only sentient being remaining in the Solar system is robot Daneel Olivaw, who resides in a small station on the moon, overseeing the progress of a humanity now spread throughout the galaxy.\n\nIn Ursula K. Le Guin's Hainish Cycle our Earth is referred to as Terra. Like all human worlds of the Hainish Cycle, Terra was populated by the humans of Hain in Earth's prehistory, but forgot our common ancestry after millennia of no contact from extraterrestrial humans after the collapse of the first Hainish interplanetary civilization.\n\nThe second period of contact with the interstellar Hainish community, now organized as \"The League Of All Worlds\" is described in \"The Word for World Is Forest\", \"The Dispossessed\", and \"Rocannon's World\". In \"The Dispossessed\", Terra's population is said to have fallen from 9 billion to only half a billion people due to a collapse of the Terran ecology, and that life has only survived there because of strict rationing of resources and help from the Hainish. In \"The Word for World Is Forest\", the people from Terra appear as reckless exploiters of other planets. Some time later, \"City of Illusions\" provides a detailed description of Terra in the depths of a second era of isolation, called \"The Age Of The Enemy\".\n\nThe post-apocalyptic Earth seen in \"City of Illusions\" shows signs of an advanced, abandoned civilization under a rewilded landscape. A small number of humans live in tiny, isolated settlements where they retain some technologies from the past but are completely cut off from any communication with neighboring regions or with other worlds. There is only one city with high technology and energy-intensive construction, and it is controlled by the alien conquerors of the League. The events of \"City of Illusions\" lead to the third period of Terran contact with other worlds, this time as the Ekumen, during which \"The Left Hand of Darkness\" takes place.\n\nIn the short story \"Dancing To Ganam,\" which takes place in the far future of the Hainish universe, it is said that an extreme religious movement called the Unists developed on Terra and engaged in mass slaughter of non-believers and then of rival Unists sects. It is described as \"the worst resurgence of theocratic violence since the Time of Pollution\". It unclear if this time of pollution refers to the ecological collapse described in \"The Dispossesed,\" the collapse seen in \"City Of Illusions\", or is another unexplored dark period on Terra. In any case, the inclusion of this story is meant to show that even after so many millennia in the League and the Ekumen, Terra is still in many ways culturally primitive and prone to violent self-destruction.\n\nVarious individuals from Terra play a part in other stories. In \"The Telling\", Terra's incorporation into the Ekumen is briefly explained. Also, the main character in \"The Left Hand of Darkness\" is from Terra.\n\nIn \"The Hitchhiker's Guide to the Galaxy\" series by Douglas Adams, the Earth is destroyed by a Vogon Constructor Fleet to make room for a hyperspace bypass. One of two surviving Earthmen, Arthur Dent, is affronted and dismayed to find that his planet's entry in the Guide consists of one word: \"Harmless\". Ford Prefect, a researcher for the Guide attempts (and fails) to placate him by informing him that he has written a more extensive article for the next edition, although the result of merciless editing of his original draft has reduced his version considerably, now reading \"Mostly harmless\". Dent also learns that the Earth was originally constructed by the inhabitants of the planet Magrathea, as a giant supercomputer built to find the Question to the Ultimate Answer of Life, The Universe and Everything. The computer was so large that it was often mistaken for a planet, and that it was destroyed five minutes before the program was due to complete (after ten million years of running). It also mentions that humans are descended from the passengers of an ark full of unwanted middlemen (hairdressers, telephone sanitizers, advertising executives and the like), tricked into leaving their own planet behind by spurious tales of impending destruction invented by the rest of the planet's civilization (it is mentioned that the population was then wiped out by a disease contracted from a dirty telephone). The Earth was located in Galactic sector ZZ9 Plural Z Alpha. An alternate version of Earth is the planet NowWhat, which occupies the same space as Earth, but not the same probability. In the 2005 film adaptation, a new Earth replaces the old one, and everything is restored to the moments leading up to its destruction, except for one thing: Arthur Dent is not part of the planet anymore, at his own request.\n\nIn the series \"Red Dwarf\", Earth is seen mainly as the goal of the crew's trip; Dave Lister is personally obsessed with revisiting it as his home world, especially since he is the only character to be from there as Arnold Rimmer was born on Io. The novel \"Infinity Welcomes Careful Drivers\" mentions the Earth, despite Lister's regard for it, as suffering from massive littering and environmental damage; with a giant toupee being installed to in order to cover up ozone depletion. The novel \"Better Than Life\", however, mentions Earth being voted out of inhabitability to be the solar system's chosen planet of refuse known as Garbage World. A methane build up \"farts\" the planet out of the system and sends it out into deep space where it becomes an ice planet; later moved and thawed by the actions of the crew of \"Red Dwarf\". The Earth is inhabited by giant cockroaches and the descendants of GELFs sent there as punishment for their rebellion and bred into the polymorph. Lister spends half a lifetime trapped there attempting to correct his species past actions before \"Red Dwarf\" can rescue him due to black hole time dilation. In \"Last Human\" a parallel Earth is doomed by the initiatives of Earth President John Milhous Nixon and humanity breeds GELFs and simulants to terraform a new home across the multi-dimensional omni-zone.\n\nIn the \"Stargate\" television series, Earth (designation: \"P2X-3YZ\") is described as one of countless inhabited worlds, and is revealed to be the original home world of humans all over the galaxy. In ancient history many groups of humans were kidnapped and enslaved by powerful alien races, primarily the Goa'uld. Others remained to form present day Earth societies, which interact covertly with other extraterrestrial races and civilizations, many of them human. Earth first became important in the scene after the Alterans occupied it as their new capital (its name was Terra at that point). When they were forced to relocate to Lantea in the Pegasus galaxy (several million years ago), they \"seeded\" the planet with a less advanced form of themselves. Eventually, the Goa'uld found the planet and determined that the human body is the ultimate host body for their parasitic race. Many humans were kidnapped through the Stargate the Supreme System Lord Ra brought to Earth (Earth already had its own Stargate in Antarctica but it was inoperable) but the leftover population wasn't touched; they eventually rebelled and drove the Goa'uld off the planet in 3000 BC. About 8000 BC, the Lantean remnants returned to the planet but the primitive civilization extinguished their last hope of rebuilding their once great civilization due to the presence of the Goa'uld; as such, the Ancients slowly died out or Ascended since their numbers were too small to survive, even by crossbreeding with regular humans.\n\nWith Earth largely left alone for millennia, its human population continued to advance until the rediscovery of the Stargate in 1928 and its subsequent reactivation in 1994 (since its DHD activation device was missing, they had been unable to determine its purpose until they were able to create a computer interface). Unlike an enormous majority of planets, the Stargate on Earth was kept secret from the general populace to prevent widespread panic because \"we are not alone\".\n\nHumans who are from Earth are referred to as the Tau'ri by most other life forms in the galaxy, including the Goa'uld. Earth is a relatively important player on account of the radical change it unwittingly brought about when troops under the command of Colonel Jack O'Neill killed Goa'uld Supreme System Lord Ra and started a guerrilla war against the Goa'uld. However, its importance pales in comparison to the power of the System Lords or the Free Jaffa Nation, even though after the extinction of the Asgard and the defeat of the Ori, the Tau'ri became the dominant race of known space — although they were initially at a huge technological disadvantage, they later managed to reverse-engineer Goa'uld technology to the point where they started building their own ships, though much of it was rendered obsolete when the Asgard granted a significant amount of non-weapon technology. The Tau'ri also created remarkable technological feats, such as fighters equipped with hyperdrives powered by an unstable isotope. Their power increased further when they discovered that, due to crossbreeding with Ancients before their extinction, some Earth-born humans actually possessed a unique gene required to operate some of the more advanced Ancient technology. The peak of their power occurred when the Asgard donated their entire technological knowledge to Earth prior to their extinction. With this and some Ancient technology, the Tau'ri actually surpassed their precursors and defeated the Ori.\n\nThe main interaction between Earth and the rest of the universe is via three organizations:\n\n\nIn the \"Star Trek\" universe, the unified human state based on Earth, was one of the founding members of the United Federation of Planets. Several major federal organizations are headquartered on Earth, such as the Federation Council which meets in Paris. The Federation President also keeps offices in Paris, and Starfleet Headquarters is located in San Francisco. Major events on Earth included first contact with the Vulcans (\"\"), barely averted attacks by the Borg (in \"\" and \"Star Trek: First Contact\"), Founder infiltration (\"\"), and numerous attempted coups. Like most other major Federation worlds, Earth is a near-utopia where poverty and war have been eradicated and environmental damage has been reversed. Earth was also the planet of origin for at least one other sentient species, the Voth, according to the \"\" episode \"Distant Origin\". Descendants of the hadrosauridæ, they are theorized to have fled Earth for the Delta Quadrant after an extinction event.\n\nIn the \"\" episode \"\", we learn that the name of the planet's actual government is named United Earth. Much of its early history is unknown, although recent Trek novels have revealed that Earth's governments founded United Earth by signing the historic \"Traite d'Unification\" in 2123. The episodes \"\" and \"Terra Prime\" imply that United Earth is a parliamentary system of government: we meet various government officials who are referred to as Ministers (such as Minister Nathan Samuels, played by Harry Groener). United Earth's leader is most likely a Prime Minister – possibly, but not necessarily, Samuels himself. In the novels, Earth's governmental structure is further developed. Earth is a parliamentary republic, with a separate head of state (the President) and head of government (the Prime Minister).\n\nIn the Mirror Universe, Earth is the capital of the despotic Terran Empire which rules over large portions of the Alpha and Beta Quadrants and is generally seen as the most powerful interstellar empire. \"\" revealed that the Empire had collapsed and fallen to a Klingon–Cardassian Alliance. \"Star Trek\" novels reveal that Earth was later liberated thanks to the efforts of anti-Alliance rebels and Memory Omega.\n\nEarth is the \"lost\" homeland of the terrans of the Koprulu Sector, often referred to as \"Old Earth\". Earth history is well known to us until the 21st century.\nHowever, by the time the 23rd century was reached, genetic engineering and cybernetics were in common use, and Earth's population had reached 23 billion. Consequently, a resource and overpopulation crisis was developing.\n\nEarth's corporate factions who supported the capitalization of genetic engineering and cybernetics were opposed by those who saw this as a degeneration of the human race. These groups included humanist factions as well as religious conservatives who resorted to terrorism in these turbulent times. The conflict was resolved by the creation of the United Powers League, which generally supported the humanist philosophy and controlled all nations except for a few volatile Latin American states. The UPL banned many religions and made English the worldwide language.\n\nThe UPL proceeded to arrest and kill many people who opposed its \"divinity of mankind\" philosophy (which included \"purity\" from cybernetics, mutations, and so forth). It was during this time that Doran Routhe set up the colonization of the Koprulu Sector. Contact between the colonists and Earth was seemingly lost, and the Koprulu Sector terrans could not have found their way back to Earth.\n\nWith the discovery of the protoss and zerg, the United Powers League reformed to become the United Earth Directorate and launched an invasion of the Koprulu Sector, ostensibly in an effort to protect itself from the distant aliens. The invasion was repelled by a tenuous alliance of the Sector's powers.\n\nIn the universe of the \"Babylon 5\" television series, Earth was located in a relatively uncontested and non-valuable portion of the Galaxy. As a result, the people of Earth were allowed to develop with relatively little outside interference or threat of invasion from alien races. Unified under the worldwide government of the Earth Alliance, first contact with the Centauri was made in the mid-22nd century, which led to trade with a number of different species.\n\nEarth remained a relatively minor power until the 2230s, when it intervened on behalf of a number of other races (which later became the League of Non-Aligned Worlds) during the Dilgar invasion. Following the Dilgar War, Earth began to expand its influence and was seen as a rising power in the galaxy. A disastrous first contact with the Minbari in the 2240s precipitated the Earth–Minbari War, in which Earth was nearly conquered: the military (EarthForce) was devastated and the planet's population was nearly annihilated. However, the Minbari mysteriously surrendered just prior to the final invasion of Earth.\n\nFollowing the war, Earth's major contribution on the galactic stage was the creation of the Babylon Stations, that are neutral trading posts and diplomatic havens. Earth turned inward and suffered from xenophobic tendencies in the late 2250s, early 2260s under the despotic regime of President Morgan Clark, until a military and civilian civil war, started by General William Hague and later concluded by Captain John Sheridan, overthrew the Clark regime and helped establish Earth as one of the major players in the Interstellar Alliance.\n\nIn the \"Robotech\" canon, based on \"Macross\", Earth is the homeworld of humanity and notable as one of the few places that \"The flower of life\" (the source of the powerful energy source Protoculture) can grow. In 1999 during a global war the (future) SDF-1 an alien warship crashed to Earth on Macross island. Discovering they were not alone in the universe (and in secret the fact that the SDF-1 was a warship for a giant sized alien race) the human race united and rebuilt the ship as well as using the technology to advance their own and to create a small defence fleet for earth.\n\nTen years later the ship was ready but as preparation for launch on a mission of exploration continued Zentradi warships arrived in orbit to search for the ship. Though humanity tried peaceful contact a booby trap in the SDF-1 fired the huge main gun at the Zentradi committing Earth to a devastating interstellar war. To lure the Zentradi away from Earth the SDF-1 attempted a space fold FTL jump. This went wrong transporting not only the SDF-1 but part of Macross Island, 70,000 civilians and two navy warships to an area near Pluto. Pressure held in sub-surface shelters long enough to evacuate the civilians while the ships were grafted on to the SDF-1 as flight decks. However the jump also caused the FTL drive to vanish (for unknown reasons) as such the SDF-1 had to return home under normal thrust fighting Zentradi all the way and unable to talk to earth due to jamming.\n\nDuring the conflict many Zentradi became fascinated by Earth culture and over a million ships eventually defected. The ship finally returned to Earth but was driven back into space to draw off the Zentradi again. However the Zentradi bought over four million ships to Earth and bombarded the planet. The SDF-1 took out most ships with an overload of its shield system but in the process the ship was left incapable of flight and most of the Earth's population was killed. Over the next two years the survivors tried to rebuild and at last the Earth began to green again.\n\nTwenty years later the Earth had recovered during the war with the Robotech masters, but even after Earth's victory the planet was then attacked and occupied by another set of aliens The Invid in 2031. The Invid collected what they could of the Protoculture on Earth but seem to have left the population (now millions once more) largely alone.\n\nIn 2042 the Robotech Mars expedition returned from deep space but was wiped out by the Invid during the battle to liberate the planet. The survivors were forced down to Earth where they hooked up with the local resistance groups.\n\nTwo years later a massive fleet arrived with more advanced technology given to them by the alien Hydenites. The Invid quickly left Earth rather than risk Earth's destruction by deadly weapons the Neutron_S missiles which were far more dangerous than man believed. As Humanity celebrated however the Hydenites were revealed as the Children of Shadow who had destroyed the Invid homeworld eons before. They launched a sneak attack on the human space station liberty. Another war then began.\n\nIn the various continuities of the \"Sonic the Hedgehog\" franchise, Earth is mentioned in one way or another:\n\n\n\nAt least a billion years ago in the Uplift Universe by David Brin, there was a semi-mythical species known as the Progenitors that started the Uplift cycle—adopting a pre-sentient race and over a period of a hundred thousand years of selective breeding and genetic engineering, raising them to full sentience. As a result, most sentient species in the universe are members of various clans and factions, often quite powerful, with varying beliefs.\n\nEarth was overlooked and humans evolved, evidently without a patron race. However, by the time of first contact with galactic civilization, humans had themselves raised chimpanzees and dolphins to sentience, giving the human race a claim to patron status. This claim is provisionally accepted by the major institutions of galactic civilization even as it is hotly contested by a number of senior patron races.\n\nGalactic civilization holds humanity's claim of having evolved to sentience independently to be highly controversial at best, and an offensive heresy at worst. Some clans holding the latter view have actively conspired to have humanity's patron status officially vacated and to adopt the \"wolfling\" race themselves, thus gaining three sentient races and control of \"fallow\" genetic material: Earth's wealth of species with uplift potential. Only the conservative and ponderous nature of galactic institutions and the rivalry of other clans reluctant to see the Earth's races claimed by another have prevented this.\n\nIn the Worldwar novels by Harry Turtledove, Earth is the human homeworld and is attacked by the aliens known as The Race in 1942. All sides in the Second World War are forced to unify to fight this threat, and despite superior technology the Lizards (a human racial epithet for the aliens) are fought to a draw by 1944.\n\nIn 1962 another Race fleet arrives carrying a civilian colony force of nearly 100 million, in 1965 the Race and the German Reich fight a major war which Germany loses.\n\nIn 1994 humanity has caught up to the Race enough to send a slower-than-light starship to the Race's home world, where it arrives in 2031. Soon after another ship arrives, an FTL-capable ship, indicating that humanity has now surpassed the Race in technology.\n\n\nIn the Shannara series of books by Terry Brooks, the setting is a post-apocalyptic earth that was destroyed after a nuclear holocaust wiped out most of humanity and mutated the survivors into Men, Gnomes, Dwarves, and Trolls. Elves are also there, but according to Allanon's recounting of history, the Elves always existed in our current world. Before the Great Wars, as the nuclear holocaust is referred to, humans had advanced to a utopian society.\n\nIn the Death gate cycle series of books by Margaret Weis and Tracy Hickman, a nuclear holocaust led to the creation of a two mutant strains of humans who developed fantastic magical powers. Other races, such as Dwarves and Elves were also present, even in pre-holocaust Earth, but were hidden from humanity. In this universe, the Earth was destroyed by the Sartan, one of the magical races that evolved from humans.\n\nAn incomplete list of what Earth natives have been referred to in various Sci-Fi worlds:\n"}
{"id": "52408383", "url": "https://en.wikipedia.org/wiki?curid=52408383", "title": "Endogenosymbiosis", "text": "Endogenosymbiosis\n\nEndogenosymbiosis is an evolutionary process, proposed by the evolutionary and environmental biologist Roberto Cazzolla Gatti, in which \"gene carriers\" (viruses, retroviruses and bacteriophages) and symbiotic prokaryotic cells (bacteria or archaea) could share parts or all of their genomes in an endogenous symbiotic relationship with their hosts.\n\nThe related process of symbiogenesis or endosymbiosis was proposed by Lynn Margulis in 1967. She argued that the internal symbiosis of bacteria-like organisms had formed organelles like chloroplasts and mitochondria. She proposed that this had created the eukaryotes, and thus driven the expansion of life on Earth. She had argued that this process of symbiotic collaboration had run alongside the classical Darwinian cycle of mutation, natural selection and adaptation.\n\nRoberto Cazzolla Gatti, Ph.D., associate professor at Tomsk State University (Russia), argued in his hypothesis that \"the main likely cause of the evolution of sexual reproduction, the parasitism, also represents the origin of biodiversity\". \nIn other terms, this theory suggests that sexual reproduction acts as a conservative system against the inclusion of new genetic variations into cells' DNA (supported by the DNA repair systems) and, instead, the evolution of species can take place only when this preservative system fails to contrast the inclusion, within the host genome, of hexogen parts of DNA (and RNA) coming from obliged \"parasitic\" elements (viruses and phages) that establish a symbiosis with their hosts. \n\"As two parallel evolutionary lines – Cazzolla Gatti wrote in his original paper – sexual reproduction seems to preserve what the endogenosymbiosis moves to diversify. Following the former process, the species can adapt slowly and indefinitely to the external factors, adjusting themselves, but not 'creating' novelty. The latter process, instead, leads to the speciation due to sudden changes in genes sequences. Not only organelles can be symbiotic with other cells, as suggested Lynn Margulis, but entire pieces of genetic material coming from symbiotic parasites, can be included in the host DNA, changing the gene expression and addressing the speciation process\".\n\nThis idea challenges the canonical natural selection models based on the gradualism of the mutation-adaptation pattern, providing more support to the punctuated equilibrium theory proposed by Stephen Jay Gould and Niles Eldredge.\n\nTwo independent studies provide support for the hypothesis. Jamie E. Henzy and Welkin E. Johnson demonstrated that the complex evolutionary history of the IFIT (Interferon Induced proteins with Tetratricopeptide repeats) family of antiviral genes has been shaped by continuous interactions between mammalian hosts and their many viruses.\n\nDavid Enard and colleagues estimated that viruses have driven close to 30% of all adaptive amino acid changes in the part of the human proteome conserved within mammals. Their results suggest that viruses are one of the most dominant drivers of evolutionary change across mammalian and human proteomes.\n\nPreviously, it was estimated that about 7–8% percent of the entire human genome carry about 100,000 pieces of DNA that came from endogenous retroviruses. This may be an underestimate.\n\nIn 2016 the biologists Sarah R. Bordestein and Seth R. Bordestein reported that genes are frequently transferred between hosts and parasites. Eukaryotic genes are often co-opted by viruses and bacterial genes are commonly found in bacteriophages. The presence of bacteriophages in symbiotic bacteria that obligately reside in eukaryotes may promote eukayotic DNA transfers to bacteriophages.\n"}
{"id": "1610231", "url": "https://en.wikipedia.org/wiki?curid=1610231", "title": "Energy density", "text": "Energy density\n\nEnergy density is the amount of energy stored in a given system or region of space per unit volume. Colloquially it may also be used for energy per unit mass, though the accurate term for this is specific energy. Often only the \"useful\" or extractable energy is measured, which is to say that inaccessible energy (such as rest mass energy) is ignored. In cosmological and other general relativistic contexts, however, the energy densities considered are those that correspond to the elements of the stress–energy tensor and therefore do include mass energy as well as energy densities associated with the pressures described in the next paragraph.\n\nEnergy per unit volume has the same physical units as pressure, and in many circumstances is a synonym: for example, the energy density of a magnetic field may be expressed as (and behaves as) a physical pressure, and the energy required to compress a compressed gas a little more may be determined by multiplying the difference between the gas pressure and the external pressure by the change in volume. In short, pressure is a measure of the enthalpy per unit volume of a system. A pressure gradient has the potential to perform work on the surroundings by converting enthalpy to work until equilibrium is reached.\n\nThere are many different types of energy stored in materials, and it takes a particular type of reaction to release each type of energy. In order of the typical magnitude of the energy released, these types of reactions are: nuclear, chemical, electrochemical, and electrical.\n\nNuclear reactions are used by stars and nuclear power plants, both of which derive energy from the binding energy of nuclei. Chemical reactions are used by animals to derive energy from food, and by automobiles to derive energy from gasoline. Liquid hydrocarbons (fuels such as gasoline, diesel and kerozene) are today the most dense way known to economically store and transport chemical energy at a very large scale (1 kg of diesel fuel burns with the oxygen contained in ~15 kg of air). Electrochemical reactions are used by most mobile devices such as laptop computers and mobile phones to release the energy from batteries.\n\nThe following is a list of the thermal energy densities (that is to say: the amount of heat energy that can be extracted) of commonly used or well-known energy storage materials; it doesn't include uncommon or experimental materials. Note that this list does not consider the mass of reactants commonly available such as the oxygen required for combustion or the energy efficiency in use. An extended version of this table is found at Energy density#Extended Reference Table. Major reference = .\n\nThe following unit conversions may be helpful when considering the data in the table: 3.6 MJ = 1 kWh ≈ 1.34 HPh.\n\nIn energy storage applications the energy density relates the mass of an energy store to the volume of the storage facility, e.g. the fuel tank. The higher the energy density of the fuel, the more energy may be stored or transported for the same amount of volume. The energy density of a fuel per unit mass is called the specific energy of that fuel. In general an engine using that fuel will generate less kinetic energy due to inefficiencies and thermodynamic considerations—hence the specific fuel consumption of an engine will always be greater than its rate of production of the kinetic energy of motion.\n\nThe greatest energy source by far is mass itself. This energy, \"E = mc\", where \"m = ρV\", \"ρ\" is the mass per unit volume, \"V\" is the volume of the mass itself and \"c\" is the speed of light. This energy, however, can be released only by the processes of nuclear fission (0.1%), nuclear fusion (1%), or the annihilation of some or all of the matter in the volume \"V\" by matter-antimatter collisions (100%). Nuclear reactions cannot be realized by chemical reactions such as combustion. Although greater matter densities can be achieved, the density of a neutron star would approximate the most dense system capable of matter-antimatter annihilation possible. A black hole, although denser than a neutron star, does not have an equivalent anti-particle form, but would offer the same 100% conversion rate of mass to energy in the form of Hawking radiation. In the case of relatively small black holes (smaller than astronomical objects) the power output would be tremendous.\n\nThe highest density sources of energy aside from antimatter are fusion and fission. Fusion includes energy from the sun which will be available for billions of years (in the form of sunlight) but so far (2018), sustained fusion power production continues to be elusive. \n\nPower from fission of uranium and thorium in nuclear power plants will be available for many decades or even centuries because of the plentiful supply of the elements on earth, though the full potential of this source can only be realised through breeder reactors, which are, apart from the BN-600 reactor, not yet used commercially. Coal, gas, and petroleum are the current primary energy sources in the U.S. but have a much lower energy density. Burning local biomass fuels supplies household energy needs (cooking fires, oil lamps, etc.) worldwide. \n\nThe density of thermal energy contained in the core of a light water reactor (PWR or BWR) of typically 1 GWe (1 000 MW electrical corresponding to ~3 000 MW thermal) is in the range of 10 to 100 MW of thermal energy per cubic meter of cooling water depending on the location considered in the system (the core itself (~30 m), the reactor pressure vessel (~50 m), or the whole primary circuit (~300 m)). This represents a considerable density of energy which requires under all circumstances a continuous water flow at high velocity in order to be able to remove the heat from the core, even after an emergency shutdown of the reactor. The incapacity to cool the cores of three boiling water reactors (BWR) at Fukushima in 2011 after the tsunami and the resulting loss of the external electrical power and of the cold source was the cause of the meltdown of the three cores in only a few hours. Meanwhile, the three reactors were correctly shut down just after the Tōhoku earthquake. This extremely high power density distinguishes nuclear power plants (NPP's) from any thermal power plants (burning coal, fuel or gas) or any chemical plants and explains the large redundancy required to permanently control the neutron reactivity and to remove the residual heat from the core of NPP's.\n\nEnergy density differs from energy conversion efficiency (net output per input) or embodied energy (the energy output costs to provide, as harvesting, refining, distributing, and dealing with pollution all use energy). Large scale, intensive energy use impacts and is impacted by climate, waste storage, and environmental consequences.\n\nNo single energy storage method boasts the best in specific power, specific energy, and energy density. Peukert's Law describes how the amount of useful energy that can be obtained (for a lead-acid cell) depends on how quickly we pull it out. To maximize both specific energy and energy density, one can compute the specific energy density of a substance by multiplying the two values together, where the higher the number, the better the substance is at storing energy efficiently.\n\nAlternative options are discussed for energy storage to increase energy density and decrease charging time.\n\nGravimetric and volumetric energy density of some fuels and storage technologies (modified from the Gasoline article):\n\nThis table lists energy densities of systems that require external components, such as oxidisers or a heat sink or source. These figures do not take into account the mass and volume of the required components as they are assumed to be freely available and present in the atmosphere. Such systems cannot be compared with self-contained systems. These values may not be computed at the same reference conditions.\n\nDivide joule/m by 10 to get MJ/L. Divide MJ/L by 3.6 to get kWh/L.\n\nElectric and magnetic fields store energy. In a vacuum, the (volumetric) energy density is given by\n\nwhere E is the electric field and B is the magnetic field. The solution will be (in SI units) in Joules per cubic metre. In the context of magnetohydrodynamics, the physics of conductive fluids, the magnetic energy density behaves like an additional pressure that adds to the gas pressure of a plasma.\n\nIn normal (linear and nondispersive) substances, the energy density (in SI units) is\n\nwhere D is the electric displacement field and H is the magnetizing field.\n\nIn the case of absence of magnetic fields, by exploting Fröhlich's relationships it is also possible to extend these equations to anisotropy and nonlinearity dielectrics, as well as to calculate the correlated Helmholtz free energy and entropy densities.\n\n\n\n"}
{"id": "268020", "url": "https://en.wikipedia.org/wiki?curid=268020", "title": "Evolutionary computation", "text": "Evolutionary computation\n\nIn computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.\n\nIn evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. Each new generation is produced by stochastically removing less desired solutions, and introducing small random changes. In biological terminology, a population of solutions is subjected to natural selection (or artificial selection) and mutation. As a result, the population will gradually evolve to increase in fitness, in this case the chosen fitness function of the algorithm.\n\nEvolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. Many variants and extensions exist, suited to more specific families of problems and data structures. Evolutionary computation is also sometimes used in evolutionary biology as an \"in silico\" experimental procedure to study common aspects of general evolutionary processes.\n\nThe use of evolutionary principles for automated problem solving originated in the 1950s. It was not until the 1960s that three distinct interpretations of this idea started to be developed in three different places.\n\n\"Evolutionary programming\" was introduced by Lawrence J. Fogel in the US, while John Henry Holland called his method a \"genetic algorithm\". In Germany Ingo Rechenberg and Hans-Paul Schwefel introduced \"evolution strategies\". These areas developed separately for about 15 years. From the early nineties on they are unified as different representatives (\"dialects\") of one technology, called \"evolutionary computing\". Also in the early nineties, a fourth stream following the general ideas had emerged – \"genetic programming\". Since the 1990s, nature-inspired algorithms are becoming an increasingly significant part of the evolutionary computation.\n\nThese terminologies denote the field of evolutionary computing and consider evolutionary programming, evolution strategies, genetic algorithms, and genetic programming as sub-areas.\n\nSimulations of evolution using evolutionary algorithms and artificial life started with the work of Nils Aall Barricelli in the 1960s, and was extended by Alex Fraser, who published a series of papers on simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s and early 1970s, who used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimise the design of systems.\n\nEvolutionary computing techniques mostly involve metaheuristic optimization algorithms. Broadly speaking, the field includes:\n\n\nEvolutionary algorithms form a subset of evolutionary computation in that they generally only involve techniques implementing mechanisms inspired by biological evolution such as reproduction, mutation, recombination, natural selection and survival of the fittest. Candidate solutions to the optimization problem play the role of individuals in a population, and the cost function determines the environment within which the solutions \"live\" (see also fitness function). Evolution of the population then takes place after the repeated application of the above operators.\n\nIn this process, there are two main forces that form the basis of evolutionary systems: Recombination and mutation create the necessary diversity and thereby facilitate novelty, while selection acts as a force increasing quality.\n\nMany aspects of such an evolutionary process are stochastic. Changed pieces of information due to recombination and mutation are randomly chosen. On the other hand, selection operators can be either deterministic, or stochastic. In the latter case, individuals with a higher fitness have a higher chance to be selected than individuals with a lower fitness, but typically even the weak individuals have a chance to become a parent or to survive.\n\nGenetic algorithms deliver methods to model biological systems and systems biology that are linked to the theory of dynamical systems, since they are used to predict the future states of the system. This is just a vivid (but perhaps misleading) way of drawing attention to the orderly, well-controlled and highly structured character of development in biology.\n\nHowever, the use of algorithms and informatics, in particular of computational theory, beyond the analogy to dynamical systems, is also relevant to understand evolution itself. \n\nThis view has the merit of recognizing that there is no central control of development; organisms develop as a result of local interactions within and between cells. The most promising ideas about program-development parallels seem to us to be ones that point to an apparently close analogy between processes within cells, and the low-level operation of modern computers . Thus, biological systems are like computational machines that process input information to compute next states, such that biological systems are closer to a computation than classical dynamical system .\n\nFurthermore, following concepts from computational theory, micro processes in biological organisms are fundamentally incomplete and undecidable (completeness (logic)), implying that “there is more than a crude metaphor behind the analogy between cells and computers .\n\nThe analogy to computation extends also to the relationship between inheritance systems and biological structure, which is often thought to reveal one of the most pressing problems in explaining the origins of life.\n\nThe list of active researchers is naturally dynamic and non-exhaustive. A network analysis of the community was published in 2007.\n\n\nThe main conferences in the evolutionary computation area include \n\n"}
{"id": "2132454", "url": "https://en.wikipedia.org/wiki?curid=2132454", "title": "Evolutionary graph theory", "text": "Evolutionary graph theory\n\nEvolutionary graph theory is an area of research lying at the intersection of graph theory, probability theory, and mathematical biology. Evolutionary graph theory is an approach to studying how topology affects evolution of a population. That the underlying topology can substantially affect the results of the evolutionary process is seen most clearly in a paper by Erez Lieberman, Christoph Hauert and Martin Nowak.\n\nIn evolutionary graph theory, individuals occupy vertices of a weighted directed graph and the weight w of an edge from vertex \"i\" to vertex \"j\" denotes the probability of \"i\" replacing \"j\". The weight corresponds to the biological notion of fitness where fitter types propagate more readily. \nOne property studied on graphs with two types of individuals is the \"fixation probability\", which is defined as the probability that a single, randomly placed mutant of type A will replace a population of type B. According to the \"isothermal theorem\", a graph has the same fixation probability as the corresponding Moran process if and only if it is isothermal, thus the sum of all weights that lead into a vertex is the same for all vertices. Thus, for example, a complete graph with equal weights describes a Moran process. The fixation probability is\nwhere \"r\" is the relative fitness of the invading type.\n\nGraphs can be classified into amplifiers of selection and suppressors of selection. If the fixation probability of a single advantageous mutation formula_2 is higher than the fixation probability of the corresponding Moran process formula_3 then the graph is an amplifier, otherwise a suppressor of selection. One example of the suppressor of selection is a linear process where only vertex \"i-1\" can replace vertex \"i\" (but not the other way around). In this case the fixation probability is formula_4 (where \"N\" is the number of vertices) since this is the probability that the mutation arises in the first vertex which will eventually replace all the other ones. Since formula_5 for all \"r\" greater than 1, this graph is by definition a suppressor of selection.\n\nEvolutionary graph theory may also be studied in a dual formulation, as a coalescing random walk, or as a stochastic process. We may consider the mutant population on a graph as a random walk between absorbing barriers representing mutant extinction and mutant fixation. For highly symmetric graphs, we can then use martingales to find the \"fixation probability\" as illustrated by Monk (2018).\n\nAlso evolutionary games can be studied on graphs where again an edge between \"i\" and \"j\" means that these two individuals will play a game against each other.\n\nClosely related stochastic processes include the voter model, which was introduced by Clifford and Sudbury (1973) and independently by Holley and Liggett (1975), and which has been studied extensively.\n\n\nA virtual laboratory for studying evolution on graphs:\n"}
{"id": "21873573", "url": "https://en.wikipedia.org/wiki?curid=21873573", "title": "Freddy Fox", "text": "Freddy Fox\n\nFreddy Fox () is a hardcover picture book written and illustrated by Ronald J. Meyer. It tells the story of a young fox and his family, following the daily activities of the fox kit, Freddy. Freddy Fox teaches the reader about red foxes, including that they are born shades of gray, how their den is built, what they eat, and family size. Then through a simple segue, Freddy's red fox mother teaches the young fox about other animals and traits attributed to them. By learning his lessons, Freddy will grow up to be a wise fox. All of the lessons, such as the bear eats a balanced diet or the raccoon washes his hands before eating, can easily be translated into lessons for young children.\n\nThe book was first published in 2004. It won the Silver Mom's Choice Award in the \"Animal Kingdom\" category in 2008.\n\nThe author is a wildlife photographer, and he used his wildlife photographs to illustrate the book. \n\n"}
{"id": "113728", "url": "https://en.wikipedia.org/wiki?curid=113728", "title": "Geothermal energy", "text": "Geothermal energy\n\nGeothermal energy is thermal energy generated and stored in the Earth. Thermal energy is the energy that determines the temperature of matter. The geothermal energy of the Earth's crust originates from the original formation of the planet and from radioactive decay of materials (in currently uncertain but possibly roughly equal proportions). The geothermal gradient, which is the difference in temperature between the core of the planet and its surface, drives a continuous conduction of thermal energy in the form of heat from the core to the surface. The adjective \"geothermal\" originates from the Greek roots \"γη (ge)\", meaning earth, and \"θερμος (thermos)\", meaning hot.\n\nEarth's internal heat is thermal energy generated from radioactive decay and continual heat loss from Earth's formation. Temperatures at the core–mantle boundary may reach over 4000 °C (7,200 °F). The high temperature and pressure in Earth's interior cause some rock to melt and solid mantle to behave plastically, resulting in portions of the mantle convecting upward since it is lighter than the surrounding rock. Rock and water is heated in the crust, sometimes up to 370 °C (700 °F).\n\nWith water from hot springs, geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation. Worldwide, 11,700 megawatts (MW) of geothermal power was available in 2013. An additional 28 gigawatts of direct geothermal heating capacity is installed for district heating, space heating, spas, industrial processes, desalination and agricultural applications as of 2010.\n\nGeothermal power is cost-effective, reliable, sustainable, and environmentally friendly, but has historically been limited to areas near tectonic plate boundaries. Recent technological advances have dramatically expanded the range and size of viable resources, especially for applications such as home heating, opening a potential for widespread exploitation. Geothermal wells release greenhouse gases trapped deep within the earth, but these emissions are much lower per energy unit than those of fossil fuels.\n\nThe Earth's geothermal resources are theoretically more than adequate to supply humanity's energy needs, but only a very small fraction may be profitably exploited. Drilling and exploration for deep resources is very expensive. Forecasts for the future of geothermal power depend on assumptions about technology, energy prices, subsidies, plate boundary movement and interest rates. Pilot programs like EWEB's customer opt in Green Power Program show that customers would be willing to pay a little more for a renewable energy source like geothermal. But as a result of government assisted research and industry experience, the cost of generating geothermal power has decreased by 25% over the past two decades. In 2001, geothermal energy costs between two and ten US cents per kWh. \n\nHot springs have been used for bathing at least since Paleolithic times. The oldest known spa is a stone pool on China's Lisan mountain built in the Qin Dynasty in the 3rd century BC, at the same site where the Huaqing Chi palace was later built. In the first century AD, Romans conquered \"Aquae Sulis\", now Bath, Somerset, England, and used the hot springs there to feed public baths and underfloor heating. The admission fees for these baths probably represent the first commercial use of geothermal power. The world's oldest geothermal district heating system in Chaudes-Aigues, France, has been operating since the 14th century. The earliest industrial exploitation began in 1827 with the use of geyser steam to extract boric acid from volcanic mud in Larderello, Italy.\n\nIn 1892, America's first district heating system in Boise, Idaho was powered directly by geothermal energy, and was copied in Klamath Falls, Oregon in 1900. The first known building in the world to utilize geothermal energy as its primary heat source was the Hot Lake Hotel in Union County, Oregon, whose construction was completed in 1907. A deep geothermal well was used to heat greenhouses in Boise in 1926, and geysers were used to heat greenhouses in Iceland and Tuscany at about the same time. Charlie Lieb developed the first downhole heat exchanger in 1930 to heat his house. Steam and hot water from geysers began heating homes in Iceland starting in 1943.\n\nIn the 20th century, demand for electricity led to the consideration of geothermal power as a generating source. Prince Piero Ginori Conti tested the first geothermal power generator on 4 July 1904, at the same Larderello dry steam field where geothermal acid extraction began. It successfully lit four light bulbs. Later, in 1911, the world's first commercial geothermal power plant was built there. It was the world's only industrial producer of geothermal electricity until New Zealand built a plant in 1958. In 2012, it produced some 594 megawatts.\n\nLord Kelvin invented the heat pump in 1852, and Heinrich Zoelly had patented the idea of using it to draw heat from the ground in 1912. But it was not until the late 1940s that the geothermal heat pump was successfully implemented. The earliest one was probably Robert C. Webber's home-made 2.2 kW direct-exchange system, but sources disagree as to the exact timeline of his invention. J. Donald Kroeker designed the first commercial geothermal heat pump to heat the Commonwealth Building (Portland, Oregon) and demonstrated it in 1946. Professor Carl Nielsen of Ohio State University built the first residential open loop version in his home in 1948. The technology became popular in Sweden as a result of the 1973 oil crisis, and has been growing slowly in worldwide acceptance since then. The 1979 development of polybutylene pipe greatly augmented the heat pump’s economic viability.\n\nIn 1960, Pacific Gas and Electric began operation of the first successful geothermal electric power plant in the United States at The Geysers in California. The original turbine lasted for more than 30 years and produced 11 MW net power.\n\nThe binary cycle power plant was first demonstrated in 1967 in the USSR and later introduced to the US in 1981. This technology allows the generation of electricity from much lower temperature resources than previously. In 2006, a binary cycle plant in Chena Hot Springs, Alaska, came on-line, producing electricity from a record low fluid temperature of .\n\nThe International Geothermal Association (IGA) has reported that 10,715 megawatts (MW) of geothermal power in 24 countries is online, which was expected to generate 67,246 GWh of electricity in 2010. This represents a 20% increase in online capacity since 2005. IGA projects growth to 18,500 MW by 2015, due to the projects presently under consideration, often in areas previously assumed to have little exploitable resources.\n\nIn 2010, the United States led the world in geothermal electricity production with 3,086 MW of installed capacity from 77 power plants. The largest group of geothermal power plants in the world is located at The Geysers, a geothermal field in California. The Philippines is the second highest producer, with 1,904 MW of capacity online. Geothermal power makes up approximately 27% of Philippine electricity generation.\n\nIn 2016, Indonesia set in third with 1,647 MW online behind USA at 3,450 MW and the Philippines at 1,870 MW, but Indonesia will become second due to an additional online 130 MW at the end of 2016 and 255 MW in 2017. Indonesia's 28,994 MW are the largest geothermal reserves in the world, and it is predicted to overtake the USA in the next decade.\n\nGeothermal electric plants were traditionally built exclusively on the edges of tectonic plates where high temperature geothermal resources are available near the surface. The development of binary cycle power plants and improvements in drilling and extraction technology enable enhanced geothermal systems over a much greater geographical range. Demonstration projects are operational in Landau-Pfalz, Germany, and Soultz-sous-Forêts, France, while an earlier effort in Basel, Switzerland was shut down after it triggered earthquakes. Other demonstration projects are under construction in Australia, the United Kingdom, and the United States of America.\n\nThe thermal efficiency of geothermal electric plants is low, around 10–23%, because geothermal fluids do not reach the high temperatures of steam from boilers. The laws of thermodynamics limits the efficiency of heat engines in extracting useful energy. Exhaust heat is wasted, unless it can be used directly and locally, for example in greenhouses, timber mills, and district heating. System efficiency does not materially affect operational costs as it would for plants that use fuel, but it does affect return on the capital used to build the plant. In order to produce more energy than the pumps consume, electricity generation requires relatively hot fields and specialized heat cycles. Because geothermal power does not rely on variable sources of energy, unlike, for example, wind or solar, its capacity factor can be quite large – up to 96% has been demonstrated. The global average was 73% in 2005.\n\nGeothermal energy comes in either \"vapor-dominated\" or \"liquid-dominated\" forms. Larderello and The Geysers are vapor-dominated. Vapor-dominated sites offer temperatures from 240 to 300 °C that produce superheated steam.\n\nLiquid-dominated reservoirs (LDRs) were more common with temperatures greater than and are found near young volcanoes surrounding the Pacific Ocean and in rift zones and hot spots. \"Flash plants\" are the common way to generate electricity from these sources. Pumps are generally not required, powered instead when the water turns to steam. Most wells generate 2-10 MWe. Steam is separated from liquid via cyclone separators, while the liquid is returned to the reservoir for reheating/reuse. As of 2013, the largest liquid system is Cerro Prieto in Mexico, which generates 750 MWe from temperatures reaching . The Salton Sea field in Southern California offers the potential of generating 2000 MWe.\n\nLower temperature LDRs (120–200 °C) require pumping. They are common in extensional terrains, where heating takes place via deep circulation along faults, such as in the Western US and Turkey. Water passes through a heat exchanger in a Rankine cycle binary plant. The water vaporizes an organic working fluid that drives a turbine. These binary plants originated in the Soviet Union in the late 1960s and predominate in new US plants. Binary plants have no emissions.\n\nLower temperature sources produce the energy equivalent of 100M BBL per year. Sources with temperatures of 30–150 °C are used without conversion to electricity as district heating, greenhouses, fisheries, mineral recovery, industrial process heating and bathing in 75 countries. Heat pumps extract energy from shallow sources at 10–20 °C in 43 countries for use in space heating and cooling. Home heating is the fastest-growing means of exploiting geothermal energy, with global annual growth rate of 30% in 2005 and 20% in 2012.\n\nApproximately 270 petajoules (PJ) of geothermal heating was used in 2004. More than half went for space heating, and another third for heated pools. The remainder supported industrial and agricultural applications. Global installed capacity was 28 GW, but capacity factors tend to be low (30% on average) since heat is mostly needed in winter. Some 88 PJ for space heating was extracted by an estimated 1.3 million geothermal heat pumps with a total capacity of 15 GW.\n\nHeat for these purposes may also be extracted from co-generation at a geothermal electrical plant.\n\nHeating is cost-effective at many more sites than electricity generation. At natural hot springs or geysers, water can be piped directly into radiators. In hot, dry ground, earth tubes or downhole heat exchangers can collect the heat. However, even in areas where the ground is colder than room temperature, heat can often be extracted with a geothermal heat pump more cost-effectively and cleanly than by conventional furnaces. These devices draw on much shallower and colder resources than traditional geothermal techniques. They frequently combine functions, including air conditioning, seasonal thermal energy storage, solar energy collection, and electric heating. Heat pumps can be used for space heating essentially anywhere.\n\nIceland is the world leader in direct applications. Some 92.5% of its homes are heated with geothermal energy, saving Iceland over $100 million annually in avoided oil imports. Reykjavík, Iceland has the world's biggest district heating system, often used to heat pathways and roads to hinder the accumulation of ice. Once known as the most polluted city in the world, it is now one of the cleanest.\n\nEnhanced geothermal systems (EGS) actively inject water into wells to be heated and pumped back out. The water is injected under high pressure to expand existing rock fissures to enable the water to freely flow in and out. The technique was adapted from oil and gas extraction techniques. However, the geologic formations are deeper and no toxic chemicals are used, reducing the possibility of environmental damage. Drillers can employ directional drilling to expand the size of the reservoir.\n\nSmall-scale EGS have been installed in the Rhine Graben at Soultz-sous-Forêts in France and at Landau and Insheim in Germany.\n\nGeothermal power requires no fuel (except for pumps), and is therefore immune to fuel cost fluctuations. However, capital costs are significant. Drilling accounts for over half the costs, and exploration of deep resources entails significant risks. A typical well doublet (extraction and injection wells) in Nevada can support 4.5 megawatts (MW) and costs about $10 million to drill, with a 20% failure rate.\n\nIn total, electrical plant construction and well drilling cost about €2–5 million per MW of electrical capacity, while the break–even price is 0.04–0.10 € per kW·h. Enhanced geothermal systems tend to be on the high side of these ranges, with capital costs above $4 million per MW and break–even above $0.054 per kW·h in 2007. Direct heating applications can use much shallower wells with lower temperatures, so smaller systems with lower costs and risks are feasible. Residential geothermal heat pumps with a capacity of 10 kilowatt (kW) are routinely installed for around $1–3,000 per kilowatt. District heating systems may benefit from economies of scale if demand is geographically dense, as in cities and greenhouses, but otherwise piping installation dominates capital costs. The capital cost of one such district heating system in Bavaria was estimated at somewhat over 1 million € per MW. Direct systems of any size are much simpler than electric generators and have lower maintenance costs per kW·h, but they must consume electricity to run pumps and compressors. Some governments subsidize geothermal projects.\n\nGeothermal power is highly scalable: from a rural village to an entire city.\n\nThe most developed geothermal field in the United States is The Geysers in Northern California.\n\nGeothermal projects have several stages of development. Each phase has associated risks. At the early stages of reconnaissance and geophysical surveys, many projects are cancelled, making that phase unsuitable for traditional lending. Projects moving forward from the identification, exploration and exploratory drilling often trade equity for financing.\n\nThe Earth's internal thermal energy flows to the surface by conduction at a rate of 44.2 terawatts (TW), and is replenished by radioactive decay of minerals at a rate of 30 TW. These power rates are more than double humanity’s current energy consumption from all primary sources, but most of this energy flow is not recoverable. In addition to the internal heat flows, the top layer of the surface to a depth of is heated by solar energy during the summer, and releases that energy and cools during the winter.\n\nOutside of the seasonal variations, the geothermal gradient of temperatures through the crust is 25–30 °C (77–86 °F) per kilometer of depth in most of the world. The conductive heat flux averages 0.1 MW/km. These values are much higher near tectonic plate boundaries where the crust is thinner. They may be further augmented by fluid circulation, either through magma conduits, hot springs, hydrothermal circulation or a combination of these.\n\nA geothermal heat pump can extract enough heat from shallow ground anywhere in the world to provide home heating, but industrial applications need the higher temperatures of deep resources. The thermal efficiency and profitability of electricity generation is particularly sensitive to temperature. The most demanding applications receive the greatest benefit from a high natural heat flux, ideally from using a hot spring. The next best option is to drill a well into a hot aquifer. If no adequate aquifer is available, an artificial one may be built by injecting water to hydraulically fracture the bedrock. This last approach is called hot dry rock geothermal energy in Europe, or enhanced geothermal systems in North America. Much greater potential may be available from this approach than from conventional tapping of natural aquifers.\n\nEstimates of the potential for electricity generation from geothermal energy vary sixfold, from depending on the scale of investments. Upper estimates of geothermal resources assume enhanced geothermal wells as deep as , whereas existing geothermal wells are rarely more than deep. Wells of this depth are now common in the petroleum industry. The deepest research well in the world, the Kola superdeep borehole, is deep.\n\nMyanmar Engineering Society has identified at least 39 locations (in Myanmar) capable of geothermal power production and some of these hydrothermal reservoirs lie quite close to Yangon which is a significant underutilized resource.\n\nAccording to the Geothermal Energy Association (GEA) installed geothermal capacity in the United States grew by 5%, or 147.05 MW, ce the last annual survey in March 2012. This increase came from seven geothermal projects that began production in 2012. GEA also revised its 2011 estimate of installed capacity upward by 128 MW, bringing current installed U.S. geothermal capacity to 3,386 MW.\n\nGeothermal power is considered to be renewable because any projected heat extraction is small compared to the Earth's heat content. The Earth has an internal heat content of 10 joules (3·10 TW·hr), approximately 100 billion times current (2010) worldwide annual energy consumption. About 20% of this is residual heat from planetary accretion, and the remainder is attributed to higher radioactive decay rates that existed in the past. Natural heat flows are not in equilibrium, and the planet is slowly cooling down on geologic timescales. Human extraction taps a minute fraction of the natural outflow, often without accelerating it.\n\nGeothermal power is also considered to be sustainable thanks to its power to sustain the Earth’s intricate ecosystems. By using geothermal sources of energy present generations of humans will not endanger the capability of future generations to use their own resources to the same amount that those energy sources are presently used. Further, due to its low emissions geothermal energy is considered to have excellent potential for mitigation of global warming.\n\nEven though geothermal power is globally sustainable, extraction must still be monitored to avoid local depletion. Over the course of decades, individual wells draw down local temperatures and water levels until a new equilibrium is reached with natural flows. The three oldest sites, at Larderello, Wairakei, and the Geysers have experienced reduced output because of local depletion. Heat and water, in uncertain proportions, were extracted faster than they were replenished. If production is reduced and water is reinjected, these wells could theoretically recover their full potential. Such mitigation strategies have already been implemented at some sites. The long-term sustainability of geothermal energy has been demonstrated at the Lardarello field in Italy since 1913, at the Wairakei field in New Zealand since 1958, and at The Geysers field in California since 1960.\n\nFalling electricity production may be boosted through drilling additional supply boreholes, as at Poihipi and Ohaaki. The Wairakei power station has been running much longer, with its first unit commissioned in November 1958, and it attained its peak generation of 173MW in 1965, but already the supply of high-pressure steam was faltering, in 1982 being derated to intermediate pressure and the station managing 157MW. Around the start of the 21st century it was managing about 150MW, then in 2005 two 8MW isopentane systems were added, boosting the station's output by about 14MW. Detailed data are unavailable, being lost due to re-organisations. One such re-organisation in 1996 causes the absence of early data for Poihipi (started 1996), and the gap in 1996/7 for Wairakei and Ohaaki; half-hourly data for Ohaaki's first few months of operation are also missing, as well as for most of Wairakei's history.\n\nFluids drawn from the deep earth carry a mixture of gases, notably carbon dioxide (), hydrogen sulfide (), methane () and ammonia (). These pollutants contribute to global warming, acid rain, and noxious smells if released. Existing geothermal electric plants emit an average of of per megawatt-hour (MW·h) of electricity, a small fraction of the emission intensity of conventional fossil fuel plants. Plants that experience high levels of acids and volatile chemicals are usually equipped with emission-control systems to reduce the exhaust.\n\nIn addition to dissolved gases, hot water from geothermal sources may hold in solution trace amounts of toxic elements such as mercury, arsenic, boron, and antimony. These chemicals precipitate as the water cools, and can cause environmental damage if released. The modern practice of injecting cooled geothermal fluids back into the Earth to stimulate production has the side benefit of reducing this environmental risk.\n\nDirect geothermal heating systems contain pumps and compressors, which may consume energy from a polluting source. This parasitic load is normally a fraction of the heat output, so it is always less polluting than electric heating. However, if the electricity is produced by burning fossil fuels, then the net emissions of geothermal heating may be comparable to directly burning the fuel for heat. For example, a geothermal heat pump powered by electricity from a combined cycle natural gas plant would produce about as much pollution as a natural gas condensing furnace of the same size. Therefore, the environmental value of direct geothermal heating applications is highly dependent on the emissions intensity of the neighboring electric grid.\n\nPlant construction can adversely affect land stability. Subsidence has occurred in the Wairakei field in New Zealand. In Staufen im Breisgau, Germany, tectonic uplift occurred instead, due to a previously isolated anhydrite layer coming in contact with water and turning into gypsum, doubling its volume.\nEnhanced geothermal systems can trigger earthquakes as part of hydraulic fracturing. The project in Basel, Switzerland was suspended because more than 10,000 seismic events measuring up to 3.4 on the Richter Scale occurred over the first 6 days of water injection.\n\nGeothermal has minimal land and freshwater requirements. Geothermal plants use per gigawatt of electrical production (not capacity) versus and for coal facilities and wind farms respectively. They use of freshwater per MW·h versus over per MW·h for nuclear, coal, or oil.\n\nSome of the legal issues raised by geothermal energy resources include questions of ownership and allocation of the resource, the grant of exploration permits, exploitation rights, royalties, and the extent to which geothermal energy issues have been recognized in existing planning and environmental laws. Other questions concern overlap between geothermal and mineral or petroleum tenements. Broader issues concern the extent to which the legal framework for encouragement of renewable energy assists in encouraging geothermal industry innovation and development.\n\n\n\n"}
{"id": "23158223", "url": "https://en.wikipedia.org/wiki?curid=23158223", "title": "Jack Collom", "text": "Jack Collom\n\nJohn Aldridge \"Jack\" Collom (November 8, 1931 – July 2, 2017) was an American poet, essayist, and creative writing pedagogue. Included among the twenty-five books he published during his lifetime were \"Red Car Goes By: Selected Poems 1955–2000\"; \"Poetry Everywhere: Teaching Poetry Writing in School and in the Community\"; and \"Second Nature\", which won the 2013 Colorado Book Award for Poetry. In the fields of education and pedagogy, he was involved in eco-literature, ecopoetics, and creative writing instruction for children.\n\nJack Collom was born John Aldridge Collom in Chicago on November 8, 1931. He grew up in the small town of Western Springs, Illinois, spent much of his time birdwatching, and over the years became an inveterate bird-watcher. Collom moved to Fraser, Colorado, in 1947. He studied Forestry at Colorado A&M College where he earned a B.S. in 1952. Afterwards, he spent four years in the U.S. Air Force, and he started writing poetry in 1955 while stationed in Tripoli, Libya. His unit was next stationed at Neubiberg, a base just south of Munich, in Bavaria. It is there he met his first wife (a native German), in 1956. After his discharge from the military, he moved back to the US after a brief time living in Germany, and worked in factories for twenty years while writing poetry.\n\nHe received his B.A. in English (1972) and M.A. in English literature (1974) from the University of Colorado, where he had studied on the G.I. Bill. In 1974, he began teaching in the \"Poetry-in-the-Schools\" programs in Colorado, Wyoming, and Nebraska. In 1980, he began teaching poetry in the public schools of New York City, by way of the \"Poets In Public Service\" and \"Teachers & Writers\" programs. Collom continued to teach creative writing to children for the next 35 years, in both elementary and secondary schools, where he developed a pedagogy for this type of educational approach.\n\nSubsequently, Teachers & Writers Collaborative published three books of Collom's essays and commentary on this experience (which included the young students' poems), notably \"Poetry Everywhere\" and \"Moving Windows\".\n\nFrom 1966 to 1977, he published the work of many writers in a little magazine called \"The\". He was twice awarded Poetry Fellowships from the National Endowment for the Arts, and received a Foundation for Contemporary Arts Grants to Artists award (2012). From 1986 until his death in 2017, Collom taught at Naropa University's Jack Kerouac School of Disembodied Poetics as an adjunct professor, where he shaped Writing Outreach, a community creative-writing project, into a course. In 1989, he pioneered Eco-Lit, one of the first ecology literature courses ever offered in the United States. Some of his accomplishments as an environmentalist-poet are documented in \"American Environmental Leaders: From Colonial Times to the Present\". His nature writings and essays about the environment were published in various venues, including \"ecopoetics\", \"The Alphabet of Trees: A Guide to Writing Nature Poetry\", and \"ISLE\", the journal of Interdisciplinary Studies in Literature and the Environment.\n\nHe read and taught throughout the United States, in Mexico, Costa Rica, Austria, Belgium, and Germany. In 2008, he was the plenary speaker at the \"Poetic Ecologies\" conference at the Université Libre de Bruxelles. In 2009, he led a three-week Creativity and Aging Program at Woodland Pattern in Milwaukee, Wisconsin.\n\nHe worked with numerous dancers, visual artists and musician/composers, and recorded three CDs: \"Calluses of Poetry\" and \"Colors Born of Shadow\", with Ken Bernstein, and \"Blue Yodel Blue Heron\", with Dan Hankin and Sierra Collom.\n\nIn 2001, his adopted hometown of Boulder, Colorado, declared and celebrated a \"Jack Collom Day\".\n\nCollom was married three times. He had three sons by his first marriage: Nathaniel, Christopher, and Franz. He had a daughter, Sierra, through a second marriage.\n\nJack Collom died in Boulder, Colorado on July 2, 2017. He is survived by his wife, Jennifer Heath, his four grown children, and a grandson.\n\n\n\n"}
{"id": "40008104", "url": "https://en.wikipedia.org/wiki?curid=40008104", "title": "Liberal naturalism", "text": "Liberal naturalism\n\nLiberal naturalism is a heterodox form of metaphysical naturalism that lies in the conceptual space between scientific (or reductive) naturalism and supernaturalism. It allows that one can respect the explanations and results of the successful sciences without supposing that the sciences are our only resource for understanding humanity and our dealings with the world and each other.\n\nThe term was introduced in 2004 by Mario De Caro & David Macarthur and, independently, by Gregg Rosenberg. This form of naturalism has been ascribed to Immanuel Kant.\n\nFor a liberal naturalist many things in our everyday world that are not explicable (or not fully explicable) by science are, nonetheless, presupposed by science—e.g. tables, persons, artworks, institutions, rational norms and values. Explaining such things might require non-scientific non-supernatural resources according to this form of naturalism. So, rather than tailoring their ontology to the posits of the successful sciences, as scientific naturalists do, liberal naturalists recognise the prima facie irreducible reality of everyday objects that are part of what Wilfrid Sellars called \"the manifest image\".\n\nLiberal naturalism is a \"liberal\" or \"catholic\" naturalism for several reasons each of which contrasts with scientific naturalist orthodoxy:\n\n\n"}
{"id": "35593749", "url": "https://en.wikipedia.org/wiki?curid=35593749", "title": "List of Cascade Range topics", "text": "List of Cascade Range topics\n\nThis article contains a list of volcanoes and a list of protected areas associated with the Cascade Range of the Pacific Northwest of North America.\n\nVolcanoes south of the Fraser River in the Cascade Volcanic Arc (a geological term) belong to the Cascade Range (a geographic term). Peaks are listed north to south.\n\n\n\nThere are four U.S. National Parks in the Cascade Range, one National Scenic Area, and many U.S. National Monuments, U.S. Wilderness Areas, and U.S. National Forests. Each classification protects the various glaciers, volcanoes, geothermal fields, rivers, lakes, forests, and wildlife to varying degrees.\n\n\n\n\n\n\n"}
{"id": "31744838", "url": "https://en.wikipedia.org/wiki?curid=31744838", "title": "List of dates predicted for apocalyptic events", "text": "List of dates predicted for apocalyptic events\n\nPredictions of apocalyptic events that would result in the extinction of humanity, a collapse of civilization, or the destruction of the planet have been made since at least the beginning of the Common Era. Most predictions are related to Abrahamic religions, often standing for or similar to the eschatological events described in their scriptures. Christian predictions typically refer to events like the Rapture, Great Tribulation, Last Judgment, and the Second Coming of Christ. End-time events are usually predicted to occur within the lifetime of the person making the prediction, and are usually made using the Bible, and in particular the New Testament, as either the primary or exclusive source for the predictions. Often this takes the form of mathematical calculations, such as trying to calculate the point where it will have been 6000 years since the supposed creation of the Earth by the Abrahamic God, which according to the Talmud marks the deadline for the Messiah to appear. Predictions of the end from natural events have also been theorised by various scientists and scientific groups. While these predictions are generally accepted as plausible within the scientific community, the events and phenomena are not expected to occur for hundreds of thousands or even billions of years from now.\n\nLittle research has been done into why people make apocalyptic predictions. Historically, it has been done for reasons such as diverting attention from actual crises like poverty and war, pushing political agendas, and promoting hatred of certain groups; antisemitism was a popular theme of Christian apocalyptic predictions in medieval times, while French and Lutheran depictions of the apocalypse were known to feature English and Catholic antagonists respectively. According to psychologists, possible explanations for why people believe in modern apocalyptic predictions include mentally reducing the actual danger in the world to a single and definable source, an innate human fascination with fear, personality traits of paranoia and powerlessness and a modern romanticism involved with end-times due to its portrayal in contemporary fiction. The prevalence of Abrahamic religions throughout modern history is said to have created a culture which encourages the embracement of a future that will be drastically different from the present. Such a culture is credited with the rise in popularity of predictions that are more secular in nature, such as the 2012 phenomenon, while maintaining the centuries-old theme that a powerful force will bring the end of humanity.\n\nPolls conducted in 2012 across 20 countries found over 14% of people believe the world will end in their lifetime, with percentages ranging from 6% of people in France to 22% in the US and Turkey. Belief in the apocalypse is most prevalent in people with lower rates of education, lower household incomes, and those under the age of 35. In the UK in 2015, 23% of the general public believed the apocalypse was likely to occur in their lifetime, compared to 10% of experts from the Global Challenges Foundation. The general public believed the likeliest cause would be nuclear war, while experts thought it would be artificial intelligence. Only 3% of Britons thought the end would be caused by the Last Judgement, compared to 16% of Americans. Between one and three percent of people from both countries thought the apocalypse would be caused by zombies or alien invasion.\n\nThis section lists eschatological predictions, mostly by religious individuals or groups. Most predictions are related to Abrahamic religions, with numerous predictions standing for the eschatological events described in their scriptures. Christian predictions typically refer to events like the Rapture, Great Tribulation, Last Judgment, and the Second Coming of Christ.\n\n\n"}
{"id": "660678", "url": "https://en.wikipedia.org/wiki?curid=660678", "title": "List of glaciers", "text": "List of glaciers\n\nA glacier ( ) or () is a persistent body of dense ice that is constantly moving under its own weight; it forms where the accumulation of snow exceeds its ablation (melting and sublimation) over many years, often centuries. Glaciers slowly deform and flow due to stresses induced by their weight, creating crevasses, seracs, and other distinguishing features. Because glacial mass is affected by long-term climate changes, e.g., precipitation, mean temperature, and cloud cover, glacial mass changes are considered among the most sensitive indicators of climate change.\n\nAfrica, specifically East Africa, has contained glacial regions, possibly as far back as the last glacier maximum 10 to 15 thousand years ago. Seasonal snow does exist on the highest peaks of East Africa as well as in the Drakensberg Range of South Africa, the Stormberg Mountains, and the Atlas Mountains in Morocco. Currently, the only remaining glaciers on the continent exist on Mount Kilimanjaro, Mount Kenya, and the Rwenzori.\n\nThere are many glaciers in the Antarctic. This set of lists does not include ice sheets, ice caps or ice fields, such as the Antarctic ice sheet, but includes glacial features that are defined by their flow, rather than general bodies of ice. The lists include outlet glaciers, valley glaciers, cirque glaciers, tidewater glaciers and ice streams. Ice streams are a type of glacier and many of them have \"glacier\" in their name, e.g. Pine Island Glacier. Ice shelves are listed separately in the List of Antarctic ice shelves. For the purposes of these lists, the Antarctic is defined as any latitude further south than 60° (the continental limit according to the Antarctic Treaty System).\n\nThere are also glaciers in the subantarctic. This includes one snow field (Murray Snowfield). Snow fields are not glaciers in the strict sense of the word, but they are commonly found at the accumulation zone or head of a glacier. For the purposes of this list, Antarctica is defined as any latitude further south than 60° (the continental limit according to the Antarctic Treaty).\n\nThe majority of Europe's glaciers are found in the Alps, Caucasus and the Scandinavian Mountains (mostly Norway) as well as in Iceland. Iceland has the largest glacier in Europe, Vatnajökull glacier, that covers between 8,100-8,300 km² in area and 3,100 km³ in volume. Norway alone has more than 2500 glaciers (including very small ones) covering an estimated 1% of mainland Norway's surface area. Several of mainland Europe's biggest glaciers are found here including; Jostedalsbreen(the largest in mainland Europe at 487 km), Vestre Svartisen(221 km), Søndre Folgefonna(168 km) and Østre Svartisen(148 km). The two Svartisen glaciers used to be one connected entity during the Little Ice Age but has since separated.\n\n\nThere are a number of glaciers existing in North America, currently or in recent centuries. In the United States, these glaciers are located in nine states, all in the Rocky Mountains or further west. The southernmost named glacier among them is the Lilliput Glacier in Tulare County, east of the Central Valley of California.\n\nMexico has about two dozen glaciers, all of which are located on Pico de Orizaba (Citlaltépetl), Popocatépetl and Iztaccíhuatl, the three tallest mountains in the country.\n\n\nGlaciers in South America develop exclusively on the Andes and are subject of the Andes various climatic regimes namely the Tropical Andes, Dry Andes and the Wet Andes. Apart from this there is a wide range of latitudes on which glaciers develop from 5000 m in the Altiplano mountains and volcanoes to reaching sealevel as tidewater glaciers from San Rafael Lagoon (45° S) and southwards. South America hosts two large ice fields, the Northern and Southern Patagonian Ice Fields, of which the second is the largest contiguous body of glaciers in extrapolar regions.\n\nThe glaciers of Chile cover 2.7% (20,188 km) of the land area of the country, excluding Antártica Chilena, and have a considerable impact on its landscape and water supply. By surface 80% of South America’s glaciers lie in Chile. Glaciers develop in the Andes of Chile from 27˚S southwards and in a very few places north of 18°30'S in the extreme north of the country: in between they are absent because of extreme aridity, though rock glaciers formed from permafrost are common. The largest glaciers of Chile are the Northern and Southern Patagonian Ice Fields. From a latitude of 47° S and south some glaciers reach sea level.\n\nApart from height and latitude, the settings of Chilean glaciers depend on precipitation patterns; in this sense two different regions exist: the Dry Andes and the Wet Andes.\n\nNo glaciers remain on the Australia mainland or Tasmania. A few, like the Heard Island glaciers are located in the territory of Heard Island and McDonald Islands in the southern Indian Ocean.\n\nNew Guinea has the Puncak Jaya glacier.\n\nNew Zealand contains many glaciers, mostly located near the Main Divide of the Southern Alps in the South Island. They are classed as mid-latitude mountain glaciers. There are eighteen small glaciers in the North Island on Mount Ruapehu.\n\nAn inventory of South Island glaciers compiled in the 1980s indicated there were about 3,155 glaciers with an area of at least one hectare (2.5 acres). Approximately one sixth of these glaciers covered more than 10 hectares. These include:\n\n\nThe following is the list of longest glaciers in the non-polar regions, generally regarded as between 60 degrees north and 60 degrees south latitude, though some definitions expand it slightly.\n\n"}
{"id": "38973180", "url": "https://en.wikipedia.org/wiki?curid=38973180", "title": "List of parson-naturalists", "text": "List of parson-naturalists\n\nParson-naturalists were ministers of religion who also studied natural history. The archetypical parson-naturalist was a priest in the Church of England in charge of a country parish, who saw the study of science as an extension of his religious work. The philosophy entailed the belief that God, as the Creator of all things, wanted man to understand his Creations and thus to study them through scientific techniques. They often collected and preserved natural artefacts such as leaves, flowers, birds' eggs, birds, insects, and small mammals to classify and study. Some wrote books or kept nature diaries.\n\n"}
{"id": "4065564", "url": "https://en.wikipedia.org/wiki?curid=4065564", "title": "List of the seven natural wonders of Georgia (U.S. state)", "text": "List of the seven natural wonders of Georgia (U.S. state)\n\nThe Seven Natural Wonders of Georgia are considered to be:\n\n\nThe first list of natural wonders was compiled by state librarian Ella May Thornton and published in the \"Atlanta Georgian\" magazine on December 26, 1926. That first list included:\n\n"}
{"id": "16554664", "url": "https://en.wikipedia.org/wiki?curid=16554664", "title": "Living systems", "text": "Living systems\n\nLiving systems are open self-organizing life forms that interact with their environment. These systems are maintained by flows of information, energy and matter.\n\nSome scientists have proposed in the last few decades that a general living systems theory is required to explain the nature of life. Such a general theory, arising out of the ecological and biological sciences, attempts to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into components, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment.\n\nLiving systems theory is a general theory about the existence of all living systems, their structure, interaction, behavior and development. This work is created by James Grier Miller, which was intended to formalize the concept of life. According to Miller's original conception as spelled out in his magnum opus \"Living Systems\", a \"living system\" must contain each of twenty \"critical subsystems\", which are defined by their functions and visible in numerous systems, from simple cells to organisms, countries, and societies. In \"Living Systems\" Miller provides a detailed look at a number of systems in order of increasing size, and identifies his subsystems in each. \nMiller considers living systems as a subset of all systems. Below the level of living systems, he defines space and time, matter and energy, information and entropy, levels of organization, and physical and conceptual factors, and above living systems ecological, planetary and solar systems, galaxies, etc.\n\nLiving systems according to Parent (1996) are by definition \"open self-organizing systems that have the special characteristics of life and interact with their environment. This takes place by means of information and material-energy exchanges. Living systems can be as simple as a single cell or as complex as a supranational organization such as the European Union. Regardless of their complexity, they each depend upon the same essential twenty subsystems (or processes) in order to survive and to continue the propagation of their species or types beyond a single generation\".\n\nMiller said that systems exist at eight \"nested\" hierarchical levels: cell, organ, organism, group, organization, community, society, and supranational system. At each level, a system invariably comprises twenty critical subsystems, which process matter–energy or information except for the first two, which process both matter–energy and information: reproducer and boundary.\n\nThe processors of matter–energy are: \n\nThe processors of information are:\n\nJames Grier Miller in 1978 wrote a 1,102-page volume to present his living systems theory. He constructed a general theory of living systems by focusing on concrete systems—nonrandom accumulations of matter–energy in physical space–time organized into interacting, interrelated subsystems or components. Slightly revising the original model a dozen years later, he distinguished eight \"nested\" hierarchical levels in such complex structures. Each level is \"nested\" in the sense that each higher level contains the next lower level in a nested fashion.\n\nHis central thesis is that the systems in existence at all eight levels are open systems composed of twenty critical subsystems that process inputs, throughputs, and outputs of various forms of matter–energy and information. Two of these subsystems—reproducer and boundary—process both matter–energy and information. Eight of them process only matter–energy. The other ten process information only. \nAll nature is a continuum. The endless complexity of life is organized into patterns which repeat themselves—theme and variations—at each level of system. These similarities and differences are proper concerns for science. From the ceaseless streaming of protoplasm to the many-vectored activities of supranational systems, there are continuous flows through living systems as they maintain their highly organized steady states.\nSeppänen (1998) says that Miller applied general systems theory on a broad scale to describe all aspects of living systems.\n\nMiller's theory posits that the mutual interrelationship of the components of a system extends across the hierarchical levels. Examples: Cells and organs of a living system thrive on the food the organism obtains from its suprasystem; the member countries of a supranational system reap the benefits accrued from the communal activities to which each one contributes. Miller says that his eclectic theory \"ties together past discoveries from many disciplines and provides an outline into which new findings can be fitted\".\n\nMiller says the concepts of space, time, matter, energy, and information are essential to his theory because the living systems exist in space and are made of matter and energy organized by information. Miller's theory of living systems employs two sorts of spaces: physical or geographical space, and conceptual or abstracted spaces. Time is the fundamental \"fourth dimension\" of the physical space–time continuum/spiral. Matter is anything that has mass and occupies physical space. Mass and energy are equivalent as one can be converted into the other. Information refers to the degrees of freedom that exist in a given situation to choose among signals, symbols, messages, or patterns to be transmitted.\n\nOther relevant concepts are system, structure, process, type, level, echelon, suprasystem, subsystem, transmissions, and steady state. A system can be conceptual, concrete or abstracted. The structure of a system is the arrangement of the subsystems and their components in three-dimensional space at any point of time. Process, which can be reversible or irreversible, refers to change over time of matter–energy or information in a system. Type defines living systems with similar characteristics. Level is the position in a hierarchy of systems. Many complex living systems, at various levels, are organized into two or more echelons. The suprasystem of any living system is the next higher system in which it is a subsystem or component. The totality of all the structures in a system which carry out a particular process is a subsystem. Transmissions are inputs and outputs in concrete systems. Because living systems are open systems, with continually altering fluxes of matter–energy and information, many of their equilibria are dynamic—situations identified as steady states or flux equilibria.\n\nMiller identifies the comparable matter–energy and information processing critical subsystems. Elaborating on the eight hierarchical levels, he defines society, which constitutes the seventh hierarchy, as \"a large, living, concrete system with [community] and lower am levels of living systems as subsystems and components\". Society may include small, primitive, totipotential communities; ancient city–states, and kingdoms; as well as modern nation–states and empires that are not supranational systems. Miller provides general descriptions of each of the subsystems that fit all eight levels.\n\nA supranational system, in Miller's view, \"is composed of two or more societies, some or all of whose processes are under the control of a decider that is superordinate to their highest echelons\". However, he contends that no supranational system with all its twenty subsystems under control of its decider exists today. The absence of a supranational decider precludes the existence of a concrete supranational system. Miller says that studying a supranational system is problematical because its subsystems\n...tend to consist of few components besides the decoder. These systems do little matter-energy processing. The power of component societies [nations] today is almost always greater than the power of supranational deciders. Traditionally, theory at this level has been based upon intuition and study of history rather than data collection. Some quantitative research is now being done, and construction of global-system models and simulations is currently burgeoning.\n\nAt the supranational system level, Miller's emphasis is on international organizations, associations, and groups comprising representatives of societies (nation–states). Miller identifies the subsystems at this level to suit this emphasis. Thus, for example, the reproducer is \"any multipurpose supranational system which creates a single purpose supranational organization\" (p. 914); and the boundary is the \"supranational forces, usually located on or near supranational borders, which defend, guard, or police them\" (p. 914).\n\nNot just those specialized in international communication, but all communication science scholars could pay particular attention to the major contributions of living systems theory (LST) to social systems approaches that Bailey has pointed out:\n\nBailey says that LST, perhaps the \"most integrative\" social systems theory, has made many more contributions that may be easily overlooked, such as: providing a detailed analysis of types of systems; making a distinction between concrete and abstracted systems; discussion of physical space and time; placing emphasis on information processing; providing an analysis of entropy; recognition of totipotential systems, and partipotential systems; providing an innovative approach to the structure–process issue; and introducing the concept of joint subsystem—a subsystem that belongs to two systems simultaneously; of dispersal—lateral, outward, upward, and downward; of inclusion—inclusion of something from the environment that is not part of the system; of artifact—an animal-made or human-made inclusion; of adjustment process, which combats stress in a system; and of critical subsystems, which carry out processes that all living systems need to survive.\n\nLST's analysis of the twenty interacting subsystems, Bailey adds, clearly distinguishing between matter–energy-processing and information-processing, as well as LST's analysis of the eight interrelated system levels, enables us to understand how social systems are linked to biological systems. LST also analyzes the irregularities or \"organizational pathologies\" of systems functioning (e.g., system stress and strain, feedback irregularities, information–input overload). It explicates the role of entropy in social research while it equates negentropy with information and order. It emphasizes both structure and process, as well as their interrelations.\n\nIt omits the analysis of subjective phenomena, and it overemphasizes concrete Q-analysis (correlation of objects) to the virtual exclusion of R-analysis (correlation of variables). By asserting that societies (ranging from totipotential communities to nation-states and non-supranational systems) have greater control over their subsystem components than supranational systems have, it dodges the issue of transnational power over the contemporary social systems. Miller's supranational system bears no resemblance to the modern world-system that Immanuel Wallerstein (1974) described, although both of them were looking at the same living (dissipative) structure.\n\n\n\n"}
{"id": "1842698", "url": "https://en.wikipedia.org/wiki?curid=1842698", "title": "Loren Eiseley", "text": "Loren Eiseley\n\nLoren Eiseley (September 3, 1907 – July 9, 1977) was an American anthropologist, educator, philosopher, and natural science writer, who taught and published books from the 1950s through the 1970s. He received many honorary degrees and was a fellow of multiple professional societies. At his death, he was Benjamin Franklin Professor of Anthropology and History of Science at the University of Pennsylvania.\n\nHe was a \"scholar and writer of imagination and grace,\" whose reputation and accomplishments extended far beyond the campus where he taught for 30 years. \"Publishers Weekly\" referred to him as \"the modern Thoreau.\" The broad scope of his writing reflected upon such topics as the mind of Sir Francis Bacon, the prehistoric origins of man, and the contributions of Charles Darwin.\n\nEiseley's reputation was established primarily through his books, including \"The Immense Journey\" (1957), \"Darwin's Century\" (1958), \"The Unexpected Universe\" (1969), \"The Night Country\" (1971), and his memoir, \"All the Strange Hours\" (1975). Science author Orville Prescott praised him as a scientist who \"can write with poetic sensibility and with a fine sense of wonder and of reverence before the mysteries of life and nature.\" Naturalist author Mary Ellen Pitts saw his combination of literary and nature writings as his \"quest, not simply for bringing together science and literature... but a continuation of what the 18th and 19th century British naturalists and Thoreau had done.\" In praise of \"The Unexpected Universe\", Ray Bradbury remarked, \"[Eiseley] is every writer's writer, and every human's human... One of us, yet most uncommon...\" \nAccording to his obituary in the \"New York Times\", the feeling and philosophical motivation of the entire body of Eiseley's work was best expressed in one of his essays, \"The Enchanted Glass:\" \"The anthropologist wrote of the need for the contemplative naturalist, a man who, in a less frenzied era, had time to observe, to speculate, and to dream.\" Shortly before his death he received an award from the Boston Museum of Science for his \"outstanding contribution to the public understanding of science\" and another from the U.S. Humane Society for his \"significant contribution for the improvement of life and environment in this country.\"\n\nBorn in Lincoln, Nebraska, Eiseley lived his childhood with a hardworking father and deaf mother who may have suffered from mental illness. Their home was located on the outskirts of town where, as author Naomi Brill writes, it was \"removed from the people and the community from which they felt set apart through poverty and family misfortune.\" His autobiography, \"All the Strange Hours\", begins with his \"childhood experiences as a sickly afterthought, weighed down by the loveless union of his parents.\"\n\nHis father, Clyde, was a hardware salesman who worked long hours for little pay, writes Brill. However, as an amateur Shakespearean actor, he was able to give his son a \"love for beautiful language and writing.\" His mother, Daisey Corey, was a self-taught prairie artist who was considered a beautiful woman. She lost her hearing as a child and sometimes exhibited irrational and destructive behavior. This left Eiseley feeling distant from her and may have contributed to his parents' unhappy marriage.\n\nLiving at the edge of town, however, led to Eiseley's early interest in the natural world, to which he turned when being at home was too difficult. There, he would play in the caves and creek banks nearby. Fortunately, there were others who opened the door to a happier life. His half-brother, Leo, for instance, gave him a copy of \"Robinson Crusoe\", with which he taught himself to read. Thereafter, he managed to find ways to get to the public library and became a voracious reader.\n\nEiseley later attended the Lincoln Public Schools; in high school, he wrote that he wanted to be a nature writer. He would later describe the lands around Lincoln as \"flat and grass-covered and smiling so serenely up at the sun that they seemed forever youthful, untouched by mind or time—a sunlit, timeless prairie over which nothing passed but antelope or wandering bird.\" But, disturbed by his home situation and the illness and recent death of his father, he dropped out of school and worked at menial jobs.\n\nEiseley enrolled in the University of Nebraska, where he wrote for the newly formed journal, \"Prairie Schooner\", and went on archaeology digs for the school's natural history museum, Morrill Hall. In 1927, however, he was diagnosed with tuberculosis and left the university to move to the western desert, believing the drier air would improve his condition. While there, he soon became restless and unhappy, which led him to hoboing around the country by hopping on freight trains (as many did during the Great Depression). Professor of religion, Richard Wentz, writes about this period:\n\nLoren Eiseley had been a drifter in his youth. From the plains of Nebraska he had wandered across the American West. Sometimes sickly, at other times testing his strength with that curious band of roving exiles who searched the land above the rippling railroad ties, he explored his soul as he sought to touch the distant past. He became a naturalist and a bone hunter because something about the landscape had linked his mind to the birth and death of life itself.\n\nEiseley eventually returned to the University of Nebraska and received a B.A. degree in English and a B.S. degree in Geology/Anthropology. While at the university, he served as editor of the literary magazine \"The Prairie Schooner\", and published his poetry and short stories. Undergraduate expeditions to western Nebraska and the southwest to hunt for fossils and human artifacts provided the inspiration for much of his early work. He later noted that he came to anthropology from paleontology, preferring to leave human burial sites undisturbed unless destruction threatened them.\n\nEiseley received his Ph.D. degree from the University of Pennsylvania in 1937 and wrote his dissertation entitled \"Three Indices of Quaternary Time and Their Bearing Upon Pre-History: a Critique\", which launched his academic career. He began teaching at the University of Kansas that same year. During World War II, Eiseley taught anatomy to reservist pre-med students at Kansas.\n\nIn 1944 he left the University of Kansas to assume the role of head of the Department of Sociology and Anthropology at Oberlin College in Ohio. In 1947 he returned to the University of Pennsylvania to head its Anthropology Department. He was elected president of the American Institute of Human Paleontology in 1949. From 1959 to 1961, he was provost at the University of Pennsylvania, and in 1961 the University of Pennsylvania created a special interdisciplinary professorial chair for him.\n\nEiseley was also a fellow of many distinguished professional societies, including the American Association for the Advancement of Science, the United States National Academy of Sciences, the National Institute of Arts and Letters and the American Philosophical Society.\n\nAt the time of his death in 1977, he was Benjamin Franklin Professor of Anthropology and History of Science, and the curator of the Early Man section at the University of Pennsylvania Museum. He had received thirty-six honorary degrees over a period of twenty years, and was the most honored member of the University of Pennsylvania since Benjamin Franklin. In 1976 he won the Bradford Washburn Award of the Boston Museum of Science for his \"outstanding contribution to the public understanding of science\" and the Joseph Wood Krutch Medal from the Humane Society of the United States for his \"significant contribution for the improvement of life and the environment in this country.\"\n\nIn addition to his scientific and academic work, Eiseley began in the mid-1940s to publish the essays which brought him to the attention of a wider audience. Anthropologist Pat Shipman writes,\n\nthe words that flowed from his pen... the images and insights he revealed, the genius of the man as a writer, outweigh his social disability. The words were what kept him in various honored posts; the words were what caused the students to flock to his often aborted courses; the words were what earned him esteemed lectureships and prizes. His contemporaries failed to see the duality of the man, confusing the deep, wise voice of Eiseley's writings with his own personal voice. He was a natural fugitive, a fox at the wood's edge (in his own metaphor)...\n\nEiseley published works in a number of different genres including poetry, autobiography, history of science, biography, and nonfictional essays. In each piece of writing, he consistently used a poetic writing style. Eiseley's style mirrors what he called the concealed essay—a piece of writing that unites the personal dimension with more scientific thoughts. His writing was unique in that it could convey complex ideas about human origin and the relationship between humans and the natural world to a nonscientific audience. Robert G. Franke describes Eiseley's essays as theatrical and dramatic. He also notes the influence his father's hobby as an amateur Shakespearean actor may have had on Eiseley's writing, pointing out that his essays often contain dramatic elements that are usually present in plays.\n\nIn describing Eiseley's writing, Richard Wentz wrote, \"As the works of any naturalist might, Eiseley's essays and poems deal with the flora and fauna of North America. They probe the concept of evolution, which consumed so much of his scholarly attention, examining the bones and shards, the arrowpoints and buried treasures. Every scientific observation leads to reflection.\" \n\nIn an interview on National Public Radio (NPR), author Michael Lind said,\n\nBefore the rise of a self-conscious intelligentsia, most educated people – as well as the unlettered majority – spent most of their time in the countryside or, if they lived in cities, were a few blocks away from farmland or wilderness... At the risk of sounding countercultural, I suspect that thinkers who live in sealed, air-conditioned boxes and work by artificial light (I am one) are as unnatural as apes in cages at zoos. Naturalists like Eiseley in that sense are the most normal human beings to be found among intellectuals, because they spend a lot of time outdoors and know the names of the plants and animals they see... For all of his scientific erudition, Eiseley has a poetic, even cinematic, imagination.\n\nRichard Wentz describes what he feels are the significance and purposes of Eiseley's writings:\"For Loren Eiseley, writing itself becomes a form of contemplation. Contemplation is a kind of human activity in which the mind, spirit and body are directed in solitude toward some other. Scholars and critics have not yet taken the full measure of contemplation as an art that is related to the purpose of all scholarly activity – to see things as they really are... Using narrative, parable and exposition, Eiseley has the uncanny ability to make us feel that we are accompanying him on a journey into the very heart of the universe. Whether he is explicating history or commenting on the ideas of a philosopher, a scientist or a theologian, he takes us with him on a personal visit.\"\n\nHowever, because of Eiseley's intense and poetic writing style and his focus on nature and cosmology, he was not accepted or understood by most of his colleagues. \"You,\" a friend told him, \"are a freak, you know. A God-damned freak, and life is never going to be easy for you. You like scholarship, but the scholars, some of them, anyhow, are not going to like you because you don't stay in the hole where God supposedly put you. You keep sticking your head out and looking around. In a university that's inadvisable.\" \n\nHis first book, \"The Immense Journey\", was a collection of writings about the history of humanity, and it proved to be that rare science book that appealed to a mass audience. It has sold over a million copies and has been published in at least 16 languages. Besides being his first book, \"The Immense Journey\" was also Eiseley's most well known book and established him as a writer with the ability to combine science and humanity in a poetic way. This book was originally published in 1946. Then, it was published again in 1957, a few years after the Piltdown Man hoax discovery.\n\nIn the book Eiseley conveys his sense of wonder at the depth of time and the vastness of the universe. He uses his own experiences, reactions to the paleontological record, and wonderment at the world to address the topic of evolution. More specifically, the text concentrates on human evolution and human ignorance. In \"The Immense Journey\", Eiseley follows the journey from human ignorance at the beginning of life to his own wonderment about the future of mankind. Marston Bates writes,\n\nIt seems to me... that Eiseley is looking at man in a quite hard-headed fashion, because he is willing to sketch problems for which he has no present and sure solution. We are not going to find the answers in human evolution until we have framed the right questions, and the questions are difficult because they involve both body and mind, physique and culture—tools and symbols as well as cerebral configurations.\n\nAuthor Orville Prescott wrote,\n\nConsider the case of Loren Eiseley, author of \"The Immense Journey\", who can sit on a mountain slope beside a prairie-dog town and imagine himself back in the dawn of the Age of mammals eighty million years ago: 'There by a tree root I could almost make him out, that shabby little Paleocene rat, eternal tramp and world wanderer, father of all mankind.' ... his prose is often lyrically beautiful, something that considerable reading in the works of anthropologists had not led me to expect. ... The subjects discussed here include the human ancestral tree, water and its significance to life, the mysteries of cellular life, 'the secret and remote abysses' of the sea, the riddle of why human beings alone among living creatures have brains capable of abstract thought and are far superior to their mere needs for survival, the reasons why Dr. Eiseley is convinced that there are no men or man-like animals on other planets, ...\n\nHe offers an example of Eiseley's style: \"There is no logical reason for the existence of a snowflake any more than there is for evolution. It is an apparition from that mysterious shadow world beyond nature, that final world which contains—if anything contains—the explanation of men and catfish and green leaves.\"\n\nThis book's subtitle is, \"Evolution and the Men Who Discovered It.\" Eiseley documented that animal variation, extinction, and a lengthy history of the earth were observed from the 1600s onward. Scientists groped towards a theory with increasingly detailed observations. They became aware that evolution had occurred without knowing how. Evolution was \"in the air\" and part of the intellectual discourse both before and after \"On the Origin of Species\" was published. The publisher describes it thus:\n\nAt the heart of the account is Charles Darwin, but the story neither begins nor ends with him. Starting with the seventeenth-century notion of the Great Chain of Being, Dr. Eiseley traces the achievements and discoveries of men in many fields of science who paved the way for Darwin; and the book concludes with an extensive discussion of the ways in which Darwin's work has been challenged, improved upon, and occasionally refuted during the past hundred years.\n\nPersons whose contributions are discussed include Sir Thomas Browne, Sir Francis Bacon, Carl Linnaeus, Benoît de Maillet, the Comte de Buffon, Erasmus Darwin, Louis Agassiz, Jean-Baptiste Lamarck, James Hutton, William Smith, Georges Cuvier, Étienne Geoffroy Saint-Hilaire, Sir Charles Lyell, Thomas Robert Malthus, William Wells, Patrick Matthew, Karl von Baer, Robert Chambers, Thomas Henry Huxley, Sir John Richardson, Alexander Humboldt, Gregor Mendel, Hugo De Vries, W. L. Johannsen, Lambert Quételet, and Alfred Russel Wallace. Critics discussed include Fleeming Jenkin, A.W. Bennett, Lord Kelvin, and Adam Sedgwick, both a mentor and a critic.\n\nAccording to naturalist author Mary Ellen Pitts, in the \"seminal\" \"Darwin's Century\", Eiseley was studying the history of evolutionary thinking, and he came to see that \"as a result of scientific studies, nature has become externalized, particularized, mechanized, separated from the human and fragmented, reduced to conflict without consideration of cooperation, confined to reductionist and positivist study.\" The results for humankind, \"as part of the 'biota' – Eiseley's concern as a writer – are far reaching.\" In the book, his unique impact as a thinker and a literary figure emerges as he reexamines science and the way man understands science. She concludes that, for Eiseley, \"Nature emerges as a metonym for a view of the physical world, of the 'biota,' and of humankind that must be reexamined if life is to survive.\"\n\nIn his conclusion, Eiseley quotes Darwin: \"If we choose to let conjecture run wild, then animals, our fellow brethren in pain, disease, suffering and famine—our slaves in the most laborious works, our companions in our amusements—they may partake of our origin in one common ancestor—we may be all melted together.\" Eiseley adds, \"If he had never conceived of natural selection, if he had never written the \"Origin,\" it would still stand as a statement of almost clairvoyant perception.\"\n\nThe book won the Phi Beta Kappa prize for best book in science in 1958.\n\nIn discussing \"The Firmament of Time\", Professor of Zoology Leslie Dunn wrote, \"How can man of 1960, burdened with the knowledge of the world external to him, and with the consciousness that scientific knowledge is attained through continually interfering with nature, 'bear his part' and gain the hope and confidence to live in the new world to which natural science has given birth? ... The answer comes in the eloquent, moving central essay of his new book.\" \"The New Yorker\" wrote, \"Dr. Eiseley describes with zest and admiration the giant steps that have led man, in a scant three hundred years, to grasp the nature of his extraordinary past and to substitute a natural world for a world of divine creation and intervention... An irresistible inducement to partake of the almost forgotten excitements of reflection.\" A review in \"The Chicago Tribune\" added, \"[This book] has a warm feeling for all natural phenomena; it has a rapport with man and his world and his problems; ... it has hope and belief. And it has the beauty of prose that characterizes Eiseley's philosophical moods.\"\n\n\"The Firmament of Time\" was awarded the 1961 John Burroughs Medal for the best publication in the field of Nature Writing.\n\n\nPoet W.H. Auden wrote, \"The main theme of \"The Unexpected Universe\" is Man as the Quest Hero, the wanderer, the voyager, the seeker after adventure, knowledge, power, meaning, and righteousness.\" He quotes from the book:\n\nEvery time we walk along a beach some ancient urge disturbs us so that we find ourselves shedding shoes and garments or scavenging among seaweed and whitened timbers like the homesick refugees of a long war... Mostly the animals understand their roles, but man, by comparison, seems troubled by a message that, it is often said, he cannot quite remember or has gotten wrong... Bereft of instinct, he must search continually for meanings... Man was a reader before he became a writer, a reader of what Coleridge once called the mighty alphabet of the universe.\n\nEvolutionary biologist Theodosius Dobzhansky described Dr. Eiseley as\n\nGregory McNamee of Amazon.com writes, \"In 1910 young Loren Eiseley watched the passage of Halley's Comet with his father. The boy who became a famous naturalist was never again to see the spectacle except in his imagination. That childhood event contributed to the profound sense of time and space that marks \"The Invisible Pyramid\". This collection of essays, first published shortly after Americans landed on the moon, explores inner and outer space, the vastness of the cosmos, and the limits of what can be known. Bringing poetic insight to scientific discipline, Eiseley makes connections between civilizations past and present, multiple universes, humankind, and nature.\n\nEiseley took the occasion of the lunar landing to consider how far humans had to go in understanding their own small corner of the universe, their home planet, much less what he called the 'cosmic prison' of space. Likening humans to the microscopic phagocytes that dwell within our bodies, he grumpily remarks, 'We know only a little more extended reality than the hypothetical creature below us. Above us may lie realms it is beyond our power to grasp.' Science, he suggests, would be better put to examining that which lies immediately before us, although he allows that the quest to explore space is so firmly rooted in Western technological culture that it was unlikely to be abandoned simply because of his urging. Eiseley's opinion continues to be influential among certain environmentalists, and these graceful essays show why that should be so.\n\nBook excerpt:\n\nMan would not be man if his dreams did not exceed his grasp. ... Like John Donne, man lies in a close prison, yet it is dear to him. Like Donne's, his thoughts at times overleap the sun and pace beyond the body. If I term humanity a slime mold organism it is because our present environment suggest it. If I remember the sunflower forest it is because from its hidden reaches man arose. The green world is his sacred center. In moments of sanity he must still seek refuge there. ... If I dream by contrast of the eventual drift of the star voyagers through the dilated time of the universe, it is because I have seen thistledown off to new worlds and am at heart a voyager who, in this modern time, still yearns for the lost country of his birth.\n\nKirkus Reviews wrote,\n\nIn a published essay, University of Pennsylvania alumnus Carl Hoffman wrote,\n\nAn old man who had done almost all of his writing late, late at night, was speaking to a younger man who liked to read in those same dark hours. In a chapter entitled 'One Night's Dying,' Eiseley said to me: 'It is thus that one day and the next are welded together, and that one night's dying becomes tomorrow's birth. I, who do not sleep, can tell you this.' Today, well into my fifties, in the midst of a lifetime of almost compulsive reading, I still regard \"The Night Country\" as my all-time favorite book.\n\n\"In \"All the Strange Hours\",\" states Amazon.com,\n\nEiseley turns his considerable powers of reflection and discovery on his own life to weave a compelling story, related with the modesty, grace, and keen eye for a telling anecdote that distinguish his work. His story begins with his childhood experiences as a sickly afterthought, weighed down by the loveless union of his parents. From there he traces the odyssey that led to his search for early postglacial man—and into inspiriting philosophical territory—culminating in his uneasy achievement of world renown. Eiseley crafts an absorbing self-portrait of a man who has thought deeply about his place in society as well as humanity's place in the natural world.\n\nHis friend and science fiction author Ray Bradbury wrote, \"The book will be read and cherished in the year 2001. It will go to the Moon and Mars with future generations. Loren Eiseley's work changed my life.\" And from the \"Philadelphia Sunday Bulletin\": \"An astonishing breadth of knowledge, infinite capacity for wonder and compassionate interest for everyone and everything in the universe.\n\n\"Darwin and the Mysterious Mr. X\" attempts to solve a mystery: \"Samuel Butler, a master of acrimonious polemic, confronted Charles Darwin with the sorest of all scientific subjects—a dispute about priority. In \"Evolution Old and New\" (1879), Butler accused Darwin of slighting the evolutionary speculations of Buffon, Lamarck, and his own grandfather, Erasmus Darwin.\" The Kirkus Reviews calls it, \"...an essay devoted to resurrecting the name and importance of Edward Blyth, a 19th-century naturalist. Eiseley credits Blyth with the development of the idea, and even the coining of the words \"natural selection,\" which Darwin absorbed and enlarged upon...[and] some thoughts on Darwin's Descent of Man; and a concluding speculation on the meaning of evolution. The last piece is very much Eiseley's poetic from-whence-do-we come/whither-do-we-go vein.\" Many experts on Darwin such as Stephen Jay Gould disagreed with Eisley. Michael Ruse, a philosopher of science, even stated \"If a work like this was handed into me for a course. I would give it a failing grade.\" Howard Gruber wrote that \"Eiseley was wrong on every count, both in the broad picture he painted of the Darwin‐Blyth relationship and in the minutiae he scratched up to support his claims.\"\n\nJust before his death Eiseley asked his wife to destroy the personal notebooks which he had kept since 1953. However, she compromised by disassembling them so they couldn't be used. Later, after great effort, his good friend Kenneth Heuer managed to reassemble most of his notebooks into readable form. \"The Lost Notebooks of Loren Eiseley\" includes a variety of Eiseley's writings including childhood stories, sketches while he was a vagabond, old family pictures, unpublished poems, portions of unfinished novels, and letters to and from literary admirers like W.H. Auden, Howard Nemerov, Lewis Mumford and Ray Bradbury.\n\nIn a review of the book, author Robert Finch writes, \"Like Melville, Eiseley thought of himself, and by extension all mankind, as 'an orphan, a wood child, a changeling,' a cosmic outcast born into a world that afforded him no true home.\" He adds that his \"distinctive gift as a writer was to take powerfully formative personal influences of family and place and fuse them with his intellectual meditations on universal topics such as evolution, human consciousness and the weight of time. ... he found metaphors that released a powerful view of man's fate in the modern world.\" As Kenneth Heuer writes, \"there are countless examples of Eiseley's empathy with life in all its forms, and particularly with its lost outcasts...the love that transcends the boundaries of species was the highest spiritual expression he knew.\n\nFinch adds, \"We are grateful for a life and a sensibility that would be welcome in any age, but never more so than in our increasingly depersonalized world. ... he made a generation of readers 'see the world through his eyes.' In an undated passage, circa 1959, Eiseley wrote, 'Man is alone in the universe...Only in the act of love, in rare and hidden communion with nature, does man escape himself.'\" \"The Lost Notebooks\" contains numerous examples of his \"creative and sympathetic imagination, even when that creation takes place in the solitude of journals never meant for public eyes.\"\n\nFrom other reviews: \"Eiseley has rightly been called 'the modern Thoreau.'\" –\"Publishers Weekly;\" \"[an] extensive and enlightening glimpses ... into the intellectual and emotional workshop of one of the most original and influential American essayists of this century.\" –\"New York Times Book Review;\" \"Eiseley's great genius for the art of the word coupled with a poetic insight into the connection between science and humanism shines through in page after page... This is a book that will be read and quoted and whose pages will grow thin with wear from hands in continued search of new meaning within its words and images.\" –\"Los Angeles Times;\" \"it will enhance any dedicated reader's knowledge of this most remarkable literary naturalist... They provide more than a glimpse into Eiseley's mind and imagination.\" –\"The Bloomsbury Review;\" \"It is a joy, like finding a lost Rembrandt in the attic, to discover that Eiseley left behind a legacy.\" –\"San Francisco Examiner-Chronicle.\"\n\nRichard Wentz, professor of religious studies, noted that \"The Christian Century\" magazine called attention to a study of Loren Eiseley by saying: \"The religious chord did not sound in him, but he vibrated to many of the concerns historically related to religion.\" Wentz adds, \"Although Eiseley may not have considered his writing as an expression of American spiritually, one feels that he was quite mindful of its religious character. As an heir of Emerson and Thoreau, he is at home among the poets and philosophers and among those scientists whose observations also were a form of contemplation of the universe.\"\n\nBut Wentz considered the inherent contradictions in the statements: \"We do not really know what to do with religiousness when it expresses itself outside those enclosures which historians and social scientists have carefully labeled religions. What, after all, does it mean to say, \"the religious chord does not sound in someone,\" but that the person vibrates to the concerns historically related to religion? If the person vibrates to such concerns, the chord is religious whether or not it manages to resound in the temples and prayer houses of the devout.\"\n\nWentz quotes Eiseley, from \"All the Strange Hours\" and \"The Star Thrower\", to indicate that he was, in fact, a religious thinker:\n\nWentz encompasses such quotes in his partial conclusion:\n\nHe was indeed a scientist – a bone hunter, he called himself. Archaeologist, anthropologist and naturalist, he devoted a great deal of time and reflection to the detective work of scientific observation. However, if we are to take seriously his essays, we cannot ignore the evidence of his constant meditation on matters of ultimate order and meaning. Science writer Connie Barlow says Eiseley wrote eloquent books from a perspective that today would be called Religious Naturalism.\nWentz writes, \"Loren Eiseley is very much in the tradition of Henry David Thoreau. He takes the circumstances of whatever \"business\" he is about as the occasion for new questioning, new searching for some sign, some glimpse into the meaning of the unknown that confronts him at every center of existence.\" He quotes Eiseley from \"The Star Thrower\", \"We are, in actuality, students of that greater order known as nature. It is into nature that man vanishes.\" \n\nIn comparing Eiseley with Thoreau, he discusses clear similarities in their life and philosophies. He notes that Eiseley was, like Thoreau, a 'spiritual wanderer through the deserts of the modern world.' However, notes Wentz, \"Thoreau had left the seclusion of Walden Pond in order to pace the fields of history, sorting out the artifacts that people had dropped along the way.\" But \"it was those 'fossil thoughts' and 'mindprints' that Eiseley himself explored in his wanderings. These explorations gave depth, a tragic dimension and catharsis to what he called the 'one great drama that concerns us most, the supreme mystery, man.'\"\n\nEiseley's writing often includes his belief that mankind does not have enough evidence to determine exactly how humans came to be. In \"The Immense Journey\", he writes, \"…many lines of seeming relatives, rather than merely one, lead to man. It is as though we stood at the heart of a maze and no longer remembered how we had come there.\" According to Wentz, Eiseley realized that there is nothing below a certain depth that can truly be explained, and quotes Eiseley as saying that there is \"nothing to explain the necessity of life, nothing to explain the hunger of the elements to become life . . . .\" and that \"the human version of evolutionary events [is] perhaps too simplistic for belief.\" \n\nEiseley talked about the illusions of science in his book, \"The Firmament of Time\":\n\nA scientist writing around the turn of the century remarked that all of the past generations of men have lived and died in a world of illusion. The unconscious irony in his observation consists in the fact that this man assumed the progress of science to have been so great that a clear vision of the world without illusion was, by his own time, possible. It is needless to add that he wrote before Einstein... at a time when Mendel was just about to be rediscovered, and before the advances in the study of radioactivity...\n\nWentz noted Eiseley's belief that science may have become misguided in its goals: \"Loren Eiseley thought that much of the modern scientific enterprise had removed humanity ever farther from its sense of responsibility to the natural world it had left in order to create an artificial world to satisfy its own insatiable appetites.\" Interpreting Eiseley's messages, he adds, \"It would be well, he tells us, to heed the message of the Buddha, who knew that 'one cannot proceed upon the path of human transcendence until one has made interiorly in one's soul a road into the future.' Spaces within stretch as far as those without.\" \n\n\"In essay after essay,\" writes Wentz, \"he writes as a magus, a spiritual master or a shaman who has seen into the very heart of the universe and shares his healing vision with those who live in a world of feeble sight. We must learn to see again, he tells us; we must rediscover the true center of the self in the otherness of nature.\" \n\nLoren Eiseley died July 9, 1977, of cardiac arrest following surgery at the University of Pennsylvania Hospital. He was buried in West Laurel Hill Cemetery in Bala Cynwyd, Pennsylvania. Eiseley's wife, Mabel Langdon Eiseley, died July 27, 1986, and is buried next to him, in the Westlawn section of the cemetery, in Lot 366. The inscription on their headstone reads, \"We loved the earth but could not stay\", which is a line from his poem \"The Little Treasures\".\n\nA library in the Lincoln City Libraries public library system is named after Eiseley.\n\nLoren Eiseley was awarded the Distinguished Nebraskan Award and inducted into the Nebraska Hall of Fame. A bust of his likeness resides in the Nebraska State Capitol.\n\nIn summarizing some of Eiseley's contributions, the editor of \"The Bloomsbury Review\" wrote,\n\nThere can be no question that Loren Eiseley maintains a place of eminence among nature writers. His extended explorations of human life and mind, set against the backdrop of our own and other universes are like those to be found in every book of nature writing currently available... We now routinely expect our nature writers to leap across the chasm between science, natural history, and poetry with grace and ease. Eiseley made the leap at a time when science was science, and literature was, well, literature... His writing delivered science to nonscientists in the lyrical language of earthly metaphor, irony, simile, and narrative, all paced like a good mystery.\n\nOn October 25, 2007, the Governor of Nebraska, Dave Heineman, officially declared that year \"The Centennial Year of Loren Eiseley.\" In a written proclamation, he encouraged all Nebraskans\n\nto read Loren Eisely's writings and to appreciate in those writings the richness and beauty of his language, his ability to depict the long, slow passage of time and the meaning of the past in the present, his portrayal of the relationships among all living things and his concern for the future.\n\n\nContains \"The Immense Journey\", \"The Firmament of Time\", \"The Unexpected Universe\", \"The Invisible Pyramid\", \"The Night Country\", essays from \"The Star Thrower\", and uncollected prose (2016) Library of America.\n\n\n\n\n\n"}
{"id": "47958635", "url": "https://en.wikipedia.org/wiki?curid=47958635", "title": "National Hydrogen and Fuel Cell Day", "text": "National Hydrogen and Fuel Cell Day\n\nNational Hydrogen and Fuel Cell Day was created by the Fuel Cell and Hydrogen Energy Association to help raise awareness of fuel cell and hydrogen technologies and to celebrate how far the industry has come as well as the vast potential the technologies have today and in future. \n\nOctober 8th (10.08) was chosen in reference to the atomic weight of hydrogen (1.008).\n\nNational Hydrogen and Fuel Cell Day was officially launched on October 8, 2015. \n\nThe Fuel Cell and Hydrogen Energy Association (FCHEA), its members, industry organizations, allied groups, state and federal governments and individuals are commemorating National Hydrogen and Fuel Cell Day with a variety of activities and events across the country.\n\n\n"}
{"id": "38890", "url": "https://en.wikipedia.org/wiki?curid=38890", "title": "Natural science", "text": "Natural science\n\nNatural science is a branch of science concerned with the description, prediction, and understanding of natural phenomena, based on empirical evidence from observation and experimentation. Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances.\n\nNatural science can be divided into two main branches: life science (or biological science) and physical science. Physical science is subdivided into branches, including physics, chemistry, astronomy and earth science. These branches of natural science may be further divided into more specialized branches (also known as fields).\n\nIn Western society's analytic tradition, the empirical sciences and especially natural sciences use tools from formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the \"laws of nature\". The social sciences also use such methods, but rely more on qualitative research, so that they are sometimes called \"soft science\", whereas natural sciences, insofar as they emphasize quantifiable data produced, tested, and confirmed through the scientific method, are sometimes called \"hard science\".\n\nModern natural science succeeded more classical approaches to natural philosophy, usually traced to ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science. Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on. Today, \"natural history\" suggests observational descriptions aimed at popular audiences.\n\nPhilosophers of science have suggested a number of criteria, including Karl Popper's controversial falsifiability criterion, to help them differentiate scientific endeavors from non-scientific ones. Validity, accuracy, and quality control, such as peer review and repeatability of findings, are amongst the most respected criteria in the present-day global scientific community.\n\nThis field encompasses a set of disciplines that examines phenomena related to living organisms. The scale of study can range from sub-component biophysics up to complex ecologies. Biology is concerned with the characteristics, classification and behaviors of organisms, as well as how species were formed and their interactions with each other and the environment.\n\nThe biological fields of botany, zoology, and medicine date back to early periods of civilization, while microbiology was introduced in the 17th century with the invention of the microscope. However, it was not until the 19th century that biology became a unified science. Once scientists discovered commonalities between all living things, it was decided they were best studied as a whole.\n\nSome key developments in biology were the discovery of genetics; evolution through natural selection; the germ theory of disease and the application of the techniques of chemistry and physics at the level of the cell or organic molecule.\n\nModern biology is divided into subdisciplines by the type of organism and by the scale being studied. Molecular biology is the study of the fundamental chemistry of life, while cellular biology is the examination of the cell; the basic building block of all life. At a higher level, anatomy and physiology looks at the internal structures, and their functions, of an organism, while ecology looks at how various organisms interrelate.\n\nConstituting the scientific study of matter at the atomic and molecular scale, chemistry deals primarily with collections of atoms, such as gases, molecules, crystals, and metals. The composition, statistical properties, transformations and reactions of these materials are studied. Chemistry also involves understanding the properties and interactions of individual atoms and molecules for use in larger-scale applications.\n\nMost chemical processes can be studied directly in a laboratory, using a series of (often well-tested) techniques for manipulating materials, as well as an understanding of the underlying processes. Chemistry is often called \"the central science\" because of its role in connecting the other natural sciences.\n\nEarly experiments in chemistry had their roots in the system of Alchemy, a set of beliefs combining mysticism with physical experiments. The science of chemistry began to develop with the work of Robert Boyle, the discoverer of gas, and Antoine Lavoisier, who developed the theory of the Conservation of mass.\n\nThe discovery of the chemical elements and atomic theory began to systematize this science, and researchers developed a fundamental understanding of states of matter, ions, chemical bonds and chemical reactions. The success of this science led to a complementary chemical industry that now plays a significant role in the world economy.\n\nPhysics embodies the study of the fundamental constituents of the universe, the forces and interactions they exert on one another, and the results produced by these interactions. In general, physics is regarded as the fundamental science, because all other natural sciences use and obey the principles and laws set down by the field. Physics relies heavily on mathematics as the logical framework for formulation and quantification of principles.\n\nThe study of the principles of the universe has a long history and largely derives from direct observation and experimentation. The formulation of theories about the governing laws of the universe has been central to the study of physics from very early on, with philosophy gradually yielding to systematic, quantitative experimental testing and observation as the source of verification. Key historical developments in physics include Isaac Newton's theory of universal gravitation and classical mechanics, an understanding of electricity and its relation to magnetism, Einstein's theories of special and general relativity, the development of thermodynamics, and the quantum mechanical model of atomic and subatomic physics.\n\nThe field of physics is extremely broad, and can include such diverse studies as quantum mechanics and theoretical physics, applied physics and optics. Modern physics is becoming increasingly specialized, where researchers tend to focus on a particular area rather than being \"universalists\" like Isaac Newton, Albert Einstein and Lev Landau, who worked in multiple areas.\n\nThis discipline is the science of celestial objects and phenomena that originate outside the Earth's atmosphere. It is concerned with the evolution, physics, chemistry, meteorology, and motion of celestial objects, as well as the formation and development of the universe.\n\nAstronomy includes the examination, study and modeling of stars, planets, comets, galaxies and the cosmos. Most of the information used by astronomers is gathered by remote observation, although some laboratory reproduction of celestial phenomena has been performed (such as the molecular chemistry of the interstellar medium).\n\nWhile the origins of the study of celestial features and phenomena can be traced back to antiquity, the scientific methodology of this field began to develop in the middle of the 17th century. A key factor was Galileo's introduction of the telescope to examine the night sky in more detail.\n\nThe mathematical treatment of astronomy began with Newton's development of celestial mechanics and the laws of gravitation, although it was triggered by earlier work of astronomers such as Kepler. By the 19th century, astronomy had developed into a formal science, with the introduction of instruments such as the spectroscope and photography, along with much-improved telescopes and the creation of professional observatories.\n\nEarth science (also known as geoscience), is an all-embracing term for the sciences related to the planet Earth, including geology, geophysics, hydrology, meteorology, physical geography, oceanography, and soil science.\n\nAlthough mining and precious stones have been human interests throughout the history of civilization, the development of the related sciences of economic geology and mineralogy did not occur until the 18th century. The study of the earth, particularly palaeontology, blossomed in the 19th century. The growth of other disciplines, such as geophysics, in the 20th century led to the development of the theory of plate tectonics in the 1960s, which has had a similar effect on the Earth sciences as the theory of evolution had on biology. Earth sciences today are closely linked to petroleum and mineral resources, climate research and to environmental assessment and remediation.\n\nThough sometimes considered in conjunction with the earth sciences, due to the independent development of its concepts, techniques and practices and also the fact of it having a wide range of sub disciplines under its wing, the atmospheric sciences is also considered a separate branch of natural science. This field studies the characteristics of different layers of the atmosphere from ground level to the edge of the time. The timescale of study also varies from days to centuries. Sometimes the field also includes the study of climatic patterns on planets other than earth.\n\nThe serious study of oceans began in the early to mid-20th century. As a field of natural science, it is relatively young but stand-alone programs offer specializations in the subject. Though some controversies remain as to the categorization of the field under earth sciences, interdisciplinary sciences or as a separate field in its own right, most modern workers in the field agree that it has matured to a state that it has its own paradigms and practices. As such a big family of related studies spanning every aspect of the oceans is now classified under this field.\n\nThe distinctions between the natural science disciplines are not always sharp, and they share a number of cross-discipline fields. Physics plays a significant role in the other natural sciences, as represented by astrophysics, geophysics, chemical physics and biophysics. Likewise chemistry is represented by such fields as biochemistry, chemical biology, geochemistry and astrochemistry.\n\nA particular example of a scientific discipline that draws upon multiple natural sciences is environmental science. This field studies the interactions of physical, chemical, geological, and biological components of the environment, with a particular regard to the effect of human activities and the impact on biodiversity and sustainability. This science also draws upon expertise from other fields such as economics, law and social sciences.\n\nA comparable discipline is oceanography, as it draws upon a similar breadth of scientific disciplines. Oceanography is sub-categorized into more specialized cross-disciplines, such as physical oceanography and marine biology. As the marine ecosystem is very large and diverse, marine biology is further divided into many subfields, including specializations in particular species.\n\nThere are also a subset of cross-disciplinary fields which, by the nature of the problems that they address, have strong currents that run counter to\nspecialization. Put another way: In some fields of integrative application, specialists in more than one field are a key part of most dialog. Such integrative fields, for example, include nanoscience, astrobiology, and complex system informatics.\n\nMaterials science is a relatively new, interdisciplinary field which deals with the study of matter and its properties; as well as the discovery and design of new materials. Originally developed through the field of metallurgy, the study of the properties of materials and solids has now expanded into all materials. The field covers the chemistry, physics and engineering applications of materials including metals, ceramics, artificial polymers, and many others. The core of the field deals with relating structure of material with it properties.\n\nIt is at the forefront of research in science and engineering. It is an important part of forensic engineering (the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property) and failure analysis, the latter being the key to understanding, for example, the cause of various aviation accidents. Many of the most pressing scientific problems that are faced today are due to the limitations of the materials that are available and, as a result, breakthroughs in this field are likely to have a significant impact on the future of technology.\n\nThe basis of materials science involves studying the structure of materials, and relating them to their properties. Once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a certain application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure, and thus its properties.\n\nSome scholars trace the origins of natural science as far back as pre-literate human societies, where understanding the natural world was necessary for survival. People observed and built up knowledge about the behavior of animals and the usefulness of plants as food and medicine, which was passed down from generation to generation. These primitive understandings gave way to more formalized inquiry around 3500 to 3000 BC in the Mesopotamian and Ancient Egyptian cultures, which produced the first known written evidence of natural philosophy, the precursor of natural science. While the writings show an interest in astronomy, mathematics and other aspects of the physical world, the ultimate aim of inquiry about nature's workings was in all cases religious or mythological, not scientific.\n\nA tradition of scientific inquiry also emerged in Ancient China, where Taoist alchemists and philosophers experimented with elixirs to extend life and cure ailments. They focused on the yin and yang, or contrasting elements in nature; the yin was associated with femininity and coldness, while yang was associated with masculinity and warmth. The five phases – fire, earth, metal, wood and water – described a cycle of transformations in nature. Water turned into wood, which turned into fire when it burned. The ashes left by fire were earth. Using these principles, Chinese philosophers and doctors explored human anatomy, characterizing organs as predominantly yin or yang and understood the relationship between the pulse, the heart and the flow of blood in the body centuries before it became accepted in the West.\n\nLittle evidence survives of how Ancient Indian cultures around the Indus River understood nature, but some of their perspectives may be reflected in the Vedas, a set of sacred Hindu texts. They reveal a conception of the universe as ever-expanding and constantly being recycled and reformed. Surgeons in the Ayurvedic tradition saw health and illness as a combination of three humors: wind, bile and phlegm. A healthy life was the result of a balance among these humors. In Ayurvedic thought, the body consisted of five elements: earth, water, fire, wind and empty space. Ayurvedic surgeons performed complex surgeries and developed a detailed understanding of human anatomy.\n\nPre-Socratic philosophers in Ancient Greek culture brought natural philosophy a step closer to direct inquiry about cause and effect in nature between 600 and 400 BC, although an element of magic and mythology remained. Natural phenomena such as earthquakes and eclipses were explained increasingly in the context of nature itself instead of being attributed to angry gods. Thales of Miletus, an early philosopher who lived from 625 to 546 BC, explained earthquakes by theorizing that the world floated on water and that water was the fundamental element in nature. In the 5th century BC, Leucippus was an early exponent of atomism, the idea that the world is made up of fundamental indivisible particles. Pythagoras applied Greek innovations in mathematics to astronomy, and suggested that the earth was spherical.\n\nLater Socratic and Platonic thought focused on ethics, morals and art and did not attempt an investigation of the physical world; Plato criticized pre-Socratic thinkers as materialists and anti-religionists. Aristotle, however, a student of Plato who lived from 384 to 322 BC, paid closer attention to the natural world in his philosophy. In his \"History of Animals\", he described the inner workings of 110 species, including the stingray, catfish and bee. He investigated chick embryos by breaking open eggs and observing them at various stages of development. Aristotle's works were influential through the 16th century, and he is considered to be the father of biology for his pioneering work in that science. He also presented philosophies about physics, nature and astronomy using inductive reasoning in his works \"Physics\" and \"Meteorology\".\n\nWhile Aristotle considered natural philosophy more seriously than his predecessors, he approached it as a theoretical branch of science. Still, inspired by his work, Ancient Roman philosophers of the early 1st century AD, including Lucretius, Seneca and Pliny the Elder, wrote treatises that dealt with the rules of the natural world in varying degrees of depth. Many Ancient Roman Neoplatonists of the 3rd to the 6th centuries also adapted Aristotle's teachings on the physical world to a philosophy that emphasized spiritualism. Early medieval philosophers including Macrobius, Calcidius and Martianus Capella also examined the physical world, largely from a cosmological and cosmographical perspective, putting forth theories on the arrangement of celestial bodies and the heavens, which were posited as being composed of aether.\n\nAristotle's works on natural philosophy continued to be translated and studied amid the rise of the Byzantine Empire and Abbasid Caliphate. \n\nIn the Byzantine Empire John Philoponus, an Alexandrian Aristotelian commentator and Christian theologian, was the first who questioned Aristotle's teaching of physics. Unlike Aristotle who based his physics on verbal argument, Philoponus instead relied on observation, and argued for observation rather than resorting into verbal argument. He introduced the theory of impetus. John Philoponus' criticism of Aristotelian principles of physics served as inspiration for Galileo Galilei during the Scientific Revolution.\n\nA revival in mathematics and science took place during the time of the Abbasid Caliphate from the 9th century onward, when Muslim scholars expanded upon Greek and Indian natural philosophy. The words \"alcohol\", \"algebra\" and \"zenith\" all have Arabic roots.\n\nAristotle's works and other Greek natural philosophy did not reach the West until about the middle of the 12th century, when works were translated from Greek and Arabic into Latin. The development of European civilization later in the Middle Ages brought with it further advances in natural philosophy. European inventions such as the horseshoe, horse collar and crop rotation allowed for rapid population growth, eventually giving way to urbanization and the foundation of schools connected to monasteries and cathedrals in modern-day France and England. Aided by the schools, an approach to Christian theology developed that sought to answer questions about nature and other subjects using logic. This approach, however, was seen by some detractors as heresy. By the 12th century, Western European scholars and philosophers came into contact with a body of knowledge of which they had previously been ignorant: a large corpus of works in Greek and Arabic that were preserved by Islamic scholars. Through translation into Latin, Western Europe was introduced to Aristotle and his natural philosophy. These works were taught at new universities in Paris and Oxford by the early 13th century, although the practice was frowned upon by the Catholic church. A 1210 decree from the Synod of Paris ordered that \"no lectures are to be held in Paris either publicly or privately using Aristotle's books on natural philosophy or the commentaries, and we forbid all this under pain of excommunication.\"\n\nIn the late Middle Ages, Spanish philosopher Dominicus Gundissalinus translated a treatise by the earlier Persian scholar Al-Farabi called \"On the Sciences\" into Latin, calling the study of the mechanics of nature \"scientia naturalis\", or natural science. Gundissalinus also proposed his own classification of the natural sciences in his 1150 work \"On the Division of Philosophy\". This was the first detailed classification of the sciences based on Greek and Arab philosophy to reach Western Europe. Gundissalinus defined natural science as \"the science considering only things unabstracted and with motion,\" as opposed to mathematics and sciences that rely on mathematics. Following Al-Farabi, he then separated the sciences into eight parts, including physics, cosmology, meteorology, minerals science and plant and animal science.\n\nLater philosophers made their own classifications of the natural sciences. Robert Kilwardby wrote \"On the Order of the Sciences\" in the 13th century that classed medicine as a mechanical science, along with agriculture, hunting and theater while defining natural science as the science that deals with bodies in motion. Roger Bacon, an English friar and philosopher, wrote that natural science dealt with \"a principle of motion and rest, as in the parts of the elements of fire, air, earth and water, and in all inanimate things made from them.\" These sciences also covered plants, animals and celestial bodies. Later in the 13th century, Catholic priest and theologian Thomas Aquinas defined natural science as dealing with \"mobile beings\" and \"things which depend on matter not only for their existence, but also for their definition.\" There was wide agreement among scholars in medieval times that natural science was about bodies in motion, although there was division about the inclusion of fields including medicine, music and perspective. Philosophers pondered questions including the existence of a vacuum, whether motion could produce heat, the colors of rainbows, the motion of the earth, whether elemental chemicals exist and where in the atmosphere rain is formed.\n\nIn the centuries up through the end of the Middle Ages, natural science was often mingled with philosophies about magic and the occult. Natural philosophy appeared in a wide range of forms, from treatises to encyclopedias to commentaries on Aristotle. The interaction between natural philosophy and Christianity was complex during this period; some early theologians, including Tatian and Eusebius, considered natural philosophy an outcropping of pagan Greek science and were suspicious of it. Although some later Christian philosophers, including Aquinas, came to see natural science as a means of interpreting scripture, this suspicion persisted until the 12th and 13th centuries. The Condemnation of 1277, which forbade setting philosophy on a level equal with theology and the debate of religious constructs in a scientific context, showed the persistence with which Catholic leaders resisted the development of natural philosophy even from a theological perspective. Aquinas and Albertus Magnus, another Catholic theologian of the era, sought to distance theology from science in their works. \"I don't see what one's interpretation of Aristotle has to do with the teaching of the faith,\" he wrote in 1271.\n\nBy the 16th and 17th centuries, natural philosophy underwent an evolution beyond commentary on Aristotle as more early Greek philosophy was uncovered and translated. The invention of the printing press in the 15th century, the invention of the microscope and telescope, and the Protestant Reformation fundamentally altered the social context in which scientific inquiry evolved in the West. Christopher Columbus's discovery of a new world changed perceptions about the physical makeup of the world, while observations by Copernicus, Tyco Brahe and Galileo brought a more accurate picture of the solar system as heliocentric and proved many of Aristotle's theories about the heavenly bodies false. A number of 17th-century philosophers, including Thomas Hobbes, John Locke and Francis Bacon made a break from the past by rejecting Aristotle and his medieval followers outright, calling their approach to natural philosophy as superficial.\n\nThe titles of Galileo's work \"Two New Sciences\" and Johannes Kepler's \"New Astronomy\" underscored the atmosphere of change that took hold in the 17th century as Aristotle was dismissed in favor of novel methods of inquiry into the natural world. Bacon was instrumental in popularizing this change; he argued that people should use the arts and sciences to gain dominion over nature. To achieve this, he wrote that \"human life [must] be endowed with new discoveries and powers.\" He defined natural philosophy as \"the knowledge of Causes and secret motions of things; and enlarging the bounds of Human Empire, to the effecting of all things possible.\" Bacon proposed scientific inquiry supported by the state and fed by the collaborative research of scientists, a vision that was unprecedented in its scope, ambition and form at the time. Natural philosophers came to view nature increasingly as a mechanism that could be taken apart and understood, much like a complex clock. Natural philosophers including Isaac Newton, Evangelista Torricelli and Francesco Redi conducted experiments focusing on the flow of water, measuring atmospheric pressure using a barometer and disproving spontaneous generation. Scientific societies and scientific journals emerged and were spread widely through the printing press, touching off the scientific revolution. Newton in 1687 published his \"The Mathematical Principles of Natural Philosophy\", or \"Principia Mathematica\", which set the groundwork for physical laws that remained current until the 19th century.\n\nSome modern scholars, including Andrew Cunningham, Perry Williams and Floris Cohen, argue that natural philosophy is not properly called a science, and that genuine scientific inquiry began only with the scientific revolution. According to Cohen, \"the emancipation of science from an overarching entity called 'natural philosophy' is one defining characteristic of the Scientific Revolution.\" Other historians of science, including Edward Grant, contend that the scientific revolution that blossomed in the 17th, 18th and 19th centuries occurred when principles learned in the exact sciences of optics, mechanics and astronomy began to be applied to questions raised by natural philosophy. Grant argues that Newton attempted to expose the mathematical basis of nature – the immutable rules it obeyed – and in doing so joined natural philosophy and mathematics for the first time, producing an early work of modern physics.\nThe scientific revolution, which began to take hold in the 17th century, represented a sharp break from Aristotelian modes of inquiry. One of its principal advances was the use of the scientific method to investigate nature. Data was collected and repeatable measurements made in experiments. Scientists then formed hypotheses to explain the results of these experiments. The hypothesis was then tested using the principle of falsifiability to prove or disprove its accuracy. The natural sciences continued to be called natural philosophy, but the adoption of the scientific method took science beyond the realm of philosophical conjecture and introduced a more structured way of examining nature.\n\nNewton, an English mathematician and physicist, was the seminal figure in the scientific revolution. Drawing on advances made in astronomy by Copernicus, Brahe and Kepler, Newton derived the universal law of gravitation and laws of motion. These laws applied both on earth and in outer space, uniting two spheres of the physical world previously thought to function independently of each other, according to separate physical rules. Newton, for example, showed that the tides were caused by the gravitational pull of the moon. Another of Newton's advances was to make mathematics a powerful explanatory tool for natural phenomena. While natural philosophers had long used mathematics as a means of measurement and analysis, its principles were not used as a means of understanding cause and effect in nature until Newton.\n\nIn the 18th century and 19th century, scientists including Charles-Augustin de Coulomb, Alessandro Volta, and Michael Faraday built upon Newtonian mechanics by exploring electromagnetism, or the interplay of forces with positive and negative charges on electrically charged particles. Faraday proposed that forces in nature operated in \"fields\" that filled space. The idea of fields contrasted with the Newtonian construct of gravitation as simply \"action at a distance\", or the attraction of objects with nothing in the space between them to intervene. James Clerk Maxwell in the 19th century unified these discoveries in a coherent theory of electrodynamics. Using mathematical equations and experimentation, Maxwell discovered that space was filled with charged particles that could act upon themselves and each other, and that they were a medium for the transmission of charged waves.\n\nSignificant advances in chemistry also took place during the scientific revolution. Antoine Lavoisier, a French chemist, refuted the phlogiston theory, which posited that things burned by releasing \"phlogiston\" into the air. Joseph Priestley had discovered oxygen in the 18th century, but Lavoisier discovered that combustion was the result of oxidation. He also constructed a table of 33 elements and invented modern chemical nomenclature. Formal biological science remained in its infancy in the 18th century, when the focus lay upon the classification and categorization of natural life. This growth in natural history was led by Carl Linnaeus, whose 1735 taxonomy of the natural world is still in use. Linnaeus in the 1750s introduced scientific names for all his species.\n\nBy the 19th century, the study of science had come into the purview of professionals and institutions. In so doing, it gradually acquired the more modern name of \"natural science.\" The term \"scientist\" was coined by William Whewell in an 1834 review of Mary Somerville's \"On the Connexion of the Sciences\". But the word did not enter general use until nearly the end of the same century.\n\nAccording to a famous 1923 textbook \"Thermodynamics and the Free Energy of Chemical Substances\" by the American chemist Gilbert N. Lewis and the American physical chemist Merle Randall, the natural sciences contain three great branches:\n\nAside from the logical and mathematical sciences, there are three great branches of \"natural science\" which stand apart by reason of the variety of far reaching deductions drawn from a small number of primary postulates — they are mechanics, electrodynamics, and thermodynamics.\n\nToday, natural sciences are more commonly divided into life sciences, such as botany and zoology; and physical sciences, which include physics, chemistry, geology, astronomy and materials science.\n\n\n\n"}
{"id": "412846", "url": "https://en.wikipedia.org/wiki?curid=412846", "title": "Naturalistic pantheism", "text": "Naturalistic pantheism\n\nNaturalistic pantheism is a kind of pantheism. It has been used in various ways such as to relate God or divinity with concrete things, determinism, or the substance of the Universe. God, from these perspectives, is seen as the aggregate of all unified natural phenomena. The phrase has often been associated with the philosophy of Baruch Spinoza, although academics differ on how it is used.\n\nThe term “pantheism\" is derived from Greek words \"pan\" (Greek: πᾶν) meaning \"all\" and \"theos\" (θεός) meaning God. It was coined by Joseph Raphson in his work \"De spatio reali\", published in 1697. The term was introduced to English by Irish writer John Toland in his 1705 work \"Socinianism Truly Stated, by a pantheist\" that described pantheism as the \"opinion of those who believe in no other eternal being but the universe.\"\n\nThe term \"naturalistic\" derives from the word \"naturalism\", which has several meanings in philosophy and aesthetics. In philosophy the term frequently denotes the view that everything belongs to the world of nature and can be studied with the methods appropriate for studying that world, \"i.e.\" the sciences. It generally implies an absence of belief in supernatural beings.\n\nJoseph Needham, a modern British scholar of Chinese philosophy and science, has identified Taoism as \"a naturalistic pantheism which emphasizes the unity and spontaneity of the operations of Nature.\" This philosophy can be dated to the late 4th century BCE.\n\nThe Hellenistic Greek philosophical school of Stoicism (which started in the early 3rd century BCE) rejected the dualist idea of the separate ideal/conscious and material realms, and identified the substance of God with the entire cosmos and heaven. However, not all philosophers who did so can be classified as naturalistic pantheists.\n\nNaturalistic pantheism was expressed by various thinkers, including Giordano Bruno, who was burned at the stake for his views. However, the 17th century Dutch philosopher Spinoza became particularly known for it.\n\nPossibly drawing upon the ideas of Descartes,\nBaruch Spinoza connected God and Nature through the phrase \"deus sive natura\" (\"God, or Nature\"), making him the father of classical pantheism. He relied upon rationalism rather than the more intuitive approach of some Eastern traditions.\n\nSpinoza's philosophy, sometimes known as Spinozism, has been understood in a number of ways, and caused disagreements such as the Pantheism controversy. However, many scholars have considered it to be a form of naturalistic pantheism. This has included viewing the pantheistic unity as natural. \nOthers focus on the deterministic aspect of naturalism.\nSpinoza inspired a number of other pantheists, with varying degrees of idealism towards nature. However, Spinoza's influence in his own time was limited.\nScholars have considered Spinoza the founder of a line of naturalistic pantheism, though not necessarily the only one.\n\nIn 1705 the Irish writer John Toland endorsed a form of pantheism in which the God-soul is identical with the material universe.\n\nGerman naturalist Ernst Haeckel (1834–1919) proposed a monistic pantheism in which the idea of God is identical with that of nature or substance.\n\nThe World Pantheist Movement, started in 1999, describes Naturalistic Pantheism as including reverence for the universe, realism, strong naturalism, and respect for reason and the scientific method as methods of understanding the world. Paul Harrison considers its position the closest modern equivalent to Toland's.\n\n"}
{"id": "14738913", "url": "https://en.wikipedia.org/wiki?curid=14738913", "title": "Peterson Field Guides", "text": "Peterson Field Guides\n\nThe Peterson Field Guides (PFG) are a popular and influential series of American field guides intended to assist the layman in identification of birds, plants, insects and other natural phenomena. The series was created and edited by renowned ornithologist Roger Tory Peterson (1908–1996). His inaugural volume was the classic 1934 book \"A Field Guide to the Birds\", published (as were all subsequent volumes) by the Houghton Mifflin Company.\n\nThe PFG series utilized what became known as the Peterson Identification System, a practical method for field identification which highlights readily noticed visual features rather than focusing on the technical features of interest to scientists. The series both reflected and contributed to awareness of the emerging environmental movement.\n\nMost books in this series use a section of plates of drawings (usually reduced from commissioned paintings) rather than photographs of the subject species, grouped at the center of the book. This allows for idealized portraits that highlight the identifying \"field marks\" of each species; such field marks are often indicated by arrows or straight lines in the plate illustrations. However, in several books in this series, the plates consist of photographs (usually without such arrows or indicators), such as in the guides for the atmosphere, coral reefs, rocks and minerals, and the (old Charles Covell 1984 guide to) Eastern moths. In many books in this series (especially older editions), a number of the plates are in black and white. For examples, older editions of the Eastern reptiles/amphibians book had many black and white plates which were colorized for the current edition, and the original 1934 Eastern bird book had only 4 color plates. At least one book (insects) was entirely in black and white. However, most newer editions are often full-color (or almost full-color) and tend to be larger. One source claims that the increased size of one of the new editions (Eastern reptiles/amphibians) was considered detrimental to its use as a field guide by its own author and was a publisher decision.\n\nIn some cases, new \"editions\" in this series are entirely new books with completely new texts and illustrations. For example, the fourth edition of the mammals guide has an entirely new text and illustrations by new author Fiona Reid, because the author (William Burt) and illustrator (Richard Grossenheider) of previous editions are both deceased. In fact, Grossenheider died prior to the publication of the \"previous\" third edition of 1976. Also, the current Northeastern moths guide by David Beadle and Seabrooke Leckie is an entirely new book than the out-of-print 1984 Eastern moths guide by Charles Covell. The Beadle/Leckie book covers a smaller geographical area and (one author claims) covers moths in greater detail. The old Covell book has been out-of-print for many years, but is currently available through the Virginia Museum of Natural History (which purchased the rights to that book).\n\nThe above situation of an old \"edition\" persisting alongside its intended replacement edition is not unique to the Eastern moths guide. George Petrides' 1988 Eastern trees book (PFG11B) was originally intended to replace Petrides' own 1958 Eastern tree and shrubs (PFG11A) book. However, both books remain popular and the original publisher still offers both books for sale (unlike the case of the old Eastern moths book).\n\nDifferences between editions can serve to indicate changes in scientific perspective as well as changes species distribution. For example, the second edition of the freshwater fishes guide by Page and Burr (2011), published 20 years after the first edition, increased the number of species included from 768 to 909, largely due to the addition of previously unrecognized species (114), as well as increased numbers of newly established exotic species (16). It also expanded coverage of marine fish commonly found in freshwater (19).\n\nBoth of these guides appeared in the Easton Press leather bound copies of the series. For that series the title of the Bond book was changed to \"Birds of the Caribbean\".\n \"Birds of the West Indies\" (1999), by James Bond\n\n\nOther volumes:\n\nAppweavers, Inc., the licensee of the Peterson field guides for mobile devices, has developed the Peterson Birds of North America and Peterson Feeder Birds of North America apps for mobile Apple products. The Peterson Birds of North America app also includes some content from other books in the Peterson field guide series.\n\n"}
{"id": "58664232", "url": "https://en.wikipedia.org/wiki?curid=58664232", "title": "Phakalane power station", "text": "Phakalane power station\n\nPhakalane Power Station is a photovoltaic pilot power plant located in Phakalane, Botswana. The power station was funded through a Japanese grant which was part of Prime Minister Hatoyama's initiative strategy called Cool Earth Partnership aimed at supporting developing countries in their efforts to combat global warming. The Cool Earth Partnership is part of the initiatives which saw Hatoyama win the Sustainable Development Leadership Award in 2010.\n\n"}
{"id": "5389553", "url": "https://en.wikipedia.org/wiki?curid=5389553", "title": "Photophoresis", "text": "Photophoresis\n\nPhotophoresis denotes the phenomenon that small particles suspended in gas (aerosols) or liquids (hydrocolloids) start to migrate when illuminated by a sufficiently intense beam of light. The existence of this phenomenon is owed to a non-uniform distribution of temperature of an illuminated particle in a fluid medium. Separately from photophoresis, in a fluid mixture of different kinds of particles, the migration of some kinds of particles may be due to differences in their absorptions of thermal radiation and other thermal effects collectively known as thermophoresis. In laser photophoresis, particles migrate once they have a refractive index different from their surrounding medium. The migration of particles is usually possible when the laser is slightly or not focused. A particle with a higher refractive index compared to its surrounding molecule moves away from the light source due to momentum transfer from absorbed and scattered light photons. This is referred to as a radiation pressure force. This force depends on light intensity and particle size but has nothing to do with the surrounding medium. Just like in Crookes radiometer, light can heat up one side and gas molecules bounce from that surface with greater velocity, hence push the particle to the other side. Under certain conditions, with particles of diameter comparable to the wavelength of light, the phenomenon of a negative indirect photophoresis occurs, due to the unequal heat generation on the laser irradiation between the back and front sides of particles, this produces a temperature gradient in the medium around the particle such that molecules at the far side of the particle from the light source may get to heat up more, causing the particle to move towards the light source.\n\nIf the suspended particle is rotating, it will also experience the Yarkovsky effect.\n\nDiscovery of photophoresis is usually attributed to Felix Ehrenhaft in the 1920s, though earlier observations were made by others including Augustin-Jean Fresnel.\n\nThe applications of photophoresis expand into the various divisions of science, thus physics, chemistry as well as in biology. Photophoresis is applied in particle trapping and levitation, in the field flow fractionation of particles, in the determination of thermal conductivity and temperature of microscopic grains and also in the transport of soot particles in the atmosphere. The use of light in the separation of particles aerosols based on their optical properties, makes possible the separation of organic and inorganic particles of the same aerodynamic size.\n\nRecently, photophoresis has been suggested as a chiral sorting mechanism for single walled carbon nanotubes. The proposed method would utilise differences in the absorption spectra of semiconducting carbon nanotubes arising from optically excited transitions in electronic structure. If developed the technique would be orders of magnitudes faster than currently established ultracentrifugation techniques.\n\nDirect photophoresis is caused by the transfer of photon momentum to a particle by refraction and reflection. Movement of particles in the forward direction occurs when the particle is transparent and has an index of refraction larger compared to its surrounding medium. Indirect photophoresis occurs as a result of an increase in the kinetic energy of molecules when particles absorb incident light only on the irradiated side, thus creating a temperature gradient within the particle. In this situation the surrounding gas layer reaches temperature equilibrium with the surface of the particle. Molecules with higher kinetic energy in the region of higher gas temperature impinge on the particle with greater momenta than molecules in the cold region; this causes a migration of particles in a direction opposite to the surface temperature gradient. The component of the photophoretic force responsible for this phenomenon is called the radiometric force. This comes as a result of uneven distribution of radiant energy (source function within a particle).\nIndirect photophoretic force depends on the physical properties of the particle and the surrounding medium.\n\nFor pressures formula_1, where the free mean path of the gas is much larger than the characteristic size formula_2 of the suspended particle (direct photophoresis), the longitudinal force is \nwhere the mean temperature of the scattered gas is (thermal accommodation coefficient formula_4, momentum accommodation coefficient formula_5)\nand the black body temperature of the particle (net light flux formula_7, Stefan Boltzmann constant formula_8, temperature of the radiation field formula_9)\nformula_11 is the thermal conductivity of the particle.\nThe asymmetry factor for spheres formula_12 is usually formula_13 (positive longitudinal photophoresis).\nFor non-spherical particles, the average force exerted on the particle is given by the same equation where the radius formula_2 is now the radius of the respective volume-equivalent sphere.\n\n"}
{"id": "169115", "url": "https://en.wikipedia.org/wiki?curid=169115", "title": "Preternatural", "text": "Preternatural\n\nThe preternatural or praeternatural is that which appears outside or beside (Latin \"\") the natural. It is \"suspended between the mundane and the miraculous\".\n\nIn theology, the term is often used to distinguish marvels or deceptive trickery, often attributed to witchcraft or demons, from the purely divine power of the genuinely supernatural to violate the laws of nature. In the early modern period the term was used by scientists to refer to abnormalities and strange phenomena of various kinds that seemed to depart from the norms of nature.\n\nMedieval theologians made a clear distinction between the natural, the preternatural and the supernatural. Thomas Aquinas argued that the supernatural consists in \"God’s unmediated actions\"; the natural is \"what happens always or most of the time\"; and the preternatural is \"what happens rarely, but nonetheless by the agency of created beings...Marvels belong, properly speaking, to the realm of the preternatural.\" Theologians, following Aquinas, argued that only God had the power to disregard the laws of nature that he has created, but that demons could manipulate the laws of nature by a form of trickery, to deceive the unwary into believing they had experienced real miracles. According to historian Lorraine Daston,\n\nAlthough demons, astral intelligences, and other spirits might manipulate natural causes with superhuman dexterity and thereby work marvels, as mere creatures they could never transcend from the preternatural to the supernatural and work genuine miracles.\n\nBy the 16th century, the term \"preternatural\" was increasingly used to refer to demonic activity comparable to the use of magic by human adepts: The Devil, \"being a natural Magician … may perform many acts in ways above our knowledge, though not transcending our natural power.\" According to the philosophy of the time, preternatural phenomena were not contrary to divine law, but used hidden, or occult powers that violated the \"normal\" pattern of natural phenomena.\n\nWith the emergence of early modern science, the concept of the preternatural increasingly came to be used to refer to strange or abnormal phenomena that seemed to violate the normal working of nature, but which were not associated with magic and witchcraft. This was a development of the idea that preternatural phenomena were fake miracles. As Daston puts it, \"To simplify the historical sequence somewhat: first, preternatural phenomena were demonized and thereby incidentally naturalized; then the demons were deleted, leaving only the natural causes.\" The use of the term was especially common in medicine, for example in John Brown's \"A Compleat Treatise of Preternatural Tumours\" (1678), or William Smellie's \"A Collection of Preternatural Cases and Observations in Midwifery\" (1754).\n\nIn the 19th century the term was appropriated in anthropology to refer to folk beliefs about fairies, trolls and other such creatures which were not thought of as demonic, but which were perceived to affect the natural world in unpredictable ways. According to Thorstein Veblen, such preternatural agents were often thought of as forces somewhere between supernatural beings and material processes. \"The preternatural agency is not necessarily conceived to be a personal agent in the full sense, but it is an agency which partakes of the attributes of personality to the extent of somewhat arbitrarily influencing the outcome of any enterprise, and especially of any contest.\"\n\nThe linguistic association between individual agents and unexplained or unfortunate circumstances remains. Many people attribute occurrences that are known to be material processes, such as \"gremlins in the engine\", a \"ghost in the machine\", or attributing motives to objects: \"the clouds are threatening\". The anthropomorphism in our daily life is a combination of the above cultural stems, as well as the manifestation of our pattern-projecting minds.\n\nIn 2011, Penn State Press began publishing a learned journal entitled \"Preternature: Critical and Historical Studies on the Preternatural\". Edited by Kirsten Uszkalo and Richard Raiswell, the journal is dedicated to publishing articles, reviews and short editions of original texts that deal with conceptions and perceptions of the preternatural in any culture and in any historical period. The journal covers \"magics, witchcraft, spiritualism, occultism, prophecy, monstrophy, demonology, and folklore.\"\n\n\n"}
{"id": "55968776", "url": "https://en.wikipedia.org/wiki?curid=55968776", "title": "Reciprocal causation", "text": "Reciprocal causation\n\nIn biology, reciprocal causation arises when developing organisms are both \"products\" of evolution as well as \"causes\" of evolution. Formally, reciprocal causation exists when process A is a cause of process B and, subsequently, process B is a cause of process A, with this feedback potentially repeated. Some researchers, particularly advocates of the extended evolutionary synthesis, promote the view that causation in biological systems is inherently reciprocal.\n\nHarvard evolutionary biologist Ernst Mayr (1961) suggested that there are two fundamentally different types of causation in biology, ‘ultimate’ and ‘proximate’. Ultimate causes (e.g. natural selection) were seen as (i) providing historical accounts for the existence of an organism’s features, and (ii) explaining the function or ‘goal-directedness’ of living beings. In contrast, proximate causes (e.g. physiology) were seen as explaining how biological systems work. According to Mayr, the evolutionary sciences study ultimate causes and the rest of biology studies proximate causes. In some of his works, Mayr considered these domains autonomous:\n“The clarification of the biochemical mechanism by which the genetic program is translated into the phenotype tells us absolutely nothing about the steps by which natural selection has built up the particular genetic program.”\nMayr, 1980\nThere has been widespread acceptance of the proximate-ultimate dichotomy within the evolutionary sciences. However, many biologists, psychologists and philosophers have taken issue with Mayr’s corollary that the proximate-ultimate distinction implies that development is irrelevant to evolution. For instance, evolutionary biologist Mary Jane West-Eberhard writes:\n“The proximate-ultimate distinction has given rise to a new confusion, namely, a belief that proximate causes of phenotypic variation have nothing to do with ultimate, evolutionary explanation.”\nWest-Eberhard, 2003\nMayr’s position implied a unidirectional or linear conception of causation for both development and evolution: genotypes cause phenotypes (proximate causation), whilst through natural selection, changes in environments cause changes in organisms (ultimate causation). Reciprocal causation was proposed as an alternative to this linear characterization. (see also ) It emphasizes how causation cycles through biological systems recursively, allowing proximate causes to feed back and thereby feature in ultimate explanations. \n\nReciprocal causation features in several explanations within contemporary evolutionary biology, including sexual selection theory, coevolution, habitat selection, and frequency-dependent selection. In these examples, the source of selection on a trait coevolves with the trait itself, therefore causation is reciprocal and developmental processes potentially become relevant to evolutionary accounts. For instance, a peacock’s tail evolves through mating preferences in peahens, and those preferences coevolve with the male trait. The ‘ultimate explanation’ for the male trait is the prior existence of female preferences, proximately manifest in differential peahen mate choice decisions, whilst the ‘ultimate explanation’ for the peahens’ mating preferences is the prior existence of variation in the peacock’s tail associated with fitness. This example illustrates how reciprocal causation is not a rejection of the proximate-ultimate distinction itself, but instead a rejection of the implication that developmental processes should not feature in evolutionary explanations.\n\nReciprocal causation also applies in other domains of evolutionary biology. The extended evolutionary synthesis emphasizes how developmental events, including both the causal effects of environments on organisms (for instance, arising through developmental plasticity, or epigenetic inheritance) and the causal effects of organisms on environments (e.g. niche construction), can direct the course of evolution. Developmental plasticity, niche construction, extra-genetic forms of inheritance and developmental bias are recognized as playing evolutionary roles that cannot be reduced to natural selection of genetically encoded characters or strategies. Proximate causes are not autonomous from natural selection, but rather feed back to influence the rate and direction of adaptive evolution. This goes beyond the recognition that ontogenetic processes can impose constraints on the action of selection, or that proximate and ultimate processes interact. Rather, developmental processes are also seen as a source of evolutionary novelty, initiators of evolutionary episodes, and co-directors of patterns of evolutionary change.\n\nAcceptance or rejection of Mayr’s proximate-ultimate distinction may lie at the centre of several major debates within contemporary biology, concerning evo devo (evolutionary developmental biology), niche construction, cultural evolution, human cooperation, and the evolution of language. According to some biologists and philosophers, these disputes share a common pattern. On one side are researchers who consider that interaction and feedback processes traditionally characterized as ‘proximate’ have explanatory value for ‘ultimate’ evolutionary questions. Their concern is that the proximate-ultimate distinction has discouraged consideration of the manner in which developmental processes can set the evolutionary agenda, for instance, by introducing innovations, channeling phenotypic variation, or initiating evolutionary episodes through modifying selection pressures. One the other side are researchers who largely adopt Mayr’s stance with a clean separation of proximate and ultimate causation. For the latter, a failure to respect Mayr’s dichotomy is considered a sign of confusing an evolutionary explanation with a mechanistic explanation.\n"}
{"id": "3090379", "url": "https://en.wikipedia.org/wiki?curid=3090379", "title": "Rubble", "text": "Rubble\n\nRubble is broken stone, of irregular size, shape and texture; undressed especially as a filling-in. Rubble naturally found in the soil is known also as 'brash' (compare cornbrash). Where present, it becomes more noticeable when the land is ploughed or worked.\n\n\"Rubble-work\" is a name applied to several types of masonry. One kind, where the stones are loosely thrown together in a wall between boards and grouted with mortar almost like concrete, is called in Italian \"muraglia di getto\" and in French \"bocage\". In Pakistan, walls made of rubble and concrete, cast in a formwork, are called 'situ', which probably derives from Sanskrit (similar to the Latin 'in situ' meaning 'made on the spot').\n\nWork executed with more or less large stones put together without any attempt at courses is called rubble walling. Where similar work is laid in courses, it is known as coursed rubble. Dry-stone walling is somewhat similar work done without the use of mortar. It is bound together by the fit of the stones and the regular placement of stones which extend through the thickness of the wall. A rubble wall built with mortar will be stronger if assembled in this way.\n\nRubble walls () are found all over the island of Malta. Similar walls are also frequently found in Sicily and the Arab countries. The various shapes and sizes of the stones used to build these walls look like stones that were found in the area lying on the ground or in the soil. It is most probable that the practice of building these walls around the field was inspired by the Arabs during their rule in Malta, as in Sicily who were also ruled by the Arabs around the same period. The Maltese farmer found that the technique of these walls was very useful especially during an era where resources were limited. Rubble walls are used to serve as borders between the property of one farm from the other. A great advantage that rubble walls offered is that when heavy rain falls, their structure would allow excessive water to pass through and therefore, excess water will not ruin the products. Soil erosion is minimised as the wall structure allows the water to pass through but it traps the soil and prevents it from being carried away from the field. One can see many rubble walls on the side of the hills and in valleys where the land slopes down and consequently the soil is in greater danger of being carried away.\n\n\n"}
{"id": "167742", "url": "https://en.wikipedia.org/wiki?curid=167742", "title": "Scientism", "text": "Scientism\n\nScientism is an ideology that promotes science as the purportedly objective means by which society should determine normative and epistemological values. The term \"scientism\" is generally used critically, pointing to the cosmetic application of science in unwarranted situations not amenable to application of the scientific method or similar scientific standards. \n\nIn philosophy of science, the term \"scientism\" frequently implies a critique of the more extreme expressions of logical positivism and has been used by social scientists such as Friedrich Hayek, philosophers of science such as Karl Popper, and philosophers such as Hilary Putnam and Tzvetan Todorov to describe (for example) the dogmatic endorsement of scientific methodology and the reduction of all knowledge to only that which is measured or confirmatory.\n\nMore generally, scientism is often interpreted as science applied \"in excess\". The term \"scientism\" has two senses:\n\n\nIt is also sometimes used to describe universal applicability of the scientific method and approach, and the view that empirical science constitutes the most authoritative worldview or the most valuable part of human learning—to the complete exclusion of other viewpoints, such as historical, philosophical, economic or cultural worldviews. It has been defined as \"the view that the characteristic inductive methods of the natural sciences are the only source of genuine factual knowledge and, in particular, that they alone can yield true knowledge about man and society\". The term \"scientism\" is also used by historians, philosophers, and cultural critics to highlight the possible dangers of lapses towards excessive reductionism in all fields of human knowledge.\n\nFor social theorists in the tradition of Max Weber, such as Jürgen Habermas and Max Horkheimer, the concept of scientism relates significantly to the philosophy of positivism, but also to the cultural rationalization for modern Western civilization. British novelist Sara Maitland has called scientism a \"myth as pernicious as any sort of fundamentalism.\"\n\nReviewing the references to scientism in the works of contemporary scholars, Gregory R. Peterson detects two main broad themes:\n\nThe term \"scientism\" was popularized by the Nobel Prize winner F.A. Hayek, who defined it as the \"slavish imitation of the method and language of Science\". Karl Popper defines scientism as \"the aping of what is widely mistaken for the method of science\".\n\nMikael Stenmark proposes the expression \"scientific expansionism\" as a synonym of scientism. In the \"Encyclopedia of science and religion\", he writes that, while the doctrines that are described as scientism have many possible forms and varying degrees of ambition, they share the idea that the boundaries of science (that is, typically the natural sciences) could and should be expanded so that something that has not been previously considered as a subject pertinent to science can now be understood as part of science (usually with science becoming the sole or the main arbiter regarding this area or dimension).\n\nAccording to Stenmark, the strongest form of scientism states that science has no boundaries and that all human problems and all aspects of human endeavor, with due time, will be dealt with and solved by science alone. This idea has also been called the Myth of Progress.\n\nE. F. Schumacher, in his \"A Guide for the Perplexed\", criticized scientism as an impoverished world view confined solely to what can be counted, measured and weighed. \"The architects of the modern worldview, notably Galileo and Descartes, assumed that those things that could be weighed, measured, and counted were more true than those that could not be quantified. If it couldn't be counted, in other words, it didn't count.\"\n\nIntellectual historian T.J. Jackson Lears argues there has been a recent reemergence of \"nineteenth-century positivist faith that a reified 'science' has discovered (or is about to discover) all the important truths about human life. Precise measurement and rigorous calculation, in this view, are the basis for finally settling enduring metaphysical and moral controversies.\" Lears specifically identifies Harvard psychologist Steven Pinker's work as falling in this category. Philosophers John N. Gray and Thomas Nagel have leveled similar criticisms against popular works by moral psychologist Jonathan Haidt, neuroscientist Sam Harris, and writer Malcolm Gladwell.\n\nGenetic biologist Austin L. Hughes wrote in conservative journal \"The New Atlantis\" that scientism has much in common with superstition: \"the stubborn insistence that something...has powers which no evidence supports.\"\n\nSeveral scholars use the term to describe the work of vocal critics of religion-as-such. Individuals associated with New Atheism have garnered this label from both religious and non-religious scholars. Theologian John Haught argues that Daniel Dennett and other new atheists subscribe to a belief system of scientific naturalism, which holds the central dogma that \"only nature, including humans and our creations, is real: that God does not exist; and that science alone can give us complete and reliable knowledge of reality.\" Haught argues that this belief system is self-refuting since it requires its adherents to assent to beliefs that violate its own stated requirements for knowledge. Christian Philosopher Peter Williams argues that it is only by conflating science with scientism that new atheists feel qualified to \"pontificate on metaphysical issues.\" Philosopher Daniel Dennett responded to religious criticism of his book \"\" by saying that accusations of scientism \"[are] an all-purpose, wild-card smear... When someone puts forward a scientific theory that [religious critics] really don't like, they just try to discredit it as 'scientism'. But when it comes to facts, and explanations of facts, science is the only game in town\".\n\nNon-religious scholars have also linked New Atheist thought with scientism. Atheist philosopher Thomas Nagel argues that neuroscientist Sam Harris conflates all empirical knowledge with that of scientific knowledge. Marxist literary critic Terry Eagleton argues that Christopher Hitchens possesses an \"old-fashioned scientistic notion of what counts as evidence\" that reduces knowledge to what can and cannot be proven by scientific procedure. Agnostic philosopher Anthony Kenny has also criticized New Atheist philosopher Alexander Rosenberg's \"The Atheist's Guide to Reality\" for resurrecting a self-refuting epistemology of logical positivism and reducing all knowledge of the universe to the discipline of physics.\n\nMichael Shermer, founder of The Skeptics Society, draws a parallel between scientism and traditional religious movements, pointing to the cult of personality that develops around some scientists in the public eye. He defines scientism as a worldview that encompasses natural explanations, eschews supernatural and paranormal speculations, and embraces empiricism and reason.\n\nThe Iranian scholar Seyyed Hossein Nasr has stated that in the Western world, many will accept the ideology of modern science, not as \"simple ordinary science\", but as a replacement for religion.\n\nGregory R. Peterson writes that \"for many theologians and philosophers, scientism is among the greatest of intellectual sins\".\n\nIn his essay \"Against Method\", Paul Feyerabend characterizes science as \"an essentially anarchic enterprise\" and argues emphatically that science merits no exclusive monopoly over \"dealing in knowledge\" and that scientists have never operated within a distinct and narrowly self-defined tradition. He depicts the process of contemporary scientific education as a mild form of indoctrination, aimed at \"making the history of science duller, simpler, more uniform, more 'objective' and more easily accessible to treatment by strict and unchanging rules.\"\n\nThomas M. Lessl argues that religious themes persist in what he calls scientism, the public rhetoric of science. There are two methodologies that illustrate this idea of scientism. One is the epistemological approach, the assumption that the scientific method trumps other ways of knowing and the ontological approach, that the rational mind reflects the world and both operate in knowable ways. According to Lessl, the ontological approach is an attempt to \"resolve the conflict between rationalism and skepticism\". Lessl also argues that without scientism, there would not be a scientific culture.\n\nPhilosopher of religion Keith Ward has said scientism is philosophically inconsistent or even self-refuting, as the truth of the statements \"no statements are true unless they can be proven scientifically (or logically)\" or \"no statements are true unless they can be shown empirically to be true\" cannot themselves be proven scientifically, logically, or empirically.\n\nIn the introduction to his collected oeuvre on the sociology of religion, Max Weber asks why \"the scientific, the artistic, the political, or the economic development [elsewhere]… did not enter upon that path of rationalization which is peculiar to the Occident?\" According to the distinguished German social theorist, Jürgen Habermas, \"For Weber, the intrinsic (that is, not merely contingent) relationship between modernity and what he called 'Occidental rationalism' was still self-evident.\" Weber described a process of rationalisation, disenchantment and the \"disintegration of religious world views\" that resulted in modern secular societies and capitalism.\nHabermas is critical of pure instrumental rationality, arguing that the \"Social Life–World\" is better suited to literary expression, the former being \"intersubjectively accessible experiences\" that can be generalized in a formal language, while the latter \"must generate an intersubjectivity of mutual understanding in each concrete case\":\n\n\n"}
{"id": "11301067", "url": "https://en.wikipedia.org/wiki?curid=11301067", "title": "Seven Wonders of Canada", "text": "Seven Wonders of Canada\n\nThe Seven Wonders of Canada was a 2007 competition sponsored by CBC Television's \"\" and CBC Radio One's \"Sounds Like Canada\". They sought to determine Canada's \"seven wonders\" by receiving nominations from viewers, and then from on-line voting of the short list. After the vote, a panel of judges, Ra McGuire, Roy MacGregor and Roberta L. Jamieson, picked the winners based on geographic and poetic criteria. Their seven picks were revealed on \"\" on June 7, 2007. The Seven Wonders as chosen by Canada were the Sleeping Giant, Niagara Falls, the Bay of Fundy, Nahanni National Park Reserve, the Northern Lights, the Rockies, and the Cabot Trail.\n\nFull voting results\n\n"}
{"id": "42752", "url": "https://en.wikipedia.org/wiki?curid=42752", "title": "Sonoluminescence", "text": "Sonoluminescence\n\nSonoluminescence is the emission of short bursts of light from imploding bubbles in a liquid when excited by sound.\n\nThe sonoluminescence effect was first discovered at the University of Cologne in 1934 as a result of work on sonar. H. Frenzel and H. Schultes put an ultrasound transducer in a tank of photographic developer fluid. They hoped to speed up the development process. Instead, they noticed tiny dots on the film after developing and realized that the bubbles in the fluid were emitting light with the ultrasound turned on. It was too difficult to analyze the effect in early experiments because of the complex environment of a large number of short-lived bubbles. (This experiment is also ascribed to N. Marinesco and J. J. Trillat in 1933, which also credits them with independent discovery). This phenomenon is now referred to as multi-bubble sonoluminescence (MBSL).\n\nIn 1960 Dr. Peter Jarman from Imperial College of London proposed the most reliable theory of SL phenomenon. The collapsing bubble generates an imploding shock wave that compresses and heats the gas at the center of the bubble to extremely high temperature.\n\nIn 1989 an experimental advance was introduced by D. Felipe Gaitan and Lawrence Crum, who produced stable single-bubble sonoluminescence (SBSL). In SBSL, a single bubble trapped in an acoustic standing wave emits a pulse of light with each compression of the bubble within the standing wave. This technique allowed a more systematic study of the phenomenon, because it isolated the complex effects into one stable, predictable bubble. It was realized that the temperature inside the bubble was hot enough to melt steel, as seen in an experiment done in 2012; the temperature inside the bubble as it collapsed reached about 12,000 kelvins. Interest in sonoluminescence was renewed when an inner temperature of such a bubble well above one million kelvins was postulated. This temperature is thus far not conclusively proven; rather, recent experiments conducted by the University of Illinois at Urbana–Champaign indicate temperatures around .\n\nSonoluminescence can occur when a sound wave of sufficient intensity induces a gaseous cavity within a liquid to collapse quickly. This cavity may take the form of a pre-existing bubble, or may be generated through a process known as cavitation. Sonoluminescence in the laboratory can be made to be stable, so that a single bubble will expand and collapse over and over again in a periodic fashion, emitting a burst of light each time it collapses. For this to occur, a standing acoustic wave is set up within a liquid, and the bubble will sit at a pressure anti-node of the standing wave. The frequencies of resonance depend on the shape and size of the container in which the bubble is contained.\n\nSome facts about sonoluminescence: \n\nSpectral measurements have given bubble temperatures in the range from to , the exact temperatures depending on experimental conditions including the composition of the liquid and gas. Detection of very high bubble temperatures by spectral methods is limited due to the opacity of liquids to short wavelength light characteristic of very high temperatures.\n\nWriting in \"Nature\", chemists David J. Flannigan and Kenneth S. Suslick describe a method of determining temperatures based on the formation of plasmas. Using argon bubbles in sulfuric acid, their data show the presence of ionized molecular oxygen O, sulfur monoxide, and atomic argon populating high-energy excited states, which confirms a hypothesis that the bubbles have a hot plasma core. The ionization and excitation energy of dioxygenyl cations, which they observed, is 18 electronvolts. From this they conclude the core temperatures reach at least 20,000 kelvins.\n\nThe dynamics of the motion of the bubble is characterized to a first approximation by the Rayleigh–Plesset equation (named after Lord Rayleigh and Milton Plesset):\n\nThis is an approximate equation that is derived from the Navier–Stokes equations (written in spherical coordinate system) and describes the motion of the radius of the bubble \"R\" as a function of time \"t\". Here, \"μ\" is the viscosity, \"p\" the pressure, and \"γ\" the surface tension. The over-dots represent time derivatives. This equation, though approximate, has been shown to give good estimates on the motion of the bubble under the acoustically driven field except during the final stages of collapse. Both simulation and experimental measurement show that during the critical final stages of collapse, the bubble wall velocity exceeds the speed of sound of the gas inside the bubble. Thus a more detailed analysis of the bubble's motion is needed beyond Rayleigh–Plesset to explore the additional energy focusing that an internally formed shock wave might produce.\n\nThe mechanism of the phenomenon of sonoluminescence is unknown. Hypotheses include: hotspot, bremsstrahlung radiation, collision-induced radiation and corona discharges, nonclassical light, proton tunneling, electrodynamic jets and fractoluminescent jets (now largely discredited due to contrary experimental evidence).\n\nIn 2002, M. Brenner, S. Hilgenfeldt, and D. Lohse published a 60-page review that contains a detailed explanation of the mechanism. An important factor is that the bubble contains mainly inert noble gas such as argon or xenon (air contains about 1% argon, and the amount dissolved in water is too great; for sonoluminescence to occur, the concentration must be reduced to 20–40% of its equilibrium value) and varying amounts of water vapor. Chemical reactions cause nitrogen and oxygen to be removed from the bubble after about one hundred expansion-collapse cycles. The bubble will then begin to emit light. The light emission of highly compressed noble gas is exploited technologically in the argon flash devices.\n\nDuring bubble collapse, the inertia of the surrounding water causes high pressure and high temperature, reaching around 10,000 kelvins in the interior of the bubble, causing the ionization of a small fraction of the noble gas present. The amount ionized is small enough for the bubble to remain transparent, allowing volume emission; surface emission would produce more intense light of longer duration, dependent on wavelength, contradicting experimental results. Electrons from ionized atoms interact mainly with neutral atoms, causing thermal bremsstrahlung radiation. As the wave hits a low energy trough, the pressure drops, allowing electrons to recombine with atoms and light emission to cease due to this lack of free electrons. This makes for a 160-picosecond light pulse for argon (even a small drop in temperature causes a large drop in ionization, due to the large ionization energy relative to photon energy). This description is simplified from the literature above, which details various steps of differing duration from 15 microseconds (expansion) to 100 picoseconds (emission).\n\nComputations based on the theory presented in the review produce radiation parameters (intensity and duration time versus wavelength) that match experimental results with errors no larger than expected due to some simplifications (e.g., assuming a uniform temperature in the entire bubble), so it seems the phenomenon of sonoluminescence is at least roughly explained, although some details of the process remain obscure.\n\nAny discussion of sonoluminescence must include a detailed analysis of metastability. Sonoluminescence in this respect is what is physically termed a bounded phenomenon meaning that the sonoluminescence exists in a bounded region of parameter space for the bubble; a coupled magnetic field being one such parameter. The magnetic aspects of sonoluminescence are very well documented.\n\nAn unusually exotic hypothesis of sonoluminescence, which has received much popular attention, is the Casimir energy hypothesis suggested by noted physicist Julian Schwinger and more thoroughly considered in a paper by Claudia Eberlein of the University of Sussex. Eberlein's paper suggests that the light in sonoluminescence is generated by the vacuum within the bubble in a process similar to Hawking radiation, the radiation generated at the event horizon of black holes. According to this vacuum energy explanation, since quantum theory holds that vacuum contains virtual particles, the rapidly moving interface between water and gas converts virtual photons into real photons. This is related to the Unruh effect or the Casimir effect. The argument has been made that sonoluminescence releases too large an amount of energy and releases the energy on too short a time scale to be consistent with the vacuum energy explanation, although other credible sources argue the vacuum energy explanation might yet prove to be correct.\n\nSome have argued that the Rayleigh–Plesset equation described above is unreliable for predicting bubble temperatures and that actual temperatures in sonoluminescing systems can be far higher than 20,000 kelvins. Some research claims to have measured temperatures as high as 100,000 kelvins, and speculates temperatures could reach into the millions of kelvins. Temperatures this high could cause thermonuclear fusion. This possibility is sometimes referred to as bubble fusion and is likened to the implosion design used in the fusion component of thermonuclear weapons.\n\nOn January 27, 2006, researchers at Rensselaer Polytechnic Institute claimed to have produced fusion in sonoluminescence experiments.\n\nExperiments in 2002 and 2005 by R. P. Taleyarkhan using deuterated acetone showed measurements of tritium and neutron output consistent with fusion. However, the papers were considered low quality and there were doubts cast by a report about the author's scientific misconduct. This made the report lose credibility among the scientific community.\n\nPistol shrimp (also called \"snapping shrimp\") produce a type of sonoluminescence from a collapsing bubble caused by quickly snapping its claw. The animal snaps a specialized claw shut to create a cavitation bubble that generates acoustic pressures of up to 80 kPa at a distance of 4 cm from the claw. As it extends out from the claw, the bubble reaches speeds of 60 miles per hour (97 km/h) and releases a sound reaching 218 decibels. The pressure is strong enough to kill small fish. The light produced is of lower intensity than the light produced by typical sonoluminescence and is not visible to the naked eye. The light and heat produced may have no direct significance, as it is the shockwave produced by the rapidly collapsing bubble which these shrimp use to stun or kill prey. However, it is the first known instance of an animal producing light by this effect and was whimsically dubbed \"shrimpoluminescence\" upon its discovery in 2001. It has subsequently been discovered that another group of crustaceans, the mantis shrimp, contains species whose club-like forelimbs can strike so quickly and with such force as to induce sonoluminescent cavitation bubbles upon impact.\n\n\n\n\nNewer research papers largely ruling out the vacuum energy explanation\n"}
{"id": "235576", "url": "https://en.wikipedia.org/wiki?curid=235576", "title": "Suction", "text": "Suction\n\nSuction is the flow of a fluid into a partial vacuum, or region of low pressure. The pressure gradient between this region and the ambient pressure will propel matter toward the low pressure area. Dust is sucked into a vacuum cleaner when it is pushed in by the higher pressure air on the outside of the cleaner.\n\nThis is similar to what happens when humans breathe or drink through a straw. Both breathing and using a straw involve contracting the diaphragm and muscles around the rib cage. The increased volume in the chest cavity or thoracic cavity decreases the pressure inside, creating an imbalance with the ambient air pressure, or atmospheric pressure. This imbalance results in air pushing into the lungs or liquid pushing up through a straw and into the mouth.\n\nPumps typically have an inlet where the fluid (or air) enters the pump and an outlet where the fluid/air comes out. The inlet location is said to be at the suction side of the pump. The outlet location is said to be at the discharge side of the pump. Operation of the pump creates suction (a lower pressure) at the suction side so that fluid/air can enter the pump through the inlet. Pump operation also causes higher pressure at the discharge side by forcing the fluid/air out at the outlet. There may be pressure sensing devices at the pump's suction and/or discharge sides which control the operation of the pump. For example, if the suction pressure of a centrifugal pump is too high, a device may trigger the fluid pump to shut off to keep it from running dry; i. e. with no fluid entering.\n\nUnder normal conditions of atmospheric pressure suction can draw pure water up to a maximum height of approximately 10.3 m (33.9 feet). \n\nIn medicine, suction devices are used to clear airways of materials that would like to impede breathing or cause infections, to aid in surgery, and for other purposes.\n\n\n"}
{"id": "8028338", "url": "https://en.wikipedia.org/wiki?curid=8028338", "title": "Systematic evolution of ligands by exponential enrichment", "text": "Systematic evolution of ligands by exponential enrichment\n\nSystematic evolution of ligands by exponential enrichment (SELEX), also referred to as \"in vitro selection\" or \"in vitro evolution\", is a combinatorial chemistry technique in molecular biology for producing oligonucleotides of either single-stranded DNA or RNA that specifically bind to a target ligand or ligands, which are commonly referred to as aptamers. \nAlthough SELEX has emerged as the most commonly used name for the procedure, some researchers have referred to it as SAAB (selected and amplified binding site) and CASTing (cyclic amplification and selection of targets) SELEX was first introduced in 1990. In 2015 a special issue was published in the Journal of Molecular Evolution in the honor of quarter century of the SELEX discovery.\n\nThe process begins with the synthesis of a very large oligonucleotide library consisting of randomly generated sequences of fixed length flanked by constant 5' and 3' ends that serve as primers. For a randomly generated region of length \"n\", the number of possible sequences in the library is 4 (\"n\" positions with four possibilities (A,T,C,G) at each position). The sequences in the library are exposed to the target ligand - which may be a protein or a small organic compound - and those that do not bind the target are removed, usually by affinity chromatography or target capture on paramagnetic beads. The bound sequences are eluted and amplified by PCR to prepare for subsequent rounds of selection in which the stringency of the elution conditions can be increased to identify the tightest-binding sequences. A caution to consider in this method is that the selection of extremely high, sub-nanomolar binding affinity entities may not in fact improve specificity for the target molecule. Off-target binding to related molecules could have significant clinical effects.\n\nSELEX has been used to develop a number of aptamers that bind targets interesting for both clinical and research purposes. Also towards these ends, a number of nucleotides with chemically modified sugars and bases have been incorporated into SELEX reactions. These modified nucleotides allow for the selection of aptamers with novel binding properties and potentially improved stability.\n\nThe first step of SELEX involves the synthesis of fully or partially randomized oligonucleotide sequences of some length flanked by defined regions which allow PCR amplification of those randomized regions and, in the case of RNA SELEX, in vitro transcription of the randomized sequence. While Ellington and Szostak demonstrated that chemical synthesis is capable of generating ~10 unique sequences for oligonucleotide libraries in their 1990 paper on in vitro selection, they found that amplification of these synthesized oligonucleotides led to significant loss of pool diversity due to PCR bias and defects in synthesized fragments. The oligonucleotide pool is amplified and a sufficient amount of the initial library is added to the reaction so that there are numerous copies of each individual sequence to minimize the loss of potential binding sequences due to stochastic events. Before the library is introduced to target for incubation and selective retention, the sequence library must be converted to single stranded oligonucleotides to achieve structural conformations with target binding properties.\n\nImmediately prior to target introduction, the single stranded oligonucleotide library is often heated and cooled slowly to renature oligonucleotides into thermodynamically stable secondary and tertiary structures. Once prepared, the randomized library is incubated with immobilized target to allow oligonucleotide-target binding. There are several considerations for this target incubation step, including the target immobilization method and strategies for subsequent unbound oligonucleotide separation, incubation time and temperature, incubation buffer conditions, and target versus oligonucleotide concentrations. Examples of target immobilization methods include affinity chromatography columns, nitrocellulose binding assay filters, and paramagnetic beads. Recently, SELEX reactions have been developed where the target is whole cells, which are expanded near complete confluence and incubated with the oligonucleotide library on culture plates. Incubation buffer conditions are altered based on the intended target and desired function of the selected aptamer. For example, in the case of negatively charged small molecules and proteins, high salt buffers are used for charge screening to allow nucleotides to approach the target and increase the chance of a specific binding event. Alternatively, if the desired aptamer function is in vivo protein or whole cell binding for potential therapeutic or diagnostic application, incubation buffer conditions similar to in vivo plasma salt concentrations and homeostatic temperatures are more likely to generate aptamers that can bind in vivo. Another consideration in incubation buffer conditions is non-specific competitors. If there is a high likelihood of non-specific oligonucleotide retention in the reaction conditions, non specific competitors, which are small molecules or polymers other than the SELEX library that have similar physical properties to the library oligonucleotides, can be used to occupy these non-specific binding sites. Varying the relative concentration of target and oligonucleotides can also affect properties of the selected aptamers. If a good binding affinity for the selected aptamer is not a concern, then an excess of target can be used to increase the probability that at least some sequences will bind during incubation and be retained. However, this provides no selective pressure for high binding affinity, which requires the oligonucleotide library to be in excess so that there is competition between unique sequences for available specific binding sites.\n\nOnce the oligonucleotide library has been incubated with target for sufficient time, unbound oligonucleotides are washed away from immobilized target, often using the incubation buffer so that specifically bound oligonucleotides are retained. With unbound sequences washed away, the specifically bound sequences are then eluted by creating denaturing conditions that promote oligonucleotide unfolding or loss of binding conformation including flowing in deionized water, using denaturing solutions containing urea and EDTA, or by applying high heat and physical force. Upon elution of bound sequences, the retained oligonucleotides are reverse-transcribed to DNA in the case of RNA or modified base selections, or simply collected for amplification in the case of DNA SELEX. These DNA templates from eluted sequences are then amplified via PCR and converted to single stranded DNA, RNA, or modified base oligonucleotides, which are used as the initial input for the next round of selection.\n\nOne of the most critical steps in the SELEX procedure is obtaining single stranded DNA (ssDNA) after the PCR amplification step. This will serve as input for the next cycle so it is of vital importance that all the DNA is single stranded and as little as possible is lost. Because of the relative simplicity, one of the most used methods is using biotinylated reverse primers in the amplification step, after which the complementary strands can be bound to a resin followed by elution of the other strand with lye. Another method is asymmetric PCR, where the amplification step is performed with an excess of forward primer and very little reverse primer, which leads to the production of more of the desired strand. A drawback of this method is that the product should be purified from double stranded DNA (dsDNA) and other left-over material from the PCR reaction. Enzymatic degradation of the unwanted strand can be performed by tagging this strand using a phosphate-probed primer, as it is recognized by enzymes such as Lambda exonuclease. These enzymes then selectively degrade the phosphate tagged strand leaving the complementary strand intact. All of these methods recover approximately 50 to 70% of the DNA. For a detailed comparison refer to the article by Svobodová et al. where these, and other, methods are experimentally compared. In classical SELEX, the process of randomized single stranded library generation, target incubation, and binding sequence elution and amplification described above are repeated until the vast majority of the retained pool consists of target binding sequences, though there are modifications and additions to the procedure that are often used, which are discussed below.\n\nIn order to increase the specificity of aptamers selected by a given SELEX procedure, a negative selection, or counter selection, step can be added prior to or immediately following target incubation. To eliminate sequences with affinity for target immobilization matrix components from the pool, negative selection can be used where the library is incubated with target immobilization matrix components and unbound sequences are retained. Negative selection can also be used to eliminate sequences that bind target-like molecules or cells by incubating the oligonucleotide library with small molecule target analogs, undesired cell types, or non-target proteins and retaining the unbound sequences.\n\nTo track the progress of a SELEX reaction, the number of target bound molecules, which is equivalent to the number of oligonucleotides eluted, can be compared to the estimated total input of oligonucleotides following elution at each round. The number of eluted oligonucleotides can be estimated through elution concentration estimations via 260 nm wavelength absorbance or fluorescent labeling of oligonucleotides. As the SELEX reaction approaches completion, the fraction of the oligonucleotide library that binds target approaches 100%, such that the number of eluted molecules approaches the total oligonucleotide input estimate, but may converge at a lower number.\n\nSome SELEX reactions can generate probes that are dependent on primer binding regions for secondary structure formation. There are aptamer applications for which a short sequence, and thus primer truncation, is desirable. An advancement on the original method allows an RNA library to omit the constant primer regions, which can be difficult to remove after the selection process because they stabilize secondary structures that are unstable when formed by the random region alone.\n\nRecently, SELEX has expanded to include the use of chemically modified nucleotides. These chemically modified oligonucleotides offer many potential advantages for selected aptamers including greater stability and nuclease resistance, enhanced binding for select targets, expanded physical properties - like increased hydrophobicity, and more diverse structural conformations.\n\nThe genetic alphabet, and thus possible aptamers, is also expanded using unnatural base pairs the use of these unnatural base pairs was applied to SELEX and high affinity DNA aptamers were generated.\n\nThe technique has been used to evolve aptamers of extremely high binding affinity to a variety of target ligands, including small molecules such as ATP and adenosine and proteins such as prions and vascular endothelial growth factor (VEGF). Moreover, SELEX has been used to select high-affinity aptamers for complex targets such as tumor cells. Clinical uses of the technique are suggested by aptamers that bind tumor markers, GFP-related fluorophores, and a VEGF-binding aptamer trade-named Macugen has been approved by the FDA for treatment of macular degeneration. Additionally, SELEX has been utilized to obtain highly specific catalytic DNA or DNAzymes. Several metal-specific DNAzymes have been reported including the GR-5 DNAzyme (lead-specific), the CA1-3 DNAzymes (copper-specific), the 39E DNAzyme (uranyl-specific) and the NaA43 DNAzyme (sodium-specific).\n\nThese developed aptamers have seen diverse application in therapies for macular degeneration and various research applications including biosensors, fluorescent labeling of proteins and cells, and selective enzyme inhibition.\n\n\n"}
{"id": "40624269", "url": "https://en.wikipedia.org/wiki?curid=40624269", "title": "Trabecular cartilage", "text": "Trabecular cartilage\n\nTrabecular cartilages (trabeculae cranii, sometimes simply trabeculae, prechordal cartilages) are paired, rod-shaped cartilages, which develop in the head of the vertebrate embryo. They are the primordia of the anterior part of the cranial base, and are derived from the cranial neural crest cells.\n\nThe trabecular cartilages generally appear as a paired, rod-shaped cartilages at the ventral side of the forebrain and lateral side of the adenohypophysis in the vertebrate embryo. During development, their anterior ends fuse and form the \"trabecula communis\". Their posterior ends fuse with the caudal-most parachordal cartilages.\n\nMost skeletons are of mesodermal origin in vertebrates. Especially axial skeletal elements, such as the vertebrae, are derived from the paraxial mesoderm (e.g., somites), which is regulated by molecular signals from the notochord. Trabecular cartilages, however, originate from the neural crest, and since they are located anterior to the rostral tip of the notochord, they cannot receive signals from the notochord. Due to these specialisations, and their essential role in cranial development, many comparative morphologists and embryologists have argued their developmental or evolutionary origins. The general theory is that the trabecular cartilage is derived from the neural crest mesenchyme which fills anterior to the mandibular arch (premandibular domain).\n\nAs clearly seen in the lamprey, Cyclostome also has a pair of cartilaginous rods in the embryonic head which is similar to the trabecular cartilages in jawed vertebrates.\nHowever, in 1916, Alexej Nikolajevich Sewertzoff pointed out that the cranial base of the lamprey is exclusively originated from the paraxial mesoderm. Then in 1948, reported the detail of the skeletogenesis of the lamprey, and showed that the “trabecular cartilages” in lamprey appear just beside the notochord, in a similar position to the parachordal cartilages in jawed vertebrates. Recent experimental studies also showed that the cartilages are derived from the head mesoderm. The “trabecular cartilages” in the Cyclostome is no longer considered to be the homologue of the trabecular in the jawed vertebrates: the (true) trabecular cartilages were firstly acquired in the Gnathostome lineage.\n\nThe trabecular cartilages were first described in the grass snake by at 1839. In 1874, Thomas Henry Huxley suggested that the trabecular cartilages are a modified part of the splanchnocranium: they arose as the serial homologues of the pharyngeal arches.\n\nThe vertebrate jaw is generally thought to be the modification of the mandibular arch (1st pharyngeal arch). Since the trabecular cartilages appear anterior to the mandibular arch, if the trabecular cartilages are serial homologues of the pharyngeal arches, ancestral vertebrates should possess more than one pharyngeal arch (so-called \"premandibular arches\") anterior to the mandibular arch. The existence of premandibular arch(es) has been accepted by many comparative embryologists and morphologists (e.g., Edwin Stephen Goodrich, Gavin de Beer). Moreover, reported premandibular arches and the corresponding branchiomeric nerves by the reconstruction of the Osteostracans (e.g., \"Cephalaspis\"; recently this arch was reinterpreted as the mandibular arch)\n\nHowever, the existence of the premandibular arch(es) has been rejected, and the trabecular cartilages are no longer assumed to be one of the pharyngeal arches.\n\n"}
{"id": "5547312", "url": "https://en.wikipedia.org/wiki?curid=5547312", "title": "Vacuum deposition", "text": "Vacuum deposition\n\nVacuum deposition is a family of processes used to deposit layers of material atom-by-atom or molecule-by-molecule on a solid surface. These processes operate at pressures well below atmospheric pressure (i.e., vacuum). The deposited layers can range from a thickness of one atom up to millimeters, forming freestanding structures. Multiple layers of different materials can be used, for example to form optical coatings. The process can be qualified based on the vapor source; physical vapor deposition uses a liquid or solid source and chemical vapor deposition uses a chemical vapor.\n\nThe vacuum environment may serve one or more purposes:\n\nCondensing particles can be generated in various ways:\n\nIn reactive deposition, the depositing material reacts either with a component of the gaseous environment (Ti + N → TiN) or with a co-depositing species (Ti + C → TiC). A plasma environment aids in activating gaseous species (N → 2N) and in decomposition of chemical vapor precursors (SiH → Si + 4H). The plasma may also be used to provide ions for vaporization by sputtering or for bombardment of the substrate for sputter cleaning and for bombardment of the depositing material to densify the structure and tailor properties (ion plating).\n\nWhen the vapor source is a liquid or solid the process is called physical vapor deposition (PVD). When the source is a chemical vapor precursor, the process is called chemical vapor deposition (CVD). The latter has several variants: \"low-pressure chemical vapor deposition\" (LPCVD), Plasma-enhanced chemical vapor deposition (PECVD), and \"plasma-assisted CVD\" (PACVD). Often a combination of PVD and CVD processes are used in the same or connected processing chambers.\n\n\nA thickness of less than one micrometre is generally called a thin film while a thickness greater than one micrometre is called a coating.\n\n\n"}
{"id": "73231", "url": "https://en.wikipedia.org/wiki?curid=73231", "title": "Weather forecasting", "text": "Weather forecasting\n\nWeather forecasting is the application of science and technology to predict the conditions of the atmosphere for a given location and time. People have attempted to predict the weather informally for millennia and formally since the 19th century. Weather forecasts are made by collecting quantitative data about the current state of the atmosphere at a given place and using meteorology to project how the atmosphere will change.\n\nOnce calculated by hand based mainly upon changes in barometric pressure, current weather conditions, and sky condition or cloud cover, weather forecasting now relies on computer-based models that take many atmospheric factors into account. Human input is still required to pick the best possible forecast model to base the forecast upon, which involves pattern recognition skills, teleconnections, knowledge of model performance, and knowledge of model biases. The inaccuracy of forecasting is due to the chaotic nature of the atmosphere, the massive computational power required to solve the equations that describe the atmosphere, the error involved in measuring the initial conditions, and an incomplete understanding of atmospheric processes. Hence, forecasts become less accurate as the difference between current time and the time for which the forecast is being made (the \"range\" of the forecast) increases. The use of ensembles and model consensus help narrow the error and pick the most likely outcome.\n\nThere are a variety of end uses to weather forecasts. Weather warnings are important forecasts because they are used to protect life and property. Forecasts based on temperature and precipitation are important to agriculture, and therefore to traders within commodity markets. Temperature forecasts are used by utility companies to estimate demand over coming days. On an everyday basis, people use weather forecasts to determine what to wear on a given day. Since outdoor activities are severely curtailed by heavy rain, snow and wind chill, forecasts can be used to plan activities around these events, and to plan ahead and survive them. In 2009, the US spent $5.1 billion on weather forecasting.\n\nFor millennia people have tried to forecast the weather. In 650 BC, the Babylonians predicted the weather from cloud patterns as well as astrology. In about 350 BC, Aristotle described weather patterns in \"Meteorologica\". Later, Theophrastus compiled a book on weather forecasting, called the \"Book of Signs\". Chinese weather prediction lore extends at least as far back as 300 BC, which was also around the same time ancient Indian astronomers developed weather-prediction methods. In New Testament times, Christ himself referred to deciphering and understanding local weather patterns, by saying, \"When evening comes, you say, 'It will be fair weather, for the sky is red', and in the morning, 'Today it will be stormy, for the sky is red and overcast.' You know how to interpret the appearance of the sky, but you cannot interpret the signs of the times.\"\n\nIn 904 AD, Ibn Wahshiyya's \"Nabatean Agriculture\", translated into Arabic from an earlier Aramaic work, discussed the weather forecasting of atmospheric changes and signs from the planetary astral alterations; signs of rain based on observation of the lunar phases; and weather forecasts based on the movement of winds.\n\nAncient weather forecasting methods usually relied on observed patterns of events, also termed pattern recognition. For example, it might be observed that if the sunset was particularly red, the following day often brought fair weather. This experience accumulated over the generations to produce weather lore. However, not all of these predictions prove reliable, and many of them have since been found not to stand up to rigorous statistical testing.\n\nIt was not until the invention of the electric telegraph in 1835 that the modern age of weather forecasting began. Before that, the fastest that distant weather reports could travel was around 100 miles per day (160 km/d), but was more typically 40–75 miles per day (60–120 km/day) (whether by land or by sea). By the late 1840s, the telegraph allowed reports of weather conditions from a wide area to be received almost instantaneously, allowing forecasts to be made from knowledge of weather conditions further upwind.\n\nThe two men credited with the birth of forecasting as a science were an officer of the Royal Navy Francis Beaufort and his protégé Robert FitzRoy. Both were influential men in British naval and governmental circles, and though ridiculed in the press at the time, their work gained scientific credence, was accepted by the Royal Navy, and formed the basis for all of today's weather forecasting knowledge.\n\nBeaufort developed the Wind Force Scale and Weather Notation coding, which he was to use in his journals for the remainder of his life. He also promoted the development of reliable tide tables around British shores, and with his friend William Whewell, expanded weather record-keeping at 200 British Coast guard stations.\n\nRobert FitzRoy was appointed in 1854 as chief of a new department within the Board of Trade to deal with the collection of weather data at sea as a service to mariners. This was the forerunner of the modern Meteorological Office. All ship captains were tasked with collating data on the weather and computing it, with the use of tested instruments that were loaned for this purpose.\nA storm in 1859 that caused the loss of the \"Royal Charter\" inspired FitzRoy to develop charts to allow predictions to be made, which he called \"forecasting the weather\", thus coining the term \"weather forecast\". Fifteen land stations were established to use the telegraph to transmit to him daily reports of weather at set times leading to the first gale warning service. His warning service for shipping was initiated in February 1861, with the use of telegraph communications. The first daily weather forecasts were published in \"The Times\" in 1861. In the following year a system was introduced of hoisting storm warning cones at the principal ports when a gale was expected. The \"Weather Book\" which FitzRoy published in 1863 was far in advance of the scientific opinion of the time.\n\nAs the electric telegraph network expanded, allowing for the more rapid dissemination of warnings, a national observational network was developed, which could then be used to provide synoptic analyses. Instruments to continuously record variations in meteorological parameters using photography were supplied to the observing stations from Kew Observatory – these cameras had been invented by Francis Ronalds in 1845 and his barograph had earlier been used by FitzRoy.\n\nTo convey accurate information, it soon became necessary to have a standard vocabulary describing clouds; this was achieved by means of a series of classifications first achieved by Luke Howard in 1802, and standardized in the \"International Cloud Atlas\" of 1896.\n\nIt was not until the 20th century that advances in the understanding of atmospheric physics led to the foundation of modern numerical weather prediction. In 1922, English scientist Lewis Fry Richardson published \"Weather Prediction By Numerical Process\", after finding notes and derivations he worked on as an ambulance driver in World War I. He described therein how small terms in the prognostic fluid dynamics equations governing atmospheric flow could be neglected, and a finite differencing scheme in time and space could be devised, to allow numerical prediction solutions to be found.\n\nRichardson envisioned a large auditorium of thousands of people performing the calculations and passing them to others. However, the sheer number of calculations required was too large to be completed without the use of computers, and the size of the grid and time steps led to unrealistic results in deepening systems. It was later found, through numerical analysis, that this was due to numerical instability. The first computerised weather forecast was performed by a team composed of American meteorologists Jule Charney, Philip Thompson, Larry Gates, and Norwegian meteorologist Ragnar Fjørtoft, applied mathematician John von Neumann, and ENIAC programmer Klara Dan von Neumann. Practical use of numerical weather prediction began in 1955, spurred by the development of programmable electronic computers.\n\nThe first ever daily weather forecasts were published in \"The Times\" on August 1, 1861, and the first weather maps were produced later in the same year. In 1911, the Met Office began issuing the first marine weather forecasts via radio transmission. These included gale and storm warnings for areas around Great Britain. In the United States, the first public radio forecasts were made in 1925 by Edward B. \"E.B.\" Rideout, on WEEI, the Edison Electric Illuminating station in Boston. Rideout came from the U.S. Weather Bureau, as did WBZ weather forecaster G. Harold Noyes in 1931.\n\nThe world's first televised weather forecasts, including the use of weather maps, were experimentally broadcast by the BBC in 1936. This was brought into practice in 1949 after World War II. George Cowling gave the first weather forecast while being televised in front of the map in 1954. In America, experimental television forecasts were made by James C Fidler in Cincinnati in either 1940 or 1947 on the DuMont Television Network. In the late 1970s and early 80s, John Coleman, the first weatherman on ABC-TV's Good Morning America, pioneered the use of on-screen weather satellite information and computer graphics for television forecasts. Coleman was a co-founder of The Weather Channel (TWC) in 1982. TWC is now a 24-hour cable network. Some weather channels have started broadcasting on live broadcasting programs such as YouTube and Periscope to reach more viewers.\n\nThe basic idea of numerical weather prediction is to sample the state of the fluid at a given time and use the equations of fluid dynamics and thermodynamics to estimate the state of the fluid at some time in the future. The main inputs from country-based weather services are surface observations from automated weather stations at ground level over land and from weather buoys at sea. The World Meteorological Organization acts to standardize the instrumentation, observing practices and timing of these observations worldwide. Stations either report hourly in METAR reports, or every six hours in SYNOP reports. Sites launch radiosondes, which rise through the depth of the troposphere and well into the stratosphere. Data from weather satellites are used in areas where traditional data sources are not available. Compared with similar data from radiosondes, the satellite data has the advantage of global coverage, however at a lower accuracy and resolution. Meteorological radar provide information on precipitation location and intensity, which can be used to estimate precipitation accumulations over time. Additionally, if a pulse Doppler weather radar is used then wind speed and direction can be determined.\nCommerce provides pilot reports along aircraft routes, and ship reports along shipping routes. Research flights using reconnaissance aircraft fly in and around weather systems of interest such as tropical cyclones. Reconnaissance aircraft are also flown over the open oceans during the cold season into systems that cause significant uncertainty in forecast guidance, or are expected to be of high impact 3–7 days into the future over the downstream continent.\n\nModels are \"initialized\" using this observed data. The irregularly spaced observations are processed by data assimilation and objective analysis methods, which perform quality control and obtain values at locations usable by the model's mathematical algorithms (usually an evenly spaced grid). The data are then used in the model as the starting point for a forecast. Commonly, the set of equations used to predict the known as the physics and dynamics of the atmosphere are called primitive equations. These equations are initialized from the analysis data and rates of change are determined. The rates of change predict the state of the atmosphere a short time into the future. The equations are then applied to this new atmospheric state to find new rates of change, and these new rates of change predict the atmosphere at a yet further time into the future. This \"time stepping\" procedure is continually repeated until the solution reaches the desired forecast time.\n\nThe length of the time step chosen within the model is related to the distance between the points on the computational grid, and is chosen to maintain numerical stability. Time steps for global models are on the order of tens of minutes, while time steps for regional models are between one and four minutes. The global models are run at varying times into the future. The Met Office's Unified Model is run six days into the future, the European Centre for Medium-Range Weather Forecasts model is run out to 10 days into the future, while the Global Forecast System model run by the Environmental Modeling Center is run 16 days into the future. The visual output produced by a model solution is known as a prognostic chart, or \"prog\". The raw output is often modified before being presented as the forecast. This can be in the form of statistical techniques to remove known biases in the model, or of adjustment to take into account consensus among other numerical weather forecasts. MOS or model output statistics is a technique used to interpret numerical model output and produce site-specific guidance. This guidance is presented in coded numerical form, and can be obtained for nearly all National Weather Service reporting stations in the United States. As proposed by Edward Lorenz in 1963, long range forecasts, those made at a range of two weeks or more, are impossible to definitively predict the state of the atmosphere, owing to the chaotic nature of the fluid dynamics equations involved. In numerical models, extremely small errors in initial values double roughly every five days for variables such as temperature and wind velocity.\n\nEssentially, a model is a computer program that produces meteorological information for future times at given locations and altitudes. Within any modern model is a set of equations, known as the primitive equations, used to predict the future state of the atmosphere. These equations—along with the ideal gas law—are used to evolve the density, pressure, and potential temperature scalar fields and the velocity vector field of the atmosphere through time. Additional transport equations for pollutants and other aerosols are included in some primitive-equation mesoscale models as well. The equations used are nonlinear partial differential equations, which are impossible to solve exactly through analytical methods, with the exception of a few idealized cases. Therefore, numerical methods obtain approximate solutions. Different models use different solution methods: some global models use spectral methods for the horizontal dimensions and finite difference methods for the vertical dimension, while regional models and other global models usually use finite-difference methods in all three dimensions.\n\nThe simplest method of forecasting the weather, persistence, relies upon today's conditions to forecast the conditions tomorrow. This can be a valid way of forecasting the weather when it is in a steady state, such as during the summer season in the tropics. This method of forecasting strongly depends upon the presence of a stagnant weather pattern. Therefore, when in a fluctuating weather pattern, this method of forecasting becomes inaccurate. It can be useful in both short range forecasts and long range forecasts.\n\nMeasurements of barometric pressure and the pressure tendency (the change of pressure over time) have been used in forecasting since the late 19th century. The larger the change in pressure, especially if more than , the larger the change in weather can be expected. If the pressure drop is rapid, a low pressure system is approaching, and there is a greater chance of rain. Rapid pressure rises are associated with improving weather conditions, such as clearing skies.\n\nAlong with pressure tendency, the condition of the sky is one of the more important parameters used to forecast weather in mountainous areas. Thickening of cloud cover or the invasion of a higher cloud deck is indicative of rain in the near future. High thin cirrostratus clouds can create halos around the sun or moon, which indicates an approach of a warm front and its associated rain. Morning fog portends fair conditions, as rainy conditions are preceded by wind or clouds that prevent fog formation. The approach of a line of thunderstorms could indicate the approach of a cold front. Cloud-free skies are indicative of fair weather for the near future. A bar can indicate a coming tropical cyclone. The use of sky cover in weather prediction has led to various weather lore over the centuries.\n\nThe forecasting of the weather within the next six hours is often referred to as nowcasting. In this time range it is possible to forecast smaller features such as individual showers and thunderstorms with reasonable accuracy, as well as other features too small to be resolved by a computer model. A human given the latest radar, satellite and observational data will be able to make a better analysis of the small scale features present and so will be able to make a more accurate forecast for the following few hours. However, there are now expert systems using those data and mesoscale numerical model to make better extrapolation, including evolution of those features in time.\n\nIn the past, the human forecaster was responsible for generating the entire weather forecast based upon available observations. Today, human input is generally confined to choosing a model based on various parameters, such as model biases and performance. Using a consensus of forecast models, as well as ensemble members of the various models, can help reduce forecast error. However, regardless how small the average error becomes with any individual system, large errors within any particular piece of guidance are still possible on any given model run. Humans are required to interpret the model data into weather forecasts that are understandable to the end user. Humans can use knowledge of local effects that may be too small in size to be resolved by the model to add information to the forecast. While increasing accuracy of forecast models implies that humans may no longer be needed in the forecast process at some point in the future, there is currently still a need for human intervention.\n\nThe analog technique is a complex way of making a forecast, requiring the forecaster to remember a previous weather event that is expected to be mimicked by an upcoming event. What makes it a difficult technique to use is that there is rarely a perfect analog for an event in the future. Some call this type of forecasting pattern recognition. It remains a useful method of observing rainfall over data voids such as oceans, as well as the forecasting of precipitation amounts and distribution in the future. A similar technique is used in medium range forecasting, which is known as teleconnections, when systems in other locations are used to help pin down the location of another system within the surrounding regime. An example of teleconnections are by using El Niño-Southern Oscillation (ENSO) related phenomena.\n\nMost end users of forecasts are members of the general public. Thunderstorms can create strong winds and dangerous lightning strikes that can lead to deaths, power outages, and widespread hail damage. Heavy snow or rain can bring transportation and commerce to a stand-still, as well as cause flooding in low-lying areas. Excessive heat or cold waves can sicken or kill those with inadequate utilities, and droughts can impact water usage and destroy vegetation.\n\nSeveral countries employ government agencies to provide forecasts and watches/warnings/advisories to the public in order to protect life and property and maintain commercial interests. Knowledge of what the end user needs from a weather forecast must be taken into account to present the information in a useful and understandable way. Examples include the National Oceanic and Atmospheric Administration's National Weather Service (NWS) and Environment Canada's Meteorological Service (MSC). Traditionally, newspaper, television, and radio have been the primary outlets for presenting weather forecast information to the public. In addition, some cities had weather beacons. Increasingly, the internet is being used due to the vast amount of specific information that can be found. In all cases, these outlets update their forecasts on a regular basis.\n\nA major part of modern weather forecasting is the severe weather alerts and advisories that the national weather services issue in the case that severe or hazardous weather is expected. This is done to protect life and property. Some of the most commonly known of severe weather advisories are the severe thunderstorm and tornado warning, as well as the severe thunderstorm and tornado watch. Other forms of these advisories include winter weather, high wind, flood, tropical cyclone, and fog. Severe weather advisories and alerts are broadcast through the media, including radio, using emergency systems as the Emergency Alert System, which break into regular programming.\n\nThe low temperature forecast for the current day is calculated using the lowest temperature found between 7pm that evening through 7am the following morning. So, in short, today's forecasted low is most likely tomorrow's low temperature.\n\nThere are a number of sectors with their own specific needs for weather forecasts and specialist services are provided to these users.\n\nBecause the aviation industry is especially sensitive to the weather, accurate weather forecasting is essential. Fog or exceptionally low ceilings can prevent many aircraft from landing and taking off. Turbulence and icing are also significant in-flight hazards. Thunderstorms are a problem for all aircraft because of severe turbulence due to their updrafts and outflow boundaries, icing due to the heavy precipitation, as well as large hail, strong winds, and lightning, all of which can cause severe damage to an aircraft in flight. Volcanic ash is also a significant problem for aviation, as aircraft can lose engine power within ash clouds. On a day-to-day basis airliners are routed to take advantage of the jet stream tailwind to improve fuel efficiency. Aircrews are briefed prior to takeoff on the conditions to expect en route and at their destination. Additionally, airports often change which runway is being used to take advantage of a headwind. This reduces the distance required for takeoff, and eliminates potential crosswinds.\n\nCommercial and recreational use of waterways can be limited significantly by wind direction and speed, wave periodicity and heights, tides, and precipitation. These factors can each influence the safety of marine transit. Consequently, a variety of codes have been established to efficiently transmit detailed marine weather forecasts to vessel pilots via radio, for example the MAFOR (marine forecast). Typical weather forecasts can be received at sea through the use of RTTY, Navtex and Radiofax.\n\nFarmers rely on weather forecasts to decide what work to do on any particular day. For example, drying hay is only feasible in dry weather. Prolonged periods of dryness can ruin cotton, wheat, and corn crops. While corn crops can be ruined by drought, their dried remains can be used as a cattle feed substitute in the form of silage. Frosts and freezes play havoc with crops both during the spring and fall. For example, peach trees in full bloom can have their potential peach crop decimated by a spring freeze. Orange groves can suffer significant damage during frosts and freezes, regardless of their timing.\n\nWeather forecasting of wind, precipitations and humidity is essential for preventing and controlling wildfires. Different indices, like the \"Forest fire weather index\" and the \"Haines Index\", have been developed to predict the areas more at risk to experience fire from natural or human causes. Conditions for the development of harmful insects can be predicted by forecasting the evolution of weather, too.\n\nElectricity and gas companies rely on weather forecasts to anticipate demand, which can be strongly affected by the weather. They use the quantity termed the degree day to determine how strong of a use there will be for heating (heating degree day) or cooling (cooling degree day). These quantities are based on a daily average temperature of . Cooler temperatures force heating degree days (one per degree Fahrenheit), while warmer temperatures force cooling degree days. In winter, severe cold weather can cause a surge in demand as people turn up their heating. Similarly, in summer a surge in demand can be linked with the increased use of air conditioning systems in hot weather. By anticipating a surge in demand, utility companies can purchase additional supplies of power or natural gas before the price increases, or in some circumstances, supplies are restricted through the use of brownouts and blackouts.\n\nIncreasingly, private companies pay for weather forecasts tailored to their needs so that they can increase their profits or avoid large losses. For example, supermarket chains may change the stocks on their shelves in anticipation of different consumer spending habits in different weather conditions. Weather forecasts can be used to invest in the commodity market, such as futures in oranges, corn, soybeans, and oil.\n\nRoyal Navy\n\nThe UK Royal Navy, working with the UK Met Office, has its own specialist branch of weather observers and forecasters, as part of the Hydrographic and Meteorological (HM) specialisation, who monitor and forecast operational conditions across the globe, to provide accurate and timely weather and oceanographic information to submarines, ships and Fleet Air Arm aircraft.\n\nA mobile unit in the RAF, working with the UK Met Office, forecasts the weather for regions in which British, allied servicemen and women are deployed. A group based at Camp Bastion provides forecasts for the British armed forces in Afghanistan.\n\nSimilar to the private sector, military weather forecasters present weather conditions to the war fighter community. Military weather forecasters provide pre-flight and in-flight weather briefs to pilots and provide real time resource protection services for military installations. Naval forecasters cover the waters and ship weather forecasts. The United States Navy provides a special service to both themselves and the rest of the federal government by issuing forecasts for tropical cyclones across the Pacific and Indian Oceans through their Joint Typhoon Warning Center.\n\nWithin the United States, Air Force Weather provides weather forecasting for the Air Force and the Army. Air Force forecasters cover air operations in both wartime and peacetime operations and provide Army support; United States Coast Guard marine science technicians provide ship forecasts for ice breakers and other various operations within their realm; and Marine forecasters provide support for ground- and air-based United States Marine Corps operations. All four military branches take their initial enlisted meteorology technical training at Keesler Air Force Base. Military and civilian forecasters actively cooperate in analyzing, creating and critiquing weather forecast products.\n\n\nThese are academic or governmental meteorology organizations. Most provide at least a limited forecast for their area of interest on their website.\n\n"}
{"id": "56106", "url": "https://en.wikipedia.org/wiki?curid=56106", "title": "Wildfire", "text": "Wildfire\n\nA wildfire or wildland fire is a fire in an area of combustible vegetation occurring in rural areas. Depending on the type of vegetation present, a wildfire can also be classified more specifically as a brush fire, bushfire, desert fire, forest fire, grass fire, hill fire, peat fire, vegetation fire, and veld fire.\n\nFossil charcoal indicates that wildfires began soon after the appearance of terrestrial plants 420 million years ago. Wildfire's occurrence throughout the history of terrestrial life invites conjecture that fire must have had pronounced evolutionary effects on most ecosystems' flora and fauna. Earth is an intrinsically flammable planet owing to its cover of carbon-rich vegetation, seasonally dry climates, atmospheric oxygen, and widespread lightning and volcanic ignitions.\n\nWildfires can be characterized in terms of the cause of ignition, their physical properties, the combustible material present, and the effect of weather on the fire. Wildfires can cause damage to property and human life, though naturally occurring wildfires may have beneficial effects on native vegetation, animals, and ecosystems that have evolved with fire. High-severity wildfire creates complex early seral forest habitat (also called \"snag forest habitat\"), which often has higher species richness and diversity than unburned old forest. Many plant species depend on the effects of fire for growth and reproduction. Wildfires in ecosystems where wildfire is uncommon or where non-native vegetation has encroached may have strongly negative ecological effects. Wildfire behavior and severity result from the combination of factors such as available fuels, physical setting, and weather. Analyses of historical meteorological data and national fire records in western North America show the primacy of climate in driving large regional fires via wet periods that create substantial fuels or drought and warming that extend conducive fire weather.\n\nStrategies for wildfire prevention, detection, and suppression have varied over the years. One common and inexpensive technique is controlled burning, intentionally igniting smaller fires to minimize the amount of flammable material available for a potential wildfire. Vegetation may be burned periodically to maintain high species diversity and limit the accumulation of plants and other debris that may serve as fuel. Wildland fire use is the cheapest and most ecologically appropriate policy for many forests. Fuels may also be removed by logging, but fuels treatments and thinning have no effect on severe fire behavior when under extreme weather conditions. Wildfire itself is reportedly \"the most effective treatment for reducing a fire's rate of spread, fireline intensity, flame length, and heat per unit of area\" according to Jan Van Wagtendonk, a biologist at the Yellowstone Field Station. Building codes in fire-prone areas typically require that structures be built of flame-resistant materials and a defensible space be maintained by clearing flammable materials within a prescribed distance from the structure.\n\nThree major natural causes of wildfire ignitions exist: \n\nThe most common direct human causes of wildfire ignition include arson, discarded cigarettes, power-line arcs (as detected by arc mapping), and sparks from equipment. Ignition of wildland fires via contact with hot rifle-bullet fragments is also possible under the right conditions. Wildfires can also be started in communities experiencing shifting cultivation, where land is cleared quickly and farmed until the soil loses fertility, and slash and burn clearing. Forested areas cleared by logging encourage the dominance of flammable grasses, and abandoned logging roads overgrown by vegetation may act as fire corridors. Annual grassland fires in southern Vietnam stem in part from the destruction of forested areas by US military herbicides, explosives, and mechanical land-clearing and -burning operations during the Vietnam War.\n\nThe most common cause of wildfires varies throughout the world. In Canada and northwest China, lightning operates as the major source of ignition. In other parts of the world, human involvement is a major contributor. In Africa, Central America, Fiji, Mexico, New Zealand, South America, and Southeast Asia, wildfires can be attributed to human activities such as agriculture, animal husbandry, and land-conversion burning. In China and in the Mediterranean Basin, human carelessness is a major cause of wildfires. In the United States and Australia, the source of wildfires can be traced both to lightning strikes and to human activities (such as machinery sparks, cast-away cigarette butts, or arson). Coal seam fires burn in the thousands around the world, such as those in Burning Mountain, New South Wales; Centralia, Pennsylvania; and several coal-sustained fires in China. They can also flare up unexpectedly and ignite nearby flammable material.\n\nThe spread of wildfires varies based on the flammable material present, its vertical arrangement and moisture content, and weather conditions. Fuel arrangement and density is governed in part by topography, as land shape determines factors such as available sunlight and water for plant growth. Overall, fire types can be generally characterized by their fuels as follows: \n\nWildfires occur when all of the necessary elements of a fire triangle come together in a susceptible area: an ignition source is brought into contact with a combustible material such as vegetation, that is subjected to sufficient heat and has an adequate supply of oxygen from the ambient air. A high moisture content usually prevents ignition and slows propagation, because higher temperatures are required to evaporate any water within the material and heat the material to its fire point. Dense forests usually provide more shade, resulting in lower ambient temperatures and greater humidity, and are therefore less susceptible to wildfires. Less dense material such as grasses and leaves are easier to ignite because they contain less water than denser material such as branches and trunks. Plants continuously lose water by evapotranspiration, but water loss is usually balanced by water absorbed from the soil, humidity, or rain. When this balance is not maintained, plants dry out and are therefore more flammable, often a consequence of droughts.\n\nA wildfire \"front\" is the portion sustaining continuous flaming combustion, where unburned material meets active flames, or the smoldering transition between unburned and burned material. As the front approaches, the fire heats both the surrounding air and woody material through convection and thermal radiation. First, wood is dried as water is vaporized at a temperature of . Next, the pyrolysis of wood at releases flammable gases. Finally, wood can smoulder at or, when heated sufficiently, ignite at . Even before the flames of a wildfire arrive at a particular location, heat transfer from the wildfire front warms the air to , which pre-heats and dries flammable materials, causing materials to ignite faster and allowing the fire to spread faster. High-temperature and long-duration surface wildfires may encourage flashover or \"torching\": the drying of tree canopies and their subsequent ignition from below.\n\nWildfires have a rapid \"forward rate of spread\" (FROS) when burning through dense, uninterrupted fuels. They can move as fast as in forests and in grasslands. Wildfires can advance tangential to the main front to form a \"flanking\" front, or burn in the opposite direction of the main front by \"backing\". They may also spread by \"jumping\" or \"spotting\" as winds and vertical convection columns carry \"firebrands\" (hot wood embers) and other burning materials through the air over roads, rivers, and other barriers that may otherwise act as firebreaks. Torching and fires in tree canopies encourage spotting, and dry ground fuels that surround a wildfire are especially vulnerable to ignition from firebrands. Spotting can create \"spot fires\" as hot embers and firebrands ignite fuels downwind from the fire. In Australian bushfires, spot fires are known to occur as far as from the fire front.\n\nEspecially large wildfires may affect air currents in their immediate vicinities by the stack effect: air rises as it is heated, and large wildfires create powerful updrafts that will draw in new, cooler air from surrounding areas in thermal columns. Great vertical differences in temperature and humidity encourage pyrocumulus clouds, strong winds, and fire whirls with the force of tornadoes at speeds of more than . Rapid rates of spread, prolific crowning or spotting, the presence of fire whirls, and strong convection columns signify extreme conditions.\n\nThe thermal heat from wildfire can cause significant weathering of rocks and boulders, heat can rapidly expand a boulder and thermal shock can occur, which may cause an object's structure to fail.\n\nHeat waves, droughts, cyclical climate changes such as El Niño, and regional weather patterns such as high-pressure ridges can increase the risk and alter the behavior of wildfires dramatically. Years of precipitation followed by warm periods can encourage more widespread fires and longer fire seasons. Since the mid-1980s, earlier snowmelt and associated warming has also been associated with an increase in length and severity of the wildfire season in the Western United States. Global warming may increase the intensity and frequency of droughts in many areas, creating more intense and frequent wildfires. A 2015 study indicates that the increase in fire risk in California may be attributable to human-induced climate change. A study of alluvial sediment deposits going back over 8,000 years found warmer climate periods experienced severe droughts and stand-replacing fires and concluded climate was such a powerful influence on wildfire that trying to recreate presettlement forest structure is likely impossible in a warmer future.\n\nIntensity also increases during daytime hours. Burn rates of smoldering logs are up to five times greater during the day due to lower humidity, increased temperatures, and increased wind speeds. Sunlight warms the ground during the day which creates air currents that travel uphill. At night the land cools, creating air currents that travel downhill. Wildfires are fanned by these winds and often follow the air currents over hills and through valleys. Fires in Europe occur frequently during the hours of 12:00 p.m. and 2:00 p.m. Wildfire suppression operations in the United States revolve around a 24-hour \"fire day\" that begins at 10:00 a.m. due to the predictable increase in intensity resulting from the daytime warmth.\n\nWildfire's occurrence throughout the history of terrestrial life invites conjecture that fire must have had pronounced evolutionary effects on most ecosystems' flora and fauna. Wildfires are common in climates that are sufficiently moist to allow the growth of vegetation but feature extended dry, hot periods. Such places include the vegetated areas of Australia and Southeast Asia, the veld in southern Africa, the fynbos in the Western Cape of South Africa, the forested areas of the United States and Canada, and the Mediterranean Basin.\n\nHigh-severity wildfire creates complex early seral forest habitat (also called “snag forest habitat”), which often has higher species richness and diversity than unburned old forest. Plant and animal species in most types of North American forests evolved with fire, and many of these species depend on wildfires, and particularly high-severity fires, to reproduce and grow. Fire helps to return nutrients from plant matter back to soil, the heat from fire is necessary to the germination of certain types of seeds, and the snags (dead trees) and early successional forests created by high-severity fire create habitat conditions that are beneficial to wildlife. Early successional forests created by high-severity fire support some of the highest levels of native biodiversity found in temperate conifer forests. Post-fire logging has no ecological benefits and many negative impacts; the same is often true for post-fire seeding.\n\nAlthough some ecosystems rely on naturally occurring fires to regulate growth, some ecosystems suffer from too much fire, such as the chaparral in southern California and lower-elevation deserts in the American Southwest. The increased fire frequency in these ordinarily fire-dependent areas has upset natural cycles, damaged native plant communities, and encouraged the growth of non-native weeds. Invasive species, such as \"Lygodium microphyllum\" and \"Bromus tectorum\", can grow rapidly in areas that were damaged by fires. Because they are highly flammable, they can increase the future risk of fire, creating a positive feedback loop that increases fire frequency and further alters native vegetation communities.\n\nIn the Amazon Rainforest, drought, logging, cattle ranching practices, and slash-and-burn agriculture damage fire-resistant forests and promote the growth of flammable brush, creating a cycle that encourages more burning. Fires in the rainforest threaten its collection of diverse species and produce large amounts of CO. Also, fires in the rainforest, along with drought and human involvement, could damage or destroy more than half of the Amazon rainforest by the year 2030. Wildfires generate ash, reduce the availability of organic nutrients, and cause an increase in water runoff, eroding away other nutrients and creating flash flood conditions. A 2003 wildfire in the North Yorkshire Moors burned off of heather and the underlying peat layers. Afterwards, wind erosion stripped the ash and the exposed soil, revealing archaeological remains dating back to 10,000 BC. Wildfires can also have an effect on climate change, increasing the amount of carbon released into the atmosphere and inhibiting vegetation growth, which affects overall carbon uptake by plants.\n\nIn tundra there is a natural pattern of accumulation of fuel and wildfire which varies depending on the nature of vegetation and terrain. Research in Alaska has shown fire-event return intervals, (FRIs) that typically vary from 150 to 200 years with dryer lowland areas burning more frequently than wetter upland areas.\n\nPlants in wildfire-prone ecosystems often survive through adaptations to their local fire regime. Such adaptations include physical protection against heat, increased growth after a fire event, and flammable materials that encourage fire and may eliminate competition. For example, plants of the genus \"Eucalyptus\" contain flammable oils that encourage fire and hard sclerophyll leaves to resist heat and drought, ensuring their dominance over less fire-tolerant species. Dense bark, shedding lower branches, and high water content in external structures may also protect trees from rising temperatures. Fire-resistant seeds and reserve shoots that sprout after a fire encourage species preservation, as embodied by pioneer species. Smoke, charred wood, and heat can stimulate the germination of seeds in a process called \"serotiny\". Exposure to smoke from burning plants promotes germination in other types of plants by inducing the production of the orange butenolide.\n\nGrasslands in Western Sabah, Malaysian pine forests, and Indonesian \"Casuarina\" forests are believed to have resulted from previous periods of fire. Chamise deadwood litter is low in water content and flammable, and the shrub quickly sprouts after a fire. Cape lilies lie dormant until flames brush away the covering and then blossom almost overnight. Sequoia rely on periodic fires to reduce competition, release seeds from their cones, and clear the soil and canopy for new growth. Caribbean Pine in Bahamian pineyards have adapted to and rely on low-intensity, surface fires for survival and growth. An optimum fire frequency for growth is every 3 to 10 years. Too frequent fires favor herbaceous plants, and infrequent fires favor species typical of Bahamian dry forests.\n\nMost of the Earth's weather and air pollution resides in the troposphere, the part of the atmosphere that extends from the surface of the planet to a height of about . The vertical lift of a severe thunderstorm or pyrocumulonimbus can be enhanced in the area of a large wildfire, which can propel smoke, soot, and other particulate matter as high as the lower stratosphere. Previously, prevailing scientific theory held that most particles in the stratosphere came from volcanoes, but smoke and other wildfire emissions have been detected from the lower stratosphere. Pyrocumulus clouds can reach over wildfires. Satellite observation of smoke plumes from wildfires revealed that the plumes could be traced intact for distances exceeding . Computer-aided models such as CALPUFF may help predict the size and direction of wildfire-generated smoke plumes by using atmospheric dispersion modeling.\n\nWildfires can affect local atmospheric pollution, and release carbon in the form of carbon dioxide. Wildfire emissions contain fine particulate matter which can cause cardiovascular and respiratory problems. Increased fire byproducts in the troposphere can increase ozone concentration beyond safe levels. Forest fires in Indonesia in 1997 were estimated to have released between 0.81 and 2.57 gigatonnes (0.89 and 2.83 billion short tons) of CO into the atmosphere, which is between 13%–40% of the annual global carbon dioxide emissions from burning fossil fuels. Atmospheric models suggest that these concentrations of sooty particles could increase absorption of incoming solar radiation during winter months by as much as 15%.\n\nIn the Welsh Borders, the first evidence of wildfire is rhyniophytoid plant fossils preserved as charcoal, dating to the Silurian period (about ). Smoldering surface fires started to occur sometime before the Early Devonian period . Low atmospheric oxygen during the Middle and Late Devonian was accompanied by a decrease in charcoal abundance. Additional charcoal evidence suggests that fires continued through the Carboniferous period. Later, the overall increase of atmospheric oxygen from 13% in the Late Devonian to 30-31% by the Late Permian was accompanied by a more widespread distribution of wildfires. Later, a decrease in wildfire-related charcoal deposits from the late Permian to the Triassic periods is explained by a decrease in oxygen levels.\n\nWildfires during the Paleozoic and Mesozoic periods followed patterns similar to fires that occur in modern times. Surface fires driven by dry seasons are evident in Devonian and Carboniferous progymnosperm forests. Lepidodendron forests dating to the Carboniferous period have charred peaks, evidence of crown fires. In Jurassic gymnosperm forests, there is evidence of high frequency, light surface fires. The increase of fire activity in the late Tertiary is possibly due to the increase of C-type grasses. As these grasses shifted to more mesic habitats, their high flammability increased fire frequency, promoting grasslands over woodlands. However, fire-prone habitats may have contributed to the prominence of trees such as those of the genera \"Eucalyptus\", \"Pinus\" and \"Sequoia\", which have thick bark to withstand fires and employ serotiny.\n\nThe human use of fire for agricultural and hunting purposes during the Paleolithic and Mesolithic ages altered the preexisting landscapes and fire regimes. Woodlands were gradually replaced by smaller vegetation that facilitated travel, hunting, seed-gathering and planting. In recorded human history, minor allusions to wildfires were mentioned in the Bible and by classical writers such as Homer. However, while ancient Hebrew, Greek, and Roman writers were aware of fires, they were not very interested in the uncultivated lands where wildfires occurred. Wildfires were used in battles throughout human history as early thermal weapons. From the Middle ages, accounts were written of occupational burning as well as customs and laws that governed the use of fire. In Germany, regular burning was documented in 1290 in the Odenwald and in 1344 in the Black Forest. In the 14th century Sardinia, firebreaks were used for wildfire protection. In Spain during the 1550s, sheep husbandry was discouraged in certain provinces by Philip II due to the harmful effects of fires used in transhumance. As early as the 17th century, Native Americans were observed using fire for many purposes including cultivation, signaling, and warfare. Scottish botanist David Douglas noted the native use of fire for tobacco cultivation, to encourage deer into smaller areas for hunting purposes, and to improve foraging for honey and grasshoppers. Charcoal found in sedimentary deposits off the Pacific coast of Central America suggests that more burning occurred in the 50 years before the Spanish colonization of the Americas than after the colonization. In the post-World War II Baltic region, socio-economic changes led more stringent air quality standards and bans on fires that eliminated traditional burning practices. In the mid-19th century, explorers from observed Australian Aborigines using fire for ground clearing, hunting, and regeneration of plant food in a method later named fire-stick farming. Such careful use of fire has been employed for centuries in the lands protected by Kakadu National Park to encourage biodiversity.\n\nWildfires typically occurred during periods of increased temperature and drought. An increase in fire-related debris flow in alluvial fans of northeastern Yellowstone National Park was linked to the period between AD 1050 and 1200, coinciding with the Medieval Warm Period. However, human influence caused an increase in fire frequency. Dendrochronological fire scar data and charcoal layer data in Finland suggests that, while many fires occurred during severe drought conditions, an increase in the number of fires during 850 BC and 1660 AD can be attributed to human influence. Charcoal evidence from the Americas suggested a general decrease in wildfires between 1 AD and 1750 compared to previous years. However, a period of increased fire frequency between 1750 and 1870 was suggested by charcoal data from North America and Asia, attributed to human population growth and influences such as land clearing practices. This period was followed by an overall decrease in burning in the 20th century, linked to the expansion of agriculture, increased livestock grazing, and fire prevention efforts. A meta-analysis found that 17 times more land burned annually in California before 1800 compared to recent decades (1,800,000 hectares/year compared to 102,000 hectares/year).\n\nAccording to a paper published in Science, the number of natural and human-caused fires decreased by 24.3% between 1998 and 2015. Researchers explain this a transition from nomadism to settled lifestyle and intensification of agriculture that lead to a drop in the use of fire for land clearing.\n\nIncreases of certain native tree species (i.e. conifers) in favor of others (i.e. leaf trees) also increases wildfire risk, especially if these trees are also planted in monocultures\n\nSome invasive species, moved in by humans (i.e., for the pulp and paper industry) have in some cases also increased the intensity of wildfires. Examples include species such as Eucalyptus in California and gamba grass in Australia.\n\nWildfire prevention refers to the preemptive methods aimed at reducing the risk of fires as well as lessening its severity and spread. Prevention techniques aim to manage air quality, maintain ecological balances, protect resources, and to affect future fires. North American firefighting policies permit naturally caused fires to burn to maintain their ecological role, so long as the risks of escape into high-value areas are mitigated. However, prevention policies must consider the role that humans play in wildfires, since, for example, 95% of forest fires in Europe are related to human involvement. Sources of human-caused fire may include arson, accidental ignition, or the uncontrolled use of fire in land-clearing and agriculture such as the slash-and-burn farming in Southeast Asia.\n\nIn 1937, U.S. President Franklin D. Roosevelt initiated a nationwide fire prevention campaign, highlighting the role of human carelessness in forest fires. Later posters of the program featured Uncle Sam, characters from the Disney movie \"Bambi\", and the official mascot of the U.S. Forest Service, Smokey Bear. Reducing human-caused ignitions may be the most effective means of reducing unwanted wildfire. Alteration of fuels is commonly undertaken when attempting to affect future fire risk and behavior. Wildfire prevention programs around the world may employ techniques such as \"wildland fire use\" and \"prescribed or controlled burns\". \"Wildland fire use\" refers to any fire of natural causes that is monitored but allowed to burn. \"Controlled burns\" are fires ignited by government agencies under less dangerous weather conditions.\n\nVegetation may be burned periodically to maintain high species diversity and frequent burning of surface fuels limits fuel accumulation. Wildland fire use is the cheapest and most ecologically appropriate policy for many forests. Fuels may also be removed by logging, but fuels treatments and thinning have no effect on severe fire behavior Wildfire models are often used to predict and compare the benefits of different fuel treatments on future wildfire spread, but their accuracy is low.\n\nWildfire itself is reportedly \"the most effective treatment for reducing a fire's rate of spread, fireline intensity, flame length, and heat per unit of area\" according to Jan van Wagtendonk, a biologist at the Yellowstone Field Station.\n\nBuilding codes in fire-prone areas typically require that structures be built of flame-resistant materials and a defensible space be maintained by clearing flammable materials within a prescribed distance from the structure. Communities in the Philippines also maintain fire lines wide between the forest and their village, and patrol these lines during summer months or seasons of dry weather. Continued residential development in fire-prone areas and rebuilding structures destroyed by fires has been met with criticism. The ecological benefits of fire are often overridden by the economic and safety benefits of protecting structures and human life.\n\nFast and effective detection is a key factor in wildfire fighting. Early detection efforts were focused on early response, accurate results in both daytime and nighttime, and the ability to prioritize fire danger. Fire lookout towers were used in the United States in the early 20th century and fires were reported using telephones, carrier pigeons, and heliographs. Aerial and land photography using instant cameras were used in the 1950s until infrared scanning was developed for fire detection in the 1960s. However, information analysis and delivery was often delayed by limitations in communication technology. Early satellite-derived fire analyses were hand-drawn on maps at a remote site and sent via overnight mail to the fire manager. During the Yellowstone fires of 1988, a data station was established in West Yellowstone, permitting the delivery of satellite-based fire information in approximately four hours.\n\nCurrently, public hotlines, fire lookouts in towers, and ground and aerial patrols can be used as a means of early detection of forest fires. However, accurate human observation may be limited by operator fatigue, time of day, time of year, and geographic location. Electronic systems have gained popularity in recent years as a possible resolution to human operator error. A government report on a recent trial of three automated camera fire detection systems in Australia did, however, conclude \"...detection by the camera systems was slower and less reliable than by a trained human observer\". These systems may be semi- or fully automated and employ systems based on the risk area and degree of human presence, as suggested by GIS data analyses. An integrated approach of multiple systems can be used to merge satellite data, aerial imagery, and personnel position via Global Positioning System (GPS) into a collective whole for near-realtime use by wireless Incident Command Centers.\n\nA small, high risk area that features thick vegetation, a strong human presence, or is close to a critical urban area can be monitored using a local sensor network. Detection systems may include wireless sensor networks that act as automated weather systems: detecting temperature, humidity, and smoke. These may be battery-powered, solar-powered, or \"tree-rechargeable\": able to recharge their battery systems using the small electrical currents in plant material. Larger, medium-risk areas can be monitored by scanning towers that incorporate fixed cameras and sensors to detect smoke or additional factors such as the infrared signature of carbon dioxide produced by fires. Additional capabilities such as night vision, brightness detection, and color change detection may also be incorporated into sensor arrays.\n\nSatellite and aerial monitoring through the use of planes, helicopter, or UAVs can provide a wider view and may be sufficient to monitor very large, low risk areas. These more sophisticated systems employ GPS and aircraft-mounted infrared or high-resolution visible cameras to identify and target wildfires. Satellite-mounted sensors such as Envisat's Advanced Along Track Scanning Radiometer and European Remote-Sensing Satellite's Along-Track Scanning Radiometer can measure infrared radiation emitted by fires, identifying hot spots greater than . The National Oceanic and Atmospheric Administration's Hazard Mapping System combines remote-sensing data from satellite sources such as Geostationary Operational Environmental Satellite (GOES), Moderate-Resolution Imaging Spectroradiometer (MODIS), and Advanced Very High Resolution Radiometer (AVHRR) for detection of fire and smoke plume locations. However, satellite detection is prone to offset errors, anywhere from for MODIS and AVHRR data and up to for GOES data. Satellites in geostationary orbits may become disabled, and satellites in polar orbits are often limited by their short window of observation time. Cloud cover and image resolution and may also limit the effectiveness of satellite imagery.\n\nin 2015 a new fire detection tool is in operation at the U.S. Department of Agriculture (USDA) Forest Service (USFS) which uses data from the Suomi National Polar-orbiting Partnership (NPP) satellite to detect smaller fires in more detail than previous space-based products. The high-resolution data is used with a computer model to predict how a fire will change direction based on weather and land conditions. The active fire detection product using data from Suomi NPP's Visible Infrared Imaging Radiometer Suite (VIIRS) increases the resolution of fire observations to 1,230 feet (375 meters). Previous NASA satellite data products available since the early 2000s observed fires at 3,280 foot (1 kilometer) resolution. The data is one of the intelligence tools used by the USFS and Department of Interior agencies across the United States to guide resource allocation and strategic fire management decisions. The enhanced VIIRS fire product enables detection every 12 hours or less of much smaller fires and provides more detail and consistent tracking of fire lines during long duration wildfires – capabilities critical for early warning systems and support of routine mapping of fire progression. Active fire locations are available to users within minutes from the satellite overpass through data processing facilities at the USFS Remote Sensing Applications Center, which uses technologies developed by the NASA Goddard Space Flight Center Direct Readout Laboratory in Greenbelt, Maryland. The model uses data on weather conditions and the land surrounding an active fire to predict 12–18 hours in advance whether a blaze will shift direction. The state of Colorado decided to incorporate the weather-fire model in its firefighting efforts beginning with the 2016 fire season.\n\nIn 2014, an international campaign was organized in South Africa's Kruger National Park to validate fire detection products including the new VIIRS active fire data. In advance of that campaign, the Meraka Institute of the Council for Scientific and Industrial Research in Pretoria, South Africa, an early adopter of the VIIRS 375m fire product, put it to use during several large wildfires in Kruger.\n\nThe demand for timely, high-quality fire information has increased in recent years. Wildfires in the United States burn an average of 7 million acres of land each year. For the last 10 years, the USFS and Department of Interior have spent a combined average of about $2–4 billion annually on wildfire suppression.\n\nWildfire suppression depends on the technologies available in the area in which the wildfire occurs. In less developed nations the techniques used can be as simple as throwing sand or beating the fire with sticks or palm fronds. In more advanced nations, the suppression methods vary due to increased technological capacity. Silver iodide can be used to encourage snow fall, while fire retardants and water can be dropped onto fires by unmanned aerial vehicles, planes, and helicopters. Complete fire suppression is no longer an expectation, but the majority of wildfires are often extinguished before they grow out of control. While more than 99% of the 10,000 new wildfires each year are contained, escaped wildfires under extreme weather conditions are difficult to suppress without a change in the weather. Wildfires in Canada and the US burn an average of per year.\n\nAbove all, fighting wildfires can become deadly. A wildfire's burning front may also change direction unexpectedly and jump across fire breaks. Intense heat and smoke can lead to disorientation and loss of appreciation of the direction of the fire, which can make fires particularly dangerous. For example, during the 1949 Mann Gulch fire in Montana, USA, thirteen smokejumpers died when they lost their communication links, became disoriented, and were overtaken by the fire. In the Australian February 2009 Victorian bushfires, at least 173 people died and over 2,029 homes and 3,500 structures were lost when they became engulfed by wildfire.\n\nIn California, the U.S. Forest Service spends about $200 million per year to suppress 98% of wildfires and up to $1 billion to suppress the other 2% of fires that escape initial attack and become large.\n\nWildland fire fighters face several life-threatening hazards including heat stress, fatigue, smoke and dust, as well as the risk of other injuries such as burns, cuts and scrapes, animal bites, and even rhabdomyolysis. Between 2000–2016, more than 350 wildland firefighters died on-duty.\n\nEspecially in hot weather condition, fires present the risk of heat stress, which can entail feeling heat, fatigue, weakness, vertigo, headache, or nausea. Heat stress can progress into heat strain, which entails physiological changes such as increased heart rate and core body temperature. This can lead to heat-related illnesses, such as heat rash, cramps, exhaustion or heat stroke. Various factors can contribute to the risks posed by heat stress, including strenuous work, personal risk factors such as age and fitness, dehydration, sleep deprivation, and burdensome personal protective equipment. Rest, cool water, and occasional breaks are crucial to mitigating the effects of heat stress.\n\nSmoke, ash, and debris can also pose serious respiratory hazards to wildland fire fighters. The smoke and dust from wildfires can contain gases such as carbon monoxide, sulfur dioxide and formaldehyde, as well as particulates such as ash and silica. To reduce smoke exposure, wildfire fighting crews should, whenever possible, rotate firefighters through areas of heavy smoke, avoid downwind firefighting, use equipment rather than people in holding areas, and minimize mop-up. Camps and command posts should also be located upwind of wildfires. Protective clothing and equipment can also help minimize exposure to smoke and ash.\n\nFirefighters are also at risk of cardiac events including strokes and heart attacks. Fire fighters should maintain good physical fitness. Fitness programs, medical screening and examination programs which include stress tests can minimize the risks of firefighting cardiac problems. Other injury hazards wildland fire fighters face include slips, trips and falls, burns, scrapes and cuts from tools and equipment, being struck by trees, vehicles, or other objects, plant hazards such as thorns and poison ivy, snake and animal bites, vehicle crashes, electrocution from power lines or lightning storms, and unstable building structures.\n\nFire retardants are used to slow wildfires by inhibiting combustion. They are aqueous solutions of ammonium phosphates and ammonium sulfates, as well as thickening agents. The decision to apply retardant depends on the magnitude, location and intensity of the wildfire. In certain instances, fire retardant may also be applied as a precautionary fire defense measure.\n\nTypical fire retardants contain the same agents as fertilizers. Fire retardant may also affect water quality through leaching, eutrophication, or misapplication. Fire retardant's effects on drinking water remain inconclusive. Dilution factors, including water body size, rainfall, and water flow rates lessen the concentration and potency of fire retardant. Wildfire debris (ash and sediment) clog rivers and reservoirs increasing the risk for floods and erosion that ultimately slow and/or damage water treatment systems. There is continued concern of fire retardant effects on land, water, wildlife habitats, and watershed quality, additional research is needed. However, on the positive side, fire retardant (specifically its nitrogen and phosphorus components) has been shown to have a fertilizing effect on nutrient-deprived soils and thus creates a temporary increase in vegetation.\n\nCurrent USDA procedure maintains that the aerial application of fire retardant in the United States must clear waterways by a minimum of 300 feet in order to safeguard effects of retardant runoff. Aerial uses of fire retardant are required to avoid application near waterways and endangered species (plant and animal habitats). After any incident of fire retardant misapplication, the U.S. Forest Service requires reporting and assessment impacts be made in order to determine mitigation, remediation, and/or restrictions on future retardant uses in that area.\n\nWildfire modeling is concerned with numerical simulation of wildfires in order to comprehend and predict fire behavior. Wildfire modeling aims to aid wildfire suppression, increase the safety of firefighters and the public, and minimize damage. Using computational science, wildfire modeling involves the statistical analysis of past fire events to predict spotting risks and front behavior. Various wildfire propagation models have been proposed in the past, including simple ellipses and egg- and fan-shaped models. Early attempts to determine wildfire behavior assumed terrain and vegetation uniformity. However, the exact behavior of a wildfire's front is dependent on a variety of factors, including windspeed and slope steepness. Modern growth models utilize a combination of past ellipsoidal descriptions and Huygens' Principle to simulate fire growth as a continuously expanding polygon. Extreme value theory may also be used to predict the size of large wildfires. However, large fires that exceed suppression capabilities are often regarded as statistical outliers in standard analyses, even though fire policies are more influenced by large wildfires than by small fires.\n\nWildfire risk is the chance that a wildfire will start in or reach a particular area and the potential loss of human values if it does. Risk is dependent on variable factors such as human activities, weather patterns, availability of wildfire fuels, and the availability or lack of resources to suppress a fire. Wildfires have continually been a threat to human populations. However, human induced geographical and climatic changes are exposing populations more frequently to wildfires and increasing wildfire risk. It is speculated that the increase in wildfires arises from a century of wildfire suppression coupled with the rapid expansion of human developments into fire-prone wildlands. Wildfires are naturally occurring events that aid in promoting forest health. Global warming and climate changes are causing an increase in temperatures and more droughts nationwide which contributes to an increase in wildfire risk.\n\nThe most noticeable adverse effect of wildfires is the destruction of property. However, the release of hazardous chemicals from the burning of wildland fuels also significantly impacts health in humans.\n\nWildfire smoke is composed primarily of carbon dioxide and water vapor. Other common smoke components present in lower concentrations are carbon monoxide, formaldehyde, acrolein, polyaromatic hydrocarbons, and benzene. Small particulates suspended in air which come in solid form or in liquid droplets are also present in smoke. 80 -90% of wildfire smoke, by mass, is within the fine particle size class of 2.5 micrometers in diameter or smaller.\n\nDespite carbon dioxide's high concentration in smoke, it poses a low health risk due to its low toxicity. Rather, carbon monoxide and fine particulate matter, particularly 2.5 µm in diameter and smaller, have been identified as the major health threats. Other chemicals are considered to be significant hazards but are found in concentrations that are too low to cause detectable health effects.\n\nThe degree of wildfire smoke exposure to an individual is dependent on the length, severity, duration, and proximity of the fire. People are exposed directly to smoke via the respiratory tract though inhalation of air pollutants. Indirectly, communities are exposed to wildfire debris that can contaminate soil and water supplies.\n\nThe U.S. Environmental Protection Agency (EPA) developed the Air Quality Index (AQI), a public resource that provides national air quality standard concentrations for common air pollutants. The public can use this index as a tool to determine their exposure to hazardous air pollutants based on visibility range.\n\nAfter a wildfire, hazards remain. Residents returning to their homes may be at risk from falling fire-weakened trees. Humans and pets may also be harmed by falling into ash pits.\n\nFirefighters are at the greatest risk for acute and chronic health effects resulting from wildfire smoke exposure. Due to firefighters' occupational duties, they are frequently exposed to hazardous chemicals at a close proximity for longer periods of time. A case study on the exposure of wildfire smoke among wildland firefighters shows that firefighters are exposed to significant levels of carbon monoxide and respiratory irritants above OSHA-permissible exposure limits (PEL) and ACGIH threshold limit values (TLV). 5–10% are overexposed. The study obtained exposure concentrations for one wildland firefighter over a 10-hour shift spent holding down a fireline. The firefighter was exposed to a wide range of carbon monoxide and respiratory irritant (combination of particulate matter 3.5 µm and smaller, acrolein, and formaldehype) levels. Carbon monoxide levels reached up to 160ppm and the TLV irritant index value reached a high of 10. In contrast, the OSHA PEL for carbon monoxide is 30ppm and for the TLV respiratory irritant index, the calculated threshold limit value is 1; any value above 1 exceeds exposure limits.\n\nBetween 2001 and 2012, over 200 fatalities occurred among wildland firefighters. In addition to heat and chemical hazards, firefighters are also at risk for electrocution from power lines; injuries from equipment; slips, trips, and falls; injuries from vehicle rollovers; heat-related illness; insect bites and stings; stress; and rhabdomyolysis.\n\nResidents in communities surrounding wildfires are exposed to lower concentrations of chemicals, but they are at a greater risk for indirect exposure through water or soil contamination. Exposure to residents is greatly dependent on individual susceptibility. Vulnerable persons such as children (ages 0–4), the elderly (ages 65 and older), smokers, and pregnant women are at an increased risk due to their already compromised body systems, even when the exposures are present at low chemical concentrations and for relatively short exposure periods.\n\nAdditionally, there is evidence of an increase in maternal stress, as documented by researchers M.H. O'Donnell and A.M. Behie, thus affecting birth outcomes. In Australia, studies show that male infants born with drastically higher average birth weights were born in mostly severely fire-affected areas. This is attributed to the fact that maternal signals directly affect fetal growth patterns.\n\nAsthma is one of the most common chronic disease among children in the United States affecting estimated 6.2 million children. A recent area of research on asthma risk focuses specifically on the risk of air pollution during the gestational period. Several pathophysiology processes are involved are in this. In human's considerable airway development occurs during the 2 and 3 trimester and continue until 3 years of age. It is hypothesized that exposure to these toxins during this period could have consequential effects as the epithelium of the lungs during this time could have increased permeability to toxins. Exposure to air pollution during parental and pre-natal stage could induce epigenetic changes which are responsible for the development of asthma. Recent Meta-Analyses have found significant association between PM, NO and development of asthma during childhood despite heterogeneity among studies. Furthermore, maternal exposure to chronic stressor, which are most like to be present in distressed communities, which is also a relevant co relate of childhood asthma which may further help explain the early childhood exposure to air pollution, neighborhood poverty and childhood risk. Living in distressed neighborhood is not only linked to pollutant source location and exposure but can also be associated with degree of magnitude of chronic individual stress which can in turn alter the allostatic load of the maternal immune system leading to adverse outcomes in children, including increased susceptibility to air pollution and other hazards.\n\nWildfire smoke contains particulate matter that may have adverse effects upon the human respiratory system. Evidence of the health effects of wildfire smoke should be relayed to the public so that exposure may be limited. Evidence of health effects can also be used to influence policy to promote positive health outcomes.\n\nInhalation of smoke from a wildfire can be a health hazard. Wildfire smoke is composed of combustion products i.e. carbon dioxide, carbon monoxide, water vapor, particulate matter, organic chemicals, nitrogen oxides and other compounds. The principal health concern is the inhalation of particulate matter and carbon monoxide.\n\nParticulate matter (PM) is a type of air pollution made up of particles of dust and liquid droplets. They are characterized into three categories based on the diameter of the particle: coarse PM, fine PM, and ultrafine PM. Coarse particles are between 2.5 micrometers and 10 micrometers, fine particles measure 0.1 to 2.5 micrometers, and ultrafine particle are less than 0.1 micrometer.  Each size can enter the body through inhalation, but the PM impact on the body varies by size. Coarse particles are filtered by the upper airways and these particles can accumulate and cause pulmonary inflammation. This can result in eye and sinus irritation as well as sore throat and coughing. Coarse PM is often composed of materials that are heavier and more toxic that lead to short-term effects with stronger impact.\n\nSmaller particulate moves further into the respiratory system creating issues deep into the lungs and the bloodstream. In asthma patients, PM causes inflammation but also increases oxidative stress in the epithelial cells. These particulates also cause apoptosis and autophagy in lung epithelial cells. Both processes cause the cells to be damaged and impacts the cell function. This damage impacts those with respiratory conditions such as asthma where the lung tissues and function are already compromised. The third PM type is ultra-fine PM (UFP). UFP can enter the bloodstream like PM however studies show that it works into the blood much quicker. The inflammation and epithelial damage done by UFP has also shown to be much more severe. PM is of the largest concern in regards to wildfire. This is particularly hazardous to the very young, elderly and those with chronic conditions such as asthma, chronic obstructive pulmonary disease (COPD), cystic fibrosis and cardiovascular conditions. The illnesses most commonly with exposure to fine particle from wildfire smoke are bronchitis, exacerbation of asthma or COPD, and pneumonia. Symptoms of these complications include wheezing and shortness of breath and cardiovascular symptoms include chest pain, rapid heart rate and fatigue.\n\nSmoke from wildfires can cause health problems, especially for children and those who already have respiratory problems. Several epidemiological studies have demonstrated a close association between air pollution and respiratory allergic diseases such as bronchial asthma. \n\nAn observational study of smoke exposure related to the 2007 San Diego wildfires revealed an increase both in healthcare utilization and respiratory diagnoses, especially asthma among the group sampled. Projected climate scenarios of wildfire occurrences predict significant increases in respiratory conditions among young children. Particulate Matter (PM) triggers a series of biological processes including inflammatory immune response, oxidative stress, which are associated with harmful changes in allergic respiratory diseases.\n\nAlthough some studies demonstrated no significant acute changes in lung function among people with asthma related to PM from wildfires, a possible explanation for these counterintuitive findings is the increased use of quick-relief medications, such as inhalers, in response to elevated levels of smoke among those already diagnosed with asthma. In investigating the association of medication use for obstructive lung disease and wildfire exposure, researchers found increases both in the usage of inhalers and initiation of long-term control as in oral steroids. More specifically, some people with asthma reported higher use of quick-relief medications (inhalers). After two major wildfires in California, researchers found an increase in physician prescriptions for quick-relief medications in the years following the wildfires than compared to the year before each occurrence. \n\nThere is consistent evidence between wildfire smoke and the exacerbation of asthma.\n\nCarbon monoxide (CO) is a colorless, odorless gas that can be found at the highest concentration at close proximity to a smoldering fire. For this reason, carbon monoxide inhalation is a serious threat to the health of wildfire firefighters. CO in smoke can be inhaled into the lungs where it is absorbed into the bloodstream and reduces oxygen delivery to the body's vital organs. At high concentrations, it can cause headache, weakness, dizziness, confusion, nausea, disorientation, visual impairment, coma and even death. However, even at lower concentrations, such as those found at wildfires, individuals with cardiovascular disease may experience chest pain and cardiac arrhythmia. A recent study tracking the number and cause of wildfire firefighter deaths from 1990–2006 found that 21.9% of the deaths occurred from heart attacks.\n\nAnother important and somewhat less obvious health effect of wildfires is psychiatric diseases and disorders. Both adults and children from countries ranging from the United States and Canada to Greece and Australia who were directly and indirectly affected by wildfires were found by researchers to demonstrate several different mental conditions linked to their experience with the wildfires. These include post-traumatic stress disorder (PTSD), depression, anxiety, and phobias.\n\nIn a new twist to wildfire health effects, former uranium mining sites were burned over in the summer of 2012 near North Fork, Idaho. This prompted concern from area residents and Idaho State Department of Environmental Quality officials over the potential spread of radiation in the resultant smoke, since those sites had never been completely cleaned up from radioactive remains.\n\nThe western US has seen an increase in both frequency and intensity of wildfires over the last several decades. This increase has been attributed to the arid climate of the western US and the effects of global warming. An estimated 46 million people were exposed to wildfire smoke from 2004 to 2009 in the Western United States. Evidence has demonstrated that wildfire smoke can increase levels of particulate matter in the atmosphere.\n\nThe EPA has defined acceptable concentrations of particulate matter in the air, through the National Ambient Air Quality Standards and monitoring of ambient air quality has been mandated. Due to these monitoring programs and the incidence of several large wildfires near populated areas, epidemiological studies have been conducted and demonstrate an association between human health effects and an increase in fine particulate matter due to wildfire smoke.\n\nThe EPA has defined acceptable concentrations of particulate matter in the air. The National Ambient Air Quality Standards are part of the Clean Air Act and provide mandated guidelines for pollutant levels and the monitoring of ambient air quality. In addition to these monitoring programs, the increased incidence of wildfires near populated areas have precipitated several epidemiological studies. Such studies have demonstrated an association between negative human health effects and an increase in fine particulate matter due to wildfire smoke. The size of the particulate matter is significant as smaller particulate matter (fine) is easily inhaled into the human respiratory tract. Often, small particulate matter can be inhaled into deep lung tissue causing respiratory distress, illness, or disease. \n\nAn increase in PM smoke emitted from the Hayman fire in Colorado in June 2002, was associated with an increase in respiratory symptoms in patients with COPD. Looking at the wildfires in Southern California in October 2003 in a similar manner, investigators have shown an increase in hospital admissions due to asthma symptoms while being exposed to peak concentrations of PM in smoke. Another epidemiological study found a 7.2% (95% confidence interval: 0.25%, 15%) increase in risk of respiratory related hospital admissions during smoke wave days with high wildfire-specific particulate matter 2.5 compared to matched non-smoke-wave days.\n\nChildren participating in the Children's Health Study were also found to have an increase in eye and respiratory symptoms, medication use and physician visits. Recently, it was demonstrated that mothers who were pregnant during the fires gave birth to babies with a slightly reduced average birth weight compared to those who were not exposed to wildfire during birth. Suggesting that pregnant women may also be at greater risk to adverse effects from wildfire. Worldwide it is estimated that 339,000 people die due to the effects of wildfire smoke each year.\n\nWhile the size of particulate matter is an important consideration for health effects, the chemical composition of particulate matter (PM) from wildfire smoke should also be considered. Antecedent studies have demonstrated that the chemical composition of PM from wildfire smoke can yield different estimates of human health outcomes as compared to other sources of smoke. Health outcomes for people exposed to wildfire smoke may differ from those exposed to smoke from alternative sources such as solid fuels. \n\n"}
