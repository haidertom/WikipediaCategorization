{"id": "30805061", "url": "https://en.wikipedia.org/wiki?curid=30805061", "title": "AHRC/ESRC Religion and Society Programme", "text": "AHRC/ESRC Religion and Society Programme\n\nThe AHRC/ESRC Religion and Society Programme is a five-year strategic research initiative funded by two UK research councils: the Arts and Humanities Research Council and the Economic and Social Research Council. running 2007-2012. It has funded 75 projects across UK universities investigating various aspects of the complex relationships between religion and society, both historical and contemporary.\n\nResearch supported includes the website \"British Religion in Numbers\", providing a one-stop shop for statistics on religion in Britain led by Professor David Voas at Manchester University, Professor Kim Knott at Leeds University's restudy of British media coverage of religion and spirituality, and an investigation of Hindu, Muslim and Sikh shrine practices across the Punjab led by Dr Tej Purewal, also at Manchester. Phase 2 of the Programme has focused on Youth and Religion specifically, supporting projects like Dr Basia Spalek at Birmingham University's research into police partnerships with young Muslims.\n\nThe Religion and Society Programme is hosted at Lancaster University, and directed by Professor Linda Woodhead helped by Dr Rebecca Catto (Research Associate) and Peta Ainsworth (Administrator). The Programme also helps to organize various relevant and topical events such as a day at the British Library asking ‘where next for religion in the public sphere’ in July 2010 and a closed seminar asking ‘Child abuse in the Catholic Church – what can be learned?’ at Heythrop College London in November 2010.\n\nSociologist have studied religion right from the beginning of the discipline, in the 19th century. It was a central concern of the first generation of sociological thinkers and those who influence them. Major influences in the history of the disciplines, such as Comte, Karl Marx, Durkheim, Weber, and Simmel all contributed in various ways to the sociology of religion. The French philosopher August Comte, coined the term sociology. Even so, religion as a replacement for existing religions as a new religion of positivism. Positivism was the view that only the methods of the natural sciences could provide knowledge of human nature and society (Kaelber, 2004). He thought positivism was valuable for its potential to solve social problems and to reorganize society. He developed a blueprint for a new social order which had a religion of humanity at its ethical basis. He did so to recognize the importance of religion in creating social bonds between people. However, his work has been largely overshadowed by the work of other scholars in sociology of religion, such as Max Weber and Durkheim's. Later work in the sociology of religion has often been developed with the work of these early scholars. Just to pick one example, one of the most famous works in the sociology of religion; Max Weber's \"the Protestant Ethic and the Spirit of Capitalism\". This was concerned with the question of the role played by religion. It's all to understand why a system of rational capitalism developing the west from about the 17th century onwards (Kaelber, 2004). Sociologist are interested in studying religion for three main reason. The first, is that religion is simply very important in the lives of many people around the world. Eighty five percent of the world's population affirmed some kind of religious belief with Christians, Muslims, and Hindus making up the three largest religious groups. Religious ideas help people to interpret their experiences. Religious values influence many people's actions and the organizations provide many people with fellowship, aid, and support. Sociologists just seeking to understand people's culture, can’t avoid than to understand the role that religion plays in their lives. Also, they must understand the consequences of religious beliefs and practices for the wider society, which might contain non-religious elements or a diversity. Second, revolves around the idea of how religion takes different forms. The difference in religious beliefs and practices between societies is something that is noticeable and needs explanation. Finally, the third reason for religious study in sociology is the fact that religion changes over time in response to different social conditions. The prime objective for this effect, is to understand why (Kaelber, 2002).\n\nReligion in America has had a relationship since the very beginning of the nation. This relationship has changed over time and has shaped the culture. It started with the Native Americans and their worship of the Great Spirits. They believed that their ancestors watched over them and that the spirits in nature blessed them. When colonist came to America, they brought Christianity along with them. They tried to integrate the religion into the lives of the Native people. Christianity branched out and separated into Protestant and Catholic (Trundle, 2012). There were many forms of Protestant like Methodist, Lutheran, and Quakers. There are other forms of Catholic as well, including the Roman Catholic and eastern Orthodox. Later other religions were introduced to America through immigration. Religions like Islam, Buddhism, and Hinduism, and many others. These began to influence the culture. America was known as the melting pot of different cultures and religions. It was seen as a safe haven for those seeking religious freedom. Religion has effected America in many ways over the years. It was the reason the colonist came to America, and was used as an excuse for dominating over the Native Americans (Trundle, 2012). It was also used as a foundation for the civil rights movement as Martin Luther King Jr. presented his speech through his own personal belief system. More recent was September 11 when the twin towers were destroyed through religious beliefs. The bombers claimed that it was for religious purposes on why they did what they did. In effect, all religions joined together in America to rebuild and grow stronger. Religion has brought people together in times of need, throughout history. It has also caused negative effects in society as explained with 9/11.\n\nFewer and fewer people believe nowadays. It's possible that in a generation, there simply won’t be religion across Europe and large sections of North America, Australia, and Asia. That's not necessarily a problem. However, it is worth thinking about why people made up religion in the first place, and what we’re doing with the needs and longings that led them to do so. At one level, religions are about asking us to believe in something. When people say, they can’t believe, they tend to stop right there with the whole religion business. They often then point out all the horrid things that religions have undoubtly done and continue to do. In this sense, belief is almost the least important and defiantly the least interesting side of religion. What's fascinating is all the other stuff religions get up to. For example, the way they regularly gather people around, and tell them to be nice to one another. Also, the way they create a sense of community, teaching others to see everyone else as human beings (Gruber, 2008). Religions use rituals to point stuff out to us and lodge it in our fickle minds. For example, that the seasons are changing or that it's the time to remember our ancestors. Religions know were not just intellectual creatures, so they carefully appeal to us via art and beauty. We think of beauty in one category, a frivolous and superficial thing, and truth and depth in another. Religions join them together. They build temples, cathedrals, and mosques that use beauty to lend depth to important ideas. They use the resources of art to remind us of what matters. Its directed at making us feel things, calm, pity, awe.\n\nWe may no longer believe, but the needs and longings that make up these stories go on. We long for beauty, wisdom, and purpose. We want to live for something more than just ourselves. Society tells us to direct our hopes in two areas. These are romantic love, and professional success, and it distracts us with news, movies, and consumption. It's not enough, as we know. We need reminders to be good, places to reawaken awe, and something to awaken our kinder, less selfish impulses. Universal things which need tending like delicate flowers and rituals that bring us together. The choice isn’t between religion and a secular world as it is now. The challenge is to learn from religions, so we can fill the secular world with replacements for the things we long ago made up religion to provide (Giovanni, 2003).\n\nGiovanni, G. D. (2003). Faith without religion, religion without faith: Kant and hegel on religion. \"Journal of the History of Philosophy, 41\"(3), 365.\n\nGruber, J., & Hungerman, D. M. (2008). The church versus the mall: What happens when religion faces increased secular competition? \"The Quarterly Journal of Economics, 123\"(2), 831.\n\nKaelber, L. (2004). Sociology of religion: A historical Introduction/Religion in society: A sociology of Religion/Religion: The social Context/Religion in sociological Perspective/Invitation to the sociology of religion. \"Teaching Sociology, 32\"(3), 329-332.\n\nKaelber, L. (2002). Sociology of religion: Theoretical and comparative perspectives / sociology of religion: Contemporary developments / A comparatice sociology of world religions: Virtuosos, priests, and popular religion. \"Teaching Sociology, 30\"(4), 496.\n\nTrundle, R. C. (2012). AMERICA'S RELIGION VERSUS RELIGION IN AMERICA: A PHILOSOPHIC PROFILE. \"Journal for the Study of Religions and Ideologies, 11\"(33), 3-20.\n"}
{"id": "1261305", "url": "https://en.wikipedia.org/wiki?curid=1261305", "title": "Act (drama)", "text": "Act (drama)\n\nAn act is a division or unit of a theatre work, including a play, film, opera, and musical theatre. The term can either refer to a conscious division placed within a work by a playwright (usually itself made up of multiple scenes) or a unit of analysis for dividing a dramatic work into sequences. The former use of the term may or may not align with the latter. The word \"act\" can also be used for major sections of other entertainment, such as variety shows, television programs, music hall performances, and cabaret.\n\nAn act is a part of a play defined by elements such as rising action, climax and resolution.:» A scene is a part of an act defined with the changing of characters.\n\nTo be more specific, the elements that create the plot of a play or any story, and divide a play into acts include the exposition, which give information, setting up the rest of the story. Another is the inciting incident, which starts all of the action that will follow. Going along with the inciting incident, the major dramatic question is formed; this holds the rest of the play. The majority of the play is made up of complications. These are the things that change the action. These complications lead up to the crisis, this is the turning point. Most of the time, at this point, the major dramatic question has been answered. Finally, there is the resolution. This is the end of the play where everything comes together and the situation has been resolved. This leaves the audience satisfied with the play as a whole. These more specific elements of plot in a play are the main things used to divide a play up into acts and sometimes scenes.\n\nRoman theatre was the first to divide plays into a number of acts separated by intervals. Acts may be further divided into scenes; in classical theater each regrouping between entrances and exits of actors is a scene, while later use describes a change of setting.\n\nThough there are no limits to the number of acts which might exist within a dramatic work, the most common formats are the three-act and five-act structures. Both of these are derived from different interpretations of Aristotle's \"Poetics\" in which he stresses the primacy of plot over character and \"an orderly arrangement of parts\".\n\nModern plays often have only one level of structure, which can be referred to as either scenes or acts at the whim of the writer; and some writers dispense with firm divisions entirely. Successive scenes are normally separated from each other in either time or place; but the division between acts is more to do with the overall dramatic structure of the piece. The end of an act often coincides with one or more characters making an important decision, else having an important decision to make. A decision which has a profound impact on the story being told.\n\nContemporary theatre, in line with screenwriting and novel forms, tends towards a three-act structure. Many operettas and most musicals are divided into just two acts, so in practice the intermission is seen as dividing them, and the word \"act\" comes to be used for the two halves of a show whether or not the script divides it into acts.\n\nA one-act play is a short drama that consists of only one act; the phrase is not used to describe a full-length play that does not utilize act-divisions. Unlike other plays which usually are published one play per book, one-act plays are often published in anthologies or collections.\n\nIn a three-act play, each act usually has a different tone to it. The most commonly used structure is the first act having a lot of introductory elements, the second act can usually be the darkest with the antagonists having a greater encompass, while the third act is the resolution and the protagonists prevailing. There is an age-old saying that \"the second act is the best\" because it was in between a starting and ending act and thus being able to delve deeper into more of the meat of the story since it does not need to have as prominent introductory or resolutive portions. This is not always so, since a third act or even a first act can have the common second act characteristics, but that type of structure is the most used.\n\nUntil the 18th century, most plays were divided into five acts. The work of William Shakespeare, for example, generally adheres to a five act structure . This format is known as the five-act play, and was famously analyzed by Gustav Freytag in \"Die Technik des Dramas\" (Dramatic techniques). The five acts played specific functions in the overall structure of the play; but in performance there was not necessarily any clear separation between them.\n\nA similar five-part structure is also used in traditional Japanese Noh drama, particularly by Zeami Motokiyo. Zeami, in his work \"Sandō\" (The Three Paths), originally described a five-part (five \"dan\") Noh play as the ideal form. It begins slowly and auspiciously in the first part (\"jo\"), building up the drama and tension in the second, third, and fourth parts (\"ha\"), with the greatest climax in the third \"dan\", and rapidly concluding with a return to peace and auspiciousness in the fifth \"dan\" (\"kyū\").\n\nAs part of a television program, each individual act can be separated by commercials.\nIn film, a number of scenes grouped together bring to an audiovisual work to life. The three act structure is commonly referred to in film adaptation of theatrical plays.\n\n"}
{"id": "60731", "url": "https://en.wikipedia.org/wiki?curid=60731", "title": "Anachronism", "text": "Anachronism\n\nAn anachronism (from the Greek , \"against\" and , \"time\") is a chronological inconsistency in some arrangement, especially a juxtaposition of persons, events, objects, or customs from different periods of time. The most common type of anachronism is an object misplaced in time, but it may be a verbal expression, a technology, a philosophical idea, a musical style, a material, a plant or animal, a custom, or anything else associated with a particular period in time that is placed outside its proper temporal domain.\n\nAn anachronism may be either intentional or unintentional. Intentional anachronisms may be introduced into a literary or artistic work to help a contemporary audience engage more readily with a historical period. Anachronism can also be used for purposes of rhetoric, comedy, or shock. Unintentional anachronisms may occur when a writer, artist, or performer is unaware of differences in technology, language, customs, attitudes, or fashions between different historical eras.\n\nA parachronism (from the Greek , \"on the side\", and , \"time\") is anything that appears in a time period in which it is not normally found (though not sufficiently out of place as to be impossible).\n\nThis may be an object, idiomatic expression, technology, philosophical idea, musical style, material, custom, or anything else so closely bound to a particular time period as to seem strange when encountered in a later era. They may be objects or ideas that were once common but are now considered rare or inappropriate. They can take the form of obsolete technology or outdated fashion.\n\nExamples of parachronisms could include a suburban housewife in the United States around 1960 using a washboard for laundry (well after washing machines had become the norm); or a teenager from that time period being an avid fan of ragtime music. Often, a parachronism is identified when a work based on a particular era's state of knowledge is read within the context of a later era—with a different state of knowledge. Many scientific works that rely on theories that have later been discredited have become anachronistic with the removal of those underpinnings, and works of speculative fiction often find their speculations outstripped by real-world technological developments or scientific discoveries.\n\nA prochronism (from the Greek , \"before\", and , \"time\") is an impossible anachronism which occurs when an object or idea has not yet been invented when the situation takes place, and therefore could not have possibly existed at the time (e.g. \"The Last Supper\" by Leonardo da Vinci at the top of the article). A prochronism may be an object not yet developed, a verbal expression that had not yet been coined, a philosophy not yet formulated, a breed of animal not yet evolved (or perhaps engineered), or use of a technology that had not yet been created.\n\nThe intentional use of older, often obsolete cultural artifacts may be regarded as anachronistic. For example, it could be considered anachronistic for a modern-day person to wear a top hat, write with a quill, or carry on a conversation in Latin. Such choices may reflect an eccentricity or an aesthetic preference.\nSome writings and works of art promoting a political, nationalist or revolutionary cause use anachronism to depict an institution or custom as being more ancient than it actually is. For example, the 19th-century Romanian painter Constantin Lecca depicts the peace agreement between Ioan Bogdan Voievod and Radu Voievod—two leaders in Romania's 16th-century history—with the flags of Moldavia (blue-red) and of Wallachia (yellow-blue) seen in the background. These flags date only from the 1830s. Here anachronism promotes legitimacy for the unification of Moldavia and Wallachia into the Kingdom of Romania at the time the painting was made.\n\nAnachronism is used especially in works of imagination that rest on a historical basis. Anachronisms may be introduced in many ways: for example, in the disregard of the different modes of life and thought that characterize different periods, or in ignorance of the progress of the arts and sciences and other facts of history. They vary from glaring inconsistencies to scarcely perceptible misrepresentation. It is only since the close of the 18th century that this kind of deviation from historical reality has jarred on a general audience. Sir Walter Scott justified the use of anachronism in historical literature: \"It is necessary, for exciting interest of any kind, that the subject assumed should be, as it were, translated into the manners as well as the language of the age we live in.\" However, as fashions move on, such attempts to use anachronisms to engage an audience may have quite the reverse effect, as the details in question are increasingly recognized as belonging neither to the historical era being represented, nor to the present, but to the intervening period in which the artwork was created. \"Nothing becomes obsolete like a period vision of an older period\", writes Anthony Grafton; \"Hearing a mother in a historical movie of the 1940s call out 'Ludwig! Ludwig van Beethoven! Come in and practice your piano now!' we are jerked from our suspension of disbelief by what was intended as a means of reinforcing it, and plunged directly into the American bourgeois world of the filmmaker.\"\n\nAnachronism can also be an aesthetic choice. Anachronisms abound in the works of Raphael and Shakespeare, as well as in those of less celebrated painters and playwrights of earlier times. Anachronisms can exist in ancient texts. Carol Meyers says that these anachronisms can be used to better understand the stories by asking what the anachronism represents. Repeated anachronisms and historical errors can become an accepted part of popular culture, such as Roman legionaries that wear leather armor.\nComedy fiction set in the past may use anachronism for humorous effect. Comedic anachronism can be used to make serious points about both historical and modern society, such as drawing parallels to political or social conventions.\n\nEven with careful research, science fiction writers risk anachronism as their works age because they cannot predict all political and technological change.\n\nFor example, many books and films nominally set in the mid-21st century or later refer to the Soviet Union, to Saint Petersburg in Russia as Leningrad, to the continuing struggle between the Eastern and Western Blocs and to divided Germany and divided Berlin. \"Star Trek\" has suffered from future anachronisms; instead of \"retconning\" these errors, the 2009 film retained them for consistency with older franchises. \n\nBuildings or natural features, such as the World Trade Center in New York City, can become out of place once they disappear.\n\nLanguage anachronisms in novels and films are quite common, both intentional and unintentional. Intentional anachronisms inform the audience more readily about a film set in the past. In this regard, language and pronunciation change so fast that most modern people (even many scholars) would find it difficult, or even impossible, to understand a film with dialogue in 15th-century English; thus, we willingly accept characters speaking an updated language, and modern slang and figures of speech are often used in these films.\n\nUnintentional anachronisms may occur even in what are intended as wholly objective and accurate records or representations of historic artifacts and artworks, because the recorder's perspective is conditioned by the assumptions and practices of his or her own times (a form of cultural bias). One example is the attribution of historically inaccurate beards to various medieval tomb effigies and figures in stained glass in records made by English antiquaries of the late 16th and early 17th centuries. Working in an age in which beards were in fashion and widespread, the antiquaries seem to have subconsciously projected the fashion back into an era in which it was rare.\n\nThe extensive science fiction subgenre depicting time travel in effect consists of deliberate, consciously created anachronisms, letting people of one time meet and interact with those of another time. Covers of time-travel books often depict deliberate anachronisms of this kind. For example, the cover of Harry Turtledove's \"The Guns of the South\" (1992) features a portrait of Confederate General Robert E. Lee holding an AK-47 rifle.\n\nIn historical writing, the most common type of anachronism is the adoption of the political, social or cultural concerns and assumptions of one era to interpret or evaluate the events and actions of another. The anachronistic application of present-day perspectives to comment on the historical past is sometimes described as presentism. Empiricist historians, working in the traditions established by Leopold von Ranke in the 19th century, regard this as a great error, and a trap to be avoided. Arthur Marwick has argued that \"a grasp of the fact that past societies are very different from our own, and ... very difficult to get to know\" is an essential and fundamental skill of the professional historian; and that \"anachronism is still one of the most obvious faults when the unqualified (those expert in other disciplines, perhaps) attempt to do history\". Anachronism in academic writing is considered at best embarrassing, as in early-20th-century scholarship's use of Translatio imperii, first formulated in the 12th century, to interpret 10th-century literature.\n\nThe use of anachronism in a rhetorical or hyperbolic sense is more complex. To refer to the Holy Roman Empire as the First Reich, for example, is technically inaccurate but may be a useful comparative exercise; the application of theory to works which predate Marxist, Feminist or Freudian subjectivities is considered an essential part of theoretical practice. In most cases, however, the practitioner will acknowledge or justify the use or context.\n\nThe ability to identify anachronisms may be employed as a critical and forensic tool to demonstrate the fraudulence of a document or artifact purporting to be from an earlier time. Anthony Grafton discusses, for example, the work of the 3rd-century philosopher Porphyry, of Isaac Casaubon (1559–1614), and of Richard Reitzenstein (1861–1931), all of whom succeeded in exposing literary forgeries and plagiarisms, such as those included in the \"Hermetic Corpus\", through – among other techniques – the recognition of anachronisms. The detection of anachronisms is an important element within the scholarly discipline of diplomatics, the critical analysis of the forms of documents, developed by the Maurist scholar Jean Mabillon (1632–1707) and his successors René-Prosper Tassin (1697–1777) and Charles-François Toustain (1700–1754). The philosopher and reformer Jeremy Bentham wrote at the beginning of the 19th century:\nThe falsehood of a writing will often be detected, by its making direct mention of, or allusions more or less indirect to, some fact posterior to the date which it bears. ... \"The mention of posterior facts;\" – first indication of forgery.<br>\n\nIn a living language there are always variations in words, in the meaning of words, in the construction of phrases, in the manner of spelling, which may detect the age of a writing, and lead to legitimate suspicions of forgery. ... \"The use of words not used till after the date of the writing;\" – second indication of forgery.\n\nThe exposure by Lorenzo Valla in 1440 of the so-called Donation of Constantine, a decree purportedly issued by the Emperor Constantine the Great in either 315 or 317 AD, as a later forgery, depended to a considerable degree on the identification of anachronisms, such as references to the city of Constantinople (a name not in fact bestowed until 330 AD). A large number of apparent anachronisms in the Book of Mormon have served to convince critics that the book was written in the 19th century, and not, as its adherents claim, in pre-Columbian America. The use of 19th- and 20th-century anti-semitic terminology demonstrates that the purported \"Franklin Prophecy\" (attributed to Benjamin Franklin, who died in 1790) is a forgery. The \"\"William Lynch speech\", an address, supposedly delivered in 1712, on the control of slaves in Virginia, is now considered to be a 20th-century forgery, partly on account of its use of anachronistic terms such as \"program\" and \"refueling\".\n\n\n"}
{"id": "18951655", "url": "https://en.wikipedia.org/wiki?curid=18951655", "title": "Archaeology", "text": "Archaeology\n\nArchaeology, or archeology, is the study of human activity through the recovery and analysis of material culture. The archaeological record consists of artifacts, architecture, biofacts or ecofacts and cultural landscapes. Archaeology can be considered both a social science and a branch of the humanities. In North America archaeology is a sub-field of anthropology, while in Europe it is often viewed as either a discipline in its own right or a sub-field of other disciplines.\n\nArchaeologists study human prehistory and history, from the development of the first stone tools at Lomekwi in East Africa 3.3 million years ago up until recent decades. Archaeology is distinct from palaeontology, the study of fossil remains. It is particularly important for learning about prehistoric societies, for whom there may be no written records to study. Prehistory includes over 99% of the human past, from the Paleolithic until the advent of literacy in societies across the world. Archaeology has various goals, which range from understanding culture history to reconstructing past lifeways to documenting and explaining changes in human societies through time.\n\nThe discipline involves surveying, excavation and eventually analysis of data collected to learn more about the past. In broad scope, archaeology relies on cross-disciplinary research. It draws upon anthropology, history, art history, classics, ethnology, geography, geology, literary history, linguistics, semiology, textual criticism, physics, information sciences, chemistry, statistics, paleoecology, paleography, paleontology, paleozoology, and paleobotany.\n\nArchaeology developed out of antiquarianism in Europe during the 19th century, and has since become a discipline practiced across the world. Archaeology has been used by nation-states to create particular visions of the past. Since its early development, various specific sub-disciplines of archaeology have developed, including maritime archaeology, feminist archaeology and archaeoastronomy, and numerous different scientific techniques have been developed to aid archaeological investigation. Nonetheless, today, archaeologists face many problems, such as dealing with pseudoarchaeology, the looting of artifacts, a lack of public interest, and opposition to the excavation of human remains.\n\nThe science of archaeology (from Greek , \"archaiologia\" from , \"arkhaios\", \"ancient\" and , \"-logia\", \"-logy\") grew out of the older multi-disciplinary study known as antiquarianism. Antiquarians studied history with particular attention to ancient artifacts and manuscripts, as well as historical sites. Antiquarianism focused on the empirical evidence that existed for the understanding of the past, encapsulated in the motto of the 18th-century antiquary, Sir Richard Colt Hoare, \"We speak from facts not theory\". Tentative steps towards the systematization of archaeology as a science took place during the Enlightenment era in Europe in the 17th and 18th centuries.\n\nIn Europe, philosophical interest in the remains of Greco-Roman civilization and the rediscovery of classical culture began in the late Middle Age. Flavio Biondo, an Italian Renaissance humanist historian, created a systematic guide to the ruins and topography of ancient Rome in the early 15th century, for which he has been called an early founder of archaeology. Antiquarians of the 16th century, including John Leland and William Camden, conducted surveys of the English countryside, drawing, describing and interpreting the monuments that they encountered.\n\nOne of the first sites to undergo archaeological excavation was Stonehenge and other megalithic monuments in England. John Aubrey (1626–1697) was a pioneer archaeologist who recorded numerous megalithic and other field monuments in southern England. He was also ahead of his time in the analysis of his findings. He attempted to chart the chronological stylistic evolution of handwriting, medieval architecture, costume, and shield-shapes.\n\nExcavations were also carried out in the ancient towns of Pompeii and Herculaneum, both of which had been covered by ash during the Eruption of Mount Vesuvius in AD 79. These excavations began in 1748 in Pompeii, while in Herculaneum they began in 1738. The discovery of entire towns, complete with utensils and even human shapes, as well the unearthing of frescos, had a big impact throughout Europe.\n\nHowever, prior to the development of modern techniques, excavations tended to be haphazard; the importance of concepts such as stratification and context were overlooked.\n\nThe father of archaeological excavation was William Cunnington (1754–1810). He undertook excavations in Wiltshire from around 1798, funded by Sir Richard Colt Hoare. Cunnington made meticulous recordings of Neolithic and Bronze Age barrows, and the terms he used to categorize and describe them are still used by archaeologists today.\n\nOne of the major achievements of 19th-century archaeology was the development of stratigraphy. The idea of overlapping strata tracing back to successive periods was borrowed from the new geological and paleontological work of scholars like William Smith, James Hutton and Charles Lyell. The application of stratigraphy to archaeology first took place with the excavations of prehistorical and Bronze Age sites. In the third and fourth decades of the 19th-century, archaeologists like Jacques Boucher de Perthes and Christian Jürgensen Thomsen began to put the artifacts they had found in chronological order.\n\nA major figure in the development of archaeology into a rigorous science was the army officer and ethnologist, Augustus Pitt Rivers, who began excavations on his land in England in the 1880s. His approach was highly methodical by the standards of the time, and he is widely regarded as the first scientific archaeologist. He arranged his artifacts by type or \"typologically, and within types by date or \"chronologically\". This style of arrangement, designed to highlight the evolutionary trends in human artifacts, was of enormous significance for the accurate dating of the objects. His most important methodological innovation was his insistence that \"all\" artifacts, not just beautiful or unique ones, be collected and catalogued.\n\nWilliam Flinders Petrie is another man who may legitimately be called the Father of Archaeology. His painstaking recording and study of artifacts, both in Egypt and later in Palestine, laid down many of the ideas behind modern archaeological recording; he remarked that \"I believe the true line of research lies in the noting and comparison of the smallest details.\" Petrie developed the system of dating layers based on pottery and ceramic findings, which revolutionized the chronological basis of Egyptology. Petrie was the first to scientifically investigate the Great Pyramid in Egypt during the 1880s. He was also responsible for mentoring and training a whole generation of Egyptologists, including Howard Carter who went on to achieve fame with the discovery of the tomb of 14th-century BC pharaoh Tutankhamun.\nThe first stratigraphic excavation to reach wide popularity with public was that of Hissarlik, on the site of ancient Troy, carried out by Heinrich Schliemann, Frank Calvert and Wilhelm Dörpfeld in the 1870s. These scholars individuated nine different cities that had overlapped with one another, from prehistory to the Hellenistic period. Meanwhile, the work of Sir Arthur Evans at Knossos in Crete revealed the ancient existence of an equally advanced Minoan civilization.\n\nThe next major figure in the development of archaeology was Sir Mortimer Wheeler, whose highly disciplined approach to excavation and systematic coverage in the 1920s and 1930s brought the science on swiftly. Wheeler developed the grid system of excavation, which was further improved by his student Kathleen Kenyon.\n\nArchaeology became a professional activity in the first half of the 20th century, and it became possible to study archaeology as a subject in universities and even schools. By the end of the 20th century nearly all professional archaeologists, at least in developed countries, were graduates. Further adaptation and innovation in archaeology continued in this period, when maritime archaeology and urban archaeology became more prevalent and rescue archaeology was developed as a result of increasing commercial development.\n\nThe purpose of archaeology is to learn more about past societies and the development of the human race. Over 99% of the development of humanity has occurred within prehistoric cultures, who did not make use of writing, thereby no written records exist for study purposes. Without such written sources, the only way to understand prehistoric societies is through archaeology. Because archaeology is the study of past human activity, it stretches back to about 2.5 million years ago when we find the first stone tools – The Oldowan Industry. Many important developments in human history occurred during prehistory, such as the evolution of humanity during the Paleolithic period, when the hominins developed from the australopithecines in Africa and eventually into modern \"Homo sapiens\". Archaeology also sheds light on many of humanity's technological advances, for instance the ability to use fire, the development of stone tools, the discovery of metallurgy, the beginnings of religion and the creation of agriculture. Without archaeology, we would know little or nothing about the use of material culture by humanity that pre-dates writing.\n\nHowever, it is not only prehistoric, pre-literate cultures that can be studied using archaeology but historic, literate cultures as well, through the sub-discipline of historical archaeology. For many literate cultures, such as Ancient Greece and Mesopotamia, their surviving records are often incomplete and biased to some extent. In many societies, literacy was restricted to the elite classes, such as the clergy or the bureaucracy of court or temple. The literacy even of aristocrats has sometimes been restricted to deeds and contracts. The interests and world-view of elites are often quite different from the lives and interests of the populace. Writings that were produced by people more representative of the general population were unlikely to find their way into libraries and be preserved there for posterity. Thus, written records tend to reflect the biases, assumptions, cultural values and possibly deceptions of a limited range of individuals, usually a small fraction of the larger population. Hence, written records cannot be trusted as a sole source. The material record may be closer to a fair representation of society, though it is subject to its own biases, such as sampling bias and differential preservation.\n\nOften, archaeology provides the only means to learn of the existence and behaviors of people of the past. Across the millennia many thousands of cultures and societies and billions of people have come and gone of which there is little or no written record or existing records are misrepresentative or incomplete. Writing as it is known today did not exist in human civilization until the 4th millennium BC, in a relatively small number of technologically advanced civilizations. In contrast, \"Homo sapiens\" has existed for at least 200,000 years, and other species of \"Homo\" for millions of years (see Human evolution). These civilizations are, not coincidentally, the best-known; they are open to the inquiry of historians for centuries, while the study of pre-historic cultures has arisen only recently. Even within a literate civilization many events and important human practices are not officially recorded. Any knowledge of the early years of human civilization – the development of agriculture, cult practices of folk religion, the rise of the first cities – must come from archaeology.\n\nIn addition to their scientific importance, archaeological remains sometimes have political or cultural significance to descendants of the people who produced them, monetary value to collectors, or simply strong aesthetic appeal. Many people identify archaeology with the recovery of such aesthetic, religious, political, or economic treasures rather than with the reconstruction of past societies.\n\nThis view is often espoused in works of popular fiction, such as Raiders of the Lost Ark, The Mummy, and King Solomon's Mines. When such unrealistic subjects are treated more seriously, accusations of pseudoscience are invariably levelled at their proponents \"(see Pseudoarchaeology)\". However, these endeavours, real and fictional, are not representative of modern archaeology.\n\nThere is no one approach to archaeological theory that has been adhered to by all archaeologists. When archaeology developed in the late 19th century, the first approach to archaeological theory to be practiced was that of cultural-history archaeology, which held the goal of explaining why cultures changed and adapted rather than just highlighting the fact that they did, therefore emphasizing historical particularism. In the early 20th century, many archaeologists who studied past societies with direct continuing links to existing ones (such as those of Native Americans, Siberians, Mesoamericans etc.) followed the direct historical approach, compared the continuity between the past and contemporary ethnic and cultural groups. In the 1960s, an archaeological movement largely led by American archaeologists like Lewis Binford and Kent Flannery arose that rebelled against the established cultural-history archaeology. They proposed a \"New Archaeology\", which would be more \"scientific\" and \"anthropological\", with hypothesis testing and the scientific method very important parts of what became known as processual archaeology.\n\nIn the 1980s, a new postmodern movement arose led by the British archaeologists Michael Shanks, Christopher Tilley, Daniel Miller, and Ian Hodder, which has become known as post-processual archaeology. It questioned processualism's appeals to scientific positivism and impartiality, and emphasized the importance of a more self-critical theoretical reflexivity. However, this approach has been criticized by processualists as lacking scientific rigor, and the validity of both processualism and post-processualism is still under debate. Meanwhile, another theory, known as historical processualism has emerged seeking to incorporate a focus on process and post-processual archaeology's emphasis of reflexivity and history.\n\nArchaeological theory now borrows from a wide range of influences, including neo-evolutionary thought,[35] phenomenology, postmodernism, agency theory, cognitive science, structural functionalism, gender-based and feminist archaeology, and systems theory.\n\nAn archaeological investigation usually involves several distinct phases, each of which employs its own variety of methods. Before any practical work can begin, however, a clear objective as to what the archaeologists are looking to achieve must be agreed upon. This done, a site is surveyed to find out as much as possible about it and the surrounding area. Second, an excavation may take place to uncover any archaeological features buried under the ground. And, third, the data collected from the excavation is studied and evaluated in an attempt to achieve the original research objectives of the archaeologists. It is then considered good practice for the information to be published so that it is available to other archaeologists and historians, although this is sometimes neglected.\n\nBefore actually starting to dig in a location, remote sensing can be used to look where sites are located within a large area or provide more information about sites or regions. There are two types of remote sensing instruments—passive and active. Passive instruments detect natural energy that is reflected or emitted from the observed scene. Passive instruments sense only radiation emitted by the object being viewed or reflected by the object from a source other than the instrument. Active instruments emit energy and record what is reflected. Satellite imagery is an example of passive remote sensing. Here are two active remote sensing instruments:\n\nLidar (Light Detection and Ranging)\nA lidar uses a laser (light amplification by stimulated emission of radiation) to transmit a light pulse and a receiver with sensitive detectors to measure the backscattered or reflected light. Distance to the object is determined by recording the time between the transmitted and backscattered pulses and using the speed of light to calculate the distance travelled. Lidars can determine atmospheric profiles of aerosols, clouds, and other constituents of the atmosphere.\n\nLaser altimeter\nA laser altimeter uses a lidar (see above) to measure the height of the instrument platform above the surface. By independently knowing the height of the platform with respect to the mean Earth's surface, the topography of the underlying surface can be determined.\nThe archaeological project then continues (or alternatively, begins) with a field survey. Regional survey is the attempt to systematically locate previously unknown sites in a region. Site survey is the attempt to systematically locate features of interest, such as houses and middens, within a site. Each of these two goals may be accomplished with largely the same methods.\n\nSurvey was not widely practiced in the early days of archaeology. Cultural historians and prior researchers were usually content with discovering the locations of monumental sites from the local populace, and excavating only the plainly visible features there. Gordon Willey pioneered the technique of regional settlement pattern survey in 1949 in the Viru Valley of coastal Peru, and survey of all levels became prominent with the rise of processual archaeology some years later.\n\nSurvey work has many benefits if performed as a preliminary exercise to, or even in place of, excavation. It requires relatively little time and expense, because it does not require processing large volumes of soil to search out artifacts. (Nevertheless, surveying a large region or site can be expensive, so archaeologists often employ sampling methods.) As with other forms of non-destructive archaeology, survey avoids ethical issues (of particular concern to descendant peoples) associated with destroying a site through excavation. It is the only way to gather some forms of information, such as settlement patterns and settlement structure. Survey data are commonly assembled into maps, which may show surface features and/or artifact distribution.\n\nThe simplest survey technique is surface survey. It involves combing an area, usually on foot but sometimes with the use of mechanized transport, to search for features or artifacts visible on the surface. Surface survey cannot detect sites or features that are completely buried under earth, or overgrown with vegetation. Surface survey may also include mini-excavation techniques such as augers, corers, and shovel test pits. If no materials are found, the area surveyed is deemed sterile.\n\nAerial survey is conducted using cameras attached to airplanes, balloons, UAVs, or even Kites. A bird's-eye view is useful for quick mapping of large or complex sites. Aerial photographs are used to document the status of the archaeological dig. Aerial imaging can also detect many things not visible from the surface. Plants growing above a buried man made structure, such as a stone wall, will develop more slowly, while those above other types of features (such as middens) may develop more rapidly. Photographs of ripening grain, which changes colour rapidly at maturation, have revealed buried structures with great precision. Aerial photographs taken at different times of day will help show the outlines of structures by changes in shadows. Aerial survey also employs ultraviolet, infrared, ground-penetrating radar wavelengths, LiDAR and thermography.\n\nGeophysical survey can be the most effective way to see beneath the ground. Magnetometers detect minute deviations in the Earth's magnetic field caused by iron artifacts, kilns, some types of stone structures, and even ditches and middens. Devices that measure the electrical resistivity of the soil are also widely used. Archaeological features whose electrical resistivity contrasts with that of surrounding soils can be detected and mapped. Some archaeological features (such as those composed of stone or brick) have higher resistivity than typical soils, while others (such as organic deposits or unfired clay) tend to have lower resistivity.\n\nAlthough some archaeologists consider the use of metal detectors to be tantamount to treasure hunting, others deem them an effective tool in archaeological surveying. Examples of formal archaeological use of metal detectors include musketball distribution analysis on English Civil War battlefields, metal distribution analysis prior to excavation of a 19th-century ship wreck, and service cable location during evaluation. Metal detectorists have also contributed to archaeology where they have made detailed records of their results and refrained from raising artifacts from their archaeological context. In the UK, metal detectorists have been solicited for involvement in the Portable Antiquities Scheme.\n\nRegional survey in underwater archaeology uses geophysical or remote sensing devices such as marine magnetometer, side-scan sonar, or sub-bottom sonar.\n\nArchaeological excavation existed even when the field was still the domain of amateurs, and it remains the source of the majority of data recovered in most field projects. It can reveal several types of information usually not accessible to survey, such as stratigraphy, three-dimensional structure, and verifiably primary context.\n\nModern excavation techniques require that the precise locations of objects and features, known as their provenance or provenience, be recorded. This always involves determining their horizontal locations, and sometimes vertical position as well (also see Primary Laws of Archaeology). Likewise, their association, or relationship with nearby objects and features, needs to be recorded for later analysis. This allows the archaeologist to deduce which artifacts and features were likely used together and which may be from different phases of activity. For example, excavation of a site reveals its stratigraphy; if a site was occupied by a succession of distinct cultures, artifacts from more recent cultures will lie above those from more ancient cultures.\n\nExcavation is the most expensive phase of archaeological research, in relative terms. Also, as a destructive process, it carries ethical concerns. As a result, very few sites are excavated in their entirety. Again the percentage of a site excavated depends greatly on the country and \"method statement\" issued. Sampling is even more important in excavation than in survey.Sometimes large mechanical equipment, such as backhoes (JCBs), is used in excavation, especially to remove the topsoil (overburden), though this method is increasingly used with great caution. Following this rather dramatic step, the exposed area is usually hand-cleaned with trowels or hoes to ensure that all features are apparent.\n\nThe next task is to form a site plan and then use it to help decide the method of excavation. Features dug into the natural subsoil are normally excavated in portions to produce a visible archaeological section for recording. A feature, for example a pit or a ditch, consists of two parts: the cut and the fill. The cut describes the edge of the feature, where the feature meets the natural soil. It is the feature's boundary. The fill is what the feature is filled with, and will often appear quite distinct from the natural soil. The cut and fill are given consecutive numbers for recording purposes. Scaled plans and sections of individual features are all drawn on site, black and white and colour photographs of them are taken, and recording sheets are filled in describing the context of each. All this information serves as a permanent record of the now-destroyed archaeology and is used in describing and interpreting the site.\n\nOnce artifacts and structures have been excavated, or collected from surface surveys, it is necessary to properly study them. This process is known as post-excavation analysis, and is usually the most time-consuming part of an archaeological investigation. It is not uncommon for final excavation reports for major sites to take years to be published.\n\nAt a basic level of analysis, artifacts found are cleaned, catalogued and compared to published collections. This comparison process often involves classifying them typologically and identifying other sites with similar artifact assemblages. However, a much more comprehensive range of analytical techniques are available through archaeological science, meaning that artifacts can be dated and their compositions examined. Bones, plants, and pollen collected from a site can all be analyzed using the methods of zooarchaeology, paleoethnobotany, and palynology, while any texts can usually be deciphered.\n\nThese techniques frequently provide information that would not otherwise be known, and therefore they contribute greatly to the understanding of a site.\n\nComputer graphics are now used to build virtual 3D models of sites, such as the throne room of an Assyrian palace or ancient Rome. Photogrammetry is also used as an analytical tool, and digital topographical models have been combined with astronomical calculations to verify whether or not certain structures (such as pillars) were aligned with astronomical events such as the sun's position at a solstice. Agent-based modeling and simulation can be used to better understand past social dynamics and outcomes. Data mining can be applied to large bodies of archaeological 'grey literature'.\n\nArchaeologists around the world use drones to speed up survey work and protect sites from squatters, builders and miners. In Peru, small drones helped researchers produce three-dimensional models of Peruvian sites instead of the usual flat maps – and in days and weeks instead of months and years.\n\nDrones costing as little as £650 have proven useful. In 2013, drones have flown over at least six Peruvian archaeological sites, including the colonial Andean town Machu Llacta above sea level. The drones continue to have altitude problems in the Andes, leading to plans to make a drone blimp, employing open source software.\n\nJeffrey Quilter, an archaeologist with Harvard University said, \"You can go up three metres and photograph a room, 300 metres and photograph a site, or you can go up 3,000 metres and photograph the entire valley.\"\n\nIn September 2014 drones weighing about were used for 3D mapping of the above-ground ruins of the Greek city of Aphrodisias. The data is being analysed by the Austrian Archaeological Institute in Vienna.\n\nAs with most academic disciplines, there are a very large number of archaeological sub-disciplines characterized by a specific method or type of material (e.g., lithic analysis, music, archaeobotany), geographical or chronological focus (e.g. Near Eastern archaeology, Islamic archaeology, Medieval archaeology), other thematic concern (e.g. maritime archaeology, landscape archaeology, battlefield archaeology), or a specific archaeological culture or civilization (e.g. Egyptology, Indology, Sinology).\n\nHistorical archaeology is the study of cultures with some form of writing.\n\nIn England, archaeologists have uncovered layouts of 14th century medieval villages, abandoned after crises such as the Black Death. In downtown New York City, archaeologists have exhumed the 18th century remains of the African Burial Ground.\n\nEthnoarchaeology is the ethnographic study of living people, designed to aid in our interpretation of the archaeological record. The approach first gained prominence during the processual movement of the 1960s, and continues to be a vibrant component of post-processual and other current archaeological approaches. Early ethnoarchaeological research focused on hunter-gatherer or foraging societies; today ethnoarchaeological research encompasses a much wider range of human behaviour.\n\nExperimental archaeology represents the application of the experimental method to develop more highly controlled observations of processes that create and impact the archaeological record. In the context of the logical positivism of processualism with its goals of improving the scientific rigor of archaeological epistemologies the experimental method gained importance. Experimental techniques remain a crucial component to improving the inferential frameworks for interpreting the archaeological record.\n\nArchaeometry aims to systematize archaeological measurement. It emphasizes the application of analytical techniques from physics, chemistry, and engineering. It is a field of research that frequently focuses on the definition of the chemical composition of archaeological remains for source analysis. Archaeometry also investigates different spatial characteristics of features, employing methods such as space syntax techniques and geodesy as well as computer-based tools such as geographic information system technology. Rare earth elements patterns may also be used. A relatively nascent subfield is that of archaeological materials, designed to enhance understanding of prehistoric and non-industrial culture through scientific analysis of the structure and properties of materials associated with human activity.\n\nArchaeology can be a subsidiary activity within Cultural resources management (CRM), also called heritage management in the United Kingdom. CRM archaeologists frequently examine archaeological sites that are threatened by development. Today, CRM accounts for most of the archaeological research done in the United States and much of that in western Europe as well. In the US, CRM archaeology has been a growing concern since the passage of the National Historic Preservation Act (NHPA) of 1966, and most taxpayers, scholars, and politicians believe that CRM has helped preserve much of that nation's history and prehistory that would have otherwise been lost in the expansion of cities, dams, and highways. Along with other statutes, the NHPA mandates that projects on federal land or involving federal funds or permits consider the effects of the project on each archaeological site.\n\nThe application of CRM in the United Kingdom is not limited to government-funded projects. Since 1990, PPG 16 has required planners to consider archaeology as a material consideration in determining applications for new development. As a result, numerous archaeological organizations undertake mitigation work in advance of (or during) construction work in archaeologically sensitive areas, at the developer's expense.\n\nIn England, ultimate responsibility of care for the historic environment rests with the Department for Culture, Media and Sport in association with English Heritage. In Scotland, Wales and Northern Ireland, the same responsibilities lie with Historic Scotland, Cadw and the Northern Ireland Environment Agency respectively.\n\nIn France, the Institut national du patrimoine (The National Institute of Cultural Heritage) trains curators specialized in archaeology. Their mission is to enhance the objects discovered. The curator is the link between scientific knowledge, administrative regulations, heritage objects and the public.\n\nAmong the goals of CRM are the identification, preservation, and maintenance of cultural sites on public and private lands, and the removal of culturally valuable materials from areas where they would otherwise be destroyed by human activity, such as proposed construction. This study involves at least a cursory examination to determine whether or not any significant archaeological sites are present in the area affected by the proposed construction. If these do exist, time and money must be allotted for their excavation. If initial survey and/or test excavations indicate the presence of an extraordinarily valuable site, the construction may be prohibited entirely.\n\nCultural resources management has, however, been criticized. CRM is conducted by private companies that bid for projects by submitting proposals outlining the work to be done and an expected budget. It is not unheard-of for the agency responsible for the construction to simply choose the proposal that asks for the least funding. CRM archaeologists face considerable time pressure, often being forced to complete their work in a fraction of the time that might be allotted for a purely scholarly endeavour. Compounding the time pressure is the vetting process of site reports that are required (in the US) to be submitted by CRM firms to the appropriate State Historic Preservation Office (SHPO). From the SHPO's perspective there is to be no difference between a report submitted by a CRM firm operating under a deadline, and a multi-year academic project. The end result is that for a Cultural Resource Management archaeologist to be successful, they must be able to produce academic quality documents at a corporate world pace.\n\nThe annual ratio of open academic archaeology positions (inclusive of post-doc, temporary, and non- tenure track appointments) to the annual number of archaeology MA/MSc and PhD students is disproportionate. Cultural Resource Management, once considered an intellectual backwater for individuals with \"strong backs and weak minds,\" has attracted these graduates, and CRM offices are thus increasingly staffed by advance degreed individuals with a track record of producing scholarly articles but who also have extensive CRM field experience.\n\nEarly archaeology was largely an attempt to uncover spectacular artifacts and features, or to explore vast and mysterious abandoned cities. Early archaeology was mostly done by upper class, scholarly men. This generalization laid the foundation for the modern popular view of archaeology and archaeologists. This generalization has been with western culture for a long time. Another popular thought that dates back to this era is that archaeology is monetarily lucrative. A large majority of the general public is under the impression that excavations are undertaken for money and not historical data. It is easy for the general public to hold this notion for that is what is presented to them through general media, and has been for many decades.\n\nThe majority of the public view archaeology as being something only available to a narrow demographic. The job of archaeologist is depicted as a \"romantic adventurist occupation\". To generalize, the public views archaeology as a fantasized hobby more than a job in the scientific community. The audience may not take away scientific methods from popular cinema but they do form a notion of \"who archaeologists are, why they do what they do, and how relationships to the past are constituted\". The modern depiction of archaeology is sensationalized so much that it has incorrectly formed the public's perception of what archaeology is. The public is often under the impression that all archaeology takes place in a distant and foreign land, only to collect monetarily or spiritually priceless artifacts.\n\nMuch thorough and productive research has indeed been conducted in dramatic locales such as Copán and the Valley of the Kings, but the bulk of activities and finds of modern archaeology are not so sensational. Archaeological adventure stories tend to ignore the painstaking work involved in carrying out modern surveys, excavations, and data processing. Some archaeologists refer to such off-the-mark portrayals as \"pseudoarchaeology\".\nArchaeologists are also very much reliant on public support; the question of exactly who they are doing their work for is often discussed.\n\nMotivated by a desire to halt looting, curb pseudoarchaeology, and to help preserve archaeological sites through education and fostering public appreciation for the importance of archaeological heritage, archaeologists are mounting public-outreach campaigns. They seek to stop looting by combatting people who illegally take artifacts from protected sites, and by alerting people who live near archaeological sites of the threat of looting. Common methods of public outreach include press releases, and the encouragement of school field trips to sites under excavation by professional archaeologists. Public appreciation of the significance of archaeology and archaeological sites often leads to improved protection from encroaching development or other threats.\n\nOne audience for archaeologists' work is the public. They increasingly realize that their work can benefit non-academic and non-archaeological audiences, and that they have a responsibility to educate and inform the public about archaeology. Local heritage awareness is aimed at increasing civic and individual pride through projects such as community excavation projects, and better public presentations of archaeological sites and knowledge. The U.S.Dept. of Agriculture, Forest Service (USFS) operates a volunteer archaeology and historic preservation program called the Passport in Time (PIT). Volunteers work with professional USFS archaeologists and historians on national forests throughout the U.S. Volunteers are involved in all aspects of professional archaeology under expert supervision.\n\nTelevision programs, web videos and social media can also bring an understanding of underwater archaeology to a broad audience. The \"Mardi Gras\" Shipwreck Project integrated a one-hour HD documentary, short videos for public viewing and video updates during the expedition as part of the educational outreach. Webcasting is also another tool for educational outreach. For one week in 2000 and 2001, live underwater video of the \"Queen Anne's Revenge\" Shipwreck Project was webcast to the Internet as a part of the \"QAR DiveLive\" educational program that reached thousands of children around the world. Created and co-produced by Nautilus Productions and Marine Grafics, this project enabled students to talk to scientists and learn about methods and technologies utilized by the underwater archaeology team.\n\nIn the UK, popular archaeology programs such as \"Time Team\" and \"Meet the Ancestors\" have resulted in a huge upsurge in public interest. Where possible, archaeologists now make more provisions for public involvement and outreach in larger projects than they once did, and many local archaeological organizations operate within the Community archaeology framework to expand public involvement in smaller-scale, more local projects. Archaeological excavation, however, is best undertaken by well-trained staff that can work quickly and accurately. Often this requires observing the necessary health and safety and indemnity insurance issues involved in working on a modern building site with tight deadlines. Certain charities and local government bodies sometimes offer places on research projects either as part of academic work or as a defined community project. There is also a flourishing industry selling places on commercial training excavations and archaeological holiday tours.\n\nArchaeologists prize local knowledge and often liaise with local historical and archaeological societies, which is one reason why Community archaeology projects are starting to become more common. Often archaeologists are assisted by the public in the locating of archaeological sites, which professional archaeologists have neither the funding, nor the time to do.\n\nArchaeological Legacy Institute (ALI), is a registered 501[c] [3] non-profit, media and education corporation registered in Oregon in 1999. ALI founded a website, The Archaeology Channel to support the organization's mission \"to nurturing and bringing attention to the human cultural heritage, by using media in the most efficient and effective ways possible.\"\n\nPseudoarchaeology is an umbrella term for all activities that falsely claim to be archaeological but in fact violate commonly accepted and scientific archaeological practices. It includes much fictional archaeological work (discussed above), as well as some actual activity. Many non-fiction authors have ignored the scientific methods of processual archaeology, or the specific critiques of it contained in post-processualism.\n\nAn example of this type is the writing of Erich von Däniken. His 1968 book, \"Chariots of the Gods?\", together with many subsequent lesser-known works, expounds a theory of ancient contacts between human civilization on Earth and more technologically advanced extraterrestrial civilizations. This theory, known as palaeocontact theory, or Ancient astronaut theory, is not exclusively Däniken's, nor did the idea originate with him. Works of this nature are usually marked by the renunciation of well-established theories on the basis of limited evidence, and the interpretation of evidence with a preconceived theory in mind.\n\nLooting of archaeological sites is an ancient problem. For instance, many of the tombs of the Egyptian pharaohs were looted during antiquity. Archaeology stimulates interest in ancient objects, and people in search of artifacts or treasure cause damage to archaeological sites. The commercial and academic demand for artifacts unfortunately contributes directly to the illicit antiquities trade. Smuggling of antiquities abroad to private collectors has caused great cultural and economic damage in many countries whose governments lack the resources and or the will to deter it. Looters damage and destroy archaeological sites, denying future generations information about their ethnic and cultural heritage. Indigenous peoples especially lose access to and control over their 'cultural resources', ultimately denying them the opportunity to know their past.\n\nIn 1937, W. F. Hodge the Director of the Southwest Museum released a statement that the museum would no longer purchase or accept collections from looted contexts. The first conviction of the transport of artifacts illegally removed from private property under the Archaeological Resources Protection Act (ARPA; Public Law 96-95; 93 Statute 721; ) was in 1992 in the State of Indiana.\n\nArchaeologists trying to protect artifacts may be placed in danger by looters or locals trying to protect the artifacts from archaeologists who are viewed as looters by the locals.\n\nIn the United States, examples such as the case of Kennewick Man have illustrated the tensions between Native Americans and archaeologists, which can be summarized as a conflict between a need to remain respectful toward sacred burial sites and the academic benefit from studying them. For years, American archaeologists dug on Indian burial grounds and other places considered sacred, removing artifacts and human remains to storage facilities for further study. In some cases human remains were not even thoroughly studied but instead archived rather than reburied. Furthermore, Western archaeologists' views of the past often differ from those of tribal peoples. The West views time as linear; for many natives, it is cyclic. From a Western perspective, the past is long-gone; from a native perspective, disturbing the past can have dire consequences in the present.\n\nAs a consequence of this, American Indians attempted to prevent archaeological excavation of sites inhabited by their ancestors, while American archaeologists believed that the advancement of scientific knowledge was a valid reason to continue their studies. This contradictory situation was addressed by the Native American Graves Protection and Repatriation Act (NAGPRA, 1990), which sought to reach a compromise by limiting the right of research institutions to possess human remains. Due in part to the spirit of postprocessualism, some archaeologists have begun to actively enlist the assistance of indigenous peoples likely to be descended from those under study.\n\nArchaeologists have also been obliged to re-examine what constitutes an archaeological site in view of what native peoples believe to constitute sacred space. To many native peoples, natural features such as lakes, mountains or even individual trees have cultural significance. Australian archaeologists especially have explored this issue and attempted to survey these sites to give them some protection from being developed. Such work requires close links and trust between archaeologists and the people they are trying to help and at the same time study.\n\nWhile this cooperation presents a new set of challenges and hurdles to fieldwork, it has benefits for all parties involved. Tribal elders cooperating with archaeologists can prevent the excavation of areas of sites that they consider sacred, while the archaeologists gain the elders' aid in interpreting their finds. There have also been active efforts to recruit aboriginal peoples directly into the archaeological profession.\n\nA new trend in the heated controversy between First Nations groups and scientists is the repatriation of native artifacts to the original descendants. An example of this occurred on 21 June 2005, when community members and elders from a number of the 10 Algonquian nations in the Ottawa area convened on the Kitigan Zibi reservation near Maniwaki, Quebec, to inter ancestral human remains and burial goods—some dating back 6,000 years. It was not determined, however, if the remains were directly related to the Algonquin people who now inhabit the region. The remains may be of Iroquoian ancestry, since Iroquoian people inhabited the area before the Algonquin. Moreover, the oldest of these remains might have no relation at all to the Algonquin or Iroquois, and belong to an earlier culture who previously inhabited the area.\n\nThe remains and artifacts, including jewelry, tools and weapons, were originally excavated from various sites in the Ottawa Valley, including Morrison and the Allumette Islands. They had been part of the Canadian Museum of Civilization's research collection for decades, some since the late 19th century. Elders from various Algonquin communities conferred on an appropriate reburial, eventually deciding on traditional redcedar and birchbark boxes lined with redcedar chips, muskrat and beaver pelts.\n\nAn inconspicuous rock mound marks the reburial site where close to 80 boxes of various sizes are buried. Because of this reburial, no further scientific study is possible. Although negotiations were at times tense between the Kitigan Zibi community and museum, they were able to reach agreement.\n\nKennewick Man is another repatriation candidate that has been the source of heated debate.\n\n\nLists\n\n"}
{"id": "13601902", "url": "https://en.wikipedia.org/wiki?curid=13601902", "title": "Arts and letters", "text": "Arts and letters\n\nArts and letters is a traditional term for the fine arts and literature considered together. The category defined as \"arts and letters\" may also include the performing arts, visual arts, or liberal arts.\n\nBy the late 19th century the term was used to name a few arts-related institutions in the United States. A subscription-based Theatre of Arts and Letters opened in New York in 1892, but closed within a year. The American Academy of Arts and Letters was founded in 1904.\n\nThe term has also figured in higher education. For example, by 1889 students at Swarthmore College were sorted into two categories of study: \"Arts and Letters\" or \"Science and Engineering.\" Since 2010, course requirements at the University of Pennsylvania College of Arts and Sciences have included a \"sector\" (category) called \"Arts and Letters.\" The college defines the sector as including \"visual arts, literature and music, together with the criticism surrounding them.\"\n"}
{"id": "31916228", "url": "https://en.wikipedia.org/wiki?curid=31916228", "title": "Body politic", "text": "Body politic\n\nThe body politic is a medieval metaphor that likens a nation to a corporation which had serious historical repercussions throughout recent history and therefore giving the Crown: \"As a legal entity today the Crown as executive is regarded as a corporation sole or aggregate\", a corporate entity. Maitland argues that the Crown (as a legal term) is a convenient cover for ignorance and traces the legal term Crown as corporation sole originally from the 16th century and argues that it was both a political and legal ploy originally reserved for the monarch of the day with the combination of medieval Roman law amalgamated into the early medieval domain of early church property law. The modern understanding of the concept means a body politic comprises all the people in a particular country considered as a single group. The analogy typically includes reference to the sovereign head of government as head of state, though the analogy may also be extended to other anatomical parts, as in political readings of Aesop's fable of \"The Belly and the Members\".\n\nA later European reference to the concept appears in \"The Book of the Body Politic\" (1407) by poet and court writer Christine de Pizan, in which she admits having borrowed the concept from a letter of Plutarch's to the Roman Emperor Trajan but does not mention John of Salisbury's work, \"Policraticus\". The metaphor appears in the French language as the \"corps-état\".\nThe metaphor was elaborated in the Renaissance, and subsequently, as medical knowledge based on Galen was challenged by thinkers such as William Harvey. Analogies were drawn between supposed causes of disease and disorder and their equivalents in the political field, viewed as plagues or infections that might be remedied with purges and nostrums.\n\n The term \"Body politic\" (the political body of society) derives from the mediæval political concept of the King's two bodies first noted by mediaeval historian Ernst Kantorowicz, as a point of theology as much as statehood. However, the person to give the concept some legal bite and codified reality, as much as legitimacy as well as Sovereignty, was the 14th century judge Sir William de Shareshull in 1351 for the offence of high treason in the aftermath of the Barons war. However, by the time of the fifteenth-century judge Sir John Fortescue the concept moves away from theology to jurisprudence in his \"The Difference between an Absolute and a Limited Monarchy\", written from exile in about 1462 . Fortescue explains that the \"character angelus\" (divine character) of the king is his royal power, derived from angels and separate from the frail physical powers of his body. However, he uses the phrase \"body politic\" itself only in its modern sense, to describe the realm, or shared rule, of Brutus, mythical first king of England, and how he and his fellow exiles had covenanted to form a body politic. Unusually for the time, Fortescue was writing in English and not Latin: \"made a body pollitike callid a reawme.\" The early medieval period (13th century) saw a general and radical shift in the idea of the political body and sovereignty and who was to be ultimate ruler of humankind.This constant shift in Translatio imperii and \"The Halo of perpetuity\"(Kantorowicz reasoning) transformed the king's persona from Liturgy to legal science, argues Kantorowicz. However, the Norman conquest proved one thing, quite surprisingly enough, the king wasn't the ultimate and major landowner with just 20% of land ownership as William the Conqueror found out to his disgust and chagrin, which ultimately led to the Domesday book being compiled. With Plenitudo potestatis, Unam sanctam (a Papal bull) extra Ecclesiam nulla salus and the Sun and moon allegory now part of the armoury of jurisdictional power of the papacy with both Canon law, and medieval political theology now becoming(part of the Scholastic movement beginning with Thomas Aquinas and John of Salisbury) a standard bearer for medieval political power which then descended into the infamous Inquisition and heresy (deviation from truth)as the ultimate legal reality which meant both the Roman Catholic Church and Papal supremacy couldn't be ignored(not without fear of excommunication and in some cases judicial execution). The church, which ultimately had powerful claims to sovereignty due to their immense wealth and vast monetary resources which included a clever taxation system establishing the legal validity of the tithe system which baffled legal experts of the day of its historical lineage and origin, such as Sir William Blackstone, for example, not its legal validity; such as their (the church's) enormous network and ownership of land, law making and the then-University and education system.\n\nIn 1550 the jurist Edmund Plowden merged Fortescue's concepts, at the same time removing them from abstraction into a real, physical manifestation in the body of the king. Plowden reports how lawyers codified this notion in an examination of a case of land-ownership turning on a disputed gift by an earlier monarch; they determined that the \"Body politic…that cannot be seen or handled…[is] constituted for the direction of the People…[and] these two bodies are incorporated in one person…the Body politic includes the [king's] Body natural.\" In 1609 Attorney General Edward Coke pronounced his dissenting opinion, that mortal power was God-made while the immortality of royal power existed only as a man-made concept; Coke later succeeded in limiting the royal power of both Charles I and James VI and I with his now infamous judgements of Case of Proclamations and his \"Petition of Right\", which triggered off the English Civil War and would have reverberations right across the country for the next 100 years. When the monarchy, in the person of Charles II, was restored at the end of the Commonwealth the idea remained current and royalty continued to use the notion, as a buttress to its authority, until an assertion of the rights of Parliament brought about the Glorious Revolution of 1688. In the 18th century English Barrister and judge Sir William Blackstone developed further the medieval idea and the legal argument (which still stands today) of the monarch in legal terms \"immortal\" with added emphasis of a quasi divine (\"character angelus\") status.\n\nThe pre-Revolution monarchs of France also claimed legitimacy on this principle, and extended it to include the idea that the king's heir assimilated the \"body politic\" of the old king, in a physical \"transfer of corporeality\", on accession.\n\n"}
{"id": "1414389", "url": "https://en.wikipedia.org/wiki?curid=1414389", "title": "Callback (comedy)", "text": "Callback (comedy)\n\nA callback, in terms of comedy, is a joke that refers to one previously told in the set. It is also known as an internal allusion, a literary device that helps give structure to the piece of writing. Callbacks are usually saved for later, towards the end of a set, the result is usually a much bigger laugh the second time as the audience is much more surprised the longer it's held back. The main principle behind the callback is to make the audience feel a sense of familiarity with the subject material, as well as with the comedian. It helps to create audience rapport. When the second joke is told, it induces a feeling similar to that of being told a personal or in-joke.\n\nIn television, the term callback has come to mean a joke or line that refers to a previous episode (or sometimes, in rare cases, movies). Particularly in earlier sitcoms—though even until the early 1990s—callbacks were rare and often frowned upon by networks, because they threaten to alienate a viewer who is new to the series, or who has missed episodes, particularly if the callback is tied to previous episodes (this is especially a threat to a show's syndication value, as shows in which the episodes are self-contained, and thus can be rerun out of order, can fetch a higher sale price than shows that must be run in sequence). \n\n\"Seinfeld\", a show built around stand-up comedy, was one of the first sitcoms to regularly use callbacks in its scripts, although on a level that would often be missed or disregarded by viewers; its use of the strategy commonly kept the callbacks confined to events in the same episodes, having the effect of bringing the episode full-circle or creating an ironic twist ending. A more recent series, \"30 Rock\", employs callbacks to reference fictitious movies and television programs created within the show. \"Arrested Development\" became well known by fans for its regular use of callbacks throughout all of its episodes. \n\nThe line between a callback and simple continuity can be ambiguous. Repeatedly calling back to the same joke is a running gag.\n\n"}
{"id": "53682", "url": "https://en.wikipedia.org/wiki?curid=53682", "title": "Calligraphy", "text": "Calligraphy\n\nCalligraphy (from Greek: καλλιγραφία) is a visual art related to writing. It is the design and execution of lettering with a broad tip instrument, brush, or other writing instruments. A contemporary calligraphic practice can be defined as \"the art of giving form to signs in an expressive, harmonious, and skillful manner\".\n\nModern calligraphy ranges from functional inscriptions and designs to fine-art pieces where the letters may or may not be readable. Classical calligraphy differs from typography and non-classical hand-lettering, though a calligrapher may practice both.\n\nCalligraphy continues to flourish in the forms of wedding invitations and event invitations, font design and typography, original hand-lettered logo design, religious art, announcements, graphic design and commissioned calligraphic art, cut stone inscriptions, and memorial documents. It is also used for props and moving images for film and television, and also for testimonials, birth and death certificates, maps, and other written works.\n\nThe principal tools for a calligrapher are the pen and the brush. Calligraphy pens write with nibs that may be flat, round, or pointed. For some decorative purposes, multi-nibbed pens—steel brushes—can be used. However, works have also been created with felt-tip and ballpoint pens, although these works do not employ angled lines. There are some styles of calligraphy, such as Gothic script, that require a stub nib pen.\n\nWriting ink is usually water-based and is much less viscous than the oil-based inks used in printing. High quality paper, which has good consistency of absorption, enables cleaner lines, although parchment or vellum is often used, as a knife can be used to erase imperfections and a light-box is not needed to allow lines to pass through it. Normally, light boxes and templates are used to achieve straight lines without pencil markings detracting from the work. Ruled paper, either for a light box or direct use, is most often ruled every quarter or half inch, although inch spaces are occasionally used. This is the case with \"litterea unciales\" (hence the name), and college-ruled paper often acts as a guideline well.\n\nCommon calligraphy pens and brushes are:\n\nWestern calligraphy is recognizable by the use of the Latin script. The Latin alphabet appeared about 600 BC, in Rome, and by the first century developed into Roman imperial capitals carved on stones, Rustic capitals painted on walls, and Roman cursive for daily use. In the second and third centuries the uncial lettering style developed. As writing withdrew to monasteries, uncial script was found more suitable for copying the Bible and other religious texts. It was the monasteries which preserved calligraphic traditions during the fourth and fifth centuries, when the Roman Empire fell and Europe entered the Dark Ages.\n\nAt the height of the Empire, its power reached as far as Great Britain; when the empire fell, its literary influence remained. The Semi-uncial generated the Irish Semi-uncial, the small Anglo-Saxon. Each region developed its own standards following the main monastery of the region (i.e. Merovingian script, Laon script, Luxeuil script, Visigothic script, Beneventan script), which are mostly cursive and hardly readable.\nChristian churches promoted the development of writing through the prolific copying of the Bible, the Breviary, and other sacred texts. Two distinct styles of writing known as uncial and half-uncial (from the Latin \"uncia\", or \"inch\") developed from a variety of Roman bookhands. The 7th-9th centuries in northern Europe were the heyday of Celtic illuminated manuscripts, such as the Book of Durrow, Lindisfarne Gospels and the Book of Kells.\n\nCharlemagne's devotion to improved scholarship resulted in the recruiting of \"a crowd of scribes\", according to Alcuin, the Abbot of York. Alcuin developed the style known as the Caroline or Carolingian minuscule. The first manuscript in this hand was the Godescalc Evangelistary (finished 783)—a Gospel book written by the scribe Godescalc. Carolingian remains the one progenitor hand from which modern booktype descends.\n\nIn the eleventh century, the Caroline evolved into the Gothic script, which was more compact and made it possible to fit more text on a page. The Gothic calligraphy styles became dominant throughout Europe; and in 1454, when Johannes Gutenberg developed the first printing press in Mainz, Germany, he adopted the Gothic style, making it the first typeface.\nIn the 15th century, the rediscovery of old Carolingian texts encouraged the creation of the humanist minuscule or \"littera antiqua\". The 17th century saw the Batarde script from France, and the 18th century saw the English script spread across Europe and world through their books.\n\nIn the mid-1600s French officials, flooded with documents written in various hands and varied levels of skill, complained that many such documents were beyond their ability to decipher. The Office of the Financier thereupon restricted all legal documents to three hands, namely the Coulee, the Rhonde, (known as Round hand in English) and a Speed Hand sometimes simply called the Bastarda.\n\nWhile there were many great French masters at the time, the most influential in proposing these hands was Louis Barbedor, who published \"Les Ecritures Financière Et Italienne Bastarde Dans Leur Naturel\" circa 1650.\n\nWith the destruction of the Camera Apostolica during the sack of Rome (1527), the capitol for writing masters moved to Southern France. By 1600, the Italic Cursiva began to be replaced by a technological refinement, the Italic Chancery Circumflessa, which in turn fathered the Rhonde and later English Roundhand.\n\nIn England, Ayres and Banson popularized the Round Hand while Snell is noted for his reaction to them, and warnings of restraint and proportionality. Still Edward Crocker began publishing his copybooks 40 years before the aforementioned.\n\nSacred Western calligraphy has some special features, such as the illumination of the first letter of each book or chapter in medieval times. A decorative \"carpet page\" may precede the literature, filled with ornate, geometrical depictions of bold-hued animals. The Lindisfarne Gospels (715–720 AD) are an early example.\n\nAs with Chinese or Islamic calligraphy, Western calligraphic script employed the use of strict rules and shapes. Quality writing had a rhythm and regularity to the letters, with a \"geometrical\" order of the lines on the page. Each character had, and often still has, a precise stroke order.\n\nUnlike a typeface, irregularity in the characters' size, style, and colors increases aesthetic value, though the content may be illegible. Many of the themes and variations of today's contemporary Western calligraphy are found in the pages of The Saint John's Bible. A particularly modern example is Timothy Botts' illustrated edition of the Bible, with 360 calligraphic images as well as a calligraphy typeface.\n\nSeveral other Western styles use the same tools and practices, but differ by character set and stylistic preferences.\nFor Slavonic lettering, the history of the Slavonic and consequently Russian writing systems differs fundamentally from the one of the Latin language. It evolved from the 10th century to today.\n\nThe Chinese name for calligraphy is ' ( in Traditional Chinese, literally \"the method or law of writing\"); the Japanese name ' (, literally \"the way or principle of writing\"); and the Korean name being \"\" (/, literally \"the art of writing\"). The calligraphy of East Asian characters is an important and appreciated aspect of traditional East Asian culture.\n\nIn ancient China, the oldest Chinese characters existing are Jiǎgǔwén characters (甲骨文) carved on ox scapulae and tortoise plastrons, because the dominators in Shang Dynasty carved pits on such animals' bones and then baked them to gain auspice of military affairs, agricultural harvest, or even procreating and weather. During the divination ceremony, after the cracks were made, the characters were written with a brush on the shell or bone to be later carved. (Keightley, 1978). With the development of \"Jīnwén\" (Bronzeware script) and \"Dàzhuàn\" (Large Seal Script) \"cursive\" signs continued. Mao Gong Ding is one of the most famous and typical Bronzeware scripts in the Chinese calligraphy history. It has 500 characters on the bronze which is the largest number of bronze inscription we have discovered so far. . Moreover, each archaic kingdom of current China had its own set of characters. \n\nIn Imperial China, the graphs on old steles—some dating from 200 BC, and in Xiaozhuan style—are still accessible.\n\nAbout 220 BC, the emperor Qin Shi Huang, the first to conquer the entire Chinese basin, imposed several reforms, among them Li Si's character unification, which created a set of 3300 standardized \"\" characters. Despite the fact that the main writing implement of the time was already the brush, few papers survive from this period, and the main examples of this style are on steles.\n\nThe Lìshū style (/)(clerical script) which is more regularized, and in some ways similar to modern text, have been also authorised under Qin Shi Huangdi.\n\nBetween clerical script and traditional regular script, there is another transition type of calligraphy works called Wei Bei. It had started at the South and North dynasty and ended before Tang Dynasty. \n\nKǎishū style (traditional regular script)—still in use today—and attributed to Wang Xizhi (, 303–361) and his followers, is even more regularized. Its spread was encouraged by Emperor Mingzong of Later Tang (926–933), who ordered the printing of the classics using new wooden blocks in Kaishu. Printing technologies here allowed a shape stabilization. The Kaishu shape of characters 1000 years ago was mostly similar to that at the end of Imperial China. But small changes have been made, for example in the shape of which is not absolutely the same in the Kangxi Dictionary of 1716 as in modern books. The Kangxi and current shapes have tiny differences, while stroke order is still the same, according to old style.\n\nStyles which did not survive include Bāfēnshū, a mix made of Xiaozhuan style at 80%, and Lishu at 20%.\nSome variant Chinese characters were unorthodox or locally used for centuries. They were generally understood but always rejected in official texts. Some of these unorthodox variants, in addition to some newly created characters, compose the Simplified Chinese character set.\n\nTraditional East Asian writing uses the Four Treasures of the Study (/): the ink brushes known as máobǐ (/) to write Chinese characters, Chinese ink, paper, and inkstone, known as the \"Four Friends of the Study\" () in Korea. In addition to these four tools, desk pads and paperweights are also used.\n\nThe shape, size, stretch, and hair type of the ink brush, the color, color density and water density of the ink, as well as the paper's water absorption speed and surface texture are the main physical parameters influencing the final result. The calligrapher's technique also influences the result. The calligrapher's work is influenced by the quantity of ink and water he lets the brush take, then by the pressure, inclination, and direction he gives to the brush, producing thinner or bolder strokes, and smooth or toothed borders. Eventually, the speed, accelerations, decelerations of the writer's moves, turns, and crochets, and the stroke order give the \"spirit\" to the characters, by greatly influencing their final shapes.\n\nCursive styles such as \"\" (/)(semi-cursive or running script) and \"\" (/)(cursive, rough script, or grass script) are less constrained and faster, where more movements made by the writing implement are visible. These styles' stroke orders vary more, sometimes creating radically different forms. They are descended from Clerical script, in the same time as Regular script (Han Dynasty), but ' and ' were used for personal notes only, and never used as a standard. The \"\" style was highly appreciated in Emperor Wu of Han reign (140–187 AD).\n\nExamples of modern printed styles are Song from the Song Dynasty's printing press, and sans-serif. These are not considered traditional styles, and are normally not written.\n\nBoth Japanese and Korean calligraphy were greatly influenced by Chinese calligraphy. The Japanese and Korean people have also developed their own specific sensibilities and styles of calligraphy while incorporating Chinese influences. For example, Japanese calligraphy go out of the set of CJK strokes to also include local alphabets such as hiragana and katakana, with specific problematics such as new curves and moves, and specific materials (Japanese paper, \"\" , and Japanese ink). In the case of Korean calligraphy, the Hangeul and the existence of the circle required the creation of a new technique which usually confuses Chinese calligraphers.\n\nTemporary calligraphy is a practice of water-only calligraphy on the floor, which dries out within minutes. This practice is especially appreciated by the new generation of retired Chinese in public parks of China. These will often open studio-shops in tourist towns offering traditional Chinese calligraphy to tourists. Other than writing the clients name, they also sell fine brushes as souvenirs and lime stone carved stamps.\n\nSince late 1980s, a few Chinese artists have branched out traditional Chinese calligraphy to a new territory by mingling Chinese characters with English letters; notable new forms of calligraphy are Xu Bing's square calligraphy and DanNie's coolligraphy or cooligraphy.\n\nMongolian calligraphy is also influenced by Chinese calligraphy, from tools to style.\n\nCalligraphy has influenced ink and wash painting, which is accomplished using similar tools and techniques. Calligraphy has influenced most major art styles in East Asia, including ink and wash painting, a style of Chinese, Japanese, and Korean based entirely on calligraphy.\n\nThe Philippines has numerous indigenous scripts collectively called as suyat. Various ethno-linguistic groups in the Philippines prior to Spanish colonization in the 16th century up to the independence era in the 21st century have used the scripts with various mediums. By the end of colonialism, only four of the suyat scripts survived and continue to be used by certain communities in everyday life. These four scripts are hanunó'o/hanunoo of the Hanuno'o Mangyan people, buhid/buid of the Buhid Mangyan people, apurahuano/tagbanwa of the Tagbanwa people, and palaw'an/pala'wan of the Palaw'an people. All four scripts were inscribed in the UNESCO Memory of the World Programme, under the name \"Philippine Paleographs (Hanunoo, Buid, Tagbanua and Pala’wan)\", in 1999.\nDue to dissent from colonialism, many artists and cultural experts have revived the usage of suyat scripts that went extinct due to Spanish persecution. These scripts being revived include the kulitan script of the Kapampangan people, the badlit script of various Visayan ethnic groups, the iniskaya script of the Eskaya people, the baybayin script of the Tagalog people, and the kur-itan script of the Ilocano people, among many others. Due to the diversity of suyat scripts, all calligraphy written in suyat scripts are collectively called as Filipino suyat calligraphy, although each are distinct from each other. Calligraphy using the Western alphabet and the Arabic alphabet are also prevalent in the Philippines due to its colonial past, but the Western alphabet and the Arabic alphabet are not considered as suyat, and therefore Western-alphabet and Arabic calligraphy are not considered as suyat calligraphy.\n\nSanskrit is the primary form of Thai calligraphy. Historically Thai calligraphy has been limited to sacred texts of the Pali Canon with few wider artistic applications where graphic calligraphy representing figures and objects is produced. Calligraphy appears on the personal flag of each member of the Thai royal family bearing its owner's initials in calligraphy. The most obvious place in the country where calligraphy is present is in graffiti. A few books have been published with calligraphic compositions.\n\n The Vietnamese name for calligraphy is (, literally \"the way of letters or words\"). Vietnamese calligraphy uses a variety of scripts, including Chinese characters, Chữ nôm, and the Latin-based Quốc Ngữ. Historically, calligraphers used the former two scripts. Due to the adoption of the Latin-based Quốc Ngữ, most modern Vietnamese calligraphy uses Roman characters. Traditional Vietnamese calligraphy is strongly affected by that of China for historical and geographic regions. As part of the East Asian cultural sphere, Chinese was often used as the written medium of communication, and as a result, Vietnamese calligraphy thus also follows Chinese calligraphy’s standard and use Han script (Chinese language) in many of its writings. In modern times, calligraphy has been done mainly in the Latin-based Quốc Ngữ script, as Chữ nôm and Chinese characters have largely fallen out of use.\n\nOn the subject of Indian calligraphy, writes:\nAśoka's edicts (c. 265–238 BC) were committed to stone. These inscriptions are stiff and angular in form. Following the Aśoka style of Indic writing, two new calligraphic types appear: Kharoṣṭī and Brāhmī. Kharoṣṭī was used in the northwestern regions of India from the 3rd century BC to the 4th century of the Christian Era, and it was used in Central Asia until the 8th century.\nIn many parts of ancient India, the inscriptions were carried out in smoke-treated palm leaves. This tradition dates back to over two thousand years. Even after the Indian languages were put on paper in the 13th century, palm leaves where considered a preferred medium of writing owing to its longevity (nearly 400 years) compared to paper. Both sides of the leaves were used for writing. Long rectangular strips were gathered on top of one another, holes were drilled through all the leaves, and the book was held together by string. Books of this manufacture were common to Southeast Asia. The palm leaf was an excellent surface for penwriting, making possible the delicate lettering used in many of the scripts of southern Asia.\n\nBurnt clay and copper were a favoured material for Indic inscriptions. In the north of India, birch bark was used as a writing surface as early as the 2nd century AD.\n\nRanjana script is the primary form of Nepalese calligraphy. The script itself, along with its derivatives (like Lantsa, Phagpa, Kutila) are used in Nepal, Tibet, Bhutan, Leh, Mongolia, coastal Japan, and Korea to write \"Om mani padme hum\" and other sacred Buddhist texts, mainly those derived from Sanskrit and Pali.\n\nCalligraphy is central in Tibetan culture. The script is derived from Indic scripts. The nobles of Tibet, such as the High Lamas and inhabitants of the Potala Palace, were usually capable calligraphers. Tibet has been a center of Buddhism for several centuries, and that religion places a great deal of significance on written word. This does not provide for a large body of secular pieces, although they do exist (but are usually related in some way to Tibetan Buddhism). Almost all high religious writing involved calligraphy, including letters sent by the Dalai Lama and other religious and secular authority. Calligraphy is particularly evident on their prayer wheels, although this calligraphy was forged rather than scribed, much like Arab and Roman calligraphy is often found on buildings. Although originally done with a reed, Tibetan calligraphers now use chisel tipped pens and markers as well.\n\nIslamic calligraphy (\"calligraphy\" in Arabic is \"\" ) has evolved alongside Islam and the Arabic language. As it is based on Arabic letters, some call it \"Arabic calligraphy\". However the term \"Islamic calligraphy\" is a more appropriate term as it comprises all works of calligraphy by the Muslim calligraphers from Andalusia in modern Spain to China.\n\nIslamic calligraphy is associated with geometric Islamic art (arabesque) on the walls and ceilings of mosques as well as on the page. Contemporary artists in the Islamic world draw on the heritage of calligraphy to use calligraphic inscriptions or abstractions.\n\nInstead of recalling something related to the spoken word, calligraphy for Muslims is a visible expression of the highest art of all, the art of the spiritual world. Calligraphy has arguably become the most venerated form of Islamic art because it provides a link between the languages of the Muslims with the religion of Islam. The Qur'an has played an important role in the development and evolution of the Arabic language, and by extension, calligraphy in the Arabic alphabet. Proverbs and passages from the Qur'an are still sources for Islamic calligraphy.\n\nIt is generally accepted that Islamic calligraphy excelled during the Ottoman era. Istanbul is an open exhibition hall for all kinds and varieties of calligraphy, from inscriptions in mosques to fountains, schools, houses, etc. \n\nEthiopian (Abyssinian)) calligraphy began with the Ge'ez script, which replaced Epigraphic South Arabian in the Kingdom of Aksum, that was developed specifically for Ethiopian Semitic languages. In those languages that use it, such as Amharic and Tigrinya, the script is called \"\", which means script or alphabet. The Epigraphic South Arabian letters were used for a few inscriptions into the 8th century, though not any South Arabian language since Dʿmt.\n\nEarly inscriptions in Ge'ez and Ge'ez script have been dated to as early as the 5th century BC, and in a sort of proto-Ge'ez written in ESA since the 9th century BC. Ge'ez literature begins with the Christianization of Ethiopia (and the civilization of Axum) in the 4th century, during the reign of Ezana of Axum. \n\nThe Ge'ez script is read from left to right and has been adapted to write other languages, usually ones that are also Semitic. The most widespread use is for Amharic in Ethiopia and Tigrinya in Eritrea and Ethiopia.\n\nThe history of calligraphy in Persia dates back to the pre-Islam era. In Zoroastrianism beautiful and clear writings were always praised.\n\nIt is believed that ancient Persian script was invented by about 600–500 BC to provide monument inscriptions for the Achaemenid kings. These scripts consisted of horizontal, vertical, and diagonal nail-shape letters, which is why it is called \"script of nails/cuneiform script\" (\"\") in Persian. Centuries later, other scripts such as \"Pahlavi\" and \"Avestan\" scripts were used in ancient Persia.\n\nThe Nasta'liq style is the most popular contemporary style among classical Persian calligraphy scripts; Persian calligraphers call it the \"bride of calligraphy scripts\". This calligraphy style has been based on such a strong structure that it has changed very little since. Mir Ali Tabrizi had found the optimum composition of the letters and graphical rules so it has just been fine-tuned during the past seven centuries. It has very strict rules for graphical shape of the letters and for combination of the letters, words, and composition of the whole calligraphy piece.\n\nMayan calligraphy was expressed via Mayan hieroglyphs; modern Mayan calligraphy is mainly used on seals and monuments in the Yucatán Peninsula in Mexico. Mayan hieroglyphs are rarely used in government offices; however in Campeche, Yucatán and Quintana Roo, Mayan calligraphy is written in Latin letters. Some commercial companies in southern Mexico use Mayan hieroglyphs as symbols of their business. Some community associations and modern Mayan brotherhoods use Mayan hieroglyphs as symbols of their groups.\n\nMost of the archaeological sites in Mexico such as Chichen Itza, Labna, Uxmal, Edzna, Calakmul, etc. have glyphs in their structures. Carved stone monuments known as stele are common sources of ancient Mayan calligraphy.\n\nAfter printing became ubiquitous from the 15th century, the production of illuminated manuscripts began to decline. However, the rise of printing did not mean the end of calligraphy.\n\nThe modern revival of calligraphy began at the end of the 19th century, influenced by the aesthetics and philosophy of William Morris and the Arts and Crafts movement. Edward Johnston is regarded as being the father of modern calligraphy. \nAfter studying published copies of manuscripts by architect William Harrison Cowlishaw, he was introduced to William Lethaby in 1898, principal of the Central School of Arts and Crafts, who advised him to study manuscripts at the British Museum.\n\nThis triggered Johnston's interest in the art of calligraphy with the use of a broad edged pen. He began a teaching course in calligraphy at the Central School in Southampton Row, London from September 1899, where he influenced the typeface designer and sculptor Eric Gill. He was commissioned by Frank Pick to design a new typeface for London Underground, still used today (with minor modifications).\n\nHe has been credited for reviving the art of modern penmanship and lettering single-handedly through his books and teachings - his handbook on the subject, \"Writing & Illuminating, & Lettering\" (1906) was particularly influential on a generation of British typographers and calligraphers, including Graily Hewitt, Stanley Morison, Eric Gill, Alfred Fairbank and Anna Simons. Johnston also devised the simply crafted round calligraphic handwriting style, written with a broad pen, known today as the \"Foundational hand\". Johnston initially taught his students an uncial hand using a flat pen angle, but later taught his hand using a slanted pen angle. He first referred to this hand as \"Foundational Hand\" in his 1909 publication, \"Manuscript & Inscription Letters for Schools and Classes and for the Use of Craftsmen\".\n\nGraily Hewitt taught at the Central School of Arts and Crafts and published together with Johnston throughout the early part of the century. Hewitt was central to the revival of gilding in calligraphy, and his prolific output on type design also appeared between 1915 and 1943. He is attributed with the revival of gilding with gesso and gold leaf on vellum. Hewitt helped to found the Society of Scribes & Illuminators (SSI) in 1921, probably the world's foremost calligraphy society.\nHewitt is not without both critics and supporters in his rendering of Cennino Cennini's medieval gesso recipes. Donald Jackson, a British calligrapher, has sourced his gesso recipes from earlier centuries a number of which are not presently in English translation. Graily Hewitt created the patent announcing the award to Prince Philip of the title of Duke of Edinburgh on November 19, 1947, the day before his marriage to Queen Elizabeth.\n\nJohnston’s pupil, Anna Simons, was instrumental in sparking off interest in calligraphy in Germany with her German translation of \"Writing and Illuminating, and Lettering\" in 1910. Austrian Rudolf Larisch, a teacher of lettering at the Vienna School of Art, published six lettering books that greatly influenced German-speaking calligraphers. Because German-speaking countries had not abandoned the Gothic hand in printing, Gothic also had a powerful effect on their styles.\n\nRudolf Koch was a friend and younger contemporary of Larisch. Koch's books, type designs, and teaching made him one of the most influential calligraphers of the 20th century in northern Europe and later in the U.S. Larisch and Koch taught and inspired many European calligraphers, notably Karlgeorg Hoefer, and Hermann Zapf.\n\nContemporary typefaces used by computers, from word processors like Microsoft Word or Apple Pages to professional designers' software like Adobe InDesign, owe a considerable debt to the past and to a small number of professional typeface designers today.\n\nUnicode provides \"Script\" and \"Fraktur\" Latin alphabets that can be used for calligraphy. See Mathematical Alphanumeric Symbols.\n\n\n\n"}
{"id": "39198876", "url": "https://en.wikipedia.org/wiki?curid=39198876", "title": "Child pirate", "text": "Child pirate\n\nIn keeping with the Paris Principles definition of a child soldier, the Roméo Dallaire Child Soldiers Initiative defines a \"child pirate\"' as any person below 18 years of age who is or who has been recruited or used by a pirate gang in any capacity, including children - boys and/or girls - used as gunmen in boarding parties, hostage guards, negotiators, ship captains, messengers, spies or for sexual purposes, whether at sea or on land. It does not only refer to a child who is taking or has taken a direct part in kinetic criminal operations.\n\nChildren may volunteer to participate in piratical activities (usually on account of socioeconomic desperation, familial suggestion or peer influence) or they may be forcibly abducted by piratical gangs.\n\nThere are a number of reasons why an adult pirate commander would view children as being of significant tactical value. These perceptions render children vulnerable to abduction or forced recruitment. As noted by Carl Conradi:\n\nIn other cases, children may volunteer to participate in piratical activities. However, as asserted by the Canada-based Roméo Dallaire Child Soldiers Initiative, \"'voluntary' enlistment must be understood in terms of the limited choices and circumstances that may exist in the context of a particular country.\"’ If a child is extremely poor, has been displaced from his or her home, has been separated from his or her family, has limited educational opportunities or has been exposed to conflict, there is an increased likelihood that he or she will view piracy as a legitimate vehicle for social advancement.\n\nIn the absence of specific international legislation on juvenile maritime piracy, the precise age of a child’s criminal responsibility when committing piratical acts differs from country to country. There are, however, a number of international conventions pertaining to either maritime law or children’s rights that may provide some guidance as to the proper handling of child pirates.\n\nWhile the United Nations Convention on the Law of the Sea (UNCLOS, 1982) does not discuss children’s involvement in maritime criminal activities, it does provide a clear definition of piracy. According to Article 101, piracy is:\n\nUNCLOS does recognise universal jurisdiction over the crime of piracy but it only applies to criminal acts that take place on international waters. If an act of piracy occurs within a country’s territorial waters, it is a matter of state jurisdiction and prosecution.\n\nArticle 3 of the International Labour Organisation’s (ILO) Worst Forms of Child Labour Convention (No. 182, 1999) stipulates that:\n\nInsofar as participation in any form of maritime criminality is dangerous (and indeed, potentially lethal), child piracy clearly constitutes a worst form of child labour.\n\nAccording to the Convention, a child is any person who is below the age of 18.\n\nAs of April 2013, eight countries had not signed ILO Convention No. 182. These include Cuba, Eritrea, India, the Marshall Islands, Myanmar, Palau, Somalia and Tuvalu. Two of these countries – India and Somalia – are currently detaining and prosecuting alleged child pirates.\n\nLike ILO Convention No. 182, Article 1 of the United Nations Convention on the Rights of the Child (1990) specifies that a child is any human being below the age of 18 years. However, the same article adds a caveat to the effect that a country’s minimum age of criminal responsibility may be lower than 18, as stipulated by national law.\n\nOther sections that may have some bearing upon the status of child pirates include Article 6, in which, “States Parties recognize that every child has the inherent right to life,” and that, “States Parties shall ensure to the maximum extent possible the survival and development of the child.”\n\nArticle 19(1) affirms that, “States Parties shall take all appropriate legislative, administrative, social and educational measures to protect the child from all forms of physical or mental violence, injury or abuse, neglect or negligent treatment, maltreatment or exploitation, including sexual abuse, while in the care of parent(s), legal guardian(s) or any other person who has the care of the child.” This clause may be particularly relevant in cases where a child’s parents have forced him or her to participate in piratical activity, or where the State is responsible for detaining, interrogating, trying and/or incarcerating a captured child pirate.\n\nArticle 32(1) echoes ILO Convention No. 182 by stipulating that, “States Parties recognize the right of the child to be protected from economic exploitation and from performing any work that is likely to be hazardous or to interfere with the child’s education, or to be harmful to the child’s health or physical, mental, spiritual, moral or social development.”\n\nArticle 35 says that, “State Parties shall take all appropriate national, bilateral and multilateral measures to prevent the abduction of, the sale of or traffic in children for any purpose or in any form.” When a piratical gang permanently separates a child from his or her family, this may constitute an act of child trafficking.\n\nArticle 37(a) affirms that, “No child shall be subjected to torture or other cruel, inhuman or degrading treatment or punishment. Neither capital punishment nor life imprisonment without possibility of release shall be imposed for offences committed by persons below eighteen years of age.” Likewise, Article 37(c) states that, “Every child deprived of liberty shall be treated with humanity and respect for the inherent dignity of the human person, and in a manner which takes into account the needs of persons of his or her age. In particular, every child deprived of liberty shall be separated from adults unless it is considered in the child’s best interests not to do so and shall have the right to maintain contact with his or her family through correspondence and visits, save in exceptional circumstances.”\n\nLastly, Article 39 stipulates that, “State Parties shall take all appropriate measures to promote physical and psychological recovery and social reintegration of a child victim of: any form of neglect, exploitation or abuse; torture or any other form of cruel, inhuman or degrading treatment or punishment; or armed conflicts. Such recovery and reintegration shall take place in an environment which fosters the health, self-respect and dignity of the child.”\n\nThe third version of the \"Best Management Practices to Deter Piracy off the Coast of Somalia and in the Arabian Sea Area\" (June 2010) recommends a sample follow-up report that should be filled by the captain of any vessel that has come under pirate attack. While the report does solicit certain details concerning the offending raiding party – such as the number of constituent pirate members, their physical appearance and the weapons that were used – it does not ask for the estimated ages of pirates.\n\nThe Regional Cooperation Agreement on Combating Piracy and Armed Robbery against Ships in Asia (ReCAAP) is a multilateral pact that facilitates information sharing between signatory countries. The reporting mechanism that it establishes does not specifically call for the collection of disaggregated data pertaining to the ages of captured pirates.\n\nAccording to the UN Office on Drugs and Crime, the UN Contact Group on Piracy off the Coast of Somalia is in the midst of drafting an apprehension and transfer protocol for juvenile pirates who are captured by international navies operating in the Horn of Africa region.\n\nAll three of the main Somali sub-regions (i.e., South-Central, Puntland and Somaliland) have signed memoranda of understanding (MoUs) with the governments of Mauritius and the Seychelles, allowing for the transfer of convicted pirates to prison facilities in Somalia. These MoUs, however, do not specify clear standard operating procedures for processing and/or transferring pirates who are determined to be under the age of 18. Kenya is also trying suspected Somali pirates but no prisoner transfer agreement is currently in place.\n\nAccording to UNICEF-Somalia, some 100 child pirates are currently being detained in a prison in Bosaso, Puntland. A second prison for juvenile pirates with a capacity of 100 is being built in Garoowe, while a 70-person prison for juvenile pirates is being built in Somaliland.\n\nThe Republic of Somaliland’s Juvenile Justice Law (2007) raised the age of criminal responsibility for 14 to 15 years and established separate judicial mechanisms for minors, such as child-specific courts of first instance and appellate courts. However, Somaliland’s Law on Combating Piracy (2012) does not make any specific reference to persons under the age of 18.\n\nThe Somali sub-region of Puntland does not have any specific legislation pertaining to child protection, though as of April 2013, a policy on orphans and other vulnerable children had been tabled in Parliament.\n\nIndia’s Piracy Bill (2012) does not make any specific reference to persons under the age of 18.\n\nAccording to the Roméo Dallaire Child Soldiers Initiative:\n\nOne of the most egregious instances of child maritime piracy off the coast of Somalia is recounted on the Canadian Naval Review’s Broadsides forum:\n\nThe \"Vega 5\" hijacking does not appear to be an isolated incident. The Broadsides article continues to report that:\n\nThe Roméo Dallaire Child Soldiers Initiative has determined that a significant proportion of those Somali pirates who are currently being tried internationally are actually under the age of 18. In India, 38 out of 61 pirates (62%) facing trial are juveniles; in Germany, 3 out of 10 (30%) are children; and in the United States of America, all three Somali pirates currently facing trial are under the age of 18.\n\nThe Roméo Dallaire Child Soldiers Initiative has noted that:\n"}
{"id": "28740906", "url": "https://en.wikipedia.org/wiki?curid=28740906", "title": "Department of Musicology (Palacký University, Faculty of Philosophy)", "text": "Department of Musicology (Palacký University, Faculty of Philosophy)\n\nThe Department of Musicology is an institute of Palacký University Faculty of Philosophy, which conducts research and provides education in the fields of musicology and fine art. While the Faculty of Philosophy dates back to 1576 and has provided education for a number of art theorists, the separate Department of Musicology was established in 1946. It is sited in the building of University Art Centre (former Jesuit College), together with other two art departments of Faculty of Philosophy and two art departments of Faculty of Education.\n\nEstablished in 1573, the Palacký University of Olomouc has a long history of musical scholarship. Olomouc prominently excels in the sphere of classical music (J. H. Gallus, W. A. Mozart, A. Dvořák, G. Mahler, J. Kubelík, E. Destinová, who all spent some part of their lives in Olomouc). Among the Olomouc University graduates are both art theorists (Karel Slavíček, Raphael Georg Kiesewetter, Rudolf Eitelberger, etc.) as well as composers (Pavel Vranický, Pavel Křížkovský, Eduard Schön, Emil Viklický, etc.).\n\nThe modern tradition of musicological research and education goes back to 1946 when the \"Institute of Music Education and Science\" was established at the Faculty of Philosophy by Robert Smetana (1904–1988). 1960’s & 70’s saw a strong growth of the department (since 1972 led by Vladimír Hudec; however, the institute was transformed into the \"Department of Music Education\" and transferred to the Faculty of Education in 1980 by the decision of the Ministry of Education.\n\nSoon after the fall of the communist régime in 1989, the musicology returned to the ground of Philosophical Faculty, this time as part of the \"Department of Combined Art Studies\". Independent Department of Musicology was renewed in 1992 and through the enthusiasm of excellent professionals grew into a modern academic institution. Since 2002 it is sited in the \"University Art Centre\", former Jesuit College building, together with another two art departments of the Faculty of Philosophy and two art departments of the Faculty of Education.\n\nResearch has a fundamental role for the Department. The Department hosts the annual \"International Musicological Conference\" (usually late November) and regular public workshops and seminars which are enriched by occasional activities such as conference on \"Zdeněk Fibich - the 19th Century Middle European Composer\" held in spring 2010.\n\nThe Department’s activities embrace a wide variety of modes of research (Regional and Historical Studies, Organology, Critical Edition of Musical Sources, Jazz, Art Rock, Film Music, and so on). Department’s research is supported by a major funds from the European Union and Czech Government (principal among these are the \"Correspondence of Vítězslav Novák between 1890-1905\", \"Correspondence of Vítězslav Novák between 1906-1949\", \"Zdeněk Fibich’s 19th century Operas\", \"Moravia and the World\" ...). Department participated on \"Music Media Training Programme For the Czech Republic (1995 - 1998)\" and \"Musical Life in Europe (1997 - 2001)\". Since 1997 the Department is linked to Saint Cloud State University through the \"Art and Music in the Czech Republic Programme\".\n\nThe Department also benefits from a number of collaborative research and educational partnerships, including those with\n\nDuring last decades the Department provided a forum for a number of academics and visiting lecturers (i.e. Charles Ansbacher, Michael Beckerman, Greg Hurworth, Thomas Christensen, Jobst Fricke, Kenton Frohrip, David Benjamin Levy, Graham Melville-Mason, Scott L. Miller, Christopher Shultis and others).\n\nAs part of the University's Faculty of Philosophy, the Department of Musicology offers Bachelor's, Master's and Doctoral degrees in the fields of Musicology and Arts integration.\n\nThe Department holds regular concerts and musical festival as well (i.e. festival of contemporary music MusicOlomouc).\n\nDuring its history department possessed a rather unusual roster of musicologists whose interest covered a great variety of scientific issues (namely Robert Smetana, Vladimír Hudec, Vladimír Gregor, František Kratochvíl, Libor Melkus, Gustav Pivoňka, Josef Schreiber (musicologist), Luděk Zenkl, Pavel Čotek, Ivan Poledňák, Mikuláš Bek, Miroslav K. Černý, Jiří Fukač, Jaroslav Jiránek, Václav Kučera, Stanislav Tesař, Vladimír Tichý, Jiří Sehnal, Miloš Štědroň, Vlastislav Matoušek). Many graduates have become leading scholars and prominent musicians (Leo Jehne, Michal Chrobák, Jan Kapusta, Pavel Klapil, Jiří Pavlica, Stanislav Pecháček).\n\n\nSince 2002 the Department of Musicology PhF PU Ol is located in renovated former Jesuits’ Convict, which houses Computer Lab, two large and one small Lecture Halls, three Noise-proof Studios, Chapel, Library and Archive and number of offices, designed to accommodate all its needs in the matter of educational and research activities\n\n\n\n\n"}
{"id": "47884159", "url": "https://en.wikipedia.org/wiki?curid=47884159", "title": "Dialect comedy", "text": "Dialect comedy\n\nThe term \"Dialect comedy\" was coined by David Marc in his essay, \"Origins of the genre\". Dialect comedies are a genre of radio (and later television) sitcoms that were popular between the 1920s and the 1950s. They relied on the exaggerated and highly stylized portrayal of stereotypes, usually based on ethnic humor. The genre has its roots on the vaudeville stage and in the minstrel shows that became popular in the 19th century. The ethnicities of the actual actors portraying the dialects did not have to match the characters; while much Jewish dialect comedy was created and portrayed by actual Jews, other dialect comedies, such as those involving blackface, were often not.\nProbably the most enduring program in this genre, Amos N Andy had two feet planted firmly in the minstrel tradition. In 1928, Freeman Gosden and Charles Correll, two white performers took their blackface act, Sam N Henry, which they did not have the rights to and changed it to Amos N Andy, which became one of the most popular shows of the time. In 1948 Variety proclaimed the show “a national habit, almost as familiar as radio itself.”\nBy the time of its final broadcast in 1960, the show had been renamed Amos N Andy Music Hall.\n\nMama (alternately “I Remember Mama”) was a series about a Norwegian family living in San Francisco, CA starring Peggy Wood, Judson Laire and Rosemary Rice.\n\n\"Beulah\" was a radio series on CBS from 1945–1954 and a TV show on ABC from 1950-1952. It was the first series to feature an African American woman in the main role. Beulah drew much of the same criticism from the black community as Amos N Andy. The show was criticized for its portrayal of the “Mammy” or the happy go lucky black servant who wanted nothing more from life than to serve white folks. When it first hit the radio waves, the character of Beulah was voiced by her creator, Marlin Hurt, a white male; the character was originally introduced nationwide as a supporting character on \"Fibber McGee and Molly\" before getting her own show. After Hurt's unexpected death in 1946, the character of Beulah was played by Bob Corley, another white man. In 1947, African American actress Hattie McDaniel took over, followed by two beautiful African American sisters Lillian Randolph and later Amanda Randolph. The TV version of Beulah was portrayed by Ethel Waters and Louise Beavers.\n\nOften overlooked in modern times were regional dialects that were not directly based on ethnic humor. Hillbilly humor was a somewhat common form of dialect comedy during the radio era, with shows such as \"Lum and Abner\" using the genre to full effect. Southerners (especially haughty aristocrats from the South and their distinctive accents) were another target of dialect comedy, with one of the most famous examples being fictional U.S. Senator Beauregard Claghorn from \"The Fred Allen Show\".\n\nOccasional use of dialect comedy continued even after political correctness led to the dramatic decline of its usage in the 1950s. José Jiménez, a fictional Bolivian immigrant who was created by non-Hispanic comedian Bill Dana, remained popular as a guest act well into the 1960s; Dana never intended the character to be offensive and killed him off in the late 1970s, complete with a mock funeral. The 1980s sitcom \"The Golden Girls\" included Swedish dialect humor in the form of Betty White's Rose Nylund character, who fit early 20th-century stereotypes of the good-hearted but dense Swede.\n"}
{"id": "4861797", "url": "https://en.wikipedia.org/wiki?curid=4861797", "title": "Dramatistic pentad", "text": "Dramatistic pentad\n\nThe dramatistic pentad forms the core structure of dramatism, a method for examining motivations that the renowned literary critic Kenneth Burke developed. Dramatism recommends the use of a metalinguistic approach to stories about human action that investigates the roles and uses of five rhetorical elements common to all narratives, each of which is related to a question. These five rhetorical elements form the \"dramatistic pentad.\" Burke argues that an evaluation of the relative emphasis that is given to each of the five elements by a human drama enables a determination of the motive for the behaviour of its characters. A character's stress on one element over the others suggests their world view.\n\nBurke introduced the pentad in his 1945 book \"A Grammar of Motives\". Burke based his pentad on the scholastic hexameter which defines \"questions to be answered in the treatment of a topic: Who, what, where, by what means, why, how, when\". Burke created the pentad by combining several of the categories in the scholastic hexameter. The result was a pentad that has the five categories of: act, scene, agent, agency, and purpose. Burke states, \"The 'who' is obviously covered by agent. Scene covers the 'where' and the 'when'. The 'why' is purpose. 'How' and 'by what means' fall under agency. All that is left to take care of is act in our terms and 'what' in the scholastic formula\".\n\nThe pentad also closely follows the journalistic 'Five Ws': who, what, when, where, why. 'Who' maps to agent. 'What' maps to action. 'When' and 'Where' map to scene. 'Why' maps to purpose. There is no direct mapping from the Five Ws to the pentads category of agency but Geoff Hart states \"Some authorities add a sixth question, “how”, to this list, but “how to” information generally fits under what, where, or when, depending on the nature of the information.\" \n\nThe dramatistic pentad comprises the five rhetorical elements: Act, Scene, Agent, Agency, and Purpose. The “world views” listed below reflect the prominent schools of thought during Burke’s time, “without dismissing any of them,\" to read them \"at once sympathetically and critically in relation to each other,\" and \"in a wider context than any of them recognizes.\"\n\nAct, which is associated with dramatic action verbs and answers the question \"what?\", is related to the world view of realism; What happened? What is the action? What is going on? What action; what thoughts? Burke defines the act as that which “names what took place, in thought or deed.” Since an act is likely composed of many separate actions, Burke states that “any verb, no matter how specific or general, that has connotations of consciousness or purpose falls under this category.”\n\nScene, which is associated with the setting of an act and answers the questions \"when?\" and \"where?\", is related to the world view of materialism and minimal or non-existent free will. Burke defines the scene as “the background of an act, the situation in which it occurred.” \n\nAgent, which answers the question \"by whom?\", reflects the world view of philosophical idealism. Burke defines the agent as “what person or kind of person performed the act.” \n\nAgency (means), which is associated with the person or the organization that committed the deed and answers the question \"how?\", implies a pragmatic point of view. Burke defines agency as “what instrument or instruments he used.”\n\nPurpose, which is associated with meaning and answers the question \"why?\", indicates that the agent seeks unity through identification with an ultimate meaning of life. Reflects the world view of mysticism. Purpose is inextricably linked to the analysis of “Motive” which, derived from the title of A Grammar of Motives, is the main subject of its analysis. Since Purpose is both the subject of analysis and an element of the dramatistic pentad, it is not a common element to be included in a ratio.\n\nIn A Grammar of Motives, Burke provides a system of analysis for the Scene-Act ratio and the Scene-Agent ratio, though his analysis of ratios in the analysis of rhetoric is not limited to these. He states that, “The principles of dramatic consistency would lead one to expect such cases of overlap among the terms; but while being aware of them we should firmly fix in our minds such cases as afford a clear differentiation. Our terms leaning themselves towards merger and division, we are here trying to divide two of them while recognizing their possibilities of merger.” Thus any two dramatistic elements can be analyzed in relation to each other, creating a ratio, and can produce individual, yet separate meanings which are equally valid. However, the rhetor’s selection of elements to compose a ratio should be scrutinized, as it can deflect attention away from or direct attention towards aspects of the rhetor’s desire.\n\nThis is what Burke calls the “Ubiquity of the Ratios,” claiming that the composition of ratios “are at the very centre of motivational assumptions,” For example, “The maxim ‘terrain determines tactics’ is a strict localization of the scene-act ratio, with ‘terrain’ as the casuistic equivalent for ‘scene’ in a military calculus of motives, and ‘tactics’ as the corresponding ‘act.’\" The analysis of a situation as a multi-faceted occurrence is central to Burke’s concept of ratios. \nLikewise, the substitution of a dramatistic element with another can change an interpretation of the motive, allowing the analyst to modify the ratio in order to highlight the importance of a specific factor. For example, “the resistance of the Russian armies to the Nazi invasion could be explained ‘scenically’ in terms of the Soviet political and economic structure; or one could use the act-agent ratio, attributing the power and tenacity to ‘Russian’ traits of character. However, in deriving the act from the scene, one would have to credit socialism as a major scenic factor, whereas a derivation of the act from agents would allow for a much more felicitous explanation from the standpoint of capitalist apologists.”\n\nThe rhetor-agent also has a significant amount of power in crafting the perception of these ratios to their effect, “If an agent acts in keeping with his nature as an agent (act-agent ratio), he may change the nature of the scene accordingly (scene-act ratio) and thereby establish a state of unity between himself and his world (scene-agent ratio).\"\n\nThe scene, or the setting, will contain the act, or the what/actions. The way in which the agent interacts with the scene draws out specific analyses usually related to the atmosphere of the setting. While Burke states that, “It is a principle of drama that the nature of acts and agents should be consistent with the nature of the scene,” some “comic and grotesque works may deliberately set these elements at odds with one another, audiences make allowances for such liberty, which reaffirms the same principle of consistency in its very violation.” This reflects the division of most plays into very literal “acts,” as the agents who construct the scene are very literally “acters” whose work is played against the backdrop of the scene.The actions that a person performs are interpreted through the setting or happenings.\n\nA relationship between the agent (person) and the scene (place or setting). A scene can pose restrictions on the agent; in a narrative, the person and place should have some connection.\n\nWhile requiring the scene to contain them, the agent does not contain the act. Burke states that, “the agent is an author of his acts,” which can “make him or remake him in accordance with their nature.” This is one of the main principles that separates act and agent, producing a linked cycle which constructs the presentation of the agent’s identity.\n\nIn A Grammar of Motives, Burke mentions that attitude is “a state of mind,” remaining exterior from the pentad, as it is often “a preparation for an act, which would make it kind of symbolic act, or incipient act. But in its character as a state of mind that may or may not lead to an act, it is clearly classified under the head of agent.”. Burke also refers to attitude as a variation of agency. Prior to 1969 the term attitude is placed under agency, act or agent and is not considered a sixth element. In 1969, Burke places attitude as a new sixth element, yet he does not refer to the pentad as a hexad. Attitude is defined as \"the preparation for an act, which would make it a kind of symbolic act, or incipient act.\" The attitude, like the agency, would answer the “how?”. There is still some uncertainty over whether attitude is an additional element, and thus makes the dramatistic pentad a hexad, or if it is still a sub-element of the original pentad. While Burke has never directly stated that the pentad is a hexad, he has admitted that the addition of attitude to the pentad is similar to gaining “another soul” or an “extra existence.” Scholars have read this as Burke’s desire not to “dispose of” ambiguity, but rather to “study and clarify the resources of ambiguity.” As a result, many diagrams of the pentad depict attitude as a derivative of agent.\n"}
{"id": "8395427", "url": "https://en.wikipedia.org/wiki?curid=8395427", "title": "Economics of the arts and literature", "text": "Economics of the arts and literature\n\nEconomics of the arts and literature or cultural economics (used below for convenience) is a branch of economics that studies the economics of creation, distribution, and the consumption of works of art, literature and similar creative and/or cultural products. For a long time, the concept of the \"arts\" were confined to visual arts (e.g., painting) and performing arts (music, theatre, dance) in the Anglo-Saxon tradition. Usage has widened since the beginning of the 1980s with the study of cultural industry (cinema, television programs, book and periodical publishing and music publishing) and the economy of cultural institutions (museums, libraries, historic buildings). The field is coded as in the \"Journal of Economic Literature\" classification system used for article searches.\n\nCultural economics is concerned with the arts in a broad sense. The goods considered have creative content, but that is not enough to qualify as a cultural good. Designer goods such as clothes and drapes are not considered usually to be works of art or culture. Cultural goods are those with a value determined by symbolic content rather than physical characteristics. (For further considerations, see also Cultural Institutions Studies). Economic thinking has been applied in ever more areas in the last decennia, including pollution, corruption and education.\n\nWorks of art and culture have a specific quality, which is their uniqueness. While other economic goods, such as crude oil or wheat are generic, interchangeable commodities (given a specific grade of the product), there is only one example of a famous painting such as the Mona Lisa, and only one example of Rodin's well-known sculpture \"The Thinker\". While copies or reproductions can be made of these works of art, and while many inexpensive posters of the Mona Lisa and small factory-made replicas of \"The Thinker\" are sold, neither full-size copies nor inexpensive reproductions are viewed as substitutes for the real artworks, in the way that a consumer views a pound of Grade A sugar from Cuba as a fully equivalent substitute for a pound of Grade A sugar from United States or Dominican Republic. As there is no equivalent item or substitute for these famous works of art, classical economist Adam Smith held it was impossible to value them. Alfred Marshall noted that the demand for a certain kind of cultural good can depend on its consumption: The more you have listened to a particular kind of music, the more you appreciate. In his economic framework, these goods do not have the usual decreasing marginal utility.\n\nKey academic works in cultural economics include those of Baumol and Bowen (Performing Arts, The Economic Dilemma, 1966), of Gary Becker on addictive goods, and of Alan Peacock (public choice). This summary has been divided into sections on the economic study of the performing arts, on the market of individual pieces of art, the art market in cultural industries, the economics of cultural heritage and the labour market in the art sector.\n\nThe seminal paper by William Baumol and Bowen introduced the term cost disease for a relative cost growth of live performances. This cost growth explains the increasing dependency of this kind of art on state subsidies. It occurs when the consumable good is labour itself. To understand this phenomenon, compare the change in the cost of performing the Molière play \"Tartuffe\" in 1664 and in 2007 with the change in cost of calculating a large number of sums from an accounting ledger. In 1664, you needed two hours and twelve actors to perform Molière's play, and it would take, say, twelve accountants working for two hours to add up all the sums in an accounting ledger. In 2007, a single accountant with a $10 calculator can add the sums in 20 minutes, but you still need two hours and twelve actors for the Molière play. Artists must make a considerable investment in human capital (e.g., training), and needs to be paid accordingly. The artists' pay needs to rise along with that of the population in general. As the latter is following the general productivity in the economy, the cost of a play will rise with general productivity, while the actors' productivity does not rise.\n\nThere are two lines of thought in subsequent literature on the economics of the performing arts:\n\n\nTwo segments of the market in the visual arts can be distinguished: works of art that are familiar and have a history, and contemporary works that are more easily influenced by fashion and new discoveries. Both markets, however, are oligopolistic, i.e., there are limited numbers of sellers and buyers (oligopsony). Two central questions on the working of the markets are: How are prices determined, and what is the return on artworks, compared to the return on financial assets?\n\nComponents of a work of art, like raw stone, tubes of paint or unpainted canvas, in general have a value much lower than the finished products, such as a sculpture or a finished painting. Also, the amount of labour needed to produce an item does not explain the big price differences between works of art. It seems that the value is much more dependent on potential buyers', and experts' perception of it. This perception has three elements: First, social value, which is the social status the buyer has by owning it. The artist thus has an \"artistic capital\". Second, the artistic value, compared to contemporary works, or as importance to later generations. Third, the price history of the item, if a buyer uses this for his expectation of a future price at which he might sell the item again (given the oligopolistic market structure).\nThree kinds of economic agents determine these values. Specific experts like gallery owners or museum directors use the first, social value. Experts like art historians and art professors use the second, artistic value. Buyers who buy works of art as an investment use the third, the price history and expectations for future price increases.\n\nSome major financial institutions, banks and insurance companies, have had considerable return rates on investments in art works in the 1990s. These rates have not slowed down at the same time as the rates on stock exchanges, in the early 90's. This may indicate a diversification opportunity. Apart from this evidence of successful investment, the amount of data available has stimulated study of the market. Many works are sold on auctions. These transactions are thus very transparent. This has made it possible to establish price databases, with prices of some items going back to 1652. Empirical studies have shown that, on average, the return on works of art has been lower than that on equity, with a volatility that is at least as high. An intangible gain in terms of pleasure of having a work of art could explain this partly. However, before interpreting the figures, it should be borne in mind that art is often exempt of many kinds of taxes. In 1986, Baumol made an estimate of an average yearly rate of return of 0.55 percent for works of art, against a rate of return of 2.5 percent for financial assets, over a 20-year period.\n\nSome famous artworks such as the \"Mona Lisa\" painting are not reproducible (at least in the sense of creating another copy that would be seen as equivalent in value), but there are many cultural goods whose value does not depend on a single, individual copy. Books, recordings, movies get some of their value from the existence of many copies of the original. These are the products of major cultural industries, which are the book industry, the music industry and the film industry. These markets are characterized by:\n\nThe important cultural industries tend to have an oligopolistic market structure. The market is dominated by a few major companies, with the rest of the market consisting of many small companies. The latter may act as a filter or as \"gatekeepers\" for the artistic supply. A small company with a successful artist or good quality roster can be bought by one of the major companies. Big conglomerates, pooling TV and film production, have existed for decades. The 1990s have seen some mergers extending beyond the industry as such, and mergers of hardware producers with content providers. Anticipated gains from synergy and market power have not been realised, and from the early 2000s there has been a trend towards organisation along sector lines.\n\nCultural heritage is reflected in goods and real estate. Management and regulation of museums has come under study in this area.\n\nMuseums, which have a conservatory role, and provide exhibitions to the general public, can be commercial, or on a non-profit base. In the second case, as they provide a public good, they pose the problems related to these goods: should they be self-financing, or be subsidized ? One of the specific issues is the imbalance between the huge value of the collections in museums, and their budgets. Also, they are often located in places (city centres) where the cost of land is high, which limits their expansion possibilities. American museums exhibit only about half of their collection. Some museums in Europe, like the Pompidou Centre in France, show less than 5 percent of their collection. Apart from providing exhibitions, museums get proceeds from derived products, like catalogues and reproductions. They also produce at a more intangible level: They make collections. Out of so many pieces in the public domain, they make a selection based on their expertise, thus adding value to the mere existence of the items.\n\nThe dual goal of conservation and providing exhibitions obviously presents a choice. On the one hand the museum has, for conservation reasons, an interest in exhibiting as few items as possible, and it would select lesser known works and a specialized audience, to promote knowledge and research. On the other hand, the exhibition argument requires showing the major pieces from different cultures, to satisfy the demands from the public and to attract a large audience. When a government has made a choice about this, application of economic contract theory will help to implement this choice by showing how to use incentives to different managers (on the financial, conservatory side) to obtain the required result.\n\nMany countries have systems that protect historically significant buildings and structures. These are buildings or other structures that are deemed to have cultural importance or which are deemed to have heritage value. Owners get tax deductions or subsidies for restoration, in return for which they accept restrictions on modifications to the buildings or provide public access. Buildings that are often classified as heritage buildings include former or current Parliament buildings, cathedrals, courthouses, houses built in a recognized historical style, and even fairly regular houses, if the house was formerly the home of a famous politician, artist or inventor. Buildings with heritage status cannot typically be demolished. Depending on the nature of the heritage restrictions, the current owner may or may not be allowed to modify the outside or inside of the building. Such a system poses the same choice problems as museums do. There has been little study of this issue.\n\nThe labour market for artists is characterized by:\n\nThe term \"star system\", coined by Sherwin Rosen, is used to explain why a very small number of the artists and creators in the market, such as the celebrity A-list actors and top pop singers, earn most of the total earnings in a sector. Rosen's 1981 paper examined the economics of superstars to determine why “relatively small numbers of people earn enormous amounts of money and seem to dominate the fields in which they engage.” Rosen argues that in superstar markets, \"small differences in talent at the top of the distribution will translate into large differences in revenue.\" Rosen points out that \"...sellers of higher talent charge only slightly higher prices than those of lower talent, but sell much larger quantities; their greater earnings come overwhelmingly from selling larger quantities than from charging higher prices\".\n\nIn cultural industries, the uncertainty about the quality of a product plays a key role in this. The consumer does not really know how good the product is, until he or she has consumed it (think of a movie), and the producer is confronted with the typical uncertainty in a cultural industry. The consumer looks for guidance in the price, reputation, or a famous name on the cover or poster. As the producer understands this using a famous director, actor or singer affects demand, he or she is prepared to pay a lot for a name considered a sign of quality (a star). Indeed, authors like Adler and Ginsburgh have given evidence that star status is determined by chance: in a musical contest, results were highly correlated with the order of performance. This randomness has been used to explain why the labor supply in the sector remains excessive: given the extreme gains of a star, and an irrational behaviour, or particular preferences, with respect to chance, unsuccessful artists keep trying, even when they are earning their money mostly in a different trade, such as waiting tables. A second argument is the possibility of intangible returns to artists' labour in terms of social status and lifestyle. For example, even a struggling DJ spends most of her time onstage on nightclubs and raves, which for some people is a desirable outcome.\n\nA case has been made for the existence of a different structure in the production of cultural goods . (See Cultural Institutions Studies.) An artist often considers a product to be an expression of himself, while the ordinary craftsperson is only concerned with his product, as far as it affects his/her pay or salary. For example, a painter who creates artworks that are displayed in museums may view her paintings as her artistic expression. On the other hand, a scene painter for a music theatre company may see herself as a craftsperson who is paid by the hour for doing painting. The artist may thus want restrict the use of his or her product, and he/she may object if a museum uses a reproduction of his/her painting to help sell cars or liquor. On the other hand, the scene painter may not object to commercial re-uses of her set painting, as she/he may see it just as a regular job.\n\n\n\n"}
{"id": "20122898", "url": "https://en.wikipedia.org/wiki?curid=20122898", "title": "Euromuse", "text": "Euromuse\n\neuromuse.net is an exhibition web portal, that, being a project of certain European museums, informs about the exhibitions of that museums, that mostly are based on cultural history and the history of arts of Europe.\n\nThe portal collects information about exhibitions, institution and collections of European museums on its website. In February 2014 information about 540 museums from 28 countries were collected. These museums are, for example the Louvre (Paris), the Uffizi (Florence), the Berlin State Museums (Berlin) and the Jewish Museum in Prague. All information about exhibitions and museums can be read in English and the national language of the museum.\n\nThe portal was developed by the National Gallery (London), the Musée du Louvre (Paris), the , Paris, the Museum of Art History (Vienna), the Rijksmuseum (Amsterdam), the Statens Museum for Kunst (Kopenhagen) and the Berlin State Museums.\n\nIn an online database, there is the possibility to research current exhibitions and over 2.970 past exhibitions. Also it includes information related to the educational and online offerings of the participating institutions.\n\nThe editing of \"euromuse.net\" takes place at the Institute for Museum Research in Berlin. Each member manages their own data to be published in \"euromuse.net\".\n\n"}
{"id": "405318", "url": "https://en.wikipedia.org/wiki?curid=405318", "title": "Expertization", "text": "Expertization\n\nExpertization is the process of authentication of an object, usually of a sort that is collected, by an individual authority or a committee of authorities. The expert, or expert committee, examines the collectible and issues a certificate typically including:\n\n\nSome experts apply a mark or signature to the item attesting its genuineness.\n\nExpertization is particularly common with valuable philatelic items, some of which are so often forged that they may be unsaleable without it.\n\n"}
{"id": "40853646", "url": "https://en.wikipedia.org/wiki?curid=40853646", "title": "Facilitas", "text": "Facilitas\n\nFacilitas /facilitas/ is facility in devising appropriate language to fit any speaking or writing situation.\n\nThe art of facilitas was most notably taught by Quintilian, the Roman rhetorician, in the latter part of the first century A.D. (c. 35 – c. 100). In Quintilian's Institutio Oratoria, Quintilian summarizes the Roman educational system. In this system, students, generally young boys, were trained daily from the age of six to about eighteen. There were two levels of masters who taught the children: the grammaticus, who helped children with imitations, speaking and writing exercises, and the rhetor, who prepared students for the final stage of declamation, when they gave fictitious speeches. The ultimate goal of Quintilian’s curriculum was for men to have facilitas: the ability to speak extemporaneously on any subject at any time. Quintilian taught the art of rhetoric, or effective communication, to his students in order for them to gain knowledge, as well as develop their analytical and political skills, in efforts to make them \"good citizens.\"\n"}
{"id": "14361510", "url": "https://en.wikipedia.org/wiki?curid=14361510", "title": "Figurae", "text": "Figurae\n\nFigurae (singular, \"figura\") are the non-signifying constituents of signifiers (signs). For example, letters of the alphabet are the figurae that comprise a written word (signifier). In the semiotic language of Louis Hjelmslev, the coiner of this term, figurae serve only to distinguish elements (e.g. words) of the expression plane from each other, independently from the content plane. That is, the letter B, in the written word expression \"bat\", distinguishes \"bat\" from the word \"sat\", but neither B nor S bears meaning on its own. On the other hand, the constituents \"foot\" and \"ball\" both bear their own individual meanings, such that in the word \"football\", they cannot be considered figurae, although their individual letters can. Hjelmslev states that in a given language a \"legion of signs\" can be constructed with a \"handful of figurae\" through ever new arrangements of them. Linguists often use the terms phonemes and morphemes to refer, respectively, to the figurae and the signifiers of human languages.\n\nThe division of the stream of speech into meaningful morphemes plus their further subdivision into meaningless elements is known as the double articulation. This duality of patterning of language is one of the few facts of language which most schools of linguistics can agree on. Occasionally, two morphemes can combine in an arbitrary way into a new morpheme, as in double names such as \"Mary-Alice, John-Paul\", and \"Sarah-Jean,\" creating a kind of triple articulation. English speakers recognize \"Mary\" and \"Alice\" as parts of the name \"Mary-Alice,\" yet they understand that a woman of that name is in no way a combination of two other women. But neither are double given names typical of English, nor are surnames meaningless, since surnames usually identify a family relationship. As far as the combination of meaningful elements is concerned, there is much less agreement on what constitutes a syntagm (e.g. \"foot-ball, I-am\") and whether any such syntax is universal.\n\nIn theory, any sign could be composed of figurae, but care must be taken in distinguishing between the control-number-like function of figurae (as in the individual digits of a telephone extension) and the syntax-like function of meaning constituents (as in the area code of a full telephone number). For example, the symbols for the lines of the New York City subway system are composed of very elementary parts, e.g., letters or numbers and colors. While the assignment of letters to trains is arbitrary, and colors are arbitrarily assigned to various avenues in Manhattan, the combination of a letter or number and color is not arbitrary. That is, the symbol for the A Train has to be blue, since it runs along Eighth Avenue and all other Eighth Avenue train symbols are blue. Therefore, these colors cannot be considered figurae. \n\nOn the other hand, the flags of a dozen countries consist of three horizontal bars, distinguished by their colors. It can be said that the colors and bars form a system of signifiers, consisting of the color figurae in a vertical order. For example, the flag of Russia is made up of a white, a blue, and a red bar, from top to bottom, whereas the flag of Estonia consists of a blue, a black and a white bar. From the point of view of a vexillologist, the colors have no meaning until they are grouped together and form the national symbol. Although white, blue and red may be \"national colors\" of Russia, combined in a different order they form the flag of Luxembourg.\n\nIn reality, most national flags do not conform to the three-horizontal-band pattern. Furthermore, national flags vary somewhat in their horizontal and vertical proportions, and shades of colors can differ. Nevertheless, this logical analysis of flags into horizontal-color-bar figurae, though not exact, would probably be arrived at by almost anyone comparing these 12 national symbols. But it is also possible to over-analyze signs. For example, a television picture of a flag would consist of thousands of meaningless pixels. A recording of speech could be digitalized on a CD into millions of meaningless bits. Neither of these mechanical divisions could be considered figurae. It would seem, then, that since signs are defined as entities recognized by sentient beings (including many animal species), the constituents of signs, figurae, must also be easily recognizable as entities, even though they have no meaning in themselves. It probably requires lot of specialization or intelligence to mentally process figurae, since it demands not only the disassociation of the characteristics of a symbol with those of its referent, but this disassociation has to be repeated for each figura that comprises the arbitrary symbol. Figurae have not yet been recognized in any non-human natural communication system. Although the honeybee waggle dance may involve some arbitrary symbols, they are combined with non-arbitrary ones, much like the subway line symbols.\n"}
{"id": "954079", "url": "https://en.wikipedia.org/wiki?curid=954079", "title": "Film studies", "text": "Film studies\n\nFilm studies is an academic discipline that deals with various theoretical, historical, and critical approaches to films. It is sometimes subsumed within media studies and is often compared to television studies. Film studies is less concerned with advancing proficiency in film production than it is with exploring the narrative, artistic, cultural, economic, and political implications of the cinema. In searching for these social-ideological values, film studies takes a series of critical approaches for the analysis of production, theoretical framework, context, and creation. In this sense the film studies discipline exists as one in which the teacher does not always assume the primary educator role; the featured film itself serves that function. Also, in studying film, possible careers include critic or production. Film theory often includes the study of conflicts between the aesthetics of visual Hollywood and the textual analysis of screenplay. Overall the study of film continues to grow, as does the industry on which it focuses. Academic journals publishing film studies work include \"Sight & Sound\", \"Screen\", \"Cinema Journal\", \"Film Quarterly\" and \"Journal of Film and Video\".\n\nFilm studies as an academic discipline emerged in the twentieth century, decades after the invention of motion pictures. Not to be confused with the technical aspects of film production, film studies exists only with the creation of film theory—which approaches film critically as an art—and the writing of film historiography. Because the modern film became an invention and industry only in the late nineteenth century, a generation of film producers and directors existed significantly before the academic analysis that followed in later generations.\n\nEarly film schools focused on the production and subjective critique of film rather than on the critical approaches, history and theory used to study academically. Since the time film was created, the concept of film studies as a whole grew to analyze the formal aspects of film as they were created. Established in 1919 the Moscow Film School was the first school in the world to focus on film. In the United States the USC School of Cinematic Arts, established in 1929, was the first cinematic based school, which was created in agreement with the Academy of Motion Picture Arts and Sciences. They were also the first to offer a major in film in 1932 but without the distinctions that are assumed in film studies. Universities began to implement different forms of cinema related curriculum without, however, the division between the abstract and practical approaches.\n\nThe \"Deutsche Filmakademie Babelsberg\" (i.e. German Film Academy Babelsberg) was founded in the Third Reich in 1938. Among the lecturers were e.g. Willi Forst and Heinrich George. To complete the studies at the Academy a student was expected to create his own film.\n\nA movement away from Hollywood productions in the 1950s turned cinema into a more artistic independent endeavor. It was the creation of the auteur theory, which asserted film as the director's vision and art, that prompted film studies to become truly considered academically worldwide in the 1960s. In 1965, film critic Robin Wood, in his writings on Alfred Hitchcock, declared that Hitchcock's films contained the same complexities of Shakespeare's plays. Similarly, Jean Luc Godard, a contributor to the influential magazine \"Cahiers du Cinema\" wrote, “Jerry Lewis...is the only one in Hollywood doing something different, the only one who isn’t falling in with the established categories, the norms, the principles. ...Lewis is the only one today who’s making courageous films.\nWith stable enrollment, proper budgets and interest in all humanities numerous universities contained the ability to offer distinct film studies programs.\n\nThere were no individuals that created the criteria for film studies; rather the growing community of the film industry and academics began to criticize, document and analyze films, eventually conforming the concepts of film studies that pertain to artistic academia. With the success in first half of the twentieth century, prominent persons in the film industry could become an endowment source for schools focusing primarily on film, creating the location for film studies as a discipline to form. An example is George Lucas' US$175 million donation to the USC School of Cinematic Arts in 2006.\n\nToday film studies exists worldwide as a discipline with specific schools dedicated to it. The aspects of film studies have grown to encompass numerous methods for teaching history, culture and society. Many liberal arts colleges and universities and contain courses specifically geared toward the analysis of film. Also exemplifying the increased diversity of film studies is the fact that high schools across the United States offer classes on film theory. Many programs conjoin film studies with media and television studies, taking knowledge from all parts of visual production in the approach. With the growing technologies such as 3-D film and YouTube, films are now concretely used to teach a reflection of culture and art around the world as a primary medium. Due to the ever-growing dynamic of film studies, a wide variety of curricula have emerged for analysis of critical approaches used in film. Although each institution has the power to form the study material, students are usually expected to grasp a knowledge of conceptual shifts in film, a vocabulary for the analysis of film form and style, a sense of ideological dimensions of film, and an awareness of extra textual domains and possible direction of film in the future.\n\nThe curriculum of tertiary level film studies programs often include but are not limited to:\nWith diverse courses that make up for film studies majors or minors come to exist within schools.\n\nIn the United States, universities offer courses specifically toward film studies, and schools committed to minor/major programs. Currently 144 different tertiary institutions nationwide offer a major program in film studies. This number continues to grow each year with new interest in the film studies discipline. Institutions offering film degrees as part of their arts or communications curriculum differ from institutions with a dedicated film program. The curriculum is in no way limited to films made in the United States; a wide variety of films can be analyzed. With the United States' film industry second worldwide only to India, the attraction for film studies is high. To obtain a degree in the United States, a person is likely to pursue careers in the production of film, especially directing and producing films. Often classes in the United States will combine new forms of media, such as television or New media, in combination with film study. Those who study film want to be able to analyze the numerous films released in the United States every year in a more academic setting, or to understand the history of cinema as an art form. Films can reflect the culture of the period not only in the United States but around the world.\n\nFilm studies throughout the world exist in over 20 countries. Due to the high cost of film production third world countries are left out of the film industry especially in the academic setting. Despite this fact more prosperous countries have the ability to study film in all the aspects of film studies. Discourse in film can be found in the schools around the world; often, international attention to the aesthetics of film emerge from film festivals. For example, the Cannes Film Festival is the most prestigious in the world. Though this discourse revolves around the film industry and promotion and does not exist within an academic school setting, numerous aspects of analysis and critical approaches are taken into account on this international stage.\n\nThe prominent persons that have influenced the study of film range from teachers to movie producers but can be subsumed into two major categories: persons in film production and persons in film criticism.\n\n\n\n\n\n"}
{"id": "38820108", "url": "https://en.wikipedia.org/wiki?curid=38820108", "title": "Gender gaps in mathematics and reading", "text": "Gender gaps in mathematics and reading\n\nThe gender gaps in mathematics and reading achievement refer to the finding that, on average, the two sexes perform differently in mathematics and reading skills on tests. On average, boys and men exceed in mathematics, while girls and women exceed in reading skills.\n\nThe Programme for International Student Assessment assesses the performance of 15-year-olds in mathematics and reading in OECD and OECD partner countries. The table below lists the scores of the PISA 2009 assessment in mathematics and reading by country, as well as the difference between boys and girls. Gaps in bold font mean that the gender gap is statistically significant (p<0.05). A positive mathematics gap means that boys outperform girls, a negative mathematics gap means that girls outperform boys. A positive reading gap means that girls outperform boys (no country has a negative reading gap). There is a negative correlation between the mathematics and reading gender gaps, that is, nations with a larger mathematics gap have a smaller reading gap and vice versa.\n"}
{"id": "14520306", "url": "https://en.wikipedia.org/wiki?curid=14520306", "title": "Gender inequality", "text": "Gender inequality\n\nGender inequality is the idea and situation that men and women are not equal. Gender inequality refers to unequal treatment or perceptions of individuals wholly or partly due to their gender. It arises from differences in gender roles. Gender systems are often dichotomous and hierarchical. Gender inequality stems from distinctions, whether empirically grounded or socially constructed. Women lag behind men in many domains, including education, labor market opportunities and political representation and in pay.\n\nNatural differences exist between the sexes base on biological and anatomic factors, most notably differing reproductive roles. Biological differences include chromosomes and hormonal differences. There is a natural difference also in the relative physical strengths (on average) of the sexes, both in the lower body and more pronouncedly in the upper-body, though this does not mean that any given man is stronger than any given woman. Men, on average, are taller, which provides both advantages and disadvantages. Women live significantly longer than men, though it is not clear to what extent this is a biological difference - see Life expectancy. Men have larger lung volumes and more circulating blood cells and clotting factors, while women have more circulating white blood cells and produce antibodies faster. Differences such as these are hypothesized to be an adaption allowing for sexual specialization.\n\nPrenatal hormone exposure influences to what extent one exhibits traditional masculine or feminine behavior. No differences between males and females exist in general intelligence. Men are significantly more likely to take risks than women. Men are also more likely than women to be aggressive, a trait influenced by prenatal and possibly current androgen exposure. It has been theorized that these differences combined with physical differences are an adaption representing sexual division of labor. A second theory proposes sex differences in intergroup aggression represent adaptions in male aggression to allow for territory, resource and mate acquisition. Females are (on average) more empathetic than males, though this does not mean that any given woman is more empathetic than any given man. Men and women have better visuospatial and verbal memory, respectively. These changes are influenced by the male sex hormone testosterone, which increases visuospatial memory in both genders when administered.\n\nFrom birth males and females are raised differently and experience different environments throughout their lives. In the eyes of society, gender has a huge role to play in many major milestones or characteristics in life; like personality. Males and females are lead on different paths before they are able to choose their own. The colour blue is most commonly associated with boys and they get toys like monster trucks or more sport related things to play with from the time that they are babies. Girls are more commonly introduced to the colour pink, dolls, dresses, and playing house where they are taking care of the dolls as if they were children. The norm of blue is for boys and pink is for girls is cultural and has not always historically been around. These paths set by parents or other adult figures in the child's life set them on certain paths. This leads to a difference in personality, career paths, or relationships. Throughout life males and females are seen as two very different species who have very different personalities and should stay on separate paths.\n\nThe gender pay gap is the average difference between men's and women's aggregate wages or salaries. The gap is due to a variety of factors, including differences in education choices, differences in preferred job and industry, differences in the types of positions held by men and women, differences in the type of jobs men typically go into as opposed to women (especially highly paid high risk jobs), differences in amount of work experiences, difference in length of the work week, and breaks in employment. These factors resolve 60% to 75% of the pay gap, depending on the source. Various explanations for the remaining 25% to 40% have been suggested, including women's lower willingness and ability to negotiate salary and sexual discrimination. According to the European Commission direct discrimination only explains a small part of gender wage differences.\n\nIn the United States, the \"average\" female's \"unadjusted\" annual salary has been cited as 78% of that of the \"average\" male. However, multiple studies from OECD, AAUW, and the US Department of Labor have found that pay rates between males and females varied by 5–6.6% or, females earning 94 cents to every dollar earned by their male counterparts, when wages were adjusted to different individual choices made by male and female workers in college major, occupation, working hours, and maternal/parental leave. The remaining 6% of the gap has been speculated to originate from deficiency in salary negotiating skills and sexual discrimination.\n\nHuman capital theories refer to the education, knowledge, training, experience, or skill of a person which makes them potentially valuable to an employer. This has historically been understood as a cause of the gendered wage gap but is no longer a predominant cause as women and men in certain occupations tend to have similar education levels or other credentials. Even when such characteristics of jobs and workers are controlled for, the presence of women within a certain occupation leads to lower wages. This earnings discrimination is considered to be a part of pollution theory. This theory suggests that jobs which are predominated by women offer lower wages than do jobs simply because of the presence of women within the occupation. As women enter an occupation, this reduces the amount of prestige associated with the job and men subsequently leave these occupations. The entering of women into specific occupations suggests that less competent workers have begun to be hired or that the occupation is becoming deskilled. Men are reluctant to enter female-dominated occupations because of this and similarly resist the entrance of women into male-dominated occupations. \n\nThe gendered income disparity can also be attributed in part to occupational segregation, where groups of people are distributed across occupations according to ascribed characteristics; in this case, gender. Occupational gender segregation can be understood to contain two components or dimensions; horizontal segregation and vertical segregation. With horizontal segregation, occupational sex segregation occurs as men and women are thought to possess different physical, emotional, and mental capabilities. These different capabilities make the genders vary in the types of jobs they are suited for. This can be specifically viewed with the gendered division between manual and non-manual labor. With vertical segregation, occupational sex segregation occurs as occupations are stratified according to the power, authority, income, and prestige associated with the occupation and women are excluded from holding such jobs.\n\nAs women entered the workforce in larger numbers since the 1960s, occupations have become segregated based on the amount femininity or masculinity presupposed to be associated with each occupation. Census data suggests that while some occupations have become more gender integrated (mail carriers, bartenders, bus drivers, and real estate agents), occupations including teachers, nurses, secretaries, and librarians have become female-dominated while occupations including architects, electrical engineers, and airplane pilots remain predominately male in composition. Based on the census data, women occupy the service sector jobs at higher rates than men. Women’s overrepresentation in service sector jobs, as opposed to jobs that require managerial work acts as a reinforcement of women and men into traditional gender roles that causes gender inequality.\n“The gender wage gap is an indicator of women’s earnings compared with men’s. It is figured by dividing the average annual earnings for women by the average annual earnings for men.” (Higgins et al., 2014) \nScholars disagree about how much of the male-female wage gap depends on factors such as experience, education, occupation, and other job-relevant characteristics. Sociologist Douglas Massey found that 41% remains unexplained, while CONSAD analysts found that these factors explain between 65.1 and 76.4 percent of the raw wage gap. CONSAD also noted that other factors such as benefits and overtime explain \"additional portions of the raw gender wage gap\".\n\nThe glass ceiling effect is also considered a possible contributor to the gender wage gap or income disparity. This effect suggests that gender provides significant disadvantages towards the top of job hierarchies which become worse as a person’s career goes on. The term glass ceiling implies that invisible or artificial barriers exist which prevent women from advancing within their jobs or receiving promotions. These barriers exist in spite of the achievements or qualifications of the women and still exist when other characteristics that are job-relevant such as experience, education, and abilities are controlled for. The inequality effects of the glass ceiling are more prevalent within higher-powered or higher income occupations, with fewer women holding these types of occupations. The glass ceiling effect also indicates the limited chances of women for income raises and promotion or advancement to more prestigious positions or jobs. As women are prevented by these artificial barriers, from either receiving job promotions or income raises, the effects of the inequality of the glass ceiling increase over the course of a woman’s career.\n\nStatistical discrimination is also cited as a cause for income disparities and gendered inequality in the workplace. Statistical discrimination indicates the likelihood of employers to deny women access to certain occupational tracks because women are more likely than men to leave their job or the labor force when they become married or pregnant. Women are instead given positions that dead-end or jobs that have very little mobility.\n\nIn Third World countries such as the Dominican Republic, female entrepreneurs are statistically more prone to failure in business. In the event of a business failure women often return to their domestic lifestyle despite the absence of income. On the other hand, men tend to search for other employment as the household is not a priority.\n\nThe gender earnings ratio suggests that there has been an increase in women’s earnings comparative to men. Men’s plateau in earnings began after the 1970s, allowing for the increase in women’s wages to close the ratio between incomes. Despite the smaller ratio between men and women’s wages, disparity still exists. Census data suggests that women’s earnings are 71 percent of men's earnings in 1999.\n\nThe gendered wage gap varies in its width among different races. Whites comparatively have the greatest wage gap between the genders. With whites, women earn 78% of the wages that white men do. With African Americans, women earn 90% of the wages that African American men do.\n\nThere are some exceptions where women earn more than men: According to a survey on gender pay inequality by the International Trade Union Confederation, female workers in the Gulf state of Bahrain earn 40 percent more than male workers.\n\nIn 2014, a report by the International Labor Organization (ILO) reveals the wage gap between Cambodian women factory workers and other male counterparts. There was a $25 USD monthly pay difference conveying that women have a much lower power and being devalued not only at home but also in the workplace.\n\nThe gender gap also appeared to narrow considerably beginning in the mid-1960s. Where some 5% of first-year students in professional programs were female in 1965, by 1985 this number had jumped to 40% in law and medicine, and over 30% in dentistry and business school. Before the highly effective birth control pill was available, women planning professional careers, which required a long-term, expensive commitment, had to \"pay the penalty of abstinence or cope with considerable uncertainty regarding pregnancy.\" This control over their reproductive decisions allowed women to more easily make long-term decisions about their education and professional opportunities. Women are highly underrepresented on boards of directors and in senior positions in the private sector.\n\nAdditionally, with reliable birth control, young men and women had more reason to delay marriage. This meant that the marriage market available to any women who \"delay[ed] marriage to pursue a career... would not be as depleted. Thus the Pill \"could\" have influenced women's careers, college majors, professional degrees, and the age at marriage.\"\n\nStudies on sexism in science and technology fields have produced conflicting results. Corinne et al. found that science faculty of both sexes rated a male applicant as significantly more competent and hireable than an identical female applicant. These participants also selected a higher starting salary and offered more career mentoring to the male applicant. Williams and Ceci, however, found that science and technology faculty of both sexes \"preferred female applicants 2:1 over identically qualified males with matching lifestyles\" for tenure-track positions. Studies show parents are more likely to expect their sons, rather than their daughters, to work in a science, technology, engineering or mathematics field – even when their 15-year-old boys and girls perform at the same level in mathematics.\n\nA survey by the U.K. Office for National Statistics in 2016 showed that in the health sector 56% of roles are held by women, while in teaching it is 68%. However equality is less evident in other area; only 30% of M.P.'s are women and only 32% of finance and investment analysts. In the natural and social sciences 43% of employees are women, and in the environmental sector 42%.\n\nA 2010 study conducted by David R. Hekman and colleagues found that customers who viewed videos featuring a black male, a white female, or a white male actor playing the role of an employee helping a customer were 19 percent more satisfied with the white male employee's performance.\n\nThis discrepancy with race can be found as early as 1947, when Kenneth Clark conducted a study in which black children were asked to choose between white and black dolls. White male dolls were the ones children preferred to play with.\n\nAlthough the disparities between men and women are decreasing in the medical field, gender inequalities still exist as social problems. From 1999 to 2008, recently qualified female doctors in the US made almost $170,000,000 less than their male counterparts. The pay discrepancy could not be explained by specialty choice, practice setting, work hours, or other characteristics. A case study carried out on Swedish medical doctors showed that the gender wage gap among physicians was greater in 2007 than in 1975.\n\nGender roles are heavily influenced by biology, with male-female play styles correlating with sex hormones, sexual orientation, aggressive traits, and pain. Furthermore, females with congenital adrenal hyperplasia demonstrate increased masculinity and it has been shown that rhesus macaque children exhibit preferences for stereotypically male and female toys.\n\nGender equality in relationships has been growing over the years but for the majority of relationships, the power lies with the male. Even now men and women present themselves as divided along gender lines. A study done by Szymanowicz and Furnham, looked at the cultural stereotypes of intelligence in men and women, showing the gender inequality in self-presentation. This study showed that females thought if they revealed their intelligence to a potential partner, then it would diminish their chance with him. Men however would much more readily discuss their own intelligence with a potential partner. Also, women are aware of people’s negative reactions to IQ, so they limit its disclosure to only trusted friends. Females would disclose IQ more often than men with the expectation that a real true friend would respond in a positive way. Intelligence continues to be viewed as a more masculine trait, than feminine trait. The article suggested that men might think women with a high IQ would lack traits that were desirable in a mate such as warmth, nurturance, sensitivity, or kindness. Another discovery was that females thought that friends should be told about one’s IQ more so than males. However, males expressed doubts about the test’s reliability and the importance of IQ in real life more so than women. The inequality is highlighted when a couple starts to decide who is in charge of family issues and who is primarily responsible for earning income. For example, in Londa Schiebinger’s book, \"Has Feminism Changed Science?\", she claims that \"Married men with families on average earn more money, live longer and happier, and progress faster in their careers,\" while \"for a working woman, a family is a liability, extra baggage threatening to drag down her career.\" Furthermore, statistics had shown that \"only 17 percent of the women who are full professors of engineering have children, while 82 percent of the men do.\"\n\nDespite the increase in women in the labour force since the mid-1900s, traditional gender roles are still prevalent in American society. Women may be expected to put their educational and career goals on hold in order to raise children, while their husbands work. However, women who choose to work as well as fulfill a perceived gender role of cleaning the house and taking care of the children. Despite the fact that different households may divide chores more evenly, there is evidence that supports that women have retained the primary caregiver role within familial life despite contributions economically. This evidence suggest that women who work outside the home often put an extra 18 hours a week doing household or childcare related chores as opposed to men who average 12 minutes a day in childcare activities. One study by van Hooff showed that modern couples, do not necessarily purposefully divide things like household chores along gender lines, but instead may rationalize it and make excuses. One excuse used is that women are more competent at household chores and have more motivation to do them. Another is that some say the demands of the males’ jobs is higher.\n\nThere was a study conducted at an \"urban comprehensive school\". They were asked questions regarding their views in sexual inequality. Many parents were for the equal pay for men and women. They also were in favor for men to help with the housework. In this study, the majority of the people who were interviewed wanted gender equality and more people wants a change in gender roles. Where men stay home, cleans, and cooks while the women can work and help support the family.\n\nGender roles have changed drastically over the past few decades. In the article, it says that in 1920-1966, there was data recorded that women spent the most time care-tending with the home and family. There was a study made with the gender roles with the males and females, The results showed that as women spend less time in the house, men have taken over the role as the mother. The article also said that women who work spend less time within the house and with their children if they have any. Furthermore, men are taking the roles of women in the homes and its changing as time goes on. Robin A. Douthitt, the author of the article, \"The Division of Labor Within the Home: Have Gender Roles Changed?\" concluded by saying, \"(1) men do not spend significnatly more time with chil- dren when their wives are employed and (2) employed women spend signifi- cantly less time in child care than their full-time homemaker counterparts, over a 10-year period both mothers and fathers are spending more total time with children.\" (703).\n\nOne survey showed that men rate their technological skills in activities such as basic computer functions and online participatory communication higher than women. However, it should be noted that this study was a self-reporting study, where men evaluate themselves on their own perceived capabilities. It thus is not data based on actual ability, but merely perceived ability, as participants' ability was not assessed. Additionally, this study is inevitably subject to the significant bias associated with self-reported data.\n\nIn contrary to such findings, a carefully controlled study that analyzed data sets from 25 developing countries led to the consistent finding that the reason why fewer women access and use digital technology is a direct result of their unfavorable conditions and ongoing discrimination with respect to employment, education and income. When controlling for these variables, women turn out to be more active users of digital tools than men. This turns the alleged digital gender divide into an opportunity: given women's affinity for ICT, and given that digital technologies are tools that can improve living conditions, ICT represents a concrete and tangible opportunity to tackle longstanding challenges of gender inequalities in developing countries, including access to employment, income, education and health services.\n\nMany countries have laws that give less inheritance of ancestral property for women compared to men.\n\nGender inequalities often stem from social structures that have institutionalized conceptions of gender differences.\n\nMarginalization occurs on an individual level when someone feels as if they are on the fringes or margins of their respective society. This is a social process and displays how current policies in place can affect people. For example, media advertisements display young girls with easy bake ovens (promoting being a housewife) as well as with dolls that they can feed and change the diaper of (promoting being a mother).\n\nCultural stereotypes, which can dictate specific roles, are engrained in both men and women and these stereotypes are a possible explanation for gender inequality and the resulting gendered wage disparity. Women have traditionally been viewed as being caring and nurturing and are designated to occupations which require such skills. While these skills are culturally valued, they were typically associated with domesticity, so occupations requiring these same skills are not economically valued. Men have traditionally been viewed as the main worker in the home, so jobs held by men have been historically economically valued and occupations predominated by men continue to be economically valued and earn higher wages.\n\nGender Stereotypes influenced greatly by gender expectations, different expectations on gender influence how people determine their roles, appearance, behaviors, etc. When expectations of gender roles deeply rooted in people's mind, people' values and ideas started to be influenced and leading to situation of stereotypes, which actualize their ideas into actions and perform different standards labelling the behaviors of people. Gender stereotypes limit opportunities of different gender when their performance or abilities were standardizing according to their gender-at-birth, that women and men may encounter limitations and difficulties when challenging the society through performing behaviors that their gender \"not supposed\" to perform. For example, men may receive judgments when they trying to stay at home and finish housework and allow their wives to go out and work instead, as men are expected to be work outside for earning money for the family. The traditional concepts of gender stereotypes are being challenged nowadays in different societies and improvement could be observed that men could also be responsible for housework, women could also be construction worker in some societies. It is still a long process when traditional concepts and values have deep-rooted in people's mind, that higher acceptance towards gender roles and characteristics is homely to be gradually developed.\n\nBonnie Spanier coined the term hereditary inequality. Her opinion is that some scientific publications depict human fertilization such that sperms seem to actively compete for the \"passive\" egg, even though in reality it is complicated (e.g. the egg has specific active membrane proteins that select sperm etc.)\n\nGender inequality can further be understood through the mechanisms of sexism. Discrimination takes place in this manner as men and women are subject to prejudicial treatment on the basis of gender alone. Sexism occurs when men and women are framed within two dimensions of social cognition.\n\nDiscrimination also plays out with networking and in preferential treatment within the economic market. Men typically occupy positions of power within the job economy. Due to taste or preference for other men because they share similar characteristics, men in these positions of power are more likely to hire or promote other men, thus discriminating against women.\n\nSonja B. Starr conducted a study in the US that found that the prison sentences that men serve are on average 63% longer than those that women serve when controlling for arrest offense and criminal history. However, the study does not purport to explain why this is the case. Starr does not believe that men are disadvantaged generally. Men's rights advocates have argued that men being over-represented in both those who commit murder and the victims of murder is evidence that men are being harmed by outmoded cultural attitudes.\n\nThe New York Film Academy took a closer look at the women in Hollywood and gathered statistics from the top 500 films from 2007 to 2012, for their history and achievements, or lack of.\n\nThere was a 5:1 ratio of men to women working in films. 30.8% of women having speaking characters, who may or may not have been a part of the 28.8% of women who were written to wear revealing clothing compared to the 7% of men who did, or the 26.2% of women who wore little to no clothing opposed to the 9.4% of men who did the same. A study analyzing five years of text from over 2,000 news sources found a similar 5:1 ratio of male to female names overall, and 3:1 for names in entertainment.\n\nHollywood actresses are paid less than actors. Topping \"Forbes\" highest paid actors list of 2013 was Robert Downey Jr. with $75 million, yet Angelina Jolie topped the highest paid actresses list with only $33 million, which tied with Denzel Washington ($33 million) and Liam Neeson ($32 million), who were the last two on the top ten highest paid actors list.\n\nIn the 2013 Academy Awards, 140 men were nominated for an award, but only 35 women were nominated. No woman was nominated for directing, cinematography, film editing, writing (original screenplay), or original score that year. Since the Academy Awards began in 1929, only seven women producers have won the Best Picture category (all of whom were co-producers with men), and only eight women have been nominated for Best Original Screenplay. Lina Wertmuller (1976), Jane Campion (1994), Sofia Coppola (2004), and Kathryn Bigelow (2012) were the only four women to be nominated for Best Director, with Bigelow being the first woman to win for her film \"The Hurt Locker\". 77% of the Academy Awards' voters are male.\n\nA group of Hollywood actors have launched their own social movement titled #AskMoreOfHim. This movement is built on the basis of men speaking out against sexual misconduct against females.   A number of male activists, specifically in the film industry, have signed an open letter explaining their responsibility in the ownership of their actions, as well as calling out the actions of others. The letter has been signed and supported by Friends actor David Schwimmer, shown above, among many others. The Hollywood Reporter published their support saying, “We applaud the courage and pledge our support to the courageous women — and men, and gender non-conforming individuals — who have come forward to recount their experiences of harassment, abuse and violence at the hands of men in our country. As men, we have a special responsibility to prevent abuse from happening in the first place... After all, the vast majority of sexual harassment, abuse and violence is perpetrated by men, whether in Hollywood or not.” This accountability is set to change the way women are seen and treated in the film and television industry, hopefully ending in the closing of the gap women are experiencing in pay, promotion, and overall respect. This initiative was created in response to the #MeToo movement. The #MeToo movement, started by a single tweet, asked women to share their stories of sexual assault against men in a professional setting. Within one day, 30,000 women had used the hashtag sharing their stories. Many women feel as if they have more power in their voices than they ever had and are choosing to make personal claims that may have been brushed under the rug prior to the internet culture we’re now living in. According to Time Magazine, 95% of women in the film and entertainment industry admit to being sexually harassed by men in their industry. In addition to the #MeToo movement, women in industry are using #TimesUp, with the goal of aiming to help prevent sexual harassment in the workplace for victims who cannot afford their own resources. \n\nGender inequality and discrimination are argued to cause and perpetuate poverty and vulnerability in society as a whole. Household and intra-household knowledge and resources are key influences in individuals' abilities to take advantage of external livelihood opportunities or respond appropriately to threats. High education levels and social integration significantly improve the productivity of all members of the household and improve equity throughout society. Gender Equity Indices seek to provide the tools to demonstrate this feature of poverty.\n\nPoverty has many different factors, one of which is the gender wage gap. Women are more likely to be living in poverty and the wage gap is one of the causes.\n\nThere are many difficulties in creating a comprehensive response. It is argued that the Millennium Development Goals (MDGs) fail to acknowledge gender inequality as a cross-cutting issue. Gender is mentioned in MDG3 and MDG5: MDG3 measures gender parity in education, the share of women in wage employment and the proportion women in national legislatures. MDG5 focuses on maternal mortality and on universal access to reproductive health. These targets are significantly off-track.\n\nAddressing gender inequality through social protection programmes designed to increase equity would be an effective way of reducing gender inequality, according to the Overseas Development Institute (ODI). Researchers at the ODI argue for the need to develop the following in social protection in order to reduce gender inequality and increase growth:\nThe ODI maintains that society limits governments' ability to act on economic incentives.\n\nNGOs tend to protect women against gender inequality and structural violence.\n\nDuring war, combatants primarily target men. Both sexes die however, due to disease, malnutrition and incidental crime and violence, as well as the battlefield injuries which predominately affect men. A 2009 review of papers and data covering war related deaths disaggregated by gender concluded \"It appears to be difficult to say whether more men or women die from conflict conditions overall.\" The ratio also depends on the type of war, for example in the Falklands War 904 of the 907 dead were men. Conversely figures for war deaths in 1990, almost all relating to civil war, gave ratios in the order of 1.3 males per female.\n\nAnother opportunity to tackle gender inequality is presented by modern information and communication technologies. In a carefully controlled study, it has been shown that women embrace digital technology more than men. Given that digital information and communication technologies have the potential to provide access to employment, education, income, health services, participation, protection, and safety, among others (ICT4D), the natural affinity of women with these new communication tools provide women with a tangible bootstrapping opportunity to tackle social discrimination.\n\nGender inequality is a result of the persistent discrimination of one group of people based upon gender and it manifests itself differently according to race, culture, politics, country, and economic situation. It is furthermore considered a causal factor of violence against women. While gender discrimination happens to both men and women in individual situations, discrimination against women is an entrenched, global pandemic. In the Democratic Republic of the Congo, rape and violence against women and girls is used as a tool of war. In Afghanistan, girls have had acid thrown in their faces for attending school. Considerable focus has been given to the issue of gender inequality at the international level by organizations such as the United Nations (UN), the Organisation for Economic Co-operation and Development (OECD), and the World Bank, particularly in developing countries. The causes and effects of gender inequality vary geographically, as do methods for combating it.\n\nOne example of the continued existence of gender inequality in Asia is the \"missing girls\" phenomenon. \"Many families desire male children in order to ensure an extra source of income. In China, females are perceived as less valuable for labor and unable to provide sustenance.\" Moreover, gender inequality also reflected in educational aspect in rural China . Gender inequality existed because of gender stereotypes in rural China, families may consider that is useless for girls to acquire knowledge at school because they will marry someone one day and their major responsibility is to take care of housework. When people have expectations on the gender roles, that considering marriage is the major goals of a girl's life in rural China, gender inequality easily existed to limit the rights and opportunities of women.\n\nA Cambodian said, \"Men are gold, women are white cloth\", emphasizing that women had a lower value and importance compared to men. In Cambodia, approximately 15% (485,000 hectares) of land was owned by women. In Asian culture, there is a stereotype that women usually have lower status than men because males carry on the family name and hold the responsibilities to take care of the family. Females have a less important role, mainly to carry out domestic chores, and taking care of husbands and children. Women are also the main victims of poverty as they have little or no access to education, low pay and low chances owning assets such as lands, homes or even basic items.\n\nIn Cambodia, the Ministry of Women's Affairs (MoWA) was formed in 1998 with the role of improving women's overall power and status in the country.\n\nThe Global Gender Gap Report put out by the World Economic Forum (WEF) in 2013 ranks nations on a scale of 0 to 1, with a score of 1.0 indicating full gender equality. A nation with 35 women and 65 men in political office would get a score of 0.538 as the WEF is measuring the gap between the two figures and not the actual percentage of women in a given category. While Europe holds the top four spots for gender equality, with Iceland, Finland, Norway and Sweden ranking 1st through 4th respectively, it also contains two nations ranked in the bottom 30 countries, Albania at 108 and Turkey at 120. The Nordic Countries, for several years, have been at the forefront of bridging the gap in gender inequality. Every Nordic country, aside from Denmark who is at 0.778, has reached above a 0.800 score. In contrast to the Nordic nations, the countries of Albania and Turkey continue to struggle with gender inequality. Albania and Turkey failed to break the top 100 nations in 2 of 4 and 3 of 4 factors, respectively. However, despite the disparity, European nations continue to make advances in the many factors that are used to determine a nation's gender gap score.\n\nWestern Europe, a region most often described as comprising the non-communist members of post-WWII Europe, has, for the most part been doing well in eliminating the gender gap. Western Europe holds 12 of the top 20 spots on the Global Gender Gap Report for overall score. While remaining mostly in the top 50 nations, four Western European nations fall below that benchmark. Portugal is just outside of the top 50 at number 51 with score of 0.706 while Italy (71), Greece (81) and Malta (84) received scores of 0.689, 0.678 and 0.676, respectively.\n\nA large portion of Eastern Europe, a region most often described as the former communist members of post-WWII Europe, resides between 40th and 100th place in the Global Gender Gap Report. A few outlier countries include Lithuania, which jumped nine places (37th to 28th) from 2011 to 2013, Latvia, which has held the 12th spot for two consecutive years, Albania and Turkey.\n\nIndia ranking remains low in gender equality measures by the World Economic Forum, although the rank has been improving in recent years. When broken down into components that contribute the rank, India performs well on political empowerment, but is scored near the bottom with China on sex selective abortion. India also scores poorly on overall female to male literacy and health rankings. India with a 2013 ranking of 101 out of 136 countries had an overall score of 0.6551, while Iceland, the nation that topped the list, had an overall score of 0.8731 (no gender gap would yield a score of 1.0). Gender inequalities impact India's sex ratio, women's health over their lifetimes, their educational attainment, and economic conditions. It is a multifaceted issue that concerns men and women alike.\n\nThe labor force participation rate of women was 80.7% in 2013. Nancy Lockwood of the Society for Human Resource Management, the world's largest human resources association with members in 140 countries, in a 2009 report wrote that female labor participation is lower than men, but has been rapidly increasing since the 1990s. Out of India's 397 million workers in 2001, 124 million were women, states Lockwood.\n\nIndia is on target to meet its Millennium Development Goal of gender parity in education before 2016. UNICEF's measures of attendance rate and Gender Equality in Education Index (GEEI) attempt to capture the quality of education. Despite some gains, India needs to triple its rate of improvement to reach GEEI score of 95% by 2015 under the Millennium Development Goals. A 1998 report stated that rural India girls continue to be less educated than the boys.\n\nThe World Economic Forum measures gender equity through a series of economic, educational, and political benchmarks. It has ranked the United States as 19th (up from 31st in 2009) in terms of achieving gender equity. The US Department of Labor has indicated that in 2009, \"the median weekly earnings of women who were full-time wage and salary workers was... 80 percent of men's\". The Department of Justice found that in 2009, \"the percentage of female victims (26%) of intimate partner violence was about 5 times that of male victims (5%)\". \"The United States ranks 41st in a ranking of 184 countries on maternal deaths during pregnancy and childbirth, below all other industrialized nations and a number of developing countries\" and women only represent 20% of members of Congress.\n\nExisting research on the topic of gender/sex and politics has found differences in political affiliation, beliefs, and voting behavior between men and women, although these differences vary across cultures. Gender is omnipresent in every culture, and while there are many factors to consider when labeling people \"Democrat\" or \"Republican\"—such as race and religion—gender is especially prominent in politics. Studying gender and political behavior poses challenges, as it can be difficult to determine if men and women actually differ in substantial ways in their political views and voting behavior, or if biases and stereotypes about gender cause people to make assumptions. However, trends in voting behavior among men and women have been proven through research.\n\nResearch shows that women in postindustrial countries like the United States, Canada, and Germany primarily identified as conservative before the 1960s; however, as time has progressed and new waves of feminism have occurred, women have become more left-wing due to shared beliefs and values between women and parties more on the left. Women in these countries typically oppose war and the death penalty, favor gun control, support environment protection, and are more supportive of programs that help people of lower socioeconomic statuses. Voting behaviors of men have not experienced as drastic of a shift over the last fifty years as women in their voting behavior and political affiliations. These behaviors tend to consistently be more conservative than women overall. These trends change with every generation, and factors such as culture, race, and religion also must be considered when discussing political affiliation. These factors make the connection between gender and political affiliation complex due to intersectionality.\n\nCandidate gender also plays a role in voting behavior. Women candidates are far more likely than male candidates to be scrutinized and have their competence questioned by both men and women when they are seeking information on candidates in the beginning stages of election campaigns. Democrat male voters tend to seek more information about female Democrat candidates over male Democrat candidates. Female Republican voters tend to seek more information about female Republican candidates. For this reason, female candidates in either party typically need to work harder to prove themselves competent more than their male counterparts.\n\nOverall, politics in the United States are dominated by men, which can pose many challenges to women who decide to enter the political sphere. As the number of women participants in politics continue to increase around the world, the gender of female candidates serves as both a benefit and a hindrance within their campaign themes and advertising practices. The overarching challenge seems to be that—no matter their actions—women are unable to win in the political sphere as different standards are used to judge them when compared to their male counterparts.\n\nOne area in particular that exemplifies varying perceptions between male and female candidates is the way female candidates decide to dress and how their choice is evaluated. When women decide to dress more masculine, they are perceived as being \"conspicuous.\" When they decide to dress more feminine, they are perceived as \"deficient.\" At the same time, however, women in politics are generally expected to adhere to the masculine standard, thereby validating the idea that gender is binary and that power is associated with masculinity. As illustrated by the points above, these simultaneous, mixed messages create a \"double-bind\" for women. Some scholars go on to claim that this masculine standard represents symbolic violence against women in politics.\n\nPolitical knowledge is a second area where male and female candidates are evaluated differently and where political science research has consistently shown women with a lower level of knowledge than their male counterparts. One reason for this finding is the argument that there are different areas of political knowledge that different groups consider. Due to this line of thought, scholars are advocating the replacement of traditional political knowledge with gender-relevant political knowledge because women are not as politically disadvantaged as it may appear.\n\nA third area that affects women's engagement in politics is their low level of political interest and perception of politics as a \"men's game.\" Despite female candidates' political contributions being equal to that of male candidates, research has shown that women perceive more barriers to office in the form of rigorous campaigns, less overall recruitment, inability to balance office and family commitments, hesitancy to enter competitive environments, and a general lack of belief in their own merit and competence. Male candidates are evaluated most heavily on their achievements, while female candidates are evaluated on their appearance, voice, verbal dexterity, and facial features in addition to their achievements.\n\nSeveral forms of action have been taken to combat institutionalized sexism. People are beginning to speak up or \"talk back\" in a constructive way to expose gender inequality in politics, as well as gender inequality and under-representation in other institutions. Researchers who have delved into the topic of institutionalized sexism in politics have introduced the term \"undoing gender.\" This term focuses on education and an overarching understanding of gender by encouraging \"social interactions that reduce gender difference.\" Some feminists argue that \"undoing gender\" is problematic because it is context-dependent and may actually reinforce gender. For this reason, researchers suggest \"doing gender differently\" by dismantling gender norms and expectations in politics, but this can also depend on culture and level of government (e.g. local versus federal).\n\nAnother key to combating institutionalized sexism in politics is to diffuse gender norms through \"gender-balanced decision-making,\" particularly at the international level, which \"establishes expectations about appropriate levels of women in decision-making positions.\" In conjunction with this solution, scholars have started placing emphasis on \"the value of the individual and the importance of capturing individual experience.\" This is done throughout a candidate's political career—whether that candidate is male or female—instead of the collective male or female candidate experience. Five recommended areas of further study for examining the role of gender in U.S. political participation are (1) realizing the \"intersection between gender and perceptions\"; (2) investigating the influence of \"local electoral politics\"; (3) examining \"gender socialization\"; (4) discerning the connection \"between gender and political conservatism\"; and (5) recognizing the influence of female political role models in recent years. Due to the fact that gender is intricately entwined in every societal institution, gender in politics can only change once gender norms in other institutions change, as well.\n\n\nHiggins, M. and Reagan, M. (n.d). The gender wage gap, 9th ed. North Mankato: Abdo Publishing, pp. 9–11\n"}
{"id": "44706", "url": "https://en.wikipedia.org/wiki?curid=44706", "title": "Genre", "text": "Genre\n\nGenre () is any form or type of communication in any mode (written, spoken, digital, artistic, etc.) with socially-agreed upon conventions developed over time. Genre is most popularly known as a category of literature, music, or other forms of art or entertainment, whether written or spoken, audio or visual, based on some set of stylistic criteria, yet genres can be aesthetic, rhetorical, communicative, or functional. Genres form by conventions that change over time as cultures invent new genres and discontinue the use of old ones. Often, works fit into multiple genres by way of borrowing and recombining these conventions. Stand-alone texts, works, or pieces of communication may have individual styles, but genres are amalgams of these texts based on agreed-upon or socially inferred conventions. Some genres may have rigid, strictly adhered-to guidelines, while others may show great flexibility.\n\nGenre began as an absolute classification system for ancient Greek literature. Poetry (odes, epics, etc.), prose, and performance each had a specific and calculated style that related to the theme of the story. Speech patterns for comedy would not be appropriate for tragedy, and even actors were restricted to their genre under the assumption that a type of person could tell one type of story best.\n\nIn later periods genres proliferated and developed in response to changes in audiences and creators. Genre became a dynamic tool to help the public make sense out of unpredictable art. Because art is often a response to a social state, in that people write/paint/sing/dance about what they know about, the use of genre as a tool must be able to adapt to changing meanings.\n\nGenre suffers from the ills of any classification system. It has been suggested that genres resonate with people because of the familiarity, the shorthand communication, as well as because of the tendency of genres to shift with public mores and to reflect the zeitgeist. While the genre of storytelling has been relegated as lesser form of art because of the heavily borrowed nature of the conventions, admiration has grown. Proponents argue that the genius of an effective genre piece is in the variation, recombination, and evolution of the codes.\n\nThe term \"genre\" is much used in the history and criticism of visual art, but in art history has meanings that overlap rather confusingly. Genre painting is a term for paintings where the main subject features human figures to whom no specific identity attachesin other words, figures are not portraits, characters from a story, or allegorical personifications. These are distinguished from staffage: incidental figures in what is primarily a landscape or architectural painting. \"Genre painting\" may also be used as a wider term covering genre painting proper, and other specialized types of paintings such as still-life, landscapes, marine paintings and animal paintings.\n\nThe concept of the \"hierarchy of genres\" was a powerful one in artistic theory, especially between the 17th and 19th centuries. It was strongest in France, where it was associated with the Académie française which held a central role in academic art. The genres in hierarchical order are:\n\nA literary genre is a category of literary composition. Genres may be determined by literary technique, tone, content, or even (as in the case of fiction) length. Genre should not be confused with age category, by which literature may be classified as either adult, young adult, or children's. They also must not be confused with format, such as graphic novel or picture book. The distinctions between genres and categories are flexible and loosely defined, often with subgroups.\n\nThe most general genres in literature are (in loose chronological order) epic, tragedy, comedy, novel, and short story. They can all be in the genres prose or poetry, which shows best how loosely genres are defined. Additionally, a genre such as satire might appear in any of the above, not only as a subgenre but as a mixture of genres. Finally, they are defined by the general cultural movement of the historical period in which they were composed. In popular fiction, which is especially divided by genres, genre fiction is the more usual term.\n\nIn literature, genre has been known as an intangible taxonomy. This taxonomy implies a concept of containment or that an idea will be stable forever.The earliest recorded systems of genre in Western history can be traced back to Plato and Aristotle. Gérard Genette, a French literary theorist and author of \"The Architext\", describes Plato as creating three Imitational genres: dramatic dialogue, pure narrative, and epic (a mixture of dialogue and narrative). Lyric poetry, the fourth and final type of Greek literature, was excluded by Plato as a non-mimetic mode. Aristotle later revised Plato's system by eliminating the pure narrative as a viable mode and distinguishing by two additional criteria: the object to be imitated, as objects could be either superior or inferior, and the medium of presentation such as words, gestures or verse. Essentially, the three categories of mode, object, and medium can be visualized along an XYZ axis.\n\nExcluding the criteria of medium, Aristotle's system distinguished four types of classical genres: tragedy (superior-dramatic dialogue), epic (superior-mixed narrative), comedy (inferior-dramatic dialogue), and parody (inferior-mixed narrative). Genette continues by explaining the later integration of lyric poetry into the classical system during the romantic period, replacing the now removed pure narrative mode. Lyric poetry, once considered non-mimetic, was deemed to imitate feelings, becoming the third leg of a new tripartite system: lyrical, epical, and dramatic dialogue. This system, which came to \"dominate all the literary theory of German romanticism (and therefore well beyond)…\" (38), has seen numerous attempts at expansion or revision. However, more ambitious efforts to expand the tripartite system resulted in new taxonomic systems of increasing scope and complexity.\n\nGenette reflects upon these various systems, comparing them to the original tripartite arrangement: \"its structure is somewhat superior to…those that have come after, fundamentally flawed as they are by their inclusive and hierarchical taxonomy, which each time immediately brings the whole game to a standstill and produces an impasse\" (74). Taxonomy allows for a structured classification system of genre, as opposed to a more contemporary rhetorical model of genre.\n\nThe basic genres of film can be regarded as drama, in the feature film and most cartoons, and documentary. Most dramatic feature films, especially from Hollywood fall fairly comfortably into one of a long list of film genres such as the Western, war film, horror film, romantic comedy film, musical, crime film, and many others. Many of these genres have a number of subgenres, for example by setting or subject, or a distinctive national style, for example in the Indian Bollywood musical.\n\nA music genre is a conventional category that identifies pieces of music as belonging to a shared tradition or set of conventions. It is to be distinguished from \"musical form\" and \"musical style\", although in practice these terms are sometimes used interchangeably. There are numerous genres in Western classical music and popular music, as well as musical theatre and the music of non-Western cultures. The term is now perhaps over-used to describe relatively small differences in musical style in modern rock music, that also may reflect sociological differences in their audiences. Timothy Laurie suggests that in the context of rock and pop music studies, the \"appeal of genre criticism is that it makes narratives out of musical worlds that often seem to lack them\".\n\nMusic can be divided into different genres in several ways. The artistic nature of music means that these classifications are often arbitrary and controversial, and some genres may overlap. There are several academic approaches to genres. In his book \"Form in Tonal Music\", Douglass M. Green lists madrigal, motet, canzona, ricercar, and dance as examples of genres from the Renaissance period. According to Green, \"Beethoven's \"Op. 61\" and Mendelssohn's \"Op. 64\" are identical in genre – both are violin concertos – but different in form. However, Mozart's \"Rondo for Piano, K. 511\", and the \"Agnus Dei\" from his \"Mass, K. 317\" are quite different in genre but happen to be similar in form.\" Some, like Peter van der Merwe, treat the terms \"genre\" and \"style\" as the same, saying that \"genre\" should be defined as pieces of music that share a certain style or \"basic musical language\".\n\nOthers, such as Allan F. Moore, state that \"genre\" and \"style\" are two separate terms, and that secondary characteristics such as subject matter can also differentiate between genres. A music genre or subgenre may be defined by the musical techniques, the styles, the context, and content and spirit of the themes. Geographical origin is sometimes used to identify a music genre, though a single geographical category will often include a wide variety of subgenres.\n\nSeveral music scholars have criticised the priority accorded to genre-based communities and listening practices. For example, Laurie argues that \"music genres do not belong to isolated, self-sufficient communities. People constantly move between environments where diverse forms of music are heard, advertised and accessorised with distinctive iconographies, narratives and celebrity identities that also touch on non-musical worlds.\"\n\nThe concept of genre is often applied, sometimes rather loosely, to other media with an artistic element, such as video game genres. Genre, and numerous minutely divided subgenres, affect popular culture very significantly, not least as they are used to classify it for publicity purposes. The vastly increased output of popular culture in the age of electronic media encourages dividing cultural products by genre to simplify the search for products by consumers, a trend the Internet has only intensified.\n\nIn philosophy of language, genre figures prominently in the works of philosopher and literary scholar Mikhail Bakhtin. Bakhtin's basic observations were of \"speech genres\" (the idea of heteroglossia), modes of speaking or writing that people learn to mimic, weave together, and manipulate (such as \"formal letter\" and \"grocery list\", or \"university lecture\" and \"personal anecdote\"). In this sense, genres are socially specified: recognized and defined (often informally) by a particular culture or community. The work of Georg Lukács also touches on the nature of literary genres, appearing separately but around the same time (1920s–1930s) as Bakhtin. Norman Fairclough has a similar concept of genre that emphasizes the social context of the text: Genres are \"different ways of (inter)acting discoursally\" (Fairclough, 2003: 26).\n\nA text's genre may be determined by its:\n\n\nIn the field of rhetoric, genre theorists usually understand genres as types of actions rather than types or forms of texts. On this perspective, texts are channels through which genres are enacted. Carolyn Miller's work has been especially important for this perspective. Drawing on Lloyd Bitzer's concept of rhetorical situation, Miller reasons that recurring rhetorical problems tend to elicit recurring responses; drawing on Alfred Schütz, she reasons that these recurring responses become \"typified\" – that is, socially constructed as recognizable types. Miller argues that these \"typified rhetorical actions\" (p. 151) are properly understood as genres.\n\nBuilding off of Miller, Charles Bazerman and Clay Spinuzzi have argued that genres understood as actions derive their meaning from other genres – that is, other actions. Bazerman therefore proposes that we analyze genres in terms of \"genre systems\", while Spinuzzi prefers the closely related concept of \"genre ecologies\".\n\nThis tradition has had implications for the teaching of writing in American colleges and universities. Combining rhetorical genre theory with activity theory, David Russell has proposed that standard English composition courses are ill-suited to teach the genres that students will write in other contexts across the university and beyond. Elizabeth Wardle contends that standard composition courses do teach genres, but that these are inauthentic \"mutt genres\" that are often of little use outside of composition courses.\n\nThis concept of genre originated from the classification systems created by Plato. Plato divided literature into the three classic genres accepted in Ancient Greece: poetry, drama, and prose. Poetry is further subdivided into epic, lyric, and drama. The divisions are recognized as being set by Aristotle and Plato; however, they were not the only ones. Many genre theorists added to these accepted forms of poetry.\n\nThe earliest recorded systems of genre in Western history can be traced back to Plato and Aristotle. Gérard Genette explains his interpretation of the history of genre in \"The Architext\". He described Plato as the creator of three imitational, mimetic genres distinguished by mode of imitation rather than content. These three imitational genres include dramatic dialogue, the drama; pure narrative, the dithyramb; and a mixture of the two, the epic. Plato excluded lyric poetry as a non-mimetic, imitational mode. Genette further discussed how Aristotle revised Plato's system by first eliminating the pure narrative as a viable mode. He then uses two additional criteria to distinguish the system. The first of the criteria is the object to be imitated, whether superior or inferior. The second criterion is the medium of presentation: words, gestures, or verse. Essentially, the three categories of mode, object, and medium can be visualized along an XYZ axis. Excluding the criteria of medium, Aristotle's system distinguished four types of classical genres: tragedy, epic, comedy, and parody.\n\nGenette explained the integration of lyric poetry into the classical system by replacing the removed pure narrative mode. Lyric poetry, once considered non-mimetic, was deemed to imitate feelings, becoming the third \"Architext\", a term coined by Gennette, of a new long-enduring tripartite system: lyrical; epical, the mixed narrative; and dramatic, the dialogue. This new system that came to \"dominate all the literary theory of German romanticism\" (Genette 38) has seen numerous attempts at expansion and revision. Such attempts include Friedrich Schlegel's triad of subjective form, the lyric; objective form, the dramatic; and subjective-objective form, the epic. However, more ambitious efforts to expand the tripartite system resulted in new taxonomic systems of increasing complexity. Gennette reflected upon these various systems, comparing them to the original tripartite arrangement: \"its structure is somewhat superior to most of those that have come after, fundamentally flawed as they are by their inclusive and hierarchical taxonomy, which each time immediately brings the whole game to a standstill and produces an impasse\".\n\nGenre is embedded in culture but may clash with it at times. There are occasions in which a cultural group may not be inclined to keep within the set structures of a genre. Anthony Pare's studied Inuit social workers in \"Genre and Identity: Individuals, Institutions and Ideology\". In this study, Pare described the conflict between the genre of Inuit social workers' record keeping forms and the cultural values that prohibited them from fully being able to fulfill the expectations of this genre. Amy Devitt further expands on the concept of culture in her 2004 essay, \"A Theory of Genre\" by adding \"culture defines what situations and genres are likely or possible\" (Devitt 24).\n\nGenre not only coexists with culture but also defines its very components. Genres abound in daily life and people often work within them unconsciously; people often take for granted their prominence and ever present residence in society. Devitt touches on Miller's idea of situation, but expands on it and adds that the relationship with genre and situation is reciprocal. Individuals may find themselves shaping the rhetorical situations, which in turn affect the rhetorical responses that arise out of the situation. Because the social workers worked closely with different families, they did not want to disclose many of the details that are standard in the genre of record keeping related to this field. Giving out such information would violate close cultural ties with the members of their community.\n\nAlthough genres are not always precisely definable, genre considerations are one of the most important factors in determining what a person will see or read. The classification properties of genre can attract or repel potential users depending on the individual's understanding of a genre.\n\nGenre creates an expectation in that expectation is met or not. Many genres have built-in audiences and corresponding publications that support them, such as magazines and websites. Inversely, audiences may call out for change in an antecedent genre and create an entirely new genre.\n\nThe term may be used in categorizing web pages, like \"news page\" and \"fan page\", with both very different layout, audience, and intention (Rosso, 2008). Some search engines like Vivísimo try to group found web pages into automated categories in an attempt to show various genres the search hits might fit.\n\nA subgenre is a subordinate within a genre. Two stories being the same genre can still sometimes differ in subgenre. For example, if a fantasy story has darker and more frightening elements of fantasy, it would belong in the subgenre of dark fantasy; whereas another fantasy story that features magic swords and wizards would belong to the subgenre of sword and sorcery.\n\n\n\n"}
{"id": "723048", "url": "https://en.wikipedia.org/wiki?curid=723048", "title": "Grave goods", "text": "Grave goods\n\nGrave goods, in archaeology and anthropology, are the items buried along with the body.\n\nThey are usually personal possessions, supplies to smooth the deceased's journey into the afterlife or offerings to the gods. Grave goods may be classed as a type of votive deposit. Most grave goods recovered by archaeologists consist of inorganic objects such as pottery and stone and metal tools but organic objects that have since decayed were also placed in ancient tombs. Funerary art is a broad term but generally means artworks made specifically to decorate a burial place, such as miniature models of possessions including slaves or servants for \"use\" in the afterlife.\n\nWhere grave goods appear, grave robbery is a potential problem. Etruscans would scratch the word \"śuθina\", Etruscan for \"from a tomb\", on grave goods buried with the dead to discourage their reuse by the living. The tomb of pharaoh Tutankhamun is famous because it was one of the few Egyptian tombs that was not thoroughly looted in ancient times.\n\nGrave goods can be regarded as a sacrifice intended for the benefit of the deceased in the afterlife. Closely related are customs of ancestor worship and offerings to the dead, in modern western culture related to All Souls' Day (Day of the Dead), in East Asia the \"hell bank note\" and related customs.\nAlso closely related is the custom of retainer sacrifice, where servants or wives of a deceased chieftain are interred with the body.\nAs the inclusion of expensive grave goods and of slaves or retainers became a sign of high status in the Bronze Age, the prohibitive cost led to the development of \"fake\" grave goods, where artwork meant to \"depict\" grave goods or retainers is produced for the burial and deposited in the grave in place of the actual sacrifice.\n\nThere are disputed claims of intentional burial of Neanderthals as old as 130,000 years. Similar claims have been made for early anatomically modern humans as old as 100,000 years. The earliest undisputed cases of burials are found in modern human sites of the Upper Palaeolithic.\n\nBeads made of basalt deposited in graves in the Fertile Crescent\ndate to the end of the Upper Paleolithic, beginning in about the 12th to 11th millennium BC.\n\nThe distribution of grave goods are a potential indicator of the social stratification of a society. Thus, early Neolithic graves tend to show equal distribution of goods, suggesting a more or less classless society, while in Chalcolithic and Bronze Age burials, rich grave goods are concentrated in \"chieftain\" graves (barrows), indicating social stratification. It is also possible that burial goods indicate a level of concern and consciousness in regard to an afterlife and related sense of spirituality.\n\nThe expression of social status in rich graves is taken to extremes in the royal graves of the Bronze Age. In the Theban Necropolis in Ancient Egypt, the pyramids and the royal graves in the Valley of the Kings are among the most elaborate burials in human history. This trend is continued into the Iron Age. An example of an extremely rich royal grave of the Iron Age is the Terracotta Army of Qin Shi Huang.\n\nIn the sphere of the Roman Empire, early Christian graves lack grave goods, and grave goods tend to disappear with the decline of Greco-Roman polytheism in the 5th and 6th centuries. Similarly, the presence of grave goods in the Early Middle Ages in Europe has often been taken as evidence of paganism, although during the period of conversion in Anglo-Saxon England and the Frankish Empire (7th century), the situation may be more complicated.\nIn the Christian Middle Ages, high-status graves are marked on the exterior, with tomb effigies or expensive tomb stones rather than by the presence of grave goods.\n\nThe practice of placing grave goods with the dead body has thus an uninterrupted history beginning in the Upper Paleolithic, if not the Middle Paleolithic, upheld until comparatively recent times, in many regions of the world ceasing only with Christianization.\n\nThe importance of grave goods, from the simple behavioural and technical to the metaphysical, in archaeology cannot be overestimated.\nBecause of their almost ubiquitous presence throughout the world and throughout prehistory, in many cases the excavation of every-day items placed in burials is the main source of such artifacts in a given prehistoric culture. \nHowever, care must be taken to avoid naive interpretation of grave goods as an objective sample of artifacts in use in a culture. Because of their ritual context, grave goods may represent a special class of artifacts, in some instances produced especially for burial. \nArtwork produced for the burial itself is known as funerary art, while grave goods in the narrow sense are items produced for actual use that are placed in the grave, but in practice the two categories overlap.\n\nGrave goods in Bronze Age and Iron Age cemeteries are a good indicator of relative social status; \nin a 2001 study on an Iron Age cemetery in Pontecagnano Faiano, Italy, a correlation was found between the quality of grave goods and Forensic indicators on the skeletons, showing that skeletons in wealthy tombs tended to show substantially less evidence of biological stress during adulthood, with fewer broken bones or signs of hard labor.\n\n\n"}
{"id": "9556155", "url": "https://en.wikipedia.org/wiki?curid=9556155", "title": "Hack (comedy)", "text": "Hack (comedy)\n\nHack is a term used primarily in stand-up comedy, but also sketch comedy, improv comedy, and comedy writing to refer to a joke or premise for a joke that is considered obvious, has been frequently used by comedians in the past, and/or is blatantly copied from its original author. Alternatively, it may refer to a comedian or performance group that uses hack material or similarly unoriginal devices in their act. Since comedians and people who work with comedians are typically exposed to many more jokes than the general public, they may recognize a topic, joke or performer as hack before the general public does; as a result, even performers who do well on stage may be considered hacks by their peers.\n\nThe word \"hack\" is derived from the British term \"hackneyed\" meaning, \"over used and thus cheapened, or trite\".\n\nOccasionally a performer will be one of the first to develop a joke about a specific topic, and later on others will follow suit to excess. This renders the topic \"hack\" to new performers, but is not considered a detriment to the originator of the material.\n\nReusing humor can also be joke thievery if it is taken without permission from another specific comedian.\n\nFrom the Catskill and Vaudeville beginnings of stand-up comedy, hacking was common as there were few chances that a performer from one area would meet one from another and a single twenty-minute set could sustain a comic for a decade.\n\nIn the late fifties and early sixties, Will Jordan perfected a caricature performance of Ed Sullivan (incorporating mispronouncing the word \"show\" as \"shoe\") that became the basis for all other impersonators that followed. Soon after, Jackie Mason, Rich Little and others began adapting Jordan's caricature to their own acts. This resulted in many of Jordan's shows being canceled due to other performers doing his bit two weeks previous to his shows at the same venue. John Byner, in turn, developed his own, oft-imitated, version of Jordan's caricature that George Carlin cited as being set up with the words, \"Now you know!\"\n\nIn the sixties, comedy took a turn for the more personal. Comics like Lenny Bruce, Richard Pryor, and George Carlin were no longer regurgitating joke after joke, but instead were offering insight to their own lives from a comedic point of view. As a result, jokes and persona were largely unique to the performer. Hacking proved more difficult, but also more offensive to the writer.\n\nIn the seventies joke theft became more prominent with the boom in popularity of comedy. The eighties and nineties saw the popularity of stand-up comedy continue to increase. With the advent of pay-cable networks, comics were afforded the opportunity to perform their routines unfettered. With this came a new type of joke theft wherein the first comic to tell a stolen joke on some sort of media became the one associated with the joke.\n\nFor many years, Denis Leary had been friends with fellow comedian Bill Hicks. However, when Hicks heard Leary's 1992 album \"No Cure For Cancer\", he felt Leary had stolen his act and material. The friendship ended abruptly as a result.\n\nAt least three stand-up comedians have gone on the record stating they believe Leary stole not just some of Hicks' material but his persona and attitude. As a result of this, it is claimed that after Hicks' death from pancreatic cancer, an industry joke began to circulate about Leary's transformation and subsequent success (roughly; \"Question: Why is Denis Leary a star while Bill Hicks is unknown? Answer: Because there's no cure for cancer\").\n\nAlso in the nineties, began a nearly universal hack of an impression of Bill Cosby, the style of which was first unveiled by Eddie Murphy in his concert \"Raw\".\n\nMore recent times have seen public rivalries between comics over the subject of hacking. Louis CK has maintained a relatively quiet rivalry with Dane Cook over three bits on Cook's album, \"Retaliation\" that allegedly bear some resemblance to three bits on CK's album \"Live in Houston\". This claim is further complicated by both artists having performed bits on naming kids that strongly resemble \"My Real Name\", a bit from Steve Martin's album, \"A Wild and Crazy Guy\".\n\nJoe Rogan, by contrast has been very open in accusing Carlos Mencia of hacking.\n\nIn 2010, Italian comic and satirist Daniele Luttazzi was accused of having plagiarised many jokes from comedians such as George Carlin, Mitch Hedberg, Eddie Izzard, Chris Rock, Bill Hicks and Robert Schimmel. But five years before those charges, Luttazzi himself told about his scheme on his personal blog: he wrote that he adds references to famous comedians' jokes to his work as a defense against the million-euro lawsuits he has to face because of his satire. (In March 2012, Luttazzi won a legal battle against La7 broadcasting company, which in 2007 abruptly closed his late show \"Decameron\", accusing him, among other charges, of plagiarism from Bill Hicks. Sentence: It was original satire, not plagiarism. La7 shall pay Luttazzi 1 million 2 hundred thousand euros.) Luttazzi calls his ruse \"the Lenny Bruce trick\" after a similar trick played by his hero, Lenny Bruce. Luttazzi asked his readers to find out the original jokes. He awards a prize to anyone who finds a \"nugget\", i.e. a reference to famous jokes: he calls the game \"treasure hunt\". Luttazzi also calls the charges \"naive\", explaining why those jokes are not \"plagiarized\", but \"calqued\", which is a fair use of original material. He uses a joke by Emo Philips to prove that the meaning of a joke depends on its context. Luttazzi's blog lists all the comedians and writers quoted in his works.\n\nIn 2011, one of the contestants on the talent quest television program \"Australia's Got Talent\" was Jordan Paris, whose act was stand-up comedy. His act went well, the judges were impressed, and he made it through to the semi-finals. However, it was later revealed that he had plagiarised his jokes from comedians Lee Mack and Geoff Keith. The television network eventually decided to give him a chance to redeem himself and he was allowed to compete in the semi-final, provided he use his own material. Paris' effort this time was self-deprecating, joking about his plagiarism and his large teeth. The first joke went well, but the rest went downhill. It was later found out that the joke that went well - \"I just sacked my two writers - Copy and Paste\" - had been done in 2009 by comedian Jeffrey Ross, about Brad Garrett, at a roast of Joan Rivers. Ross had said, \"This guy has two writers, their names are Cut and Paste.\"\n\nIn January 2012, Blogger and comedian Troy Holm was ridiculed on the social networking site Facebook for stealing jokes and stories from comedian Doug Stanhope and posting them to his Blog from 2010, claiming them as his own work, including Stanhope's \"Fuck someone uglier than you\" routine, which was found on Stanhope's Acid Bootleg. Troy Holm also plagiarized Stanhope's story of an encounter with a transsexual prostitute nearly verbatim, substituting himself as Stanhope, and changing a few small details, causing a backlash from Stanhope's fans. This catapulted Troy Holm into an internet icon which started the \"Occupy Troy Holm\" Movement. Stanhope commented on the Occupy Troy Holm Facebook page that \"To the few people who seem to think this is overboard...and it is...I don't think that you know the levels to which this guy has been ripping me off. He didn't take a tit-fuck joke and use it as a status update. He's been living my entire life as though it was his, changing some names and then promoting with twitters... Look at his site and most the entirety of it is me, including the comments where he uses my stuff to pass as his own conversation. And on Twitter. So who is he ripping off for that stuff that \"isn't\" mine?\"\n\nHacking is not limited to stand-up comedy. Often entire premises in film and television shows are taken from comics or even other media.\n\nDick Cavett and Woody Allen often cited to each other the many instances of their jokes appearing in television shows without their permission, sometimes even falsely attributed to each other.\n\nAllen's jokes and topics were regularly stolen by the highly successful television show, \"Laugh In\". This proved extremely painful to Allen.\n\nSeveral episodes of \"The Simpsons\", including \"\", \"Treehouse of Horror XIII\", and \"The Italian Bob\" have poked fun at \"Family Guy\", implying that MacFarlane's show is guilty of stealing jokes and premises from \"The Simpsons\". However, the producers of both shows have said that there is no serious feud between the two of them and their shows.\n\nThere is, historically, very little legal recourse taken in cases of hacking. Some comics, however, have chosen to exact their own justice. W. C. Fields reportedly paid fifty dollars to have a hack comic's legs broken.\n\nTypically, the repercussions of hacking are limited to personal animosity. On this issue, it sometimes appears that the offended comics are alone in their concern. For example, on February 10, 2007 at the Comedy Store in Los Angeles, Joe Rogan argued on-stage with Carlos Mencia, accusing him of hacking other comedians' work. According to Rogan's account, he had just finished his act and introduced the next performer, Ari Shaffir, as a comedian who opens for \"Carlos Men-steal-ia\". Mencia took offense and walked on the stage. The Comedy Store later cancelled Rogan's shows and suggested he \"take a break\" from the Comedy Store, which was then followed by Rogan's manager (who also manages Mencia) dropping Rogan. The entire incident was filmed as part of Rogan's internet reality show, \"JoeShow\". It was then made available to watch or download at numerous websites, including Rogan's.\n\nJoe Rogan said, \"People take plagiarism so seriously in all other forms of media, whether it's music, newspapers, books, but with comedy, it's like, 'You're on your own, fucker.'\"\n\nThe internet, however, has opened up a new medium for \"outing\" a hack. Websites like YouTube allow users to upload videos and share them with others. This has made it much easier to show evidence of joke thievery in a public forum.\n\nIn January 2012, Troy Holm, an amateur Comic, stole several jokes from Doug Stanhope and posted them to his blog under the guise of having written them, himself. Stanhope discovered the blog and tipped-off his fans who then deluged Holms blog with negative and berating comments. The blog has since been taken down.\n\nSteven Rosenthal and Steve Silberberg have published a \"Guide to Hack\" to help new comics avoid hacking, which references (and gives credit to) an earlier work on the same subject by Andy Kindler called, \"The Hacks Handbook: A Starter Kit\".\n\n"}
{"id": "2104491", "url": "https://en.wikipedia.org/wiki?curid=2104491", "title": "Haiga", "text": "Haiga\n\nStylistically, \"haiga\" vary widely based on the preferences and training of the individual painter, but generally show influences of formal Kanō school painting, minimalist Zen painting, and Ōtsu-e, while sharing much of the aesthetic attitudes of the \"nanga\" tradition. Some were reproduced as woodblock prints. The subjects painted likewise vary widely, but are generally elements mentioned in the calligraphy, or poetic images which add meaning or depth to that expressed by the poem. The moon is a common subject in these poems and paintings, sometimes represented by the Zen circle \"ensō\", which evokes a number of other meanings, including that of the void. Other subjects, ranging from Mount Fuji to rooftops, are frequently represented with a minimum of brushstrokes, thus evoking elegance and beauty in simplicity.\n\nNonoguchi Ryūho (1595–1669), a student of Kanō Tan'yū, is sometimes credited with founding the style; though poetry was commonly accompanied by images for centuries prior, Ryūho was the first poet to regularly include paintings alongside his calligraphy.\n\nMatsuo Bashō, known worldwide as the definitive master of haiku, frequently painted as well. \"Haiga\" became a major style of painting as a result of association with his famous works of haiku. Like his poems, Bashō's paintings are founded in a simplicity which reveals great depth, complementing the poems they are paired with. Towards the end of his life, he studied painting under Morikawa Kyoriku, his pupil in poetry; the works of both men benefited from the exchange, and a number of works were produced combining Morikawa's painting with Bashō's poetry and calligraphy.\n\nComposing haiku, and painting accompanying pictures, was a common pastime of Edo period aesthetes, who would pursue these activities in their spare time, or at friendly gatherings as a communal form of entertainment. The famous novelist Ihara Saikaku was one of many people not normally associated with either poetry or painting, who took part. By contrast, the \"nanga\" painter Yosa Buson, widely considered second only to Bashō as a master of haiku, is said to be \"the only artist to be included in surveys both of great poets and great painters in Japanese history.\"\n\nUnlike other schools of painting which maintained a standard set of styles passed from master to apprentice, the genre of \"haiga\" encompassed a variety of artists with different approaches. Some, like Bashō, were primarily poets, accompanying their compositions with simple sketches, while others, like Buson, were primarily painters, devoting more space and centrality of focus to the image. Maruyama Goshun and Ki Baitei were among those who tended to paint portraits of poets and other figures in a relatively quick, loose style which looks somewhat cartoonish to the modern eye. Some \"haiga\" paintings, such as those by Morikawa Kyoriku, reflect the formal training of the artists, while others, like those by Nakahara Nantenbō, reflect the artist's background in Zen.\n\nOne overall trend that developed over time, despite this wide variety, was a shift from the circles of literati (\"bunjin\") painters to the orbit of the Shijō school of the naturalistic painter Maruyama Ōkyo. This move was effected primarily by Maruyama Goshun, and can be seen as well in the works of Yamaguchi Soken. Some later painters, such as Takebe Sōchō, were influenced by \"ukiyo-e\" styles, and used color in highly detailed works.\n\nThough traditional-style \"haiga\" are still produced today, contemporary artists experiment with the style, coupling haiku with digital imagery, photography, and other media.\n\n\n\n"}
{"id": "2542546", "url": "https://en.wikipedia.org/wiki?curid=2542546", "title": "He Who Gets Slapped", "text": "He Who Gets Slapped\n\nHe Who Gets Slapped is a 1924 American silent tragedy and drama film starring Lon Chaney, Norma Shearer, and John Gilbert, and directed by Victor Sjöström. The film is based on the Russian play \"Тот, кто получает пощёчины\" (\"He Who Gets Slapped\", transliterated as \"Tot, kto polučájet poščóčiny\") by playwright Leonid Andreyev, which was published in 1914 and in English, as \"He Who Gets Slapped\", in 1922. The Russian original was made into a Russian movie in 1916.\n\n\"He Who Gets Slapped\" was the first film produced entirely by the newly formed Metro-Goldwyn-Mayer. It was not, however, MGM's first released movie, as the film was held until the Christmas season when higher attendance was expected. The movie was highly profitable for the fledgling MGM, and was critically hailed upon release. It was also the first film to feature Leo the Lion as the mascot for MGM. Previously, Leo appeared in the logo for Goldwyn Pictures Corporation in 1916, and the trademark was retained by MGM when the companies merged.\n\nThe film was important in the careers of Chaney, Shearer, Gilbert, and Sjöström. In 2017, the film was selected for preservation in the United States National Film Registry by the Library of Congress as being \"culturally, historically, or aesthetically significant\".\n\nPaul Beaumont (Lon Chaney) is a scientist who labored for years alone to prove his radical theories on the origin of mankind. Baron Regnard (Marc McDermott) becomes his patron, enabling him to do research while living in his mansion. One day, Beaumont announces to his beloved wife Marie and the Baron that he has proved all his theories and is ready to present them before the Academy of the Sciences. He leaves the arrangements to the Baron. However, after Beaumont goes to sleep, Marie steals his key, opens the safe containing his papers, and gives them to the baron.\n\nOn the appointed day, Paul travels to the Academy with the Baron. He is aghast when the Baron, instead of introducing him, takes credit for Paul's work himself. After he recovers from the shock, Paul confronts him in front of everyone, but the Baron tells them that Paul is merely his assistant and slaps him. All of the academicians laugh at his humiliation. Paul later seeks comfort from his wife, but she brazenly admits she and the baron are having an affair and calls him a clown. Paul leaves them.\n\nFive years pass by. Paul is now a clown calling himself \"HE who gets slapped\", the star attraction of a small circus near Paris. His act consists of his getting slapped every evening by other clowns, and includes Paul pretending to present in front of the Academy of Science.\n\nAnother of the performers is Bezano (John Gilbert), a daredevil horseback rider. Consuelo (Norma Shearer), the daughter of the impoverished Count Mancini, applies to join his act. Bezano falls in love with Consuelo, as does Paul. Consuelo and her father, however, are planning to restore the family's fortunes with a marriage to her father's wealthy friend.\n\nOne night, during HE's performance, he spots the baron in the audience. The baron goes backstage and begins flirting with Consuelo, which she does not like. The next day, the baron sends Consuelo jewelry, but she rejects it.\n\nWhen her father leaves for a meeting with the baron, Bezano takes Consuelo out to the countryside for a romantic meeting, where they declare their love for each other. Meanwhile, Count Mancini convinces the reluctant baron that the only way he can have Consuelo is by marrying her. The baron discards the heartbroken Marie, leaving her with a check.\n\nLater, HE admits to Consuelo he, too, is in love with her. She thinks he is kidding and laughingly slaps him. They are interrupted by the baron and the count, who inform Consuelo she will marry the baron after the night's performance. When HE tries to interfere, he is locked in an adjoining room, where an angry lion is kept in a cage. He moves the cage so that, when he carefully opens it, only the door to the next room prevents the lion from escaping. HE re-enters the other room through the only other entrance (making sure to lock it behind him) and reveals his identity to the baron. HE threatens the baron, but the count stabs him with a sword.\n\nThe baron and the count try to leave but, finding the main entrance locked, open the side door, releasing the lion. The animal kills the count, then the baron. However, the lion tamer shows up and saves HE from the same fate. HE goes on stage and collapses. He assures Consuelo he is happy and that she will be happy, before dying in her arms.\n\n\nThe film was given a newly composed score by Will Gregory from the band Goldfrapp for use at live concert screenings of the film, initially in the Colston Hall in Bristol, UK on 1 December 2007. \nThe score was later broadcast on BBC Radio 3 in February 2008 with linking narration by actor Samuel West to relay to listeners the plot of the film.\nThe Alloy Orchestra has also composed a score for the film.\n\nThe film made a profit of $349,000.\n\n\"He Who Gets Slapped\" was released on DVD by Warner Brothers Digital Distribution on November 30, 1999. Warner has re-released the film several times as a part of its 6-disk Warner Archive Collection, first on November 22, 2011 and later on June 23, 2015.\n\n\"He Who Gets Slapped\" received mostly positive reviews from critics upon its release, with many praising the film and Chaney's performance.\nIn his 1924 review of the film for \"New York Times\", Mordaunt Hall praised the film's direction, performances, and story, calling it' \"the finest production we have yet seen\".\nAuthor and film critic Leonard Maltin gave the film three out of a possible four stars, calling it \"a Pagliacci–type vehicle for Chaney.\"\nChristopher Meeks from \"Variety\" gave the film a positive review, commending the film's inventive staging, lighting and sound design, and performances, but felt that the ending was predictable and drawn out.\nHans J. Wollstein from Allmovie gave the film a positive review, praising Chaney's and McDermott's performances.\n\nThe film is recognized by American Film Institute in these lists:\n\n\n"}
{"id": "46187677", "url": "https://en.wikipedia.org/wiki?curid=46187677", "title": "History of human migration", "text": "History of human migration\n\nHuman migration is the movement by people from one place to another with the intention of settling temporarily or permanently in the new location. It typically involves movements over long distances and from one country or region to another. \n\nHistorically, early human migration includes the peopling of the world, i.e. migration to world regions where there was previously no human habitation, during the Upper Paleolithic. Since the Neolithic, most migrations (except for the peopling of remote regions such as the Arctic or the Pacific), migration was predominantly warlike, consisting of conquest or \"Landnahme\" on the part of expanding populations. Colonialism involves expansion of sedentary populations into previously only sparsely settled territories or territories with no permanent settlements. In the modern period, human migration has primarily taken the form of migration within and between existing sovereign states, either controlled (legal immigration) or uncontrolled and in violation of immigration laws (illegal immigration).\n\nMigration can be voluntary or involuntary. Involuntary migration includes forced displacement (in various forms such as deportation, slave trade, trafficking in human beings) and flight (war refugees, ethnic cleansing).\n\nThe pre-modern migration of human populations begins with the movement of \"Homo erectus\" out of Africa across Eurasia about 1.75 million years ago. \"Homo sapiens\" appears to have occupied all of Africa about 150,000 years ago; some members of this species moved out of Africa 70,000 years ago (or, according to more recent studies, as early as 125,000 years ago into Asia, and even as early as 270,000 years ago), and had spread across Australia, Asia and Europe by 40,000 BC.Migration to the Americas took place 20,000 to 15,000 years ago. By 2000 years ago humans had established settlements in most of the Pacific Islands. Major population-movements notably include those postulated as associated with the Neolithic Revolution and with Indo-European expansion. The Early Medieval Great Migrations including Turkic expansion have left significant traces. In some places, such as Turkey and Azerbaijan, there was a substantial cultural transformation after the migration of relatively small elite populations. Historians see elite-migration parallels in the Roman and Norman conquests of Britain, while \"the most hotly debated of all the British cultural transitions is the role of migration in the relatively sudden and drastic change from Romano-Britain to Anglo-Saxon Britain\", which may be explained by a possible \"substantial migration of Anglo-Saxon Y chromosomes into Central England (contributing 50%–100% to the gene pool at that time).\"\n\nEarly humans migrated due to many factors, such as changing climate and landscape and inadequate food-supply. The evidence indicates that the ancestors of the Austronesian peoples spread from the South Chinese mainland to the island of Taiwan around 8,000 years ago. Evidence from historical linguistics suggests that seafaring peoples migrated from Taiwan, perhaps in distinct waves separated by millennia, to the entire region encompassed by the Austronesian languages. Scholars believe that this migration began around 6,000 years ago. Indo-Aryan migration from the Indus Valley to the plain of the River Ganges in Northern India is presumed to have taken place in the Middle to Late Bronze Age, contemporary with the Late Harappan phase in India (around 1700 to 1300 BC). From 180 BC, a series of invasions from Central Asia followed in the northwestern Indian subcontinent, including those led by the Indo-Greeks, Indo-Scythians,Indo-Parthians and Kushans.\n\nFrom 728 BC, the Greeks began 250 years of expansion, settling colonies in several places, including Sicily and Marseille. Europe provides evidence of two major migration movements: the Celtic peoples in the first millennium BC, and the later Migration Period of the first millennium AD from the North and East. Both may be examples of general cultural change sparked by primarily elite and warrior migration. A smaller migration (or sub-migration) involved the Magyars moving into Pannonia (modern-day Hungary) in the 9th century AD. Turkic peoples spread from their homeland in modern Turkestan across most of Central Asia into Europe and the Middle East between the 6th and 11th centuries AD. Recent research suggests that Madagascar was uninhabited until Austronesian seafarers from Indonesia arrived during the 5th and 6th centuries AD. Subsequent migrations from both the Pacific and Africa further consolidated this original mixture, and Malagasy people emerged.\n\nBefore the expansion of the Bantu languages and their speakers, the southern half of Africa is believed to have been populated by Pygmies and Khoisan-speaking people, whose descendants today occupy the arid regions around the Kalahari Desert and the forests of Central Africa. By about 1000 AD, Bantu migration had reached modern-day Zimbabwe and South Africa. The Banu Hilal and Banu Ma'qil were a collection of Arab Bedouin tribes from the Arabian Peninsula who migrated westwards via Egypt between the 11th and 13th centuries. Their migration strongly contributed to the Arabisation and Islamisation of the western Maghreb, which was until then dominated by Berber tribes. Ostsiedlung was the medieval eastward migration and settlement of Germans. The 13th century was the time of the great Mongol and Turkic migrations across Eurasia.\n\nBetween the 11th and 18th centuries, numerous migrations took place in Asia. The Vatsayan Priests migrated from the eastern Himalaya hills to Kashmir during the Shan invasion in the 13th century. They settled in the lower Shivalik Hills in the 13th century to sanctify the manifest goddess. In the Ming occupation, the Vietnamese started expanded southward in the 11th century; this is known in Vietnamese as nam tiến (southward expansion). Manchuria was separated from China proper by the Inner Willow Palisade, which restricted the movement of the Han Chinese into Manchuria during the early Qing Dynasty (founded in 1636), as the area was off-limits (British English: out of bounds) to the Han until the Qing started colonizing the area with them (late 18th century) later on in the dynasty's rule.\n\nThe Age of Exploration and European colonialism has led to an accelerated pace of migration since Early Modern times. In the 16th century, perhaps 240,000 Europeans entered American ports. In the 19th century over 50 million people left Europe for the Americas alone. The local populations or tribes, such as the Aboriginal people in Canada, Brazil, Argentina, Australia, Japan\nand the United States, were often numerically overwhelmed by incoming settlers.\n\nWhen the pace of migration had accelerated since the 18th century already (including the involuntary slave trade), it would increase further in the 19th century. Manning distinguishes three major types of migration: labor migration, refugee migrations, and urbanization. Millions of agricultural workers left the countryside and moved to the cities causing unprecedented levels of urbanization. This phenomenon began in Britain in the late 18th century and spread around the world and continues to this day in many areas.\n\nIndustrialization encouraged migration wherever it appeared. The increasingly global economy globalized the labour market. The Atlantic slave trade diminished sharply after 1820, which gave rise to self-bound contract labour migration from Europe and Asia to plantations. Overpopulation, open agricultural frontiers, and rising industrial centres attracted voluntary migrants. Moreover, migration was significantly made easier by improved transportation techniques.\n\nRomantic nationalism also rose in the 19th century, and, with it, ethnocentrism. The great European industrial empires also rose. Both factors contributed to migration, as some countries favored their own ethnicity over outsiders and other countries appeared to be considerably more welcoming. For example, the Russian Empire identified with Eastern Orthodoxy, and confined Jews, who were not Eastern Orthodox, to the Pale of Settlement and imposed restrictions. Violence was also a problem. The United States was promoted as a better location, a \"golden land\" where Jews could live more openly. Another effect of imperialism, colonialism, led to the migration of some colonizing parties from \"home countries\" to \"the colonies\", and eventually the migration of people from \"colonies\" to \"home countries\".\n\nTransnational labor migration reached a peak of three million migrants per year in the early twentieth century. Italy, Norway, Ireland and the Guangdong region of China were regions with especially high emigration rates during these years. These large migration flows influenced the process of nation state formation in many ways. Immigration restrictions have been developed, as well as diaspora cultures and myths that reflect the importance of migration to the foundation of certain nations, like the American melting pot. The transnational labor migration fell to a lower level from the 1930s to the 1960s and then rebounded.\n\nThe United States experienced considerable internal migration related to industrialization, including its African American population. \nFrom 1910 to 1970, approximately 7 million African Americans migrated from the rural Southern United States, where blacks faced both poor economic opportunities and considerable political and social prejudice, to the industrial cities of the Northeast, Midwest and West, where relatively well-paid jobs were available. This phenomenon came to be known in the United States as its own Great Migration, although historians today consider the migration to have two distinct phases. The term \"Great Migration\", without a qualifier, is now most often used to refer the first phase, which ended roughly at the time of the Great Depression. \nThe second phase, lasting roughly from the start of U.S. involvement in World War II to 1970, is now called the Second Great Migration. With the demise of legalised segregation in the 1960s and greatly improved economic opportunities in the South in the subsequent decades, millions of blacks have returned to the South from other parts of the country since 1980 in what has been called the New Great Migration.\n\nThe First and Second World Wars, and wars, genocides, and crises sparked by them, had an enormous impact on migration. Muslims moved from the Balkan to Turkey, while Christians moved the other way, during the collapse of the Ottoman Empire. In April 1915 the Ottoman government embarked upon the systematic decimation of its civilian Armenian population. The persecutions continued with varying intensity until 1923 when the Ottoman Empire ceased to exist and was replaced by the Republic of Turkey. The Armenian population of the Ottoman state was reported at about two million in 1915. An estimated one million had perished by 1918, while hundreds of thousands had become homeless and stateless refugees. By 1923 virtually the entire Armenian population of Anatolian Turkey had disappeared. Four hundred thousand Jews had already moved to Palestine in the early twentieth century, and numerous Jews to America, as already mentioned. The Russian Civil War caused some three million Russians, Poles, and Germans to migrate out of the new Soviet Union. Decolonization following the Second World War also caused migrations.\n\nThe Jewish communities across Europe, the Mediterranean and the Middle East were formed from voluntary and involuntary migrants. After the Holocaust (1938 to 1945), there was increased migration to the British Mandate of Palestine, which became the modern state of Israel as a result of the United Nations Partition Plan for Palestine.\n\nProvisions of the Potsdam Agreement from 1945 signed by victorious Western Allies and the Soviet Union led to one of the largest European migrations, and the largest in the 20th century. It involved the migration and resettlement of close to or over 20 million people. The largest affected group were 16.5 million Germans expelled from Eastern Europe westwards. The second largest group were Poles, millions of whom were expelled westwards from eastern Kresy region and resettled in the so-called Recovered Territories (see Allies decide Polish border in the article on the Oder-Neisse line). Hundreds of thousands of Poles, Ukrainians (Operation Vistula), Lithuanians, Latvians, Estonians and some Belarusians were expelled eastwards from Europe to the Soviet Union. Finally, many of the several hundred thousand Jews remaining in Eastern Europe after the Holocaust migrated outside Europe to Israel and the United States.\n\nIn 1947, upon the Partition of India, large populations moved from India to Pakistan and vice versa, depending on their religious beliefs. The partition was created by the Indian Independence Act 1947 as a result of the dissolution of the British Indian Empire. The partition displaced up to 17 million people in the former British Indian Empire, with estimates of loss of life varying from several hundred thousand to a million. Muslim residents of the former British India migrated to Pakistan (including East Pakistan, now Bangladesh), whilst Hindu and Sikh residents of Pakistan and Hindu residents of East Pakistan (now Bangladesh) moved in the opposite direction.\n\nIn modern India, estimates based on industry sectors mainly employing migrants suggest that there are around 100 million circular migrants in India. Caste, social networks and historical precedents play a powerful role in shaping patterns of migration. \n\nResearch by the Overseas Development Institute identifies a rapid movement of labor from slower- to faster-growing parts of the economy. Migrants can often find themselves excluded by urban housing policies, and migrant support initiatives are needed to give workers improved access to market information, certification of identity, housing and education.\n\nIn the riots which preceded the partition in the Punjab region, between 200,000 and 500,000 people were killed in the retributive genocide. U.N.H.C.R. estimates 14 million Hindus, Sikhs and Muslims were displaced during the partition. Scholars call it the largest mass migration in human history: Nigel Smith, in his book \"Pakistan: History, Culture, and Government\", calls it \"history's greatest migration.\"\n\n\n\nBooks\n\nJournals\n\nOnline Books\n\n\n"}
{"id": "39356537", "url": "https://en.wikipedia.org/wiki?curid=39356537", "title": "Hockett's design features", "text": "Hockett's design features\n\nIn the 1960s, linguistic anthropologist Charles F. Hockett defined a set of features that characterize human language and set it apart from animal communication. He called these characteristics the design features of language. Hockett originally believed there to be 13 design features. While primate communication utilizes the first 9 features, the final 4 features (displacement, productivity, cultural transmission, and duality) are reserved for humans. Hockett later added prevarication, reflexiveness, and learnability to the list as uniquely human characteristics. He asserted that even the most basic human languages possess these 16 features.\n\nCharles Hockett was an American linguist and anthropologist, who lived from 1916 to 2000. Hockett graduated from Yale in 1939, and later taught at both Cornell and Rice. Hockett made significant contributions to structural linguistics, as well as the study of Native American, Chinese, and Fijian languages. His work focused on detailed linguistic analysis, particularly morphology and phonology, and on the concepts and tools that facilitated such analysis.\nUp until the 1950s, language was largely viewed as a social-behavioral phenomenon. Hockett was challenged in this belief by Noam Chomsky, who suggested that language is biologically-based and innately learned. He believed that humans share a universal grammar that ties all languages together. Hockett staunchly opposed this \"Chomskyan\" concept of the nature of language. However, Hockett is most famous for defining what he called the design features of language, which demonstrate his beliefs about the commonalities between human languages.\n\nVocal-auditory channel.\nRefers to the idea that speaking/hearing is the mode humans use for language. When Hockett first defined this feature, it did not take sign language into account, which reflects the ideology of orality that was prevalent during the time (See, for instance, the argumentation of Christin, 1995) . This feature has since been modified to include other channels of language, such as tactile-visual or chemical-olfactory.\n\nBroadcast transmission and directional reception\nWhen humans speak, sounds are transmitted in all directions; however, listeners perceive the direction from which the sounds are coming. Similarly, signers broadcast to potentially anyone within the line of sight, while those watching see who is signing. This is characteristic of most forms of human and animal communication.\n\nTransitoriness\nAlso called rapid fading, transitoriness refers to the idea of temporary quality of language. Language sounds exist for only a brief period of time, after which they are no longer perceived. Sound waves quickly disappear once a speaker stops speaking. This is also true of signs. In contrast, other forms of communication such as writing and Inka khipus (knot-tying) are more permanent.\n\nInterchangeability\nRefers to the idea that humans can give and receive identical linguistic signals; humans are not limited in the types of messages they can say/hear. One can say \"I am a boy\" even if one is a girl. This is not to be confused with lying (prevarication). The importance is that a speaker can physically create any and all messages regardless of their truth or relation to the speaker. In other words, anything that one can hear, one can also say.\n\nNot all species possess this feature. For example, in order to communicate their status, queen ants produce chemical scents that no other ants can produce (see animal communication below).\n\nTotal feedback\nSpeakers of a language can hear their own speech and can control and modify what they are saying as they say it. Similarly, signers see, feel, and control their signing.\n\nSpecialization\nThe purpose of linguistic signals is communication and not some other biological function. When humans speak or sign, it is generally intentional.\n\nAn example of \"non\"-specialized communication is dog panting. When a dog pants, it often communicates to its owner that it is hot or thirsty; however, the dog pants in order to cool itself off. This is a biological function, and the communication is a secondary matter.\n\nSemanticity\nSpecific sound signals are directly tied to certain meanings.\n\nArbitrariness ** languages are generally made up of both arbitrary and iconic symbols. In spoken languages this takes the form of onomatopoeias. In English \"murmur\", in Mandarin \"māo\" (cat). In ASL \"cup\", \"me\" \"up/down\", etc. \nThere is no intrinsic or logical connection between a sound form (signal) and its meaning. Whatever name a human language attributes an object is purely arbitrary. The word \"car\" is nothing like an actual car. Spoken words are really nothing like the objects they represent. This is further demonstrated by the fact that different languages attribute very different names to the same object.\n\nSigned languages are transmitted visually and this allows for a certain degree of iconicity. For example, in the ASL sign HOUSE, the hands are flat and touch in a way that resembles the roof and walls of a house. However, many other signs are not iconic, and the relationship between form and meaning is arbitrary. Thus, while Hockett did not account for the possibility of non-arbitrary form-meaning relationships, the principle still generally applies. \n\nDiscreteness\nLinguistic representations can be broken down into small discrete units which combine with each other in rule-governed ways. They are perceived categorically, not continuously. For example, English marks number with the plural morpheme /s/, which can be added to the end of any noun. The plural morpheme is perceived categorically, not continuously: we cannot express smaller or larger quantities by varying how loudly we pronounce the /s/. \n\nDisplacement\nRefers to the idea that humans can talk about things that are not physically present or that do not even exist. Speakers can talk about the past and the future, and can express hopes and dreams. A human's speech is not limited to here and now. Displacement is one of the features that separates human language from other forms of primate communication.\n\nProductivity\nRefers to the idea that language-users can create and understand novel utterances. Humans are able to produce an unlimited amount of utterances. Also related to productivity is the concept of grammatical patterning, which facilitates the use and comprehension of language. Language is not stagnant, but is constantly changing. New idioms are created all the time and the meaning of signals can vary depending on the context and situation.\n\nTraditional transmission\nAlso called cultural transmission. While humans are born with innate language capabilities, language is learned after birth in a social setting. Children learn how to speak by interacting with experienced language users. Language and culture are woven together.\n\nDuality of patterning\nMeaningful messages are made up of distinct smaller meaningful units (words and morphemes) which themselves are made up of distinct smaller, meaningless units (phonemes).\n\nPrevarication\nPrevarication is the ability to lie or deceive. When using language, humans can make false or meaningless statements.\n\nReflexiveness\nHumans can use language to talk about language.\n\nLearnability\nLanguage is teachable and learnable. In the same way as a speaker learns their first language, the speaker is able to learn other languages. It is worth noting that young children learn language with competence and ease; however, language acquisition is constrained by a critical period such that it becomes more difficult once children pass a certain age.\n\nHockett distinguished language from communication. While almost all animals communicate in some way, a communication system is only considered language if it possesses \"all\" of the above characteristics. Some animal communication systems are impressively sophisticated.\n\nAnts make use of the chemical-olfactory channel of communication. Ants produce chemicals called pheromones, which are released through body glands and received by the tips of the antenna. Ants can produce up to twenty different pheromone scents, each a unique signal used to communicate things such as the location of food and danger, or even the need to defend or relocate the colony. When an ant is killed, it releases a pheromone that alerts others of potential danger. Pheromones also help ants distinguish family members from strangers. The queen ant has special pheromones which she uses to signal her status, orchestrate work, and let the colony know when they need to raise princesses or drones.\nAnts will even engage in warfare to protect the colony or a food source. This warfare involves tactics that resemble human warfare. Marauder ants will capture and hold down an enemy while another ant crushes it. Ants are loyal to their colony to the death; however, the queen will kill her own in order to be the last one standing. This level of \"planning\" among an animal species requires an intricate communication.\n\nBird communication demonstrates many features, including the vocal-auditory channel, broadcast transmission/directional reception, rapid fading, semanticity, and arbitrariness. Bird communication is divided into songs and calls. Songs are used primarily to attract mates, while calls are used to alert of food and danger and coordinate movement with the flock. Calls are acoustically simple, while songs are longer and more complex. Bird communication is both discrete and non-discrete. Birds use syntax to arrange their songs, where musical notes act as phonemes. The order of the notes is important to the meaning of the song, thus indicating that discreteness exists. Bird communication is also continuous in the sense that it utilizes duration and frequency. However, the fact that birds have \"phonemes\" does not necessarily mean that they can combine them in an infinite way. Birds have a limited number of songs that they can produce. The male indigo bunting only has one song, while the brown thrasher can sing over 2000 songs. Birds even have unique dialects, depending on where they are from.\n\nHoneybee communication is distinct from other forms of animal communication. Rather than vocal-auditory, bees use the space-movement channel to communicate. Honeybees use two kinds of dances to communicate—the round dance and the waggle dance. They use the round dance to communicate that food is 50–75 meters from the hive. They use the waggle dance when it is farther than this. To do the waggle dance, a bee moves in a zig-zag line and then does a loop back to the beginning of the line, forming a figure-eight. The direction of the line points to the food. The speed of the dance indicates the distance to the food. In this way, bee dancing is also continuous, rather than discrete. Their communication is also not arbitrary. They move in a direction and pattern that physically points out where food is located.\n\nHoneybee dancing also demonstrates displacement, which is generally considered a human characteristic. Most animals will only give a food-found call in the physical presence of food, yet bees can talk about food that is over 100 meters away.\n\n\n"}
{"id": "866522", "url": "https://en.wikipedia.org/wiki?curid=866522", "title": "Human zoo", "text": "Human zoo\n\nHuman zoos, also called ethnological expositions, were 19th-, 20th-, and 21st-century public exhibitions of humans, usually in a so-called natural or primitive state. The displays often emphasized the cultural differences between Europeans of Western civilization and non-European peoples or with other Europeans who practiced a lifestyle deemed more primitive. Some of them placed indigenous populations in a continuum somewhere between the great apes and Europeans. Ethnological expositions have been extensively criticized and ascertained as highly degrading and racist, depending on the show and individuals involved.\n\nIn the late 19th century German ethnographic museums were an attempt at empirical study of human culture. They contained artifacts from cultures around the world organized by continent allowing visitors to see the similarities and differences between the groups and form their own ideas. The ethnographic museums of Germany were explicitly designed to steer away from projecting certain principles or instructing its viewers to interpret the material in a particular manner. They were instead left open for museum guests to form their own opinions.\n\nThe directors of Germany's ethnographic museums intended to create a unifying history of mankind, to show how humans had progressed to the cosmopolitan creatures that walked the halls of these museums. Imperialism influenced the supporters, visitors, and collectors of these museums, the displays, directions, and political rhetoric.\n\nEthnology studies in Germany took a new approach in the 1870s as human displays were incorporated into zoos. These exhibits were lauded as educational to the general population by the scientific community of the time, because they were informing of the way people lived across the world. Very quickly the exhibits were used as a way to show that Europeans had evolved into a superior cosmopolitan life.\n\nAs Ethnogenic expositions were discontinued in Germany around 1931, there were many repercussions for the performers. Many of the people brought from their homelands to work in the exhibits had created families in Germany, and there were many children that had been born in Germany. Once they no longer worked in the zoos or for performance acts these people were stuck living in Germany where they had no rights and were harshly discriminated against. During the rise of the Nazi party the foreign actors in these stage shows were typically able to stay out of concentration camps because there were so few of them that the Nazis did not see them as a real threat. Although they were able to avoid concentration camps, they were not able to participate in German life as citizens of ethnically German origin could. The Hitler Youth did not allow children of foreign parents to participate, and adults were rejected as German soldiers. Many ended up working in war industry factories or foreign laborer camps. After WWII ended, racism in Germany became more concealed or invisible but did not go away. Many people of foreign descent intended to leave after the war, but because of their German nationality, it was difficult for them to emigrate.\n\nCarl Hagenbeck was a German exotic animal businessman, who became famous for his conquering of the animal trade market during the mid to late 1800’s. Due to the costs of acquiring and keeping animals, the financial implications started to worry Hagenbeck, and he began looking for other ways to alleviate the company’s monetary strains. Heinrich Leutemann, an old friend of Hagenbeck suggested bringing along the people from the foreign lands to accompany the animals. The idea struck Hagenbeck as brilliant and he had a group of Laplanders accompany his next shipment of Reindeer. They set up traditional houses and went about their business as usual on the Hagenbeck property. The display was so successful that Carl was organizing his second show before the first was over. Although the concept of parading peoples captured from conquered lands goes back to the Romans, Hagenbeck claimed to have the first show’s displaying “cultures” from foreign lands. Carl Hagenbeck continued to bring indigenous people along with the animals he was importing from across the globe. The people would come with their hunting equipment, homes, and other facets of their daily life. Hagenbeck’s displays evolved in complexity as the years went by. In 1876 Hagenbeck had a group of 6 Sami accompany a herd of reindeer, and by 1874 his acts included close to 67 men, women, and children in his \"Ceylon\" show performing with 25 elephants. The performances also expanded from showing every day activities such as milking reindeer and building huts, to displaying some of the more extravagant parts of the cultures such as magicians, jugglers, and devil dancers.\n\nThe notion of the human curiosity has a history at least as long as colonialism. For instance, in the Western Hemisphere, one of the earliest-known zoos, that of Moctezuma in Mexico, consisted not only of a vast collection of animals, but also exhibited humans, for example, dwarves, albinos and hunchbacks.\n\nDuring the Renaissance, the Medici developed a large menagerie in the Vatican.\nIn the 16th century, Cardinal Hippolytus Medici had a collection of people of different races as well as exotic animals.\nHe is reported as having a troupe of so-called Savages, speaking over twenty languages; there were also Moors, Tartars, Indians, Turks and Africans.\n\nDuring the 1850s, Maximo and Bartola, two microcephalic children from El Salvador, were exhibited in the US and Europe under the names Aztec Children and Aztec Lilliputians. However, human zoos would become common only in the 1870s in the midst of the New Imperialism period.\n\nIn the 1870s, exhibitions of exotic populations became popular in various countries. Human zoos could be found in Paris, Hamburg, Antwerp, Barcelona, London, Milan, and New York City. Carl Hagenbeck, a merchant in wild animals and future entrepreneur of many zoos in Europe, decided in 1874 to exhibit Samoan and Sami people as \"purely natural\" populations. In 1876, he sent a collaborator to the Egyptian Sudan to bring back some wild beasts and Nubians. The Nubian exhibit was very successful in Europe and toured Paris, London, and Berlin. In 1880, Hagenbeck dispatched an agent to Labrador to secure a number of Esquimaux (Eskimo / Inuit) from the moravian mission of Hebron; these Inuit were exhibited in his Hamburg Tierpark. Other ethnological expositions included Egyptian and Bedouin mock settlements. Hagenbeck would also employ agents to take part in his ethnological exhibits, with the aim of exposing his audience to various different subsistence modes and lifestyles. Among these hired workers were Hersi Egeh and his lineage from Berbera in present-day northwestern Somalia, who in the process accumulated much wealth, which they later reinvested in real estate in their homeland. The viceroy of India likewise gave Hagenbeck permission to hire local inhabitants for an exhibit, on the condition that Hagenbeck would first have to deposit funds into the royal treasury.\n\nGeoffroy de Saint-Hilaire, director of the Jardin d'acclimatation, decided in 1877 to organize two ethnological spectacles that presented Nubians and Inuit. That year, the audience of the Jardin d'acclimatation' doubled to one million. Between 1877 and 1912, approximately thirty ethnological exhibitions were presented at the \"Jardin zoologique d'acclimatation\".\n\nBoth the 1878 and the 1889 Parisian World's Fair presented a Negro Village (\"village nègre\"). Visited by 28 million people, the 1889 World's Fair displayed 400 indigenous people as the major attraction. The 1900 World's Fair presented the famous diorama living in Madagascar, while the Colonial Exhibitions in Marseilles (1906 and 1922) and in Paris (1907 and 1931) also displayed humans in cages, often nude or semi-nude. The 1931 exhibition in Paris was so successful that 34 million people attended it in six months, while a smaller counter-exhibition entitled \"The Truth on the Colonies\", organized by the Communist Party, attracted very few visitors—in the first room, it recalled Albert Londres and André Gide's critiques of forced labour in the colonies. Nomadic Senegalese Villages were also presented.\n\nIn 1883, native people of Suriname were displayed in the International Colonial and Export Exhibition in Amsterdam, held behind the Rijksmuseum.\n\nIn the late 1800s, Hagenbeck organized exhibitions of indigenous populations from various parts of the globe. He staged a public display in 1886 of Sinhalese autochthones from the Sri Lanka. In 1893/1894, he also put together an exhibition of Sami/Lapps in Hamburg-Saint Paul.\n\nAt the 1901 Pan-American Exposition and at the 1893 World's Columbian Exposition, where Little Egypt performed bellydance, and where the photographers Charles Dudley Arnold and Harlow Higginbotham took depreciative photos, presenting indigenous people as catalogue of \"types\", along with sarcastic legends.\n\nIn 1896, to increase the number of visitors, the Cincinnati Zoo invited one hundred Sioux Native Americans to establish a village at the site. The Sioux lived at the zoo for three months.\n\nIn 1904, Apaches and Igorots (from the Philippines) were displayed at the Saint Louis World Fair in association with the 1904 Summer Olympics. The US had just acquired, following the Spanish–American War, new territories such as Guam, the Philippines, and Puerto Rico, allowing them to \"display\" some of the native inhabitants. According to the Rev. Sequoyah Ade:\n\nTo further illustrate the indignities heaped upon the Philippine people following their eventual loss to the Americans, the United States made the Philippine campaign the centrepoint of the 1904 World's Fair held that year in St. Louis, MI [sic]. In what was enthusiastically termed a \"parade of evolutionary progress,\" visitors could inspect the \"primitives\" that represented the counterbalance to \"Civilisation\" justifying Kipling's poem \"The White Man's Burden\". Pygmies from New Guinea and Africa, who were later displayed in the Primate section of the Bronx Zoo, were paraded next to American Indians such as Apache warrior Geronimo, who sold his autograph. But the main draw was the Philippine exhibition complete with full size replicas of Indigenous living quarters erected to exhibit the inherent backwardness of the Philippine people. The purpose was to highlight both the \"civilising\" influence of American rule and the economic potential of the island chains' natural resources on the heels of the Philippine–American War. It was, reportedly, the largest specific Aboriginal exhibition displayed in the exposition. As one pleased visitor commented, the human zoo exhibition displayed \"the race narrative of odd peoples who mark time while the world advances, and of savages made, by American methods, into civilized workers.\"\n\nIn 1906, Madison Grant—socialite, eugenicist, amateur anthropologist, and head of the New York Zoological Society—had Congolese pygmy Ota Benga put on display at the Bronx Zoo in New York City alongside apes and other animals. At the behest of Grant, the zoo director William Hornaday placed Benga displayed in a cage with the chimpanzees, then with an orangutan named Dohong, and a parrot, and labeled him \"The Missing Link\", suggesting that in evolutionary terms Africans like Benga were closer to apes than were Europeans. It triggered protests from the city's clergymen, but the public reportedly flocked to see it.\nBenga shot targets with a bow and arrow, wove twine, and wrestled with an orangutan. Although, according to \"The New York Times\", \"few expressed audible objection to the sight of a human being in a cage with monkeys as companions\", controversy erupted as black clergymen in the city took great offense. \"Our race, we think, is depressed enough, without exhibiting one of us with the apes\", said the Reverend James H. Gordon, superintendent of the Howard Colored Orphan Asylum in Brooklyn. \"We think we are worthy of being considered human beings, with souls.\"\n\nNew York City Mayor George B. McClellan Jr. refused to meet with the clergymen, drawing the praise of Hornaday, who wrote to him: \"When the history of the Zoological Park is written, this incident will form its most amusing passage.\"\n\nAs the controversy continued, Hornaday remained unapologetic, insisting that his only intention was to put on an ethnological exhibition. In another letter, he said that he and Grant—who ten years later would publish the racist tract \"The Passing of the Great Race\"—considered it \"imperative that the society should not even seem to be dictated to\" by the black clergymen.\n\nOn Monday, September 8, 1906, after just two days, Hornaday decided to close the exhibition, and Benga could be found walking the zoo grounds, often followed by a crowd \"howling, jeering and yelling.\"\n\nIn 1925, a display at Belle Vue Zoo in Manchester, England, was entitled \"Cannibals\" and featured black Africans depicted as savages.\n\nBy the 1930s, a new kind of human zoo appeared in America, nude shows masquerading as education. These included the Zoro Garden Nudist Colony at the Pacific International Exposition in San Diego, California (1935-6) and the Sally Rand Nude Ranch at the Golden Gate International Exposition in San Francisco (1939). The former was supposedly a real nudist colony, which used hired performers instead of actual nudists. The latter featured nude women performing in western attire. The Golden Gate fair also featured a \"Greenwich Village\" show, described in the Official Guide Book as “Model artists’ colony and revue theatre.”\n\nA Congolese village was displayed at the Brussels 1958 World's Fair.\n\nIn April 1994, an example of an Ivory Coast village was presented as part of an African safari in Port-Saint-Père, near Nantes, in France, later called Planète Sauvage.\n\nAn African village, intended as a craft and cultural festival, was held in Augsburg Zoo in Germany in July 2005, and was subject to widespread criticism.\n\nIn August 2005, London Zoo displayed four human volunteers wearing fig leaves (and bathing suits) for four days.\n\nIn 2007, Adelaide Zoo ran a Human Zoo exhibition which consisted of a group of people who, as part of a study exercise, had applied to be housed in the former ape enclosure by day, but then returned home by night. The inhabitants took part in several exercises, and spectators were asked for donations towards a new ape enclosure.\n\nAlso in 2007, pygmy performers at the Festival of Pan-African Music (Fespam) were housed at a zoo in Brazzaville, Congo. Although members of the group of twenty people - among them an infant, age three-months - were not officially on display, it was necessary for them to \"collect firewood in the zoo to cook their food, and [they] were being stared at and filmed by tourists and passers-by\".\n\n\n\n"}
{"id": "17893852", "url": "https://en.wikipedia.org/wiki?curid=17893852", "title": "Humor research", "text": "Humor research\n\nHumor research (also humor studies) is a multifaceted field which enters the domains of linguistics, history, and literature. Research in humor has been done to understand the psychological and physiological effects, both positive and negative, on a person or groups of people. Research in humor has revealed many different theories of humor and many different kinds of humor including their functions and effects personally, in relationships, and in society.\n\nHumor research deals with a wide variety of issues, which can be categorized according to several theories of humor. Because of its interdisciplinary nature, humor research has many areas of study which aim to explain the phenomenon of humor.\n\nCognitive neuroscience has provided insight into how humor is neurologically realized. Brain imaging techniques such as fMRI and PET scans have been implemented in this subfield of humor research.\nThere are a few main regions of the human brain associated with humor and laughter. The production of laughter involves two primary brain pathways, one for involuntary and one for voluntary laughter (i.e., Duchenne and non-Duchenne laughter). Involuntary laughter is usually emotionally driven and includes key emotional brain areas such as the amygdala, thalamic areas, and the brainstem. Voluntary laughter, however, begins in the premotor opercular area in the temporal lobe and moves to the motor cortex and pyramidal tract before moving to the brainstem. Wild et al. (2003) propose that the generation of laughter is mostly influenced by neural pathways that go from the premotor and motor cortex to the ventral side of the brainstem through the cerebral peduncles. It is also suggested that real laughter is not produced from the motor cortex, but that the normal inhibition of cortical frontal areas stops during laughter.\n\nWhen the electrical activity of the brain is measured during and after hearing a joke, a prominent response can be seen approximately 300ms after the punchline, followed by a depolarization about 100ms later. The fact that humor response occurs in two separate waves of activity supports the idea that humor processing occurs in two stages.\n\nFunctional MRI and PET studies further illuminate which parts of the brain are participating in the experience of humor. A study by Ozawa et al. (2000) found that, when participants heard sentences that they rated as humorous, the Broca's area and the middle frontal gyrus were activated. Additionally, Wernicke's area and the transverse temporal gyri were activated, but these areas also were also found to be active in control (non-humorous) conditions.\n\nAnother study using fMRI showed that the linguistic basis of jokes participants found to be humorous impacted which parts of the brain were activated. In response to puns, the left posterior middle temporal gyrus and the left inferior frontal gyrus were activated. When listening to semantic jokes, the left posterior middle temporal gyrus was again activated, as were the left posterior inferior temporal gyrus, the right posterior middle temporal gyrus, and the cerebellum. Brain activity in the medial ventral prefrontal cortex was associated with ratings of funniness that the participants gave after the brain scan and initial humor response. This response may stem from the mood or emotional change that occurs after hearing humor.\n\nInduction of laughter through direct brain stimulation has been reported in a number of studies, and includes areas such as ACC, globus pallidus, floor of the third ventricle, and most recently left superior frontal gyrus – though these results are hard to draw inferences from (may be inhibitory, may be artefacts, etc.) Because the nature of laughter is so complex—involving facial muscles, respiratory actions, etc.--a control center has been hypothesized in the upper pons.\n\nStudies within play research have provided correlates to the study of humor, as play often takes on a humorous demeanor. A child's social play often invokes the use of jokes, non-serious social incongruity, physical slapstick humor. Studies on how play \"promot[es] social cohesion, cooperation, and even altruism,\" have been used to describe humor's function. Laughter is often a byproduct of playful social interactions, and can therefore be viewed as serving a similar function as play. It can be said that the perception and appreciation of humor decreases aggression and stress while promoting cooperation and fairness. Play research can provide a functional look at humor in its relation to social interactions.\n\nEvolutionary theorists have attempted to study and explain the phenomena of laughter and humor in terms of survival benefit. Laughter-like behavior is not unique to humans, but humans do display a much more consistent and complex use of humor and laughter than other animals The evolution and functions of laughter and humor have been explored in an attempt to understand how and why humor and laughter have become part of human existence.\n\nDuchenne laughter refers to laughter that is stimulus-driven and linked to some positive experience. This is usually the result of a perceived social incongruity. Non-Duchenne laughter refers to laughter that is unconnected to any emotional experience, but rather laughter that is originated voluntarily. Duchenne laughter evolved before non-Duchenne laughter with the function of making play and playful emotions contagious. In these situations, laughter serves as an in-group designator. Early evolutionary laughter is also theorized to serve the function of dissolving social tension. During moments of displays of social superiority, laughter could be used to dissolve tensions that otherwise could lead to fighting or exclusion from a group, by designating those situations as play rather than as a real challenge. Duchenne laughter, which should be thought of as emotionally-valenced rather than simply spontaneous, can increase positive affect and mood of an individual as well as a group. So, early laughter most likely provided survival benefits through effects of emotional contagion that served to strengthen within-group ties. Non-duchenne laughter developed later than Duchenne laughter. Non-duchenne laughter was co-opted for a variety of other social situations as laughter and humor already existed. Voluntarily accessing the laughter system that was already in place would serve to strengthen in-group fitness just as genuine, emotional laughter did before it, but now it could be done at will. Thus, non-Duchenne laughter developed to express things like aggression, nervousness, hierarchy positions, and to do things such as manipulate, appease, and make fun of others. Even when non-Duchenne laughter is unconscious, that does not mean it is necessarily emotional or using the same brain pathways as genuine Duchenne laughter. As social situations became more complex in later hominid stages of evolution, so did the functions and usefulness of laughter. As linguistics developed, proto-laughter and proto-humor were both co-opted for assistance in the production of and response to effective communication.\n\nIt has been hypothesized that intentional humor evolved as indicator of intelligence. Humor is often rated as important in mate selection, and it may be that partners are selecting for genetic fitness. General intelligence predicted higher ratings on humor tasks, even after controlling for Big Five personality factors.\n\nGreengross & Miller propose that humor plays into sexual selection. Rather than being sexually attractive for its own sake, humor is attractive because it is an indicator of other things which humor is correlated with, such as intelligence, [creativity], and other desirable traits. Intelligence has been shown to be a mental fitness indicator and is highly sexually desirable for both traits Intelligence and humor ability are positively correlated and the ability to produce humor on the spot predicts mating success. In line with theories regarding sex differences of mating strategies, humor ability was rated higher by a blind panel of judges when produced from males than females.\n\nA large proportion of humor is produced during informal conversation and involves the derision of a specific person (either oneself or another). The adaptiveness of deprecating humor in social interactions is a point of interest among researchers. Drawing attention to one's own faults seems counter-intuitive when considering humor's function of strengthening within-group fitness and sexual desirability. Other-deprecating humor has functions which are implicated in establishing same-sex rivalries. This sort of humor usually targets fitness indicators of rivals, which can be risky if an opponent perceives the humor as a threat worthy of retaliation. Self-deprecating humor is proposed to fit within cost signaling theory. Greengross and Miller found that self-deprecating humor had predictive value for long-term sexual attractiveness. However, this occurred as an interaction effect, and only was predictive for high-status individuals. Self-deprecating humor may only work for high-status individuals because it ironically points out the desirable traits by way of discrepancy. In low-status individuals, this approach is unproductive, as the humor is pointing to actual weak fitness traits rather than a discrepancy as with high-status individuals.\n\nThere are two adaptive styles of humor and two maladaptive styles of humor. Affiliative and self-enhancing humor are the two adaptive styles. The defining feature of affiliative humor is humor that is used to strengthen interpersonal relationships or ease tensions within those relationships. Self-enhancing humor involves the use of a humorous outlook on situations in life as a coping tool. In this instance, humor is used to mitigate stress without targeting others or the self in a hurtful way. The two maladaptive humor styles are aggressive humor, which uses sarcasm and other humor styles to target or put down others and self-defeating humor, which uses self-deprecating tactics for the enjoyment of others. Martin et al. (2003) found that higher levels of adaptive humor are related to psychological well-being in terms of lower levels of depression and higher self-esteem. Self-defeating humor contrasts with the adaptive types, and is associated with poorer psychological health. Kuiper and McHale (2009) found support for humor styles being mediators between self-evaluation and psychological well-being. Simply put, rather than humor styles directly affecting psychological well-being, they are involved with how people self-evaluate, which results in different amounts of psychological well-being.\n\nIn a study examining humor as an intervention, random assignment to a humor group, a social group, and a non-intervention control group was used. A standardized manual and booklet were used in the intervention group, and results showed that the humor group demonstrated significant increases in emotional well-being. Importantly, self-efficacy, positive affect, optimism, and perceptions of control were found to go up while other negative measures decreased. Crawford & Caltabiano (2011) found support for humor as an intervention which may increase positive aspects of emotional well-being. Humor skills of the intervention group were targeted through the use of a manual. Humor increased several measures of well-being including self-efficacy, positive affect, optimism, and perception of control, even more than a “social group” control or a non-intervention control group. The exposure to humor and humor skills training was also found to decrease certain negative aspects of emotional well-being, including perceived stress, certain depressive symptoms, and anxiety.\n\nAdaptive components of humor show facilitative effects on psychological well-being. Maladaptive styles that were self-focused showed detrimental effects, while maladaptive styles that did not focus on self were unrelated to personal well beings. Self-deprecating humor is the specific component of maladaptive humor that results in decreased psychological well-being, while both of the adaptive styles of humor (affiliative and self-enhancing) are associated with positive psychological outcomes, such as greater self-esteem, lower depression and anxiety levels, and greater endorsement of self-efficacy. It is important to note that the relevant psychological states may have preceded the humor style rather than vice versa.\n\nHumor research includes investigations into the positive benefits of humor, sense of humor, and laughter on physical health. In recent decades, humor research has seen a surge in publications in part because of Norman Cousins and his claims that he became cured from ankylosing spondilitis due to a daily regimen which included humor and laughter.\n\nPositive affect is theorized to influence physical health by means of the immune system. The immune system could be influenced positively by humor in a number of ways. The possibilities include simple immunoenhancement, stress moderation, or both. The pleasant experience of humor is thought to positively influence affect so that stress is moderated. The antibody secretory immunoglobin A (S-IgA) is associated with both positive and negative affect states, with negative states decreasing effectiveness and positive states increasing effects of the antibody. Levels of immune response can be predicted by the number of desirable events (and presumably mood) which occur in preceding days to the immune assessment. However, there is also evidence that significant mood changes in either direction can induce an increase in immune response. Laughter as an overt behavior is also associated with immune response, specifically NK cell activity, which are part of the body’s immune response. Glucocorticoids are lower in subjects who laughed while watching a humorous video, and glucocorticoids decrease NK cell activity, so humor theory posits this pathway as a possible mechanism for humor as a benefit to immune responses. However, it is unclear whether these effects on this specific immune response measurement have long-term effects with clinical significance. Using humor as a coping style is a way which these long-term effects on the immune system may be enacted. In one study, levels of S-IgA and the use of humor as a coping style were positively correlated, although the saliva sampling procedure of this study was subsequently called into question.\n\nHumor is thought to play a role in the levels of stress that people experience, both in the short-term and long-term. Watching a humorous video for 20 minutes can significantly lower stress levels, on a level comparable to 20 minutes of exercise. Since watching a video is more widely feasible than exercising for 20 minutes for large portions of the population, humor researchers hope that the acute anxiety-alleviating effects of humor can be used in a therapeutic setting. Although acute benefits of humor may be obtained, this does not guarantee any long term benefits.\n\nHumor's effect on pain tolerance is another point of interest within humor research. Hypotheses for this research include the idea that the positive feelings of humor will increase the threshold of pain that a person can endure. These ideas are implicit in some folk wisdom as well as in anecdotes such as Norman Cousins' recovery story. Though not all studies have showed support for the pain-reducing effect of humor, humor and comedy stimuli do have effects on pain tolerance and perception, particularly when pain is assessed after the humor stimuli is completed. Based on the measures of most studies examining pain, it is difficult to tell if simply consuming humorous material is sufficient for pain reduction, or if actual laughter is a necessary component. Further, some studies that used negative affect control groups found that pain thresholds also increased for people experiencing unsettling stimuli, such as a clip from a horror movie. So, humor does have an effect on pain, but it is unclear what exact behavioral requirements are necessary as well as if humor has an effect greater than other emotionally salient experiences.\n\nEven though the positive benefits of humor have been lauded by popular media and have found support among scientific research, the field of humor research also has received criticism. The scientific support for the physical benefits of humor are somewhat sparse, and the findings from the existing research have been called into question in some cases. Humor is assessed differently based on the theories which underpin the research, such as whether humor is a stable personality trait or not. Because of the non-standard measurements of humor in research, it is difficult to tell whether researchers are measuring the same thing across studies. Sense of humor, internal humor response, and laughter can all be targeted to measure humor, but are not necessarily interchangeable. Martin (2001) critiques several studies for not measuring laughter along with their self-report humor measures.\n\nAside from theoretical issues, the methods of some key humor research was also called into question by Martin. For example, the immune response sampling techniques of some humor research may have not been reliable because of how baseline immune levels were set. According to Martin (2001), within the nine studies assessing S-IgA levels in response to humor, three did not have control groups, five had control groups which involved a documentary viewing where interest was not assessed, and only one study had a negative affect control. Proper use of control groups is among the major issues within humor research. Some experimental findings are encouraging, but no firm conclusions involving humor and physical health where controls were not used correctly can be drawn. Thus, there is only partial support of immune effects of humor.\n\nIn 2002, a study was conducted on the functions of humor in close relationships. The study examined three types of humor:\n\n\nPositive humor is expected to be related to intimacy and marital satisfaction. This relation was true for the wives, but not for the husbands. This suggests that whether or not husbands use positive humor, it has nothing to do with how closely they feel to their partner or their marital satisfaction. Research suggests that instrumental humor is negatively related to marital satisfaction and is an indicator of potential future marital deterioration. Results showed that husbands acknowledged using instrumental humor in their relationship in relation to demand-withdrawal, in which the wife demands and the husband withdraws. Wives, on the other hand, did not acknowledge the use of instrumental humor in this communication pattern. This seems to indicate that men more often withdraw than women, in this case, with the use of humor. Negative humor was not related to any of the other humor measure in the study. This makes sense considering that put-downs or hurtful humor does not tend to lead to laughter or humor appreciation. Negative humor items in the questionnaire given to the participants described expressing negative emotion. Negative humor somewhat correlated with demand-withdrawal for husbands and wives. This humor can be a form of passive aggressiveness where the one using the humor is not taking responsibility for the put-down or hurtful joke, and in doing so, avoiding, or withdrawing from, conflict.\n\nWhen someone finds something funny, there are different ways of expressing it. Common responses of humor include laughing and smiling. In 2008, a study was conducted using 155 undergraduate students at North London University in order to measure responses to humor using a British comedy. The participants were divided into one of three categories: watched a video of the comedy, listened to an audiotape of the comedy, or read a script of the comedy. Approximately half of the participants were observed by an overt video camera and half were observed by a covert video camera. Results showed that participants laughed and smiled much more frequently when watching a video of the comedy and listening to the audiotape of the comedy than when reading a script. The difference in the frequency of smiling and laughing between the video and the audiotape was not significant. Participants laughed and smiled more frequently when observed by a covert video camera than an overt video camera. Aspects of the video and audiotape such as visualization of the acting, auditory representation, and also the presence of audience laughter significantly increase the frequency of laughter and smiling.\n\n\n"}
{"id": "25454239", "url": "https://en.wikipedia.org/wiki?curid=25454239", "title": "Masculism", "text": "Masculism\n\nMasculism or masculinism may variously refer to advocacy of the rights or needs of men and boys; and the adherence to or promotion of attributes (opinions, values, attitudes, habits) regarded as typical of men and boys. \n\nThe \"Oxford English Dictionary\" (OED) defines \"masculism\", or synonymously \"masculinism\", as the \"advocacy of the rights of men; adherence to or promotion of opinions, values, etc., regarded as typical of men; (more generally) anti-feminism, machismo.\"\n\nHowever, philosopher Ferrell Christensen differentiates \"masculism\" from \"masculinism\", defining the latter as promoting the attributes of manliness. \nPolitical scientist Georgia Duerst-Lahti also distinguishes the two terms, with masculism expressing the ethos of the early gender-egalitarian men's movement, while masculinism refers to the ideology of patriarchy.\n\nChristensen differentiates between \"progressive masculism\" and an \"extremist version\". The former welcomes many of the societal changes promoted by feminists, while believing that some measures reducing sexism against women have increased it against men. Nicholas Davidson, in his book \"The Failure of Feminism\", describes an extremist version of masculism which he termed \"virism\": \"What ails society is 'effeminacy'. The improvement of society requires that the influence of female values be decreased and the influence of male values increased….\" \n\nGender theories, which have frequently focused on woman-based or feminist approaches, have come to examine oppression in a masculist society also from the perspectives of men, many of whom are also subjected by that society. From a feminist perspective to philosophy, masculinism seeks to value and include only male views, and claim \"that anything that cannot be reduced or translated in men's experience should be excluded from the subject-matter of philosophy.\n\nMany masculists oppose co-educational schooling, believing that single-sex schools better promote the well-being of boys. Other masculists and equity feminists say that boys lag behind girls in educational achievement.\n\nData from the U.S. in 1994 reported that men suffer 94% of workplace fatalities. Masculist Warren Farrell has argued that men do a disproportionate share of dirty, physically demanding, and hazardous jobs.\n\nMasculists cite higher rates of suicide in men than women. Masculists express concern about violence against men being depicted as humorous, in the media and elsewhere.\n\nThey also express concern about violence against men being ignored or minimized in comparison to violence against women, some asserting gender symmetry in domestic violence. Another of their concerns is that traditional assumptions of female innocence or sympathy for women, termed benevolent sexism, may lead to unequal penalties for women and men who commit similar crimes, to lack of sympathy for male victims in domestic violence cases, and to dismissal of female-on-male sexual assault and sexual harassment cases.\n\nAccording to David Benatar, head of philosophy at the University of Cape Town, \"Custody law is perhaps the best-known area of men's rights activism\", as it is more likely in most parts of the world for the mother to obtain custody of children in case of divorce. He argues: \"When the man is the primary care-giver his chances of winning custody are lower than when the woman is the primary care-giver. Even when the case is not contested by the mother, he's still not as likely to get custody as when the woman's claim is uncontested\".\n\nIn the wake of the abolition of apartheid, South Africa has seen a resurgence of masculist Christian evangelical groups, led by two complementary men's and women's movements, the Mighty Men movement and the Worthy Women movement. The Mighty Men movement harkens back to the Victorian idea of Muscular Christianity and the movement does not lead discussions about institutionalized racism. Feminist scholars argue that the movement's lack of attention to women's rights and their historical struggle with racial equality makes it a threat to women and to the stability of the country. Scholar Miranda Pillay argues that the Mighty Men movement appeal lies in its resistance to gender equality as incompatible with Christian values, and in raising patriarchy to a \"hyper-normative status,\" beyond challenge by other claims to power.\n\nThe Worthy Women movement is a women's auxiliary to Mighty Men in advocating \"men\"ism, a belief in the inherent superiority of men over women. Their leader is Gretha Wiid. She blames South Africa's disorder on the liberation of women, and aims to restore the nation through its families, making women again subservient to their men. Her success is attributed to her balancing claims that God created the gender hierarchy, but that women are no less valuable than men, and that restoration of traditional gender roles relieves existential anxiety in post-apartheid South Africa.\n\nSome critics believe masculism focuses on male superiority or male dominance (i.e. androcentrism) to the exclusion of women. Some masculinists believe that differentiated gender roles are natural. There is some evidence for social influences (e.g. gender division of labor, socialization) as the sole or primary origin of gender differentiation. Some parts of the masculinist movement have to some extent borrowed concepts from evolutionary psychology: this theory argues that adaptation during prehistory resulted in complementary but different roles for the different genders, and that this balance has been destabilized by feminism since the 1960s.\n\nSome masculinists have been described as explicitly antifeminist by feminist activists. According to Blais and Dupuis-Déri, \"the contents of [masculinist] websites and the testimony of feminists that we questioned confirm that masculinists are generally critical of even moderate feminists and feminists at the head of official feminist organizations.\" Some masculinist activism has involved disruption of events organized by feminists and lawsuits against feminist academics, journalists, or activists. Furthermore, masculinist actions are sometimes extreme; father's rights activists have bombed family courts in Australia and have issued bomb threats in the UK, although it is ambiguous whether there was public and organized militant group involvement. They have also engaged in \"tire-slashing, the mailing of excrement-filled packages, threats against politicians and their children.\" Spokesmen for these groups have also spoken out against public awareness campaigns to prevent sexual assault, arguing that they portray a negative image of men, and one masculinist group harassed administrators of dozens of battered women's shelters and women's centers. \n\nPhilosopher Ferrell Christensen states that if masculism and feminism refer to the belief that men/women are systematically discriminated against, and that this discrimination should be eliminated, there is not necessarily a conflict between feminism and masculism, and some assert that they are both. However, many believe that one sex is more discriminated against, and thus use one label and reject the other.\n\n\n\n\n\n\n\n"}
{"id": "5075805", "url": "https://en.wikipedia.org/wiki?curid=5075805", "title": "Media evaluation", "text": "Media evaluation\n\nMedia evaluation is a discipline of the social sciences and centres on the analysis of media content rating the exposure using a number of pre-designated criteria commonly including tonal value and presence of key messages. It is said to be one of the fastest growing areas of mass communications research. \n\nThe International Association for Measurement and Evaluation of Communication (AMEC) is the industry appointed trade body for companies and individuals involved in research, measurement and evaluation in editorial media coverage and related communications issues. To be a full member of AMEC, companies must be able to a) offer comprehensive media evaluation, research and interpretation services, b) been in business for at least two years and c) have media evaluation turnover in excess of £150,000 when applying. In addition all companies abide by a strict code of ethics and must implement tight quality control procedures. These conditions ensure that the media avaluation services offered are all to the very highest standard. Further detailed information on the industry is available from AMEC's website. Another organisation is the Commission on Public Relations Measurement & Evaluation which was formed under the auspices of the Institute for Public Relations in 1998. The Commission exists to establish standards and methods for public relations research and measurement, and to issue authoritative best-practices white papers.\n"}
{"id": "1574427", "url": "https://en.wikipedia.org/wiki?curid=1574427", "title": "Medical humanities", "text": "Medical humanities\n\nMedical humanities is an interdisciplinary field of medicine which includes the humanities (philosophy, ethics, history, literature and religion), social science ( psychology, sociology,anthropology, cultural studies, health geography) and the arts (literature, theater, film, and visual arts) and their application to medical education and practice. The core strengths of the medical humanities are the imaginative nonconformist qualities and practices. \n\nMedical humanities is mainly concerned with training medical practitioners. This contrasts with health humanities which more broadly links health and social care disciplines with the arts and humanities.\n\nMedical humanities can be defined as an interdisciplinary, and increasingly international endeavor that draws on the creative and intellectual strengths of diverse disciplines, including literature, art, creative writing, drama, film, music, philosophy, ethical decision making, anthropology, and history, in pursuit of medical educational goals. The humanistic sciences are relevant when multiple people’s perspectives on issues are compiled together to answer questions or even create questions. The arts can provide additional perspective to the sciences.\n\nCritical medical humanities is an approach which argues that the arts and humanities have more to offer to healthcare than simply improving medical education. It proposes that the arts and humanities offer different ways of thinking about human history, culture, behaviour and experience which can be used to dissect, critique and influence healthcare practices and priorities.\n\nMedical books, pictures, and diagrams help medical students build an appreciation for anything in the medical field from the human body to diseases.\n\nThe medical humanities can assist medical practitioners with viewing issues from more than one perspective, such as the visual arts and culture are supposed to do. Both patients and doctors/medical professionals deal with facing decision-making. Each person’s perspective of medical ethics is different from one another due to different cultures, religions, societies, and traditions. The humanities also assist and attempt to create a closer or more meaningful relationship between medical practitioners and their peers/patients. Ethics are perceived differently from person to person, so answering ethical questions requires the viewpoints of several different people who may have different opinions of what is right from wrong.\n\nThe first category is bioethics, which includes the morals of healthcare. As science and technology develop, so does healthcare and medicine, and there is discussion and debate in society and healthcare committees that go over the ethics of these certain situations that pertain to medical humanities. For example, one of these cases involves the practice of body enhancements in which the ethics of this practice are questioned due to the fact that bio-medical and technological practices are making changes to a person’s body to improve the body and/or its appearance.\n\nThe second category in ethics of the medical humanities is clinical ethics, which refers to the respect that healthcare professionals have for patients and families, and this helps develop a sort of professionalism, respectability, and expertise that healthcare professionals must use in respect to their patients. Another example in the ethics of the medical humanities is bias people and society have against others with disabilities, and how these disabilities correlate with success or what the disabled person is able to do. It is unethical to judge or assume the incapability of a disabled person because disabled people are able to find ways to become successful through modern technology and even through self-determination.\n\nVarious academic institutions offer courses of study in the ethics of medical humanities. These programs help their students learn professionalism in the medical field so that they may respectfully help their patients and do what it is right in any situation that may arise.\n\nFormerly called medicine in literature, literature and medicine is an interdisciplinary subfield of the medical humanities considered a \"dialogue rather than a merger\" between the literary and the medical. Literature and medicine is flourishing in undergraduate programs and in medical schools at all levels. The Pennsylvania State University College of Medicine-Hershey was the first to introduced literature into a medical school curriculum when Joanne Trautmann (Banks), an English professor, was appointed to a position in literature there in 1972. The rationale for using literature and medicine in medical education is three-fold: reading the stories of patients and writing about their experiences gives doctors in training the tools they need to better understand their patients; discussing and reflecting on literature brings the medical practitioner's biases and assumptions into focus, heightening awareness; and reading literature requires critical thinking and empathetic awareness about moral issues in medicine.\n\n\n\n"}
{"id": "57184777", "url": "https://en.wikipedia.org/wiki?curid=57184777", "title": "Mierzanowice culture", "text": "Mierzanowice culture\n\nThe Mierzanowice culture appeared in the area of the upper and middle basin of the Vistula, during the Early Bronze Age. It evolved from the so-called Protomierzanowice cultural unit. The name of the culture comes from an eponymous site in Mierzanowice, where the cemetery was located. This entity was part of the precarpathian sphere epicorded cultures and it has been divided into three local groups: samborzecka, iwanowicka and pleszowska. Initial phases of functioning of the mentioned culture are characterized by a small number of burials, seasonal settlements and single artifacts. The area of the Mierzanowice culture spread over from western Slovakia, through south - eastern Poland, reaching in the east the areas of the Volhynian Upland.\n\nBased on relative dating, Mierzanowice culture appeared in the Early Bronze Age. According to the archaeologist Jan Machnik, we distinguish the older and younger phase of this cultural unit. The discovery in Szarbia Zwierzyniecka allowed for a certain \"rejuvenation\" of the Mierzanowice culture as a result of the distinction of its late phase called \"szarbiańska\". The younger phase of the Mierzanowice culture was expended at the end of bronze A1 and the beginning of bronze A2 according to Paul Reinecke chronology. \n\nCemeteries of the Mierzanowice cultural population were established near the settlements. The largest cemeteries were from 150 to 300 burials. The burial occurred mainly in skeletal form. Human remains were put into oval or rectangular burial pits or in coffins made of wooden logs. There were two systems for arranging human corpses: in the shrunken position and in straight position. The bodies of men were buried on the right side while corpses of women on the left side. The results of investigations conducted at the cemeteries of the Mierzanowice culture, showed a little advantage of men's graves over women's graves, which had slightly poorer grave inventory. \n\nSettlements of the Mierzanowice culture in most cases are represented by small and seasonal camps. Settlements with a larger area were founded on the hills with a naturally defensive character, near water reservoirs. A relatively large part of the archaeological sites of this culture are found on loess uplands. The best – known settlement of the Mierzanowice culture is archaeological site called Iwanowice. In the classical phase of the Mierzanowice culture, the settlements were mostly accompanied by cemeteries. \n\nOne of the most common objects discovered in archaeological sites in the Mierzanowice culture are axes and sickles. Another type of artifacts are neklaces made of faience and bones. The Mierzanowice culture is well known for its earrings in the shape of a willow leaf, often produced in local workshops. Military objects discovered in the settlements are primarily arrowheads on the shape of leaves. The faience beads are an extremely common element of the funeral inventory. The next category is pottery. In vascular ceramics the influences of the Corded Ware culture are visible. Pottery of the late phase of Mierzanowice culture is characterized by a huge variety of forms and ornamentation\n\n"}
{"id": "55223808", "url": "https://en.wikipedia.org/wiki?curid=55223808", "title": "Miscarriage and grief", "text": "Miscarriage and grief\n\nMiscarriage and grief are both an event and subsequent process of grieving that develops in response to a miscarriage. Almost all those experiencing a miscarriage experience grief. This event is often considered to be identical to the loss of a child and has been described as traumatic. \"Devastation\" is another descriptor of miscarriage. Grief differs from the emotion sadness. Sadness is an emotion along with grief, on the other hand, is a response to the loss a of the bond or affection was formed and is a process rather than one single emotional response. Grief is not equivalent to depression. Grief also has physical, cognitive, behavioral, social, cultural, and philosophical dimensions. Bereavement and mourning refer to the ongoing state of loss, and grief is the reaction to that loss. Emotional responses may be bitterness, anxiety, anger, surprise, fear, and disgust and blaming others; these responses may persist for months. Self-esteem can be diminished as another response to miscarriage. Not only does miscarriage tend to be a traumatic event, women describe their treatment afterwards to be worse than the miscarriage itself.\n\nA miscarriage can often be \"heart-breaking\". A miscarriage can effect the women, husband, partner, siblings, grandparents, the whole family system and friends. Almost all those experiencing a miscarriage go through a grieving process. Serious emotional impact is usually experienced immediately after the miscarriage. Some may go through the same loss when an ectopic pregnancy is terminated. In some, the realization of the loss can take weeks. Providing family support to those experiencing the loss can be challenging because some find comfort in talking about the miscarriage while others may find the event painful to discuss. The father of the baby can have the same sense of loss. Expressing feelings of grief and loss can sometimes be harder for men. Some women are able to begin planning their next pregnancy after a few weeks of having the miscarriage. For others, planning another pregnancy can be difficult. Organizations exist that provide information and counselling to help those who have had a miscarriage. Some women have a higher risk of developing prolonged grief and complicated grief than others.\n\nMiscarriage has an emotional effect and can also lead to psychological disorders. One discorder that can develop is primary maternal preoccupation. This is defined as a \" ...'special psychiatric condition' in which the pregnant woman identifies with her baby, highlights the crisis a woman faces when the baby with whom she is preoccupied and identified dies...\" Grieving manifests itself differently for each woman after miscarriage. It may often go unrecognized. The grief that follows a miscarriage resembles, but is not the same as, the grief experienced after the loss of a family member. Disbelief, depression, anger, and yearning, are described as being a part of the normal grieving process. These reactions remain from three to nine months after the loss. Forty-one percent of parents experience a normal, expected decline in grief in the first two years while 59% were delayed in the resolution of their grief.\n\nGrieving can create feelings of loneliness.\nThis grieving has been called a type of psychological trauma. Other serious consequences can develop including depression, anxiety disorder, post-traumatic stress disorder, and somatoform disorder. These responses all are associated with grieving after a miscarriage. Some women are able to complete the grieving process a few weeks after the miscarriage and start anticipating their next pregnancy. Planning another pregnancy is traumatic for others. The impact of a miscarriage can be \"crippling\" psychologically. Anger can be directed toward those who have had successful pregnancies and children. A woman can grieve the \"loss of a future child\" and question her own role as a mother. They may blame themselves or their partner for the miscarriage.\n\nUnsuccessful attempts to become pregnant through in vitro fertilization (IVF) can also illicit a similar grief response in women. Those experiencing a late miscarriage may have more significant distress compared to those who have experienced a miscarriage in the first trimester. Even depression can occur.\n\n\"Women today...are aught in a unique historical moment: technology encourages them to form emotional attachments to their pregnancies, but society has not developed traditions to cushion the shock when those attachmets are shattered.\"\n\nDescriptions of the miscarriage are expressed in non-clinical terms by those who have experienced the event.\n\nMiscarriage has been found to be a traumatic event and a major loss for women. Pregnancy loss, including induced abortion is a risk factor for mental illness. The impact of miscarriage can be underestimated. The trauma can be compounded if the miscarriage was accompanied by visible and relatively large amounts of blood loss.\nThe trauma of miscarriage can be compounded if the miscarriage was accompanied by visible and relatively large amounts of blood loss.\nBipolar disorders are associated with miscarriage. Depression and bilpolar disorder becomes evident after a miscarriage in 43% of women. Some women are more likely to experience complicated and prolonged grief than others.\n\nWomen experiencing miscarriage are at risk for grief reactions, anxiety or depression. Obsessiveness regarding the miscarriage can develop. Primary maternal preoccupation is also considered a consequence of miscarriage. This condition can occur if a woman who develop a close bond \"with her baby\" experiences the loss of the pregnancy.\n\nDifferent grieving \"styles\" can exist and vary between individuals. There can be a complete avoidance of dealing with the memories of the miscarriage and there can be an \"obsessive\" concentration on an event in the miscarriage. This is in contrast with the expected ability to \"reminisce about the loss of a loved one\". Complicated grief differs from the more common form of grief that occurs after a miscarriage. The grieving process associated with other events such as the loss of a spouse or parent is expected to decline in predictable and steady rate. This not true for those experiencing grief after a miscarriage because only 41% follow the expected decline in grief while most, 59% do not fit this pattern.\n\nMiscarriage is associated with post traumatic stress disorder.\nRisks for developing PTSD after miscarriage are: emotional pain, expressions of emotion, and low levels of social support. Even if relatively low levels of stress occur after the miscarriage, symptoms of PTSD including flashbacks, intrusive thoughts, dissociation and hyperarousal can later develop. The effects of stress can complicate miscarriage. Miscarriage is a stressful event and because stress is a risk factor for subsequent miscarriage, its presence can become part of cycle that continues. Lower stress levels are associated with more favorable outcomes in future pregnancies while higher stress levels increase the risk.\n\nPhysical recovery from miscarriage can have an effect on emotional disturbances. The body has to recover from the sudden pregnancy loss. In some instances ffatigue is present. Insomnia can be a problem. The miscarriage is very upsetting to the family and can generate very strong emotions. Some women may feel that the miscarriage occurred because they somehow had caused it. Others may blame the father or partner for the miscarriage. Coping with a miscarriage can very greatly between women and families. Some find it difficult to talk about the miscarriage. The narratives of women tend to coincide with quantified and measurable effects. Some women engage in activities that are believed to aid in recovery such as therapy, religion and art.\nCounseling can be offered but effective interventions to assist in recovery have been difficult to identify due to the reports of efficacy and ineffective counseling. Comparisons are hard to make. Despite the lack of studies that describe effective interventions for those with grief after a miscarriage, some clinicians still offer counselling and follow-up to help women recover and adapt to the loss.\n\nRecommendations to help recover from the event include:\n\nGenerally, the impact of experiencing miscarriage is underestimated. Other methods used to promote recovery are be relaxation techniques, guided imagery, and \"thought-stopping\". Even Gestalt role-playing has been used. Some women can \"emotionally relocate the child\", redefine a relationship \"with the missing child\", and engage in \"continuing the bond\" to incorporate the loss into their life experiences.\n\nWomen who have miscarried report that they were dissatisfied with the care they received from physicians and nurses. One observer highlights the insensitivity of some health care providers when they approach the grieving mother \"...by playing down her emotion as somehow an irrational response...\" Clinicians may not recognize the psychological impact of the miscarriage and can \"expect parents to move on with their lives.\"\n\nSince the experiences of women can vary so widely, sensitive nursing care afterward is appropriate.\n\nOne emotional response to miscarriage is the strong apprehension that can develop anticipating a subsequent pregnancy. Procreation abilities may also be questioned by the woman. Significant distress can develop in the other children in the family when they think a sibling has died. They may regard this as a baby they did not get to meet. They can also experience grief and guilt find it difficult to express these emotions to their parents. The siblings may feel a need to act as if everything is the same and that they are unaffected in an attempt to protect their parents from their own feelings. Children can also need help, understanding and the ability to make sense of the event.\n\nRituals that recognize the loss can be important in coping. Family and friends often conduct a memorial or burial service. Hospitals also can provide support and help memorialize the event. Depending on locale others desire to have a private ceremony. The religious faith of the woman and family impacts the grief process. Conversely, the lack of recognition that the miscarriage has occurred by family and friends can be troubling and add to the trauma of the event.\nGrieving after the loss of a child through miscarriage in other cultures can vary from western culture. An individual’s culture plays a large role in determining an inappropriate pattern of grief, and it is appropriate to take into account cultural norms before reaching a complicated grief diagnosis. There are cultural differences in emotional levels, how these are expressed and how long they are expressed. External symptoms of grief differ in non-Western cultures, presenting increased somatization. Narratives by Swedish women include their own perception of losing a child. Investigations describe their grief over their miscarriage: \"When miscarriage occurs it is not a gore, an embryo, or a fetus they lose, it is their child. They feel that they are the cause of the miscarriage through something they have done, eaten, or thought. They feel abandonment and they grieve for their profound loss; they are actually in bereavement.\" Native American women have cut their long hair following the death of a family member. The narratives of women tend to coincide with quantified and measurable effects. In women who are induced to have an abortion, an identical grieving process can occur. The emotional responses to a spontaneous abortion (miscarriage) and an elective abortion are sometimes identical. Spanish women experience grief in much the same way in the rest of Western culture. Some women find online forums helpful.\n\n\n\n"}
{"id": "14120220", "url": "https://en.wikipedia.org/wiki?curid=14120220", "title": "Neuroarthistory", "text": "Neuroarthistory\n\nNeuroarthistory is a term coined by Professor John Onians, an art historian at the University of East Anglia in 2005. Neuroarthistory is an approach that concerns the neurological study of artists, both living and dead.\n\nIn 2004 Onians taught the Postgraduate module \"Art and the Brain\" named after the 1999 paper by Professor Semir Zeki which was the first postgraduate course in an art history department that applied neuroscientific principles. In 2005, he began working with Zeki, who is a professor of Neurobiology at University College London (and founder of Neuroesthetics) to study what goes on in the brains of artists. They used neuroimaging and studied the neurobiological processes of artists such as the painters of the paleolithic Chauvet Cave art.\n\nIn May 2005 Onians founded Neuroarthistory in a lecture at the Neuroaesthetics Conference Goldsmiths May 2005. \nIn 2006, he wrote and presented the paper 'Neuroarthistory: making more sense of art' which, according to The Art Book \"explored the ways in which our ever-expanding knowledge of the brain invites art historians to reconsider the interaction between the senses and cognition.\"\n\nIn September 2006, Onians presented the results of the research to the BA Festival of Science in a lecture called 'Cracking the real Da Vinci Code: what happens in the artist’s brain?'. The object of the study was to learn more about how artists think, and how these thought processes differ between artists of different eras and locations, as well as the differences between professional and amateur artists. Professor Onians has said that neuroarthistory can be used \"both to better understand the nature of familiar artistic phenomena such as style, and to crack so far intractable problems such as ‘what is the origin of art?’\" According to a press release, neuroarthistory can explain why \"Florentine painters made more use of line and Venetian painters more of colour. The reason is that 'neural plasticity' ensured that passive exposure to different natural and man-made environments caused the formation of different visual preferences.\"\n\nA book, \"Neuroarthistory: From Aristotle and Pliny to Baxandall and Zeki\" was published in 2008 by Yale University Press, discussing a number of case studies from art history, and being the 'preface' to a further two books on the subject. \n"}
{"id": "15841839", "url": "https://en.wikipedia.org/wiki?curid=15841839", "title": "Nordicom", "text": "Nordicom\n\nThe Nordic Information Centre for Media and Communication Research (Nordicom) is a non-profit knowledge center based at the University of Gothenburg that works to collect and communicate media and communication research and facts conducted in the Nordic countries. The purpose of Nordicom work is to develop the knowledge of media´s role in society. This is done through: \n\nNordicom is financed by the Nordic Council of Ministers, the Swedish Ministry of Culture and the University of Gothenburg. \n\n"}
{"id": "504974", "url": "https://en.wikipedia.org/wiki?curid=504974", "title": "Patronage", "text": "Patronage\n\nPatronage is the support, encouragement, privilege, or financial aid that an organization or individual bestows to another. In the history of art, arts patronage refers to the support that kings, popes, and the wealthy have provided to artists such as musicians, painters, and sculptors. It can also refer to the right of bestowing offices or church benefices, the business given to a store by a regular customer, and the guardianship of saints. The word \"patron\" derives from the (\"patron\"), one who gives benefits to his clients (see Patronage in ancient Rome).\n\nIn some countries the term is used to describe political patronage, which is the use of state resources to reward individuals for their electoral support. Some patronage systems are legal, as in the Canadian tradition of the Prime Minister to appoint senators and the heads of a number of commissions and agencies; in many cases, these appointments go to people who have supported the political party of the Prime Minister. As well, the term may refer to a type of corruption or favoritism in which a party in power rewards groups, families, ethnicities for their electoral support using illegal gifts or fraudulently awarded appointments or government contracts.\n\nFrom the ancient world onward, patronage of the arts was important in art history. It is known in greatest detail in reference to medieval and Renaissance Europe, though patronage can also be traced in feudal Japan, the traditional Southeast Asian kingdoms, and elsewhere—art patronage tended to arise wherever a royal or imperial system and an aristocracy dominated a society and controlled a significant share of resources. Samuel Johnson defined a patron as \"one who looks with unconcern on a man struggling for life in the water, and, when he has reached ground, encumbers him with help\".\n\nRulers, nobles and very wealthy people used patronage of the arts to endorse their political ambitions, social positions, and prestige. That is, patrons operated as sponsors. Most languages other than English still use the term \"mecenate\", derived from the name of Gaius Maecenas, generous friend and adviser to the Roman Emperor Augustus. Some patrons, such as the Medici of Florence, used artistic patronage to \"cleanse\" wealth that was perceived as ill-gotten through usury. Art patronage was especially important in the creation of religious art. The Roman Catholic Church and later Protestant groups sponsored art and architecture, as seen in churches, cathedrals, painting, sculpture and handicrafts.\n\nWhile sponsorship of artists and the commissioning of artwork is the best-known aspect of the patronage system, other disciplines also benefited from patronage, including those who studied natural philosophy (pre-modern science), musicians, writers, philosophers, alchemists, astrologers, and other scholars. Artists as diverse and important as Chrétien de Troyes, Leonardo da Vinci and Michelangelo, William Shakespeare, and Ben Jonson all sought and enjoyed the support of noble or ecclesiastical patrons. Figures as late as Wolfgang Amadeus Mozart and Ludwig van Beethoven also participated in the system to some degree; it was only with the rise of bourgeois and capitalist social forms in the middle 19th century that European culture moved away from its patronage system to the more publicly supported system of museums, theaters, mass audiences and mass consumption that is familiar in the contemporary world. \nThis kind of system continues across many fields of the arts. Though the nature of the sponsors has changed—from churches to charitable foundations, and from aristocrats to plutocrats—the term \"patronage\" has a more neutral connotation than in politics. It may simply refer to direct support (often financial) of an artist, for example by grants. In the latter part of the 20th century, the academic sub-discipline of patronage studies began to evolve, in recognition of the important and often neglected role that the phenomenon of patronage had played in the cultural life of previous centuries.\n\nCharitable and other non-profit making organisations often seek an influential figurehead to act as patron. The relationship often does not involve money. As well as conferring credibility, these people can use their contacts and charisma to assist the organisation to raise funds or to affect government policy. The British Royal Family are especially prolific in this respect, devoting a large proportion of their time to a wide range of causes.\n\nSometimes consumers support smaller or local businesses or corporations out of loyalty even if less expensive options exist. Their regular custom is referred to as 'patronage'. Patronage may entitle members of a cooperative to a share of the surplus or profit generated by the co-op, called a \"patronage refund\". This refund is a form of dividend.\n\nIn the Church of England, \"patronage\" is the commonly used term for the right to present a candidate to a benefice.\n\nThe liturgical feast of the Patronage of Our Lady was first permitted by Decree of the Sacred Congregation of Rites on 6 May 1679, for all the ecclesiastical provinces of Spain, in memory of the victories obtained over the Saracens, heretics and other enemies from the sixth century to the reign of Philip IV of Spain. Pope Benedict XII ordered it to be kept in the Papal States on the third Sunday of November. To other places it is granted, on request, for some Sunday in November, to be designated by the ordinary. In many places the feast of the Patronage is held with an additional Marian title of Queen of All Saints, of Mercy, Mother of Graces. The Office is taken entirely from the Common of the Blessed Virgin, and the Mass is the \"Salve sancta parens\".\n\nThe Church Patronage (Scotland) Act 1711, (in force until 1874) resulted in multiple secessions from the Church of Scotland, including the secession of 1733, which led to the formation of the Associate Presbytery, the secession of 1761, which led to the formation of the Relief Church, and the Disruption of 1843, which led to the formation of the Free Church of Scotland.\n\nWhile most news companies, particularly in North America are funded through advertising revenue, secondary funding sources include audience members and philanthropists who donate to for-profit and non-profit organizations.\n\nPolitical leaders have at their disposal a great deal of patronage, in the sense that they make decisions on the appointment of officials inside and outside government (for example on quangos in the UK). Patronage is therefore a recognized power of the executive branch. In most countries the executive has the right to make many appointments, some of which may be lucrative (see also sinecures). In some democracies, high-level appointments are reviewed or approved by the legislature (as in the advice and consent of the United States Senate); in other countries, such as those using the Westminster system, this is not the case. Other types of political patronage may violate the laws or ethics codes, such as when political leaders engage in nepotism (hiring family members) and cronyism such as fraudulently awarding non-competitive government contracts to friends or relatives or pressuring the public service to hire an unqualified family member or friend.\n\nPolitical patronage, also known as \"Padrino System\" also a slang call as \"balimbing\" (starfruit), in the Philippines, has been the source of many controversies and corruption. It has been an open secret that one cannot join the political arena of the Philippines without mastery of the Padrino System. From the lowest Barangay official, to the President of the Republic, it is expected that one gains political debts and dispenses political favor to advance one's career or gain influence, if not wealth.\n\nAfter Soviet leader Vladimir Lenin's retirement from politics in March 1923 following a stroke, a power struggle began between Soviet Premier Alexei Rykov, Pravda editor Nikolai Bukharin, Profintern leader Mikhail Tomsky, Red Army founder Leon Trotsky, former Premier Lev Kamenev, Comintern leader Grigory Zinoviev, and General Secretary Joseph Stalin. Stalin used patronage to appoint many Stalinist delegates (such as Vyacheslav Molotov, Lazar Kaganovich, Grigory Ordzhonikidze, and Mikhail Kalinin) to the Party Politburo and Sovnarkom in order to sway the votes in his favour, making Stalin the effective leader of the country by 1929.\n\nDuring 2012, the African National Congress (ANC) mayor of Beaufort West in the Western Cape Province wrote a letter which openly and illegally solicited funds from the Construction Education and Training Authority for the ANC's 2016 election campaign. This episode, amongst many others including instances revolving around president Jacob Zuma, revealed how the African National Congress as ruling political party utilized patronage to reward supporters and strengthen the leading faction of the party's control over governmental institutions.\n\nIn the United States during the Gilded Age, patronage became a controversial issue. Tammany boss William M. Tweed was an American politician who ran what is considered now to have been one of the most corrupt political machines in the country's history. Tweed and his cronies ruled for a brief time with absolute power over the city and state of New York. At the height of his influence, Tweed was the third-largest landowner in New York City, a director of the Erie Railway, the Tenth National Bank, and the New-York Printing Company, as well as proprietor of the Metropolitan Hotel. At times he was a member of the United States House of Representatives, the New York City Board of Advisors, and the New York State Senate. In 1873, Tweed was convicted for diverting between $40 million and $200 million of public monies.\n\nSix months after James Garfield became president in 1881, Charles J. Guiteau, a disappointed office-seeker, assassinated him. To prevent further political violence and to assuage public outrage, Congress passed the Pendleton Act in 1883, which set up the Civil Service Commission. Henceforth, applicants for most federal government jobs would have to pass an examination. Federal politicians' influence over bureaucratic appointments waned, and patronage declined as a national political issue.\n\nBeginning in 1969, a Supreme Court case in Chicago, Michael L. Shakman v. Democratic Organization of Cook County, occurred involving political patronage and its constitutionality. Shakman claimed that much of the patronage going on in Chicago politics was unlawful on the grounds of the first and fourteenth amendments. Through a series of legal battle and negotiations, the two parties agreed upon The Shakman Decrees. Under these decrees it was declared that the employment status of most public employees could not be affected positively or negatively based on political allegiance, with exceptions for politically inclined positions. The case is still in negotiation today, as there are points yet to be decided.\n\nPolitical patronage is not always considered corrupt. In the United States, the U.S. Constitution provides the president with the power to appoint individuals to government positions. He or she also may appoint personal advisers without congressional approval. Not surprisingly, these individuals tend to be supporters of the president. Similarly, at the state and local levels, governors and mayors retain appointments powers. Some scholars have argued that patronage may be used for laudable purposes, such as the \"recognition\" of minority communities through the appointment of their members to a high-profile positions. Bearfield has argued that patronage be used for four general purposes: create or strengthen a political organization; achieve democratic or egalitarian goals; bridge political divisions and create coalitions; and to alter the existing patronage system.\n\nBoliburguesía is a term that was coined by journalist Juan Carlos Zapata in order to \"define the oligarchy that has developed under the protection of the Chavez government\". During Hugo Chávez's tenure, he seized thousands of properties and businesses while also reducing the footprint of foreign companies. Venezuela's economy was then largely state-run and was operated by military officers that had their business and government affairs connected. Senior fellow at the Brookings Institution, Harold Trinkunas, stated that involving the military in business was \"a danger\", with Trinkunas explaining that the Venezuelan military \"has the greatest ability to coerce people, into business like they have\". According to \"Bloomberg Business\", \"[b]y showering contracts on former military officials and pro-government business executives, Chavez put a new face on the system of patronage\".\n\nThere are historical examples where the noble classes financed scientific pursuits.\n\nMany Barmakids were patrons of the sciences, which greatly helped the propagation of Indian science and scholarship from the neighbouring Academy of Gundishapur into the Arabic world. They patronized scholars such as Gebir and Jabril ibn Bukhtishu. They are also credited with the establishment of the first paper mill in Baghdad. The power of the Barmakids in those times is reflected in \"The Book of One Thousand and One Nights\"; the vizier Ja'far appears in several stories, as well as a tale that gave rise to the expression \"Barmecide feast\".\n\nIn the same manner as commercial patronage, those who attend a sporting event may be referred to as patrons, though the usage in much of the world is now considered archaic—with some notable exceptions. Those who attend the Masters Tournament, one of the four major championship of professional golf, are still traditionally referred to as \"patrons,\" largely at the insistence of the Augusta National Golf Club. This insistence is occasionally made fun of by sportswriters and other media. In polo, a \"patron\" is a person who puts together a team by hiring one or more professionals. The rest of the team may be amateurs, often including the patron himself (or, increasingly, herself).\n\nAlso, people who attend hurling or Gaelic football games organised by the Gaelic Athletic Association are referred to as patrons.\n\n\n"}
{"id": "1666883", "url": "https://en.wikipedia.org/wiki?curid=1666883", "title": "Phenomenology of religion", "text": "Phenomenology of religion\n\nThe phenomenology of religion concerns the experiential aspect of religion, describing religious phenomena in terms consistent with the orientation of worshippers. It views religion as made up of different components, and studies these components across religious traditions in order to gain some understanding of them.\n\nThe first explicit use of the phrase \"phenomenology of religion\" occurs in the \"Lehrbuch der Religionsgeschichte\" (Handbook of the History of Religions), written by Pierre Daniël Chantepie de la Saussaye in 1887, wherein he articulates the task of the science of religion and gives an \"Outline of the phenomenology of religion\". Employing the terminology of Georg Wilhelm Friedrich Hegel, Chantepie divides his science of religion into two areas of investigation, essence and manifestations, which are approached through investigations in philosophy and history, respectively. However, Chantepie’s phenomenology \"belongs neither to the history nor the philosophy of religion as Hegel envisioned them\". For Chantepie, it is the task of phenomenology to prepare historical data for philosophical analysis through \"a collection, a grouping, an arrangement, and a classifying of the principal groups of religious conceptions\". This sense of phenomenology as a grouping of manifestations is similar to the conception of phenomenology articulated by Robison and the British; however, insofar as Chantepie conceives of phenomenology as a preparation for the philosophical elucidation of essences, his phenomenology is not completely opposed to that of Hegel.\n\nChantepie’s \"Lehrbuch\" was highly influential, and many researchers began similar efforts after its publication and its subsequent translation into English and French. One such researcher was William Brede Kristensen. In 1901, Kristensen was appointed the first professorship relating to the phenomenology of religion at the University of Leiden. Some of the material from Kristensen’s lectures on the phenomenology of religion was edited posthumously, and the English translation was published in 1960 as \"The Meaning of Religion\". James notes that Kristensen’s phenomenology \"adopts many of the features of Chantepie’s grouping of religious phenomena,\" and penetrates further into the intricacies of Chantepie’s phenomenological approach.\nFor Chantepie, phenomenology is affected by the philosophy and history of religion, but for Kristensen, it is also the medium whereby the philosophy and history of religion interact with and affect one another. In this sense, Kristensen’s account of the relationship between historical manifestations and philosophy is more similar to that of Hegel than it is to Chantepie. In defining the religious essence of which he explores historical manifestations, Kristensen appropriates Rudolf Otto’s conception of \"das Heilige\" (\"the holy\" or \"the sacred\"). Otto describes \"das Heilige\" with the expression \"mysterium tremendum et fascinans\"—a numinous power revealed in a moment of \"awe\" that admits of both the horrible shuddering of \"religious dread\" (\"tremendum\") and fascinating wonder (\"fascinans\") with the overpowering majesty (\"majestas\") of the ineffable, \"wholly other\" mystery (\"mysterium\").\n\nLike Chantepie, Kristensen argues that phenomenology seeks the “meaning” of religious phenomena. Kristensen clarifies this supposition by defining the meaning that his phenomenology is seeking as “the meaning that the religious phenomena have for the believers themselves”. Furthermore, Kristensen argues that phenomenology is not complete in grouping or classifying the phenomena according to their meaning, but in the act of understanding. “Phenomenology has as its objects to come as far as possible into contact with and to understand the extremely varied and divergent religious data”.\n\nBeing a phenomenologist, Kristensen was less interested in philosophical presuppositions than in his concrete depth-research in the incidental religious phenomena. These subjects concerned mythological material (such as Creation, the Flood etc.) as well as human action (such as baptism, Olympic Games etc.), and objects of nature and handicrafts.\nIn all of this he only made use of the authentic sources: writings and images by the believers themselves. This procedure compelled him to reduce the field of his research - he had to profoundly master all relating languages and writings in order to be able to understand his sources in a way as they would have wanted to be understood themselves.\nConsequently, he reduced his field of research to the phenomena in religions living around the origin of Christianity: during the millennia before and the centuries after Christ, in Iran (Avesta), Babylonia and Assyria, Israel, Egypt, Greece and Rome.\nThe required knowledge of speeches, also, is one of the causes that only few (Van der Leeuw, Bleeker) of his pupils did carry on in his line, although many scholars showed interests in the results of his research.\nApart from his synopsis \"The Meaning of Religion\", and a just simple \"Introduction in History of Religion\", his publications are mostly restricted to the results of his incidental partial researches, published in the shape of a Communication of the Royal Academy of the Netherlands.\n\nThe phenomenological approach to religion developed in Gerardus van der Leeuw’s \"Phänomenologie der Religion\" (1933) follows Kristensen in many respects, while also appropriating the phenomenology of Martin Heidegger and the hermeneutics of Wilhelm Dilthey.\n\nFor van der Leeuw, understanding is the subjective aspect of phenomena, which is inherently intertwined with the objectivity of that which is manifest. Van der Leeuw articulates the relation of understanding to understood phenomena according to the schema outlined in Dilthey’s definition of the human sciences (\"Geisteswissenschaften\") as sciences that are “based on the relations between experience, expression and understanding” (“\"Verhältnis von Erlebnis, Ausdruck, und Verstehen\"”). Van der Leeuw correlates subjective experience, expression, and understanding with three objective levels of appearing—relative concealment (\"Verborgenheit\"), relative transparency (\"Durchsichtigkeit\"), and gradually becoming manifest or revealed (\"Offenbarwerden\"), wherein the understanding of what is becoming revealed is the primordial level of appearing from which the experienced concealment and expressed transparency of appearing are derived.\n\nBecause van der Leeuw, like Kristensen, appropriates Otto’s concept of \"das Heilige\" in defining the essential category of religion, the transcendence becoming revealed in all human understanding can be further described as sacred — an overpowering “wholly other,” which becomes revealed in astonishing moments of dreadful awe (\"Scheu\") and wonderful fascination. Van der Leeuw argues that this concept of religious dread is also present in Kierkegaard’s work on \"Angst\" and in Heidegger’s statement that “what arouses dread is ‘being in the world’ itself”. Moreover, van der Leeuw recognizes that, although dreadful, Being-in-the-world is fundamentally characterized as care (\"Sorge\"), the existential structure whereby \"Dasein\" is concerned with meaningful relationships in the world alongside other beings.\n\nBecause all experiences disclose concealed (wholly other) transcendence to the understanding, all experiences of Being-in-the-world are ultimately religious experiences of the sacred, whether explicitly recognized as such or not. Human being as such is \"homo religiosus\", the opposite of \"homo negligens\".\n\nIt is the task of the phenomenology of religion to interpret the various ways in which the sacred appears to human beings in the world, the ways in which humans understand and care for that which is revealed to them, for that which is ultimately wholly other mystery.\nAmong other great phenomenologists who worked and influenced phenomenology of religion are Henry Corbin, Ninian Smart, Mircea Eliade.\n\n\n\n"}
{"id": "30048655", "url": "https://en.wikipedia.org/wiki?curid=30048655", "title": "Proper right and proper left", "text": "Proper right and proper left\n\nProper right and proper left are conceptual terms used to unambiguously convey relative direction when describing an image or other object. The \"proper right\" hand of a figure is the hand that would be regarded by that figure as its right hand. In a frontal representation, that appears on the left as the viewer sees it, creating the potential for ambiguity if the hand is just described as the \"right hand\".\n\nThe terms are mainly used in discussing images of humans, whether in art history, medical contexts such as x-ray images, or elsewhere, but they can be used in describing any object that has an unambiguous front and back (for example furniture) or, when describing things that move or change position, with reference to the original position. However a more restricted use may be preferred, and the internal instructions for cataloguing objects in the \"Inventory of American Sculpture\" at the Smithsonian American Art Museum say that \"The terms \"proper right\" and \"proper left\" should be used when describing figures only\". In heraldry, right and left is always used in the meaning of proper right and proper left, as for the imaginary bearer of a coat of arms; to avoid confusion, the Latin terms dexter and sinister are often used.\n\nThe alternative is to use language that makes it clear that the viewer's perspective is being used. The swords in the illustrations might be described as: \"to the left as the viewer sees it\", \"at the view's left\", \"at the viewer's left\", and so on. However these formulations do not work for freestanding sculpture in the round, where the viewer might be at any position around the sculpture. A British 19th-century manual for military drill contrasts \"proper left\" with \"present left\" when discussing the orientation of formations performing intricate movements on a parade ground, \"proper\" meaning the orientraion at the start of the drill.\n\nThe terms are analogous to the nautical port and starboard, where \"port\" is to a watercraft as \"proper left\" is to a sculpture, and they are used for essentially the same reason. Their use obviates the need for potentially ambiguous language such as \"my right,\" \"your left,\" and so on, by expressing the direction in a manner that holds true regardless of the relative orientations of the object and observer. Another example is stage right and left in the theatre, which uses the actor's orientation, \"stage right\" equating to the audience's \"house left\".\n\nThis is from the auction catalogue description of an African wood figure: There is extensive insect loss in the proper right leg, some at the proper right elbow, and at the fronts of both feet. There is a chip off the proper right breast, and the proper right leg was broken off and reglued.\n\nDescribing an Indian sculpture:The figure standing on the yakṣī's proper left, however, is not a mirror image of the other male ...\n"}
{"id": "16448388", "url": "https://en.wikipedia.org/wiki?curid=16448388", "title": "Religious image", "text": "Religious image\n\nA religious image, sometimes called a votive image, is a work of visual art that is representational and has a religious purpose, subject or connection. All major historical religions have made some use of religious images, although their use is strictly controlled and often controversial in many religions, especially Abrahamic ones. General terms associated with religious images include cult image, a term for images, especially in sculpture which are or have been claimed to be the object of religious worship in their own right, and icon strictly a term for Eastern Orthodox religious images, but often used more widely, in and outside the area of religion.\n\nImages flourished within the Christian world, but by the 6th century, certain factions arose within the Eastern Church to challenge the use of icons, and in 726-30 they won Imperial support. The Iconoclasts actively destroyed icons in most public places, replacing them with the only religious depiction allowed, the cross. The Iconodules (those who favored the veneration of images), on the other hand, argued that icons had always been used by Christians and should continue to be allowed. They further argued that not only should the use of icons be permitted, it was necessary to the Christian faith as a testimony of the dogma of the Incarnation of Christ. Saint John Damascene argued:\n\n\"Of old God the incorporial and uncircumscribed was not depicted at all. But now that God has appeared in the flesh and lived among men, I make an image of the God who can be seen. I do not worship matter, but I worship the Creator of matter, who for my sake became material and deigned to dwell in matter, who through matter effected my salvation.\"\n\nFinally, after much debate at the Second Council of Nicaea, held in 787, the Iconodules, supported by the Empress, upheld the use of icons as an integral part of Christian tradition, and the Western Church, which had been almost totally unaffected by the dispute, confirmed this. According to the definition of the council, icons of Jesus are not intended to depict his divinity, but only the Incarnate Word. Saints are depicted because they reflect the grace of God, as depicted by their halos.\n\nThe Eastern Orthodox Church fully ascribes to the teachings of the Seventh Ecumenical Council and celebrates the restoration of the use of icons after the period of Iconoclasm on the First Sunday of Great Lent. So important are the icons in Orthodox theology that the ceremony celebrating their restoration is known as the Triumph of Orthodoxy.\n\nIn the traditions of Eastern Christianity, only flat images or \"bas relief\" images are used (no more than 3/4 relief). Because the Eastern Church teaches that icons should represent the spiritual reality rather than the physical reality, the traditional style of Orthodox iconography was developed in which figures were stylized in a manner that emphasized their holiness rather than their humanity.\n\nTraditional icons differ from Western art in that they are not romantic or emotional, but call the viewer to \"sobriety\" (\"nipsis\"). The manner of depicting the face, and especially the eyes, is intended to produce in the viewer a sense of calm, devotion, and a desire for asceticism. Icons also differ from Western art in that they use inverse perspective (giving the impression that the icon itself is the source of light), and for this reason make very little use of shadow or highlight. The background of icons is usually covered with gold leaf to remind the viewer that the subject pictured is not earthly but otherworldly (gold being the closest earthly medium in which to signify heavenly glory).\n\nJesus and the Apostles are depicted wearing the robes of philosophers. The precise manner of depicting the face of Jesus and many of the saints is also fixed by tradition. Even the colours used in depicting the clothing of Jesus, the Virgin Mary, and other saints is fixed by tradition, with symbolic meaning attached to each color. Icons of Jesus depict him with a halo that displays three bars of a cross and the Greek letters which signify I AM (the Divine Name which God revealed to Moses at the Burning Bush). The halos of saints, even the Theotokos (Mother of God) are usually simple circles, filled with gold leaf. Over the centuries, painter's manuals have developed to help preserve the traditions and techniques of Orthodox iconography, one of the best-known is the manual from the Stroganov School of iconography in Russia. Despite these strict guidelines, the Orthodox iconographic style is not stilted, and the individual artist is always permitted to bring his own style and spiritual insight into his work, so long as he remains faithful to Sacred Tradition, and many icons display remarkable movement and depth.\n\nThe thoughtful use of symbolism allows the icon to present complex teaching in a simple way, making it possible to educate even the illiterate in theology. The interiors of Orthodox Churches are often completely covered in icons of Christ, Mary and the saints. Most are portrait figures in various conventional poses, but many narrative scenes are also depicted. It is not unusual in narrative icons for the same individual to be depicted more than one time.\n\nOrthodox Christians do not pray \"to\" icons; rather, they pray \"before\" them. An icon is a medium of communication, rather than a medium of art. Gazing at an icon is intended to help draw the worshipper into the heavenly kingdom. As with all of Orthodox theology, the purpose is \"theosis\" (mystical union with God).\n\nIcons are venerated by the faithful by bowing and kissing them. Traditionally, the faithful would not kiss the face of the one depicted on the icon, but rather the right hand or foot depicted on the icon. The composition of an icon is planned with this veneration in mind, and the iconographer will usually portray his subject so that the right hand is raised in blessing, or if it is the saint's full figure is depicted, the right foot is visible.\n\nIcons are also honored with incense and by burning lampadas (oil lamps) in front of them. Icons are carried in processions, and the bishop or priest may bless the people by holding an icon upright and making the sign of the cross with it over them.\n\nUntil the 13th century, icons followed a broadly similar pattern in West and East, although very few such early examples survive from either tradition. Western icons, which are not usually so termed, were largely patterned on Byzantine works, and equally conventional in composition and depiction. From this point on the Western tradition came slowly to allow the artist far more flexibility, and a more realistic approach to the figures.\n\nIn the 15th century the use of icons in the West was enormously increased by the introduction of prints on paper, mostly woodcuts which were produced in vast numbers. With the Reformation, after an initial uncertainty among early Lutherans, Protestants came down firmly against icon-like portraits, especially larger ones, even of Christ. Many Protestants found these idolatrous. Catholics maintained and even intensified the traditional use of icons, both printed and on paper, using the different styles of the Renaissance and Baroque. Popular Catholic imagery to a certain extent has remained attached to a Baroque style of about 1650, especially in Italy and Spain.\n\nIn the Church of England, the Royal Arms of the United Kingdom has been used like an icon, owing to its absence of human portraiture, as a representation of the sovereign as Head of the Church. It has been done in wood and stone as well as painted.\n\nImages of Hindu gods and goddesses use a rich symbolism. Some figures are blue-skinned (the color of heaven) or have multiple arms holding various symbols which depict aspects of the god.\n\nMuslims view sanctified icons as idols, and strictly forbid their worship, nor do they pray in front of one. However, the various divisions of Islam take different positions on the role of visual depictions of living (or once-living) creatures, including people. At one end of the spectrum, sects such as the Wahhabis totally ban drawings and photography. Some branches of Islam forbid only the former but allow the latter. The majority of Sunni Muslims permit both. Some Shia allow even the depiction of Muhammad and the twelve Imams, a position totally unacceptable to most Sunnis.\n\nIt is commonly thought that the Jews absolutely prohibit \"graven images\"; this, however, is not entirely true. There are numerous instances within the scriptures that describe the creation and use of images for religious purposes (the angels on the Ark of the Covenant, the bronze snake Moses mounted on a pole, etc.). What is important to note is that none of these are worshipped as God. Since God is incorporeal and has no form, He cannot be depicted. During the Late Antique period of Jewish history it is clear that restrictions on representation were relaxed considerably; for example, the synagogue at Dura Europas had large figurative wall paintings. It is also clear there was a tradition of painted scrolls, of which the Joshua Roll and the Utrecht Psalter are medieval Christian copies, none of the originals having survived. There are also many medieval illuminated manuscripts, especially of the Haggadah of Pesach (Passover).\n\nA unique Jewish tradition of animal iconography was developed in Eastern Europe, which included symbolic depictions of God's attributes and powers as various animal scenes and plant ornaments in the wooden synagogues in the Polish-Lithuanian commonwealth, as well as some mystical imagery on the gravestones. A part of the same imagery also appears on the Ashkenazic Shivisi - meditative images used for contemplation over God's name, not unlike the Eastern Mandalas.\n\nSome synagogue wall paintings contained over 80 various animals, including lions, unicorns, dragons, lion-headed mermaids, three hares, three intertwined fishes, Uroboros, elephants, deer, leopards, bears, foxes, wolves, squirrels, turkeys, ostriches and many others.\n\nGod himself was usually represented as a two-headed golden eagle in the center of the Sun, painted on the ceiling of the synagogue, and surrounded by the Zodiac circle. This system was based on the Kabbalistic symbolic tradition; unfortunately, the meaning of some forgotten symbols is hard to recover.\n\nThomas Hubka has traced the style of decorative painting in the wooden synagogues to the medieval Hebrew illuminated manuscripts of Ashkenazi Jewry, and its meaning to the Jewish mystical literature, such as the Zohar and the works of Rabbi Elazar Rokeach.\n"}
{"id": "354220", "url": "https://en.wikipedia.org/wiki?curid=354220", "title": "Religious studies", "text": "Religious studies\n\nReligious studies, alternately known as the study of religion, is an academic field devoted to research into religious beliefs, behaviors, and institutions. It describes, compares, interprets, and explains religion, emphasizing systematic, historically based, and cross-cultural perspectives.\n\nWhile theology attempts to understand the nature of transcendent or supernatural forces (such as deities), religious studies tries to study religious behavior and belief from outside any particular religious viewpoint. Religious studies draws upon multiple disciplines and their methodologies including anthropology, sociology, psychology, philosophy, and history of religion.\n\nReligious studies originated in the 19th century, when scholarly and historical analysis of the Bible had flourished, and Hindu and Buddhist texts were first being translated into European languages. Early influential scholars included Friedrich Max Müller, in England, and Cornelius P. Tiele, in the Netherlands. Today religious studies is practiced by scholars worldwide. In its early years, it was known as \"comparative religion\" or the science of religion and, in the USA, there are those who today also know the field as the History of religion (associated with methodological traditions traced to the University of Chicago in general, and in particular Mircea Eliade, from the late 1950s through to the late 1980s). \n\nThe religious studies scholar Walter Capps described the purpose of the discipline as to provide \"training and practice... in directing and conducting inquiry regarding the subject of religion\". At the same time, Capps stated that its other purpose was to use \"prescribed modes and techniques of inquiry to make the subject of religion intelligible.\"\nReligious studies scholar Robert A. Segal characterised the discipline as \"a subject matter\" that is \"open to many approaches\", and thus it \"does not require either a distinctive method or a distinctive explanation to be worthy of disciplinary status.\"\n\nDifferent scholars operating in the field have different interests and intentions; some for instance seek to defend religion, while others seek to explain it away, and others wish to use religion as an example with which to prove a theory of their own. Some scholars of religious studies are interested in primarily studying the religion to which they belong.\n\nScholars of religion have argued that a study of the subject is useful for individuals because it will provide them with knowledge that is pertinent in inter-personal and professional contexts within an increasingly globalised world. It has also been argued that studying religion is useful in appreciating and understanding sectarian tensions and religious violence.\n\nThe term \"religion\" originated from the Latin noun \"religio\", that was nominalized from one of three verbs: \"relegere\" (to turn to constantly/observe conscientiously); \"religare\" (to bind oneself [back]); and \"reeligere\" (to choose again). Because of these three different potential meanings, an etymological analysis alone does not resolve the ambiguity of defining religion, since each verb points to a different understanding of what religion is. During the Medieval Period, the term \"religious\" was used as a noun to describe someone who had joined a monastic order (a \"religious\").\n\nThroughout the history of religious studies, there have been many attempts to define the term \"religion\". Many of these have been \"monothetic\", seeking to determine a key, essential element which all religions share, which can be used to define \"religion\" as a category, and which must be necessary in order for something to be classified as a \"religion\". There are two forms of monothetic definition; the first are \"substantive\", seeking to identify a specific core as being at the heart of religion, such as a belief in a God or gods, or an emphasis on power. The second are \"functional\", seeking to define \"religion\" in terms of what it does for humans, for instance defining it by the argument that it exists to assuage fear of death, unite a community, or reinforce the control of one group over another. Other forms of definition are \"polythetic\", producing a list of characteristics that are common to religion. In this definition there is no one characteristic that need be common to every form of religion.\n\nCausing further complications is the fact that there are various secular world views, such as nationalism and Marxism, which bear many of the same characteristics that are commonly associated with religion, but which rarely consider themselves to be religious.\n\nConversely, other scholars of religious studies have argued that the discipline should reject the term \"religion\" altogether and cease trying to define it. In this perspective, \"religion\" is argued to be a Western concept that has been forced upon other cultures in an act of intellectual imperialism. According to scholar of religion Russell T. McCutcheon, \"many of the peoples that we study by means of this category have no equivalent term or concept whatsoever\". There is, for instance, no word for \"religion\" in languages like Sanskrit.\n\nBefore religious studies became a field in its own right, flourishing in the United States in the late 1960s, several key intellectual figures explored religion from a variety of perspectives. One of these figures was the famous pragmatist William James. His 1902 Gifford lectures and book \"The Varieties of Religious Experience\" examined religion from a psychological-philosophical perspective and is still influential today. His essay \"The Will to Believe\" defends the rationality of faith.\n\nMax Weber studied religion from an economic perspective in \"The Protestant Ethic and the Spirit of Capitalism\" (1904-1905), his most famous work. As a major figure in sociology, he has no doubt influenced later sociologists of religion.\nÉmile Durkheim also holds continuing influence as one of the fathers of sociology. He explored Protestant and Catholic attitudes and doctrines regarding suicide in his work \"Suicide\". In 1912, he published his most memorable work on religion, \"The Elementary Forms of the Religious Life\".\n\nInterest in the general study of religion dates back to at least Hecataeus of Miletus (ca. 550 BCE – ca. 476 BCE) and Herodotus (ca. 484 BCE – 425 BCE). Later, during the Middle Ages, Islamic scholars such as Ibn Hazm (d. 1064 CE) studied Persian, Jewish, Christian, and Indian religions, among others. The first history of religion was the \"Treatise on the Religious and Philosophical Sects\" (1127 CE), written by the Muslim scholar Muhammad al-Shahrastani. Peter the Venerable, also working in the twelfth century, studied Islam and made possible a Latin translation of the Qur'an.\nNotwithstanding the long interest in the study of religion, the academic discipline Religious Studies is relatively new. Dr. Chris Partridge notes that the \"first professorships were established as recently as the final quarter of the nineteenth century.\"\nIn the nineteenth century, the study of religion was done through the eyes of science. Max Müller was the first Professor of Comparative Philology at Oxford University, a chair created especially for him. In his \"Introduction to the Science of Religion\" (1873) he wrote that it is \"the duty of those who have devoted their life to the study of the principal religions of the world in their original documents, and who value and reverence it in whatever form it may present itself, to take possession of this new territory in the name of true science.\"\n\nMany of the key scholars who helped to establish the study of religion did not regard themselves as scholars of religious studies, but rather as theologians, philosophers, anthropologists, sociologists, psychologists, and historians.\n\nPartridge writes that \"by the second half of the twentieth century the study of religion had emerged as a prominent and important field of academic enquiry.\" He cites the growing distrust of the empiricism of the nineteenth century and the growing interest in non-Christian religions and spirituality coupled with convergence of the work of social scientists and that of scholars of religion as factors involved in the rise of Religious Studies.\n\nOne of the earliest academic institutions where Religious Studies was presented as a distinct subject was University College Ibadan, now the University of Ibadan, where Geoffrey Parrinder was appointed as lecturer in Religious Studies in 1949.\n\nIn the 1960s and 1970s, the term \"religious studies\" became common and interest in the field increased. New departments were founded and influential journals of religious studies were initiated (for example, \"Religious Studies and Religion\"). In the forward to \"Approaches to the Study of Religion\", Ninian Smart wrote that \"in the English-speaking world [religious studies] basically dates from the 1960s, although before then there were such fields as 'the comparative study of religion', the 'history of religion', the 'sociology of religion' and so on...\"\n\nIn the 1980s, in both Britain and America, \"the decrease in student applications and diminishing resources in the 1980s led to cut backs affecting religious studies departments.\" (Partridge) Later in the decade, religious studies began to pick up as a result of integrating religious studies with other disciplines and forming programs of study that mixed the discipline with more utilitarian study.\n\nPhilosophy of religion uses philosophical tools to evaluate religious claims and doctrines. Western philosophy has traditionally been employed by English speaking scholars. (Some other cultures have their own philosophical traditions including Indian, Muslim, and Jewish.) Common issues considered by the (Western) philosophy of religion are the existence of God, belief and rationality, cosmology, and logical inferences of logical consistency from sacred texts.\n\nAlthough philosophy has long been used in evaluation of religious claims (\"e.g.\" Augustine and Pelagius's debate concerning original sin), the rise of scholasticism in the 11th century, which represented \"the search for order in intellectual life\" (Russell, 170), more fully integrated the Western philosophical tradition (with the introduction of translations of Aristotle) in religious study.\n\nThere is some amount of overlap between subcategories of religious studies and the discipline itself. Religious studies seeks to study religious phenomena as a whole, rather than be limited to the approaches of its subcategories.\n\nThe anthropology of religion is principally concerned with the common basic human needs that religion fulfills.\n\nThe cultural anthropology of religion is principally concerned with the cultural aspects of religion. Of primary concern to the cultural anthropologist of religions are rituals, beliefs, religious art, and practices of piety.\n\nGallup surveys have found that the world's poorest countries may be the most religious. Of those countries with average per-capita incomes under $2000, 95% reported that religion played an important role in their daily lives. This is contrasted by the average of 47% from the richest countries, with incomes over $25000 (with the United States breaking the trend by reporting at 65%).\nSocial scientists have suggested that religion plays a functional role (helping people cope) in poorer nations.\nThe New York Times offers a graphic illustrating the correlation (not necessarily causation) between religion and poverty.\n\nThe geography of religion is principally concerned with the spatial elements of religious practice and embodiment. In the 1960s and 1970s, geographers of religion such as Wilbur Zelinsky and David Sopher were mostly associated with the \"Berkeley school\" of cultural geography and focused mostly on the cultural imprints of religion on the landscape. \nSince the turn in the new cultural geography of religion through the work of James Duncan on the City as Text, geographers of religion have focused on what Lily Kong has called the \"politics and poetics\" of religion, especially in relation to the political geographies of secular nation-states. Recent interest in the geography of religion has focused on how religious practitioners enact sacred space through their embodied sacred practices as well as the relationship between religion and geopolitics.\n\nThe history of religions is not concerned with theological claims apart from their historical significance. Some topics of this discipline are the historicity of religious figures, events, and the evolution of doctrinal matters.\n\nThere are many approaches to the study of sacred texts. One of these approaches is to interpret the text as a literary object. Metaphor, thematic elements, and the nature and motivations of the characters are of interest in this approach. An example of this approach is \"\", by Jack Miles.\n\nThe temporal lobe has been of interest which has been termed the \"God center\" of the brain. (Ramachandran, ch. 9) Neurological findings in regard to religious experience is not a widely accepted discipline within religious studies. Scientific investigators have used a SPECTscanner to analyze the brain activity of both Christian contemplatives and Buddhist meditators, finding them to be quite similar.\n\nThe \"origin of religion\" refers to the emergence of religious behavior in prehistory, before written records.\n\nThe psychology of religion is concerned with the psychological principles operative in religious communities and practitioners. William James's The Varieties of Religious Experience analyzed personal experience as contrasted with the social phenomenon of religion. Some issues of concern to the psychologist of religions are the psychological nature of religious conversion, the making of religious decisions, religion and happiness, and the psychological factors in evaluating religious claims.\n\nSigmund Freud was another figure in the field of psychology and religion. He used his psychoanalytic theory to explain religious beliefs, practices, and rituals, in order to justify the role of religion in the development of human culture.\n\nThe sociology of religion concerns the dialectical relationship between religion and society; the practices, historical backgrounds, developments, universal themes and roles of religion in society. There is particular emphasis on the recurring role of religion in all societies and throughout recorded history. The sociology of religion is distinguished from the philosophy of religion in that it does not set out to assess the validity of religious beliefs, though the process of comparing multiple conflicting dogmas may require what Peter L. Berger has described as inherent \"methodological atheism\". Whereas the sociology of religion broadly differs from theology in assuming the invalidity of the supernatural, theorists tend to acknowledge socio-cultural reification of religious practise.\n\nIt may be said that the modern formal discipline of sociology \"began\" with the analysis of religion in Durkheim's 1897 study of suicide rates amongst Catholic and Protestant populations. The works of Max Weber emphasised the relationship between religious belief and the economic foundations of society. Contemporary debates have centred on issues such as secularization, civil religion, and the cohesiveness of religion in the context of globalization and multiculturalism.\n\nThe sociology of religion also deals with how religion impacts society regarding the positive and negatives of what happens when religion is mixed with society. Theorist such as Marx states that “religion is the opium of the people” - the idea that religion has become a way for people to deal with their problems. At least one comprehensive study refutes this idea. Research has found that secular democracies like France or Scandinavia outperform more theistic democracies on various measures of societal health. The authors explains, \"Pressing questions include the reasons, whether theistic or non-theistic, that the exceptionally wealthy U.S. is so inefficient that it is experiencing a much higher degree of societal distress than are less religious, less wealthy prosperous democracies. Conversely, how do the latter achieve superior societal health while having little in the way of the religious values or institutions?\"\n\nVogel reports that in the 1970s a new \"law and religion\" approach has progressively built its own contribution to religious studies. Over a dozen scholarly organizations and committees were formed by 1983, and a scholarly quarterly, the \"Journal of Law and Religion\" first published that year and the \"Ecclesiastical Law Journal\" opened in 1999. Many departments and centers have been created around the world during the last decades. As of 2012, major Law and Religion organizations in the U.S. included 500 law professors, 450 political scientists, and specialists in numerous other fields such as history and religious studies. Between 1985 and 2010, the field saw the publication of some 750 books and 5000 scholarly articles. Scholars are not only focused on strictly legal issues about religious freedom or non establishment but also on the study of religions as they are qualified through judicial discourses or legal understanding on religious phenomena. Exponents look at canon law, natural law, and state law, often in comparative perspective. Specialists have explored themes in western history regarding Christianity and justice and mercy, rule and equity, discipline and love. Common topics on interest include marriage and the family, and human rights. Moving beyond Christianity, scholars have looked at law and religion interrelations in law and religion in the Muslim Middle East, and pagan Rome.\n\nThe earliest serious writing on the interface between religion and film appeared in the work of film critics like Jean Epstein in the 1920s.\nThe subject has grown in popularity with students and is cited as having particular relevance given the pervasiveness of film in modern culture. Approaches to the study of religion and film differ among scholars; functionalist approaches for instance view film as a site in which religion is manifested, while theological approaches examine film as a reflection of God's presence in all things.\n\nA number of methodologies are used in Religious Studies. Methodologies are hermeneutics, or interpretive models, that provide a structure for the analysis of religious phenomena.\n\nPhenomenology is \"arguably the most influential approach to the study of religion in the twentieth century.\" (Partridge) The term is first found in the title of the work of the influential philosopher of German Idealism, Georg Wilhelm Friedrich Hegel, entitled The Phenomenology of Spirit. Phenomenology had been practiced long before its being made explicit as a philosophical method by Edmund Husserl, who is considered to be its founder. In the context of Phenomenology of religion however, the term was first used by Pierre Daniel Chantepie de la Saussaye in his work \"Lehrbuch der Religiongeschichte\" (1887). Chantepie's phenomenology catalogued observable characteristics of religion much like a zoologist would categorize animals or an entomologist would categorize insects.\n\nIn part due to Husserl's influence, \"phenomenology\" came to \"refer to a method which is more complex and claims rather more for itself than did Chantepie’s mere cataloguing of facts.\" (Partridge) Husserl argued that the foundation of knowledge is consciousness. He recognized \"how easy it is for prior beliefs and interpretations to unconsciously influence one’s thinking, Husserl’s phenomenological method sought to shelve all these presuppositions and interpretations.\" (Partridge) Husserl introduced the term \"eidetic vision\" to describe the ability to observe without \"prior beliefs and interpretations\" influencing understanding and perception.\n\nHis other main conceptual contribution is the idea of the epoche: setting aside metaphysical questions and observing phenomena in and of themselves, without any bias or commitments on the part of the investigator. The epoche, also known as phenomenological reduction or bracketing, involves approaching a phenomenon or phenomena from a neutral standpoint, instead of with our own particular attitudes. In performing this reduction, whatever phenomenon or phenomena we approach are understood in themselves, rather than from our own perspectives. In the field of religious studies, a contemporary advocate of the phenomenological method is Ninian Smart. He suggests that we should perform the epoche as a means to engage in cross-cultural studies. In doing so, we can take the beliefs, symbols, rituals etc. of the other from within their own perspective, rather than imposing ours on them. Another earlier scholar who employs the phenomenological method for studying religion is Gerardus van der Leeuw. In his \"Religion in Essence and Manifestation\" (1933), he outlines what a phenomenology of religion should look like:\n\nThe subjectivity inherent to the phenomenological study of religion makes complete and comprehensive understanding highly difficult. However, phenomenologists aim to separate their formal study of religion from their own theological worldview and to eliminate, as far as possible, any personal biases (e.g., a Christian phenomenologist would avoid studying Hinduism through the lens of Christianity).\n\nThere are a number of both theoretical and methodological attitudes common among phenomenologists:source\n\nMany scholars of religious studies argued that phenomenology was \"the distinctive method of the discipline\".\nIn 2006, the phenomenologist of religion Thomas Ryba noted that this approach to the study of religion had \"entered a period of dormancy\".\nPhenomenological approaches were largely taxonomical, with Robert A. Segal stating that it amounted to \"no more than data gathering\" alongside \"the classification of the data gathered\".\n\nFunctionalism, in regard to religious studies, is the analysis of religions and their various communities of adherents using the functions of particular religious phenomena to interpret the structure of religious communities and their beliefs. The approach was introduced by British anthropologist Alfred Radcliffe-Brown. A major criticism of functionalism is that it lends itself to teleological explanations. An example of a functionalist approach is understanding the dietary restrictions contained in the Pentateuch as having the function of promoting health or providing social identity (\"i.e.\" a sense of belonging though common practice).\n\nLived religion is the ethnographic and holistic framework for understanding the beliefs, practices, and everyday experiences of religious and spiritual persons in religious studies. The name lived religion comes from the French tradition of sociology of religion \"la religion vécue\".\n\nThe concept of lived religion was popularized in the late 20th century by religious study scholars like Robert A. Orsi and David Hall. The study of lived religion has come to include a wide range of subject areas as a means of exploring and emphasizing what a religious person does and what they believe. Today, the field of lived religion is expanding to include many topics and scholars.\n\nWestern philosophy of religion, as the basic ancestor of modern religious studies, is differentiated from theology and the many Eastern philosophical traditions by generally being written from a third party perspective. The scholar need not be a believer. Theology stands in contrast to the philosophy of religion and religious studies in that, generally, the scholar is first and foremost a believer employing both logic \"and\" scripture as evidence. Theology according to this understanding fits with the definition which Anselm of Canterbury gave to it in the 11th century, credo ut intelligam, or faith seeking understanding (literally, \"I believe so that I may understand\"). The theologian then has the task of making intelligible, or clarifying, the religious commitments to which he or she subscribes. The scholar of religious studies has no such allegiances.\n\nA group of scholars have criticized religious studies beginning in the 1990s as a theological project which actually imposes views onto the people it aims to survey. Prominent voices in this critical view include Jonathan Z. Smith, Timothy Fitzgerald, Talal Asad, Tomoko Masuzawa, Geoffrey A. Oddie, Richard E. King, Russell T. McCutcheon, and Daniel Dubuisson. Their areas of research overlap heavily with postcolonial studies.\n\nMuch of the latest scholarship appears in the scholarly journals, which also typically review and evaluate new monographs. There are a large numbers of peer-reviewed scholarly journals in the discipline of Religious Studies. Many journals focus on historical or sociological topics or concentrate on particular religious traditions, such as Judaism or Islam. Religious studies journals have been laggard in gaining accessibility through the Internet, but libraries specializing in religious history have started to catch up. Among the prominent journals published in English are:\n\n\n\n\n\n"}
{"id": "41305762", "url": "https://en.wikipedia.org/wiki?curid=41305762", "title": "Satendra Singh (doctor)", "text": "Satendra Singh (doctor)\n\nDr Satendra Singh is a medical doctor at the University College of Medical Sciences and Guru Tegh Bahadur Hospital, Delhi. A physiologist by profession, he contracted poliomyelitis at the age of 9 months but went on to complete Bachelor of Medicine, Bachelor of Surgery from Ganesh Shankar Vidyarthi Memorial Medical College, Kanpur and later on Doctor of Medicine in Physiology. He is the first ever Indian to win the prestigious Henry Viscardi Achievement Awards given to extraordinary leaders in global disability community. He is a noted disability activist especially for his sustained efforts in making public places accessible for disabled persons.\n\nUnlocked Central posts for doctors with disability\n\nThe disabled fighter fought for not only his but other disabled doctor’s rights. He also brought into open the discrimination by UPSC which disallowed doctors with disability to apply for various Central Health Services (CHS) posts. He himself was rejected in 2014 but fought and was later allowed to appear for the interview. In 2013, his application was again rejected when he highlighted that the same discrimination was also present for the other posts. He was allowed to apply in a quick intervention. His RTI revealed that doctors with disabilities are not considered eligible for specialist CHS posts in teaching, non-teaching as well as public health specialist cadres. Undeterred, he complained again and requested the health ministry to allow all eligible doctors with disability to apply for these posts. His single handed relentless fight for justice over four years ultimately forced Health Ministry to unlock 1,674 specialist central posts for disabled doctors.\n\nChallenged unfair guidelines of Medical Council of India\n\nHe filed writ petition in the Supreme Court of India & lead successful advocacy of doctors with disabilities to the Central Government to bring amendments in the controversial MCI guidelines lifting the bar on admission of candidates with specified disabilities. \n\nAccessibility of all the medical institutions in India\n\nOn his petition the Court of the Chief Commissioner for Persons with Disabilities, under Ministry of Social Justice and Empowerment, Government of India instructed Medical Council of India to issue directives to all the medical institutions in India to be disabled-friendly. Despite these efforts, most medical institutions have yet to comply with the directions. His black armband protest on the United Nations' International Day of Persons with Disabilities in 2012 led authorities to construct ramps outside all the hostels at his medical college.\n\nDisabled friendly polling booths\n\nHe played an integral role of whistleblower to raise the issue of problems faced by voters with disability before and during the Delhi legislative assembly elections, 2013 through Right to Information. He also wrote to the election commission as well as the Chief Justice of India to allow disabled voters to vote again before declaration of final results. His RTI on fourth National Voters' Day revealed how Election Commission of India was caught unprepared to enfranchise electors with disability. It showed violation of Supreme Court orders of 2004 to empower voters with disabilities. After the expose Chief Electoral Office, Delhi involved him in making elections in Delhi disabled-friendly. Not only did he sensitise the election officers on how to help electors with disability but he also helped in setting up a disability registration helpline. His sustained efforts led to making election booths across the capital accessible to disabled and elderly voters. Further, on his petition the disability court warned the Election Commission of India to make its website disabled friendly.\n\nFiled first case under the new Disability Law against a cabinet Minister\n\nHe filed the first ever case under the new Rights of Persons with Disabilities Act, 2016, which has strict punishment for contravention of provisions of Act, against Satyadev Pachauri for publicly ridiculing a disabled employee. If convicted, Pachauri who is minister of khadi and village industries in the Government of Uttar Pradesh, will faces a six month to five-year prison term.\n\nWithdrawal of Union Public Service Commission and Indian Institutes of Technology Joint Entrance Examinations ‘discriminatory proforma’\n\nDr Singh challenged Union Public Service Commission’s format whereby disabled applicants were asked to paste photographs showing their disability as a proof. The Court of Chief Commissioner for Persons with Disabilities directed the UPSC to refrain from asking disabled candidates to submit photographs showing their disabilities and to consider the ‘permanent disability certificate’ issued from a government hospital as a valid proof. His advocacy led to the withdrawal of similar prOforma by IITJEE\n\nDisabled-friendly websites of Delhi Government hospitals\n\nHis judicial activism led to the web accessibility of all the hospitals under Government of Delhi.\n\nPolicy change in duty concession for drivers with disability\nCentral Government provides excise duty concession to disabled customers however Maruti Suzuki denied him the concession on the pretext of right leg disability which was not only discriminatory but illegal. His relentless fight led the Ministry of Heavy Industries and Public Enterprises to amend the rules and quashing manufacturer's certificate and allowing RTO's certificate only.\n\nDignified screening of people with disabilities at airports\n\nAfter being harassed by security personnel at the Hyderabad airport because of his orthosis, Dr Singh approached Directorate General of Civil Aviation (DGCA) and Bureau of Civil Aviation Security (BCAS) to amend rustic screening procedures and modify guidelines taking Convention on the Rights of Persons with Disabilities into consideration. The disability watchdog has directed both DGCA and BCAS to make sure people with disabilities are not being harassed or humiliated at airports and that security personnel have been sensitized towards them.\n\nCorrected wrong celebration of Salk's birthday on World Polio Day\n\nRotary International established World Polio Day (24 October) to commemorate the birthday of virologist Jonas Salk who developed polio vaccine. This erroneous observance was first challenged by Dr Singh in his publication in the official journal of the Edward Jenner Society's Vaccine (journal). Even Google dedicated a doodle on Salk's 100th birthday to confirm 28 October as his birthday. Rotary still celebrates the day on 24 October despite media reporting on the contrary.\n\nHis sustained advocacy also lead to ramps being constructed at ATMs. He also raised awareness by highlighting inaccessible ATMs and post offices in the capital to persons with disabilities through Right to Information Act.\n\nSingh is the Coordinator of the Enabling Unit for persons with disabilities and founder of Infinite Ability, a medical humanities group on disability. These are the first such bodies in any medical college in India. He organised the first ever Theatre of the Oppressed workshop for medical students in India. He was also instrumental in organising a unique 'Blind with Camera' workshop for the visually impaired and blind students of University of Delhi in 2012 with Partho Bhowmick.\n\nHe won the prestigious Henry Viscardi Achievement Awards in 2017 which is given by the Viscardi center to recognise exemplary leaders within the disability community for their extraordinary societal contributions. He is the first Indian to get this global award. President, Medical Council of India conferred him 'Medical Personality of the Year' award in 2017. He is also the recipient of State Award by the Government of Delhi for exceptional achievement by a person with disability in the field of social work in 2016. He was also awarded the NCPEDP MphasiS Universal Design Awards 2013 for promoting accessibility and ensuring life of equality and dignity for persons with disabilities.\n"}
{"id": "25445767", "url": "https://en.wikipedia.org/wiki?curid=25445767", "title": "Shadowgraphy (performing art)", "text": "Shadowgraphy (performing art)\n\nShadowgraphy or ombromanie is the art of performing a story or show using images made by hand shadows. It can be called \"cinema in silhouette\". Performers are titled as a shadowgraphist or shadowgrapher.\n\nThe art has declined since the late 19th century when electricity became available to homes because light bulbs and electric lamps do not give off good shadows and because cinema and television were becoming a new form of entertainment. Shadows are greatly defined by candlelight; therefore hand shadows were common in earlier centuries.\n\nThe modern art of hand shadows was made popular by the French entertainer Félicien Trewey in the 19th century. He popularized the art by making silhouettes of famous personalities.\n\nSince shadows have existed since the existence of objects obstructing light, it is hard to say when the art was first used by humans for entertainment. It could have been practiced by ancient or later humans, but it probably originated in the Far East. The French entertainer Félicien Trewey was interested in the art of Chinese shadow puppetry called Ombres Chinoises, which means \"Chinese shadows\". He popularized the art of hand shadows when he developed shadows of famous silhouettes. It then became popular in Europe in the 19th century.\n\nAlthough the art is popular amongst different kinds of entertainers it seems prominent amongst magicians, because it was popularized by a magician who inspired many other magicians. Félicien Trewey perfected the widely known elephant, bird, and cat hand shadows and created some of his own such as The Volunteer, Robinson Crusoe, The Jockey, The Rope Dancer and more. In 1889, Trewey joined with Alexander Herrmann who most likely learned it from him. David Tobias Bamberg most likely learned it from Alexander who then passed it down to his son Okito (Tobias Leendert Bamberg) who then passed it down to \"his\" son Fu Manchu (David Theodore Bamberg). Fu Manchu passed his skill to Marcelo Contento, one of his apprentices, who became famous worldwide for it. Contento died before he could pass it on to his son.\n\nOther magicians who used hand shadows in their act include David Devant, Edward Victor, and the duo Holden and Graham in which Holden was famous for his \"Monkey in the Belfry\" shadow.\n\nThe hands and fingers are exercised and different finger positions are practiced to help aid in forming shadows.\n\nThe light source to be used should be small and bright. The best shadows come from light proceeding from the smallest possible point. Albert Almoznino suggests a candle, a flashlight (with the lens and reflector removed) or any very small light. If a bulb is used, it should be clear. J. C. Cannell suggests in his book, \"Modern Conjuring For Amateurs\", that the best source of light is the electric arc, which Almoznino agrees to the small arc lamp, and the second best being the limelight (if used with a high-class jet). Trewey suggests the chalk for the limelight to be cut in a triangular form, or else it will produce a gray border around the shadow. Cannell states another favorite amongst shadowgraphists is the use of acetylene gas (i.e. acetylene gas lamp or carbide lamp). Nowadays it is possible to use a single lensless (for example, SMD) LED.\n\nAlbert Almoznino suggests to use a white or light-colored wall, sheet or table cloth for a small audience as in a private home. If a wall is dark-colored, the sheet or table cloth can be hung against it. If performing for a large audience such as in an auditorium or on a stage, he suggests a screen made of muslin or other thin cloth attached to a frame. In a nightclub, hall or small theater, he suggests a nylon screen on a pliable aluminum frame. It is a screen sometimes used for TV projection called a rear projection screen, but the light must be stronger like a small spotlight without the projector, lenses, or diffusers, or a motion-picture projector with the front lenses removed.\n\nThe performer sits or stands between the light source and the blank surface, while having the option to perform in front of the performance surface or behind it, with each having different advantages. The performer has another option to perform from the left or the right of the light source. The farther the hands are from the light, the smaller the shadows will be, while the closer the hands are to the light, the larger the shadows will be. Also, the closer the hands are to the blank surface, the sharper the shadows will be. Trewey suggests that the most convenient distance for the light from the hands is four feet while the hands from the performance surface should be about six feet. The performer should always watch their shadows instead of their hands.\n\nMovement helps give the shadows character and brings them to life. Some shadows are performed with accessories attached to the hands or fingers to achieve movements or images not applicable to hands alone.\n\n\n"}
{"id": "8217939", "url": "https://en.wikipedia.org/wiki?curid=8217939", "title": "Theme (arts)", "text": "Theme (arts)\n\nIn art, theme is usually about life, society or human nature, but can be any other subject. Themes are the fundamental and often universal ideas explored in a work. Themes are usually implied rather than explicitly stated. Deep thematic content is not required in a work, but the great majority of works have some kind of thematic content, not always intended by the author. Analysis of changes (or implied change) in dynamic characteristics of the work can provide insight into a particular theme.\n\nA theme is not the same as the subject of a work. For example, the \"subject\" of \"Star Wars\" is \"the battle for control of the galaxy between the Galactic Empire and the Rebel Alliance\". The \"themes\" explored in the films might be \"moral ambiguity\" or \"the conflict between technology and nature\".\n\nThemes differ from motifs in the visual arts in that themes are ideas conveyed by the visual experience as a whole, while motifs are elements of the content. In the same way, a literary story with repeated symbolism related to chess does not make the story's theme the similarity of life to chess. Themes arise from the interplay of the plot, the characters, and the attitude the author takes to them, and the same story can be given very different themes in the hands of different authors. \n\n"}
{"id": "6904987", "url": "https://en.wikipedia.org/wiki?curid=6904987", "title": "Virtual heritage", "text": "Virtual heritage\n\nVirtual heritage or cultural heritage and technology is the body of works dealing with information and communication technologies (ICT) and their application to cultural heritage, such as \"virtual archaeology\". Virtual heritage and cultural heritage have independent meanings: \"cultural heritage\" refers to sites, monuments, buildings and objects \"with historical, aesthetic, archaeological, scientific, ethnological or anthropological value\", whereas \"virtual heritage\" refers to instances of these within a technological domain, usually involving computer visualization of artefacts or Virtual Reality environments.\n\nOne technology that is frequently employed in virtual heritage applications is augmented reality, which is used to provide on-site reconstructions of archaeological sites or artefacts.\n\nMany virtual heritage projects focus on the tangible aspects of cultural heritage, for example 3D modelling, graphics and animation. In doing so they often overlook the intangible aspects of cultural heritage associated with objects and sites, such as stories, performances and dances. The tangible aspects of cultural heritage are not inseparable from the intangible and one method for combining them is the use of virtual heritage serious games, such as the 'Digital Songlines' and 'Virtual Songlines' which modified computer game technology to preserve, protect and present the cultural heritage of Australian Aborigines.\n\nThe first use of virtual heritage as a museum exhibit, and the derivation of the name virtual tour, was in 1994 as a museum visitor interpretation, providing a 'walk-through' of a 3D reconstruction of Dudley Castle in England as it was in 1550.\nThis consisted of a computer controlled laserdisc based system designed by British-based engineer Colin Johnson. It is a little-known fact that one of the first users of Virtual Heritage was Queen Elizabeth II, when she officially opened the visitor centre in June 1994.\nBecause the Queen's officials had requested titles, descriptions and instructions of all activities, the system was named 'Virtual Tour', being a cross between virtual reality and Royal Tour.\n\n\n"}
{"id": "29584466", "url": "https://en.wikipedia.org/wiki?curid=29584466", "title": "Vyaz (Cyrillic calligraphy)", "text": "Vyaz (Cyrillic calligraphy)\n\nVyaz ( from , ; ( from \n, 'to bind, to tie') is a type of Cyrillic ornate lettering.\n\n\n"}
{"id": "35424118", "url": "https://en.wikipedia.org/wiki?curid=35424118", "title": "Women in engineering", "text": "Women in engineering\n\nWomen are often under-represented in the fields of engineering, both in academia and in the profession of engineering, yet many have contributed to the diverse fields of engineering historically and currently. A number of organizations and programs have been created to understand and overcome this tradition of gender disparity. Some have decried this gender gap, saying that it indicates the absence of potential talent. Though the gender gap as a whole is narrowing, there is still a growing gap with minority women compared to their white counterparts.\n\nThe history of women as designers and builders of machines and structures predates the development of engineering as a trade. Prior to the creation of the term \"engineer\" in the 11th century, women had contributed to the technological advancement of societies around the globe, including Hypatia of Alexandria (350 or 370–415 AD), who is credited with the invention of the hydrometer. By the 19th century, women who participated in engineering work often had academic training in mathematics or science. Ada Lovelace was privately schooled in mathematics before beginning her collaboration with Charles Babbage on his analytical engine that would earn her the designation of the \"first computer programmer.\" In the early years of the 20th century, greater numbers of women began to be admitted to engineering programs, but they were generally looked upon as anomalies by the men in their departments.\n\nThe first University to award an engineering's bachelor's degree for women was University of California, Berkeley. Elizabeth Bragg was the recipient of a bachelor's degree in civil engineering in 1876, becoming the first female engineer in the United States. Prior to the 19th century, it was very rare for women to earn bachelor's degree in any field because they did not have the opportunity to enroll in universities due to gender disparities. Some universities started to admit women to their colleges by the early 1800s and by the mid-1800s they started to admit them into all academic programs including engineering.\n\nThe entry of the United States into World War II created a serious shortage of engineering talent in that country as men were drafted into the armed forces. To address the shortage, initiatives like GE on-the-job engineering training for women with degrees in mathematics and physics and the Curtiss-Wright Engineering Program among others created new opportunities for women in engineering. Curtiss-Wright partnered with Cornell, Penn State, Purdue, the University of Minnesota, the University of Texas, Rensselaer Polytechnic Institute, and Iowa State University to create an engineering curriculum that lasted ten months and focused primarily on aircraft design and production.\n\nDuring this time, since the female representation in the engineering field, there were barely public attacks on women. Chiefly, these attacks were kept quiet inside institutions due to the fact that women did not pressure aggressively to shift the gender gap between men and women in the engineering field. Another reason why these “attacks” were kept private is due to how men believed that it was impossible for engineering to stop being a male-dominated field.\n\nWomen's roles in the workforce, specifically in engineering fields changed greatly during the Post–World War II period. As women started to marry at later ages, have fewer children, divorce more frequently and stopped depending on male breadwinners for economic support, they started to become even more active in the engineering labor force despite the fact that their salaries were less than men's.\n\nWomen also played a crucial role in programming the ENIAC from its construction during the World War II period through the next several decades. Originally recruited by the Army in 1943, female ENIAC programmers made considerable advancements in programming techniques, such as the invention of breakpoints, now a standard debugging tool.\n\nIn addition to the wartime shortage of engineers, women also made inroads in engineering fields due to the gradual increase in public universities admitting female students. For example, Georgia Tech began to admit women engineering students in 1952, while the École Polytechnique in Paris, a premier French engineering institution, began to admit female students in 1972.\n\nStereotype threat may contribute to the under-representation of women in engineering. Because engineering is a traditionally male-dominated field, women may be less confident about their abilities, even when performing equally. At a young age, girls do not express the same level of interest in engineering as boys, possibly due in part to gender stereotypes. There is also significant evidence of the remaining presence of implicit bias against female engineers, due to the belief that men are mathematically superior and better suited to engineering jobs. Women who persist are able to overcome these difficulties, enabling them to find fulfilling and rewarding experiences in the engineering profession.\nDue to this gender bias, women's choice in entering an engineering field for college is also highly correlated to the background and exposure they have had with mathematics and other science courses during high school. Most women that do choose to study engineering have significant experience with regarding themselves better at these types of courses and as a result, think they are capable of studying in a male-dominated field.\n\nWomen's self-efficacy is also a contributor to the gender stereotype that plays a role in the underrepresentation of women in engineering. Women's ability to think critically that they can be successful and perform accomplishments is correlated to the choices they have when choosing a college career. Women that show high self-efficacy personalities are more prone to choose to study in the engineering field. Self-efficacy is also correlated to gender roles because men often present higher self-efficacy than women, which can also be a cause to why when choosing a major, most women opt to not choose the engineering major.\n\nOver the past few years 40 percent of women are leaving the engineering field, there are many factors leading to why they don't go into engineering because of women being judged about going into a difficult major such as engineering, working in difficult workplace conditions. According to the Society of Women Engineers one in four female leave the field after a certain age.\n\nWomen are under-represented in engineering education programs as in the workforce (see Statistics). Enrollment and graduation rates of women in post-secondary engineering programs are very important determinants of how many women go on to become engineers. Because undergraduate degrees are acknowledged as the \"latest point of standard entry into scientific fields\", the under-representation of women in undergraduate programs contributes directly to under-representation in scientific fields. Additionally, in the United States, women who hold degrees in science, technology, and engineering fields are less likely than their male counterparts to have jobs in those fields.\n\nThis degree disparity varies across engineering disciplines. Women tend to be more interested in the engineering disciplines that have societal and humane developments, such as agricultural and environmental engineering. They are therefore well-represented in environmental and biomedical engineering degree programs, receiving 40-50% of awarded degrees in the U.S. (2014–15), women are far less likely to receive degrees in fields like mechanical, electrical and computer engineering.\n\nA study made by the Harvard Business Review discussed the reasons why the rates of women representation in the engineering field are still low. The study discovered that rates of female students in engineering programs are continuous because of the collaboration aspects in the field. The results of the study chiefly determined how women are treated differently in group works in which there are more male than female members and how male members “excluded women from the real engineering work”. Aside from this, women in this study also described how professors treated female students differently “just because they were women”.\n\nDespite the fact that fewer women enroll in engineering programs across the nation, the representation of women in STEM-based careers can potentially increase when college and university administrators work on implementing mentoring programs and work-life policies for women. \nResearch shows that these rates have a hard time increasing since women are judged as less competent than men to perform supposedly “masculine jobs”.\n\nAnother possible reason for lower female participation in engineering fields is the prevalence of values \"associated with the male gender role\" in workplace culture. For example, some women in engineering have found it difficult to re-enter the workforce after a period of absence. Because men are less likely to take time off to raise a family, this disproportionately affects women.\n\nMales are also associated with taking leadership roles in the workplace. By holding a position of power over the women, they may create an uncomfortable environment for them. For example, lower pay, more responsibilities, less appreciation as compared to men.\n\nCommunication is also a contributing factor to the divide between men and women in the workplace. A male to male communication is said to be more direct,\nbut when a man explains a task to a woman, they tend to talk down, or “dumb down” terms. This comes from the stereotype that men are more qualified than women for engineering, causing men to treat women as inferiors instead of equals.\n\nPart of the male dominance in the engineering field is explained by their perception towards engineering itself. A study in 1964 found that both women and men believed that engineering was in fact masculine.\n\nThe masculinity dominating engineering majors and fields proves the issues that men themselves believe that they “naturally” excel in fields related to mathematics and sciences while women “naturally” excel in linguistics and liberal arts. In the past few decades, women's representation in the workforce in STEM fields, specifically engineering, has significantly improved. In 1960 women made up around 1% of all the engineers and by the year 2000 women have made up 11% of all engineers.\n\nSeveral colleges and universities nationwide want to decrease the gender gap between men and women in the engineering field by recruiting more women into their programs. The strategies used for recruiting more female undergraduate students are: increasing women's exposure to stem-courses during high school, planting the idea of positivism relating gender from the engineering culture, producing a more female-friendly environment inside and outside the classroom. These strategies have helped institutions encourage more women to enroll in engineering programs as well as other STEM-based majors. \nFor universities to encourage women to enroll in their graduate programs, institutions have to emphasize the importance of recruiting women, emphasize the importance of STEM education in the undergraduate level, offer financial aid, and develop more efficient methods for recruiting women to their programs.\n\nFemales are underrepresented as both graduate students in engineering and working engineers. The number of bachelor's degrees awarded to women dropped from 20.4% in 2003, down to 17.8% in 2009, and back up to 18.9% in 2012. Women's underrepresentation in the engineering workforce varies by field. In the year 2008 women Mechanical Engineers made up 6.7%, Electrical and Electronics made 7.7%, Aerospace and Civil made 10.4%, Chemical made 13.1% and Computer and Software Engineers made up 20.9% of the workforce. These values are even more outnumbered while quantifying the number of women who hold doctorates.\n\nThe percentage of master's degrees awarded to women has not changed much from 2003 (22.3%) to 2012 (23.1%). The percentage of doctoral degrees awarded to women in engineering increased from 11.6% in 1995, to 17.4% in 2004, to 21.1% in 2008, then to 22.2% in 2012.\n\nSince 1997, the percentage of Asian females enrolling in engineering majors has risen from about 30% to 34% but somehow also dropped in 2002. African American females have increased their representation in engineering from 21% to 33% in the same time frame. Mexican American (Chicana) and Puerto Rican females have had an increase in their representation from 25% to 31%. Even if ethnicities are included in these statistics, men from all ethnicities still outnumber the proportion of women who enroll in engineering bachelor programs.\n\nThere is a significant drop-off rate regarding the number of women who earn a bachelor's degree and the women who afterward enroll in graduate school. Over the last 35 years, women have been more likely than men to enroll in graduate school right after receiving their bachelor's degree. Women who do not enroll in a graduate program right after earning their bachelor's degree tend to be caregivers who face work-family conflicts in the context of family women.\nThe workforce remains the area of lowest representation for women. In 2009, women comprised 48% of the total workforce, but only 14% of the engineering workforce.\n\nOnly 14% of engineers in Australia are women. The retention of female engineers is also disproportionally low; in 2006, 62.6% of qualified male engineers were employed in engineering professions, as opposed to 47.1% of qualified female engineers.\n\nThough women tend to make up more than half of the undergraduate population in Canada, the number of women in engineering is disproportionately low. Whereas in 2001, 21 percent of students in engineering programs were female, by 2009, this had fallen to 17 percent. One commentator attributed this drop to a number of factors, such as the failure of higher education programs to explain how engineering can improve others' lives, a lack of awareness of what engineers do, and discomfort of being in a male-dominated environment and the perception that women must adapt to fit in.\n\nIn the 1990s, undergraduate enrollment of women in engineering fluctuated from 17 to 18%, while in 2001, it rose to 20.6%. In 2010, 17.7% of students in undergraduate engineering were women.\n\nFemale undergraduate enrollment was highest in 2010 in environmental, biosystems, and geological engineering.\n\nThe number of women enrolled in undergraduate, graduate, and doctoral engineering programs tends to vary by province, with the highest number in Saskatchewan, Alberta, and British Columbia.\n\nOn average, 11% of engineering faculty are women and the percentage of leadership roles held by women is an average of 9%. The University of Toronto has the highest female faculty rate in Canada at 17% and École Polytechnique de Montréal, University of British Columbia, and Dalhousie University all have a female faculty rate of 13%.\n\nIn 2011, the INWES Education and Research Institute (ERI) held a national workshop, Canadian Committee of Women in Engineering (CCWE+20), to determine ways of increasing the number of women in the engineering field in Canada. CCWE+20 identified a goal of increasing women's interest in engineering by 2.6 percent by 2016 to a total of 25 percent through more incentives such as through collaboration and special projects. The workshop identifies early education as one of the main barriers in addition to other factors, such as: \"the popular culture of their generation, the guidance they receive on course selection in high school and the extent to which their parents, teachers, and counselors recognize engineering as an appropriate and legitimate career choice for women.\" The workshop report compares enrollment, teaching, and professional statistics from the goals identified in 1997 compared to the actual data from 2009, outlining areas of improvement (see table, right).\n\n\n"}
{"id": "27200925", "url": "https://en.wikipedia.org/wiki?curid=27200925", "title": "Women in the military in the Americas", "text": "Women in the military in the Americas\n\nThis article is about the role played by women in the military in the Americas, particularly in the United States and Canada from the First World War to modern times.\n\nThe first participation of a woman in combat occurred in 1823. Maria Quitéria de Jesus fought for the maintenance of the independence of Brazil, and is considered the first woman to enlist in a military unit.\nHowever, it was not until 1943, during World War II, that women officially entered the Brazilian Army. They included 73 nurses, 67 of them registered nurses and six air transport specialists. They served in four different hospitals in the US Army. All volunteered for the mission and were the first women to join the active service of the Brazilian armed forces. After the war, as well as the rest of the FEB, the nurses, most have been awarded, they won the official patent and licensed the active military service.\n\nIn 1992, the School of Army Administration (Salvador - BA) enrolled the first group of 49 women, by conducting tender. And in 1996, Maria Quitéria de Jesus, the Paladina of Independence, was recognized in the army ranks, as Patron of Table Complementary Brazilian Army officers. The Army established the Military Female Volunteer for Medical, Dental, Pharmaceutical, Veterinary and top-level Nurses (MFDV) in 1996. At that time, they entered the first class of 290 female volunteers to provide military service in healthcare. This merger took place in all twelve military regions of the country. \n\nIn 1997 the Military Institute of Engineering—IME (Rio de Janeiro—RJ) enrolled the first group of 10 women students to be included in Table Military Engineers (QEM). In the same year the School of the Army Health—Essex (Rio de Janeiro RJ) enrolled and graduated the first group of medical officers, dentists, pharmaceutical, veterinary and top-level nurses in the framework of the Army Health.\n\nIn 1998, the Army established the Stage Technical Service for higher education professionals than healthcare. At that time, he entered the first class of 519 women lawyers, administrators of businesses, accountants, teachers, computer analysts, engineers, architects, journalists, and other areas of human and exact sciences, serving the needs of Official Temporary Technical (OTT) of Institution. In 2001 the Army Health School allowed the enrollment of women to participate in the public tender for the filling of vacancies in the Health Sergeant Course, which started to operate in 2002.\n\nDuring the First World War, over 2,300 women served overseas in the Canadian Army Medical Corps. Canadian women were also organized into possible uniformed home guard units, undertaking military training in paramilitary groups. During the Second World War, 5,000 women of the Royal Canadian Army Medical Corps again served overseas, however they were not permitted to serve on combat warships or in combat teams. The Canadian Army Women's Corps was created during the Second World War, as was the Royal Canadian Air Force (Women's Division). As well, 45,000 women served as support staff in every theatre of the conflict, driving heavy equipment, rigging parachutes, and performing clerical work, telephone operation, laundry duties and cooking. Some 5,000 women performed similar occupations during Canada’s part in the Korean War of 1950–1953.\n\nIn 1965 the Canadian government decided to allow a maximum of 1,500 women to serve directly in all three branches of its armed forces, and the former \"women's services\" were disbanded. In 1970 the government created a set of rules for the armed forces designed to encourage equal opportunities. These included the standardization of enlistment criteria, equal pay and pensions, and allowing women to enroll in all aspects of the Canadian armed forces and making it possible for women to reach any rank. In 1974 the first woman, Major Wendy Clay, earned her pilot's wings in the newly integrated Canadian Forces, and four years later the first woman qualified for the Canadian skydiving demonstration team, the Skyhawks.\n\nBetween 1979 and 1985 the role of women expanded further, with military colleges allowing women to enroll. 1981 saw the first female navigator and helicopter pilot, and in 1982 laws were passed ending all discrimination in employment, and combat related roles in the Canadian armed forces were opened for women, with no restrictions in place, with the exception of the submarine service. In 1986 further laws were created to the same effect. The following years saw Canada’s first female infantry soldier, first female gunner, and a female Brigadier-General.\n\nIn 1990 the Ministers Advisory Board on Women in the Canadian Forces was created, and in 1994 Wendy Clay was promoted to Major-General. In 2000 Major Micky Colton became the first female to log 5,000 flying hours in a C-130 Hercules. Women were permitted to serve on board Canadian submarines in 2002 with the acquisition of the \"Victoria\"-class submarine. Master Seaman Colleen Beattie became the first female submariner in 2003.\n\nCanadian women have also become clearance divers, and commanded large infantry units and Canadian warships.\n\nOn May 17, 2006 Captain Nichola Goddard became the first Canadian woman killed in combat during operations in Afghanistan.\n\nOne of the first American woman soldiers was Deborah Sampson of Massachusetts. She enlisted as a Continental Army soldier under the name of \"Robert Shurtliff\". She served for three years in the Revolutionary War and was wounded twice; she cut a musket ball out of her own thigh so no doctor would find out she was a woman. Finally, at the end of the hostilities her secret was discovered—even so, George Washington gave her an honorable discharge. She later lectured on her experiences and became a champion of women's rights. Another female soldier in the Revolutionary War was Anna Maria Lane of Virginia, who enlisted with her husband in 1776 and fought in several battles, including the Battle of Germantown. \n\nDuring the American Civil War, about 250 women have been found to have enlisted, counting both sides, and specialists believe that the total is higher. It was not generally a crime to for a woman to enlist, but it was considered improper and women only signed up claiming to be men. For example, Sarah Rosetta Wakeman enlisted under the alias of Private Lyons Wakeman. She served in the 153rd Regiment, New York State Volunteers. Her complete letters describing her experiences as a female soldier in the Union Army are reproduced in the book, \"An Uncommon Soldier: The Civil War Letters of Sarah Rosetta Wakeman, alias Pvt. Lyons Wakeman, 153rd Regiment, New York State Volunteers, 1862–1864\".\n\nA letter written by Annie Oakley to President William McKinley on April 5, 1898 may represent the earliest documentary proof of a political move towards recognizing a woman's right to serve in the United States military. Annie Oakley, sharpshooter and star in the Buffalo Bill Show, wrote a letter to President William McKinley on April 5, 1898 \"offering the government the services of a company of 50 'lady sharpshooters' who would provide their own arms and ammunition should war break out with Spain.\" Oakley's offer was not accepted for the Spanish–American War.\n\nIn World War I, the first American women enlisted into the regular armed forces were 13,000 women admitted into active duty in the Navy and Marines and a much smaller number admitted into the Coast Guard. The Yeoman (F) recruits and women Marines primarily served in clerical positions. They received the same benefits and responsibilities as men, including identical pay (US$28.75 per month), and were treated as veterans after the war. These women were quickly demobilized when hostilities ceased, and aside from the Nurse Corps the soldiery became once again exclusively male.\n\nThe Woman's Army Auxiliary Corps was established in the United States in 1942. However, political pressures stalled attempts to create more roles for women in the American Armed Forces. Women saw combat during World War II, first as nurses in the Pearl Harbor attacks on December 7, 1941. The Woman's Naval Reserve and Marine Corps Women’s Reserve were also created during this conflict. In July 1943 a bill was signed removing 'auxiliary' from the Women's Army Auxiliary Corps, making it an official part of the regular army. In 1944 WACs arrived in the Pacific and landed in Normandy on D-Day. During the war, 67 Army nurses and 16 Navy nurses were captured and spent three years as Japanese prisoners of war. There were 350,000 American women who served during World War Two and 16 were killed in action; in total, they gained over 1,500 medals, citations and commendations.\n\nVirginia Hall, serving with the Office of Strategic Services, received the second-highest US combat award, the Distinguished Service Cross, for action behind enemy lines in France. Hall, who had one artificial leg, landed clandestinely in occupied territory aboard a British Motor Torpedo Boat.\n\nAfter World War Two, demobilization led to the vast majority of serving women being returned to civilian life. Law 625, The Women's Armed Services Act of 1948, was signed by President Truman, allowing women to serve in the armed forces in fully integrated units during peace time, with only the WAC remaining a separate female unit. During the Korean War of 1950–1953 many women served in the Mobile Army Surgical Hospitals, with women serving in Korea numbering 120,000 during the conflict.\n\nRecords regarding American women serving in the Vietnam War are vague. However, it is recorded that 600 women served in the country as part of the Air Force, along with 500 members of the WAC, and over 6,000 medical personnel and support staff.\n\nThe Ordnance Corps began accepting female missile technicians in 1974, and female crewmembers and officers were accepted into Field Artillery missile units.\n\nIn 1974, the first six women aviators earned their wings as Navy pilots: Jane Skiles O'Dea, Barbara Allen Rainey, Rosemary Bryant Mariner, Judith Ann Neuffer, Ana Marie Fuqua, and Joellen Drag Oslund. The Congressionally mandated prohibition on women in combat places limitations on the pilots' advancement, but at least two retired as captains.\n\nAmerica’s involvement in Grenada in 1983 saw over 200 women serving; however, none of these took part in direct combat. Some women, such as Lt Col Eileen Collins or Lt Celeste Hayes, flew transport aircraft carrying wounded or assault teams, however they were not deemed to have been in direct combat. Several hundred women took part in operations in Panama in 1989, in non-combat roles.\n\nOn December 20, 1989, Capt Linda L. Bray, 29, became the first woman to command American soldiers in battle, during the invasion of Panama. She was assigned to lead a force of 30 men and women MPs to capture a kennel holding guard dogs that was defended by elements of the Panamanian Defense force. From a command center about a half-mile from the kennel she ordered her troops to fire warning shots. The Panamanians returned fire until threatened by artillery attack, fleeing into nearby woods. Bray advanced to the kennel to try to stop them, using the cover of a ditch to reach the building. No enemy dead were found, but a cache of weapons was recovered.\n\nThe 1991 Gulf War brought greater media attention to the role of women in the American armed forces. A senior woman pilot at the time, Colonel Kelly Hamilton, commented that \"[t]he conflict was an awakening for the people in the US. They suddenly realised there were a lot of women in the military.\" Over 40,000 women served in almost every role the armed forces had to offer. They were not permitted to participate in deliberate ground engagements. Many came under fire, however, and there are many reports of women engaging enemy forces. One example is that of the USS Mount HOOD, AE-29, Pacific Fleet ammunitions carrier in Battle Group Bravo. The Mt. Hood's sister ship, which was all male, was grounded after hitting a mine. The Mt. Hood, regardless of having at least 32 women on board, the first women to board that class of vessel in the USN, filled in. The women aboard the USS Mt. Hood, AE-29, may be the enlisted Navy's very first CONGRESSIONALLY RECOGNIZED females ordered to combat in a Congressionally declared war. That is not to say that they were the first females in combat because, as above illustrates, women have fought the front lines, whether they were afforded official Congressional recognition for their service or not. At any rate, this small ship tackled her duties and from the women leaders aboard came the first African-American female Admiral in the history of the United States Navy, Admiral and second in command of the United States Navy as Vice Chief of Naval Operations: Admiral Michelle Howard. \n\nToday, women can serve on American combat ships, including in command roles, and on submarines. They are not permitted to participate in special forces programs such as Navy SEALs. Women enlisted soldiers are barred from serving in Infantry, Special Forces, however female enlisted members and officers can hold staff positions in every branch of the Army except infantry and armor. Women can however serve on the staffs of infantry and armor units at Division level and above, and be members of Special Operations Forces. Women can fly military aircraft and make up 2% of all pilots in the U.S. military. Although Army regulations ban women from infantry assignments, some females are detailed to accompany male infantry units to handle searches of Iraqi women.\nThe case \"United States v. Virginia\", in which the Supreme Court ordered that the Virginia Military Institute allow women to register as cadets, gave women soldiers a weapon against laws which (quoting J. Ruth Bader Ginsburg) “[deny] to women, simply because they are women, full citizenship stature—equal opportunity to aspire, achieve, participate in and contribute to society.”\n\nDuring Battle of Nasiriyah in 2003, American soldiers Shoshana Johnson, the first African-American and first Hispanic female prisoner of war, and Jessica Lynch were captured while serving in Iraq. In the same action, Lori Piestewa, a U.S. soldier, died after driving her Humvee through enemy fire in an attempt to escape an ambush, earning a Purple Heart. She had just rescued Jessica Lynch, whose vehicle had crashed.\n\nAlso in 2003, Major Kim Campbell was awarded the Distinguished Flying Cross for landing her combat damaged A-10 Thunderbolt II with no hydraulic control and only one functional engine after being struck by hostile fire over Baghdad.\n\nIn a recent scandal, U.S Army Reservists Lynndie England and Sabrina Harman were convicted by court martial of cruelty and maltreatment of prisoners at Abu Ghraib prison.\nSGT Leigh Ann Hester became the first woman to receive the Silver Star, the third-highest US decoration for valor, for direct participation in combat. Female medical personnel had been awarded the same medal, but not for actual combat. She was a team leader of Raven 42, a Military Police squad that broke up an ambush roughly three to four times its strength. Specialist Ashley Pullen received the Bronze Star. The squad leader, SSG Timothy Nein, had originally received the Silver Star, but his award was later upgraded to the Distinguished Service Cross. SGT Jason Mike, the unit's medic, also received the Silver Star.\nIn Afghanistan, Monica Lin Brown, was presented the Silver Star for shielding wounded soldiers with her body, and then treating life-threatening injuries. As of March 2012, the U.S. military has two women, Ann E. Dunwoody and Janet C. Wolfenbarger, with the rank of four-star general.\n\n\n"}
