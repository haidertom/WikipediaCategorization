{"id": "23580511", "url": "https://en.wikipedia.org/wiki?curid=23580511", "title": "20th-century Western painting", "text": "20th-century Western painting\n\n20th-century Western painting begins with the heritage of late-19th-century painters Vincent van Gogh, Paul Cézanne, Paul Gauguin, Georges Seurat and Henri de Toulouse-Lautrec, and others who were essential for the development of modern art. At the beginning of the 20th century, Henri Matisse and several other young artists including the pre-cubist Georges Braque, André Derain, Raoul Dufy and Maurice de Vlaminck revolutionized the Paris art world with \"wild\", multi-colored, expressive landscapes and figure paintings that the critics called Fauvism. Matisse's second version of \"The Dance\" signified a key point in his career and in the development of modern painting. It reflected Matisse's incipient fascination with primitive art: the intense warm color of the figures against the cool blue-green background and the rhythmical succession of the dancing nudes convey the feelings of emotional liberation and hedonism.\n\nInitially influenced by Toulouse-Lautrec, Gauguin, and other late-19th-century innovators, Pablo Picasso made his first cubist paintings based on Cézanne's idea that all depiction of nature can be reduced to three solids: cube, sphere and cone. With the painting \"Les Demoiselles d'Avignon\" (1907; see gallery) Picasso created a new and radical picture depicting a raw and primitive brothel scene with five prostitutes, violently painted women, reminiscent of African tribal masks and his own new proto-Cubist inventions. Analytic cubism, exemplified by \"Violin and Candlestick, Paris\", was jointly developed by Pablo Picasso and Georges Braque from about 1908 through 1912. Analytic cubism was followed by Synthetic cubism, characterized by the introduction of different textures, surfaces, collage elements, papier collé and a large variety of merged subject matter.\n\nCrystal Cubism was a distilled form of Cubism consistent with a shift between 1915 and 1916 towards a strong emphasis on flat surface activity and large overlapping geometric planes, practised by Braque, Picasso, Jean Metzinger, Albert Gleizes, Juan Gris, Diego Rivera, Henri Laurens, Jacques Lipchitz, Alexander Archipenko, Fernand Léger and several other artists into the 1920s.\n\nDuring the years between 1910 and the end of World War I and after the heyday of cubism, several movements emerged in Paris. Giorgio de Chirico moved to Paris in July 1911, where he joined his brother Andrea (the poet and painter known as Alberto Savinio). Through his brother he met Pierre Laprade, a member of the jury at the Salon d'Automne, where he exhibited three of his dreamlike works: \"Enigma of the Oracle\", \"Enigma of an Afternoon\" and \"Self-Portrait\". During 1913 he exhibited his work at the Salon des Indépendants and Salon d’Automne, where his work was noticed by Pablo Picasso, Guillaume Apollinaire, and others. His compelling and mysterious paintings are considered instrumental to the early beginnings of Surrealism. \"Song of Love\" (1914) is one of the most famous works by de Chirico and is an early example of the surrealist style, though it was painted ten years before the movement was \"founded\" by André Breton in 1924.\n\nIn the first two decades of the 20th century, as Cubism evolved, several other important movements emerged; Futurism (Balla), Abstract art (Kandinsky), Der Blaue Reiter (Wassily Kandinsky and Franz Marc), Bauhaus (Kandinsky and Klee), Orphism, (Delaunay and Kupka), Synchromism (Russell and Macdonald-Wright), De Stijl (van Doesburg and Mondrian), Suprematism (Malevich), Constructivism (Tatlin), Dadaism (Duchamp, Picabia and Arp), and Surrealism (de Chirico, André Breton, Miró, Magritte, Dalí and Ernst). Modern painting influenced all the visual arts, from Modernist architecture and design, to avant-garde film, theatre and modern dance and became an experimental laboratory for the expression of visual experience, from photography and concrete poetry to advertising art and fashion. Van Gogh's painting exerted great influence upon 20th-century Expressionism, as can be seen in the work of the Fauves, Die Brücke (a group led by German painter Ernst Kirchner), and the Expressionism of Edvard Munch, Egon Schiele, Marc Chagall, Amedeo Modigliani, Chaim Soutine and others.\n\nWassily Kandinsky, a Russian painter, printmaker and art theorist, is generally considered the first important painter of modern abstract art. As an early modernist, in search of new modes of visual expression, and spiritual expression, he theorized—as did contemporary occultists and theosophists—that pure visual abstraction had corollary vibrations with sound and music. They posited that pure abstraction could express pure spirituality. His earliest abstractions were generally titled (as the example in the above gallery) \"Composition VII\", making connection to the work of the composers of music. Kandinsky included many of his theories about abstract art in his book \"Concerning the Spiritual in Art\". Piet Mondrian's art was also related to his spiritual and philosophical studies. In 1908 he became interested in the theosophical movement launched by Helena Petrovna Blavatsky in the late 19th century. Blavatsky believed that it was possible to attain a knowledge of nature more profound than that provided by empirical means, and much of Mondrian's work for the rest of his life was inspired by his search for that spiritual knowledge. Other major pioneers of early abstraction include Swedish painter Hilma af Klint, Russian painter Kazimir Malevich, and Swiss painter Paul Klee. Robert Delaunay was a French artist who is associated with Orphism, (reminiscent of a link between pure abstraction and cubism). His later works were more abstract, reminiscent of Paul Klee. His key contributions to abstract painting refer to his bold use of color, and a clear love of experimentation of both depth and tone. At the invitation of Kandinsky, Delaunay and his wife the artist Sonia Delaunay, joined The Blue Rider (Der Blaue Reiter), a Munich-based group of abstract artists, in 1911, and his art took a turn to the abstract. Still other important pioneers of abstract painting include Czech painter, František Kupka as well as American artists Stanton Macdonald-Wright and Morgan Russell who, in 1912, founded Synchromism, an art movement that closely resembles Orphism.\n\nLes Fauves (French for \"The Wild Beasts\") were early-20th-century painters, experimenting with freedom of expression through color. The name was given, humorously and not as a compliment, to the group by art critic Louis Vauxcelles. Fauvism was a short-lived and loose grouping of artists whose works emphasized painterly qualities and the imaginative use of deep color over the representational values. Fauvists made the subject of the painting easy to read and exaggerated perspectives. A prescient prediction of the Fauves was expressed in 1888 by Paul Gauguin to Paul Sérusier:\n\n\"How do you see these trees? They are yellow. So, put in yellow; this shadow, rather blue, paint it with pure ultramarine; these red leaves? Put in vermilion.\"\n\nThe leaders of the movement were Henri Matisse and André Derain—friendly rivals of a sort, each with his own followers. Ultimately Matisse became the \"yang\" to Picasso's \"yin\" in the 20th century. Fauvist painters included Albert Marquet, Charles Camoin, Maurice de Vlaminck, Raoul Dufy, Othon Friesz, the Dutch painter Kees van Dongen, and Picasso's partner in Cubism, Georges Braque amongst others.\n\nFauvism, as a movement, had no concrete theories, and was short lived, beginning in 1905 and ending in 1907. The Fauves had only three exhibitions. Matisse was seen as the leader of the movement, due to his seniority in age and prior self-establishment in the academic art world. His 1905 portrait of Mme. Matisse, \"The Green Line\" (above), caused a sensation in Paris when it was first exhibited. He said he wanted to create art to delight; art as a decoration was his purpose and it can be said that his use of bright colors tries to maintain serenity of composition. In 1906 at the suggestion of his dealer Ambroise Vollard, André Derain went to London and produced a series of paintings like \"Charing Cross Bridge, London\" (above) in the Fauvist style, paraphrasing the famous series by the Impressionist painter Claude Monet.\nBy 1907 Fauvism no longer was a shocking new movement, soon it was replaced by Cubism on\nthe critics radar screen as the latest new development in Contemporary Art of the time.\nIn 1907 Appolinaire, commenting about Matisse in an article published in La Falange, said, \"We are not here in the presence of an extravagant or an extremist undertaking: Matisse's art is eminently reasonable.\"\n\nDer Blaue Reiter was a German movement lasting from 1911 to 1914, fundamental to Expressionism, along with Die Brücke, a group of German expressionist artists formed in Dresden in 1905. Founding members of Die Brücke were Fritz Bleyl, Erich Heckel, Ernst Ludwig Kirchner and Karl Schmidt-Rottluff. Later members included Max Pechstein, Otto Mueller and others. This was a seminal group, which in due course had a major impact on the evolution of modern art in the 20th century and created the style of Expressionism.\n\nWassily Kandinsky, Franz Marc, August Macke, Alexej von Jawlensky, whose psychically expressive painting of the Russian dancer \"Portrait of Alexander Sakharoff,\" 1909 is in the gallery above, Marianne von Werefkin, Lyonel Feininger and others founded the Der Blaue Reiter group in response to the rejection of Kandinsky's painting \"Last Judgement\" from an exhibition. Der Blaue Reiter lacked a central artistic manifesto, but was centered around Kandinsky and Marc. Artists Gabriele Münter and Paul Klee were also involved.\n\nThe name of the movement comes from a painting by Kandinsky created in 1903. It is also claimed that the name could have derived from Marc's enthusiasm for horses and Kandinsky's love of the colour blue. For Kandinsky, \"blue\" is the colour of spirituality: the darker the blue, the more it awakens human desire for the eternal.\n\n\"Expressionism\" and \"Symbolism\" are broad rubrics encompassing several important and related movements in 20th-century painting that dominated much of the avant-garde art being made in Western, Eastern, and Northern Europe. Expressionist works were painted largely between World War I and World War II, mostly in France, Germany, Norway, Russia, Belgium, and Austria. Expressionist styles are related to those of both Surrealism and Symbolism and are each uniquely and somewhat eccentrically personal. Fauvism, Die Brücke, and Der Blaue Reiter are three of the best known groups of Expressionist and Symbolist painters. Artists as interesting and diverse as Marc Chagall, whose painting \"I and the Village,\" (above) tells an autobiographical story that examines the relationship between the artist and his origins, with a lexicon of artistic Symbolism. Gustav Klimt, Egon Schiele, Edvard Munch, Emil Nolde, Chaim Soutine, James Ensor, Oskar Kokoschka, Ernst Ludwig Kirchner, Max Beckmann, Franz Marc, Käthe Schmidt Kollwitz, Georges Rouault, Amedeo Modigliani and some of the Americans abroad like Marsden Hartley, and Stuart Davis, were considered influential expressionist painters. Although Alberto Giacometti is primarily thought of as an intense Surrealist sculptor, he made intense expressionist paintings as well.\nIn the USA during the period between World War I and World War II painters tended to go to Europe for recognition. Modernist artists like Marsden Hartley, Patrick Henry Bruce, Gerald Murphy and Stuart Davis, created reputations abroad. While Patrick Henry Bruce, created cubist related paintings in Europe, both Stuart Davis and Gerald Murphy made paintings that were early inspirations for American pop art and Marsden Hartley experimented with expressionism. During the 1920s photographer Alfred Stieglitz exhibited Georgia O'Keeffe, Arthur Dove, Alfred Henry Maurer, Charles Demuth, John Marin and other artists including European Masters Henri Matisse, Auguste Rodin, Henri Rousseau, Paul Cézanne, and Pablo Picasso, at his New York City gallery \"the 291\". In Europe masters like Henri Matisse and Pierre Bonnard continued developing their narrative styles independent of any movement.\n\n \nMarcel Duchamp came to international prominence in the wake of the New York City Armory Show in 1913 where his \"Nude Descending a Staircase\" became the cause celebre. He subsequently created \"The Bride Stripped Bare by Her Bachelors, Even, Large Glass\". The \"Large Glass\" pushed the art of painting to radical new limits being part painting, part collage, part construction. Duchamp (who was soon to renounce artmaking for chess) became closely associated with the Dada movement that began in neutral Zürich, Switzerland, during World War I and peaked from 1916 to 1920. The movement primarily involved visual arts, literature (poetry, art manifestoes, art theory), theatre, and graphic design, and concentrated its anti war politic through a rejection of the prevailing standards in art through anti-art cultural works. Francis Picabia, Man Ray, Kurt Schwitters, Tristan Tzara, Hans Richter, Jean Arp, Sophie Taeuber-Arp, along with Duchamp and many others are associated with the Dadaist movement. Duchamp and several Dadaists are also associated with Surrealism, the movement that dominated European painting in the 1920s and 1930s.\nIn 1924 André Breton published the \"Surrealist Manifesto.\" The Surrealist movement in painting became synonymous with the avant-garde and which featured artists whose works varied from the abstract to the super-realist. With works on paper like \"Machine Turn Quickly,\" (above) Francis Picabia continued his involvement in the Dada movement through 1919 in Zürich and Paris, before breaking away from it after developing an interest in Surrealist art. Yves Tanguy, René Magritte and Salvador Dalí are particularly known for their realistic depictions of dream imagery and fantastic manifestations of the imagination. Joan Miró's \"The Tilled Field\" of 1923–1924 verges on abstraction, this early painting of a complex of objects and figures, and arrangements of sexually active characters; was Miró's first Surrealist masterpiece. The more abstract Joan Miró, Jean Arp, André Masson, and Max Ernst were very influential, especially in the United States during the 1940s.\n\nThroughout the 1930s, Surrealism continued to become more visible to the public at large. A Surrealist group developed in Britain and, according to Breton, their 1936 London International Surrealist Exhibition was a high-water mark of the period and became the model for international exhibitions. Surrealist groups in Japan, and especially in Latin America, the Caribbean and in Mexico produced innovative and original works.\n\nDalí and Magritte created some of the most widely recognized images of the movement. The 1928/1929 painting \"This Is Not A Pipe\" by Magritte is the subject of a Michel Foucault 1973 book, \"This is not a Pipe\" (English edition, 1991), that discusses the painting and its paradox. Dalí joined the group in 1929, and participated in the rapid establishment of the visual style between 1930 and 1935.\n\nSurrealism as a visual movement had found a method: to expose psychological truth by stripping ordinary objects of their normal significance, in order to create a compelling image that was beyond ordinary formal organization, and perception, sometimes evoking empathy from the viewer, sometimes laughter and sometimes outrage and bewilderment.\n\n1931 marked a year when several Surrealist painters produced works which marked turning points in their stylistic evolution: in one example liquid shapes become the trademark of Dalí, particularly in his \"The Persistence of Memory\", which features the image of watches that sag as if they are melting. Evocations of time and its compelling mystery and absurdity.\n\nThe characteristics of this style – a combination of the depictive, the abstract, and the psychological – came to stand for the alienation which many people felt in the modernist period, combined with the sense of reaching more deeply into the psyche, to be \"made whole with one's individuality.\"\n\nMax Ernst studied philosophy and psychology in Bonn and was interested in the alternative realities experienced by the insane. His paintings, such as \"Murdering Airplane\" (1920), may have been inspired by the psychoanalyst Sigmund Freud's study of the delusions of a paranoiac, Daniel Paul Schreber. Freud identified Schreber's fantasy of becoming a woman as a \"castration complex\". The central image of two pairs of legs refers to Schreber's hermaphroditic desires. Ernst's inscription on the back of the painting reads: \"The picture is curious because of its symmetry. The two sexes balance one another.\"\n\nDuring the 1920s André Masson's work was enormously influential in helping the young artist Joan Miró find his roots in the new Surrealist painting. Miró acknowledged in letters to his dealer Pierre Matisse the importance of Masson as an example to him in his early years in Paris.\n\nLong after personal, political and professional tensions have fragmented the Surrealist group into thin air and ether, Magritte, Miró, Dalí and the other Surrealists continue to define a visual program in the arts. Other prominent surrealist artists include Giorgio de Chirico, Méret Oppenheim, Toyen, Grégoire Michonze, Roberto Matta, Kay Sage, Leonora Carrington, Dorothea Tanning, and Leonor Fini among others.\n\nDuring the 1920s and the 1930s and the Great Depression, the European art scene was characterized by Surrealism, late Cubism, the Bauhaus, De Stijl, Dada, Neue Sachlichkeit, and Expressionism; and was occupied by masterful modernist color painters like Henri Matisse and Pierre Bonnard.\n\nAmerican Scene painting and the Social Realism and Regionalism movements that contained both political and social commentary dominated the art world in the USA. Artists like Ben Shahn, Thomas Hart Benton, Grant Wood, George Tooker, John Steuart Curry, Reginald Marsh, and others became prominent. In Latin America besides the Uruguayan painter Joaquín Torres García and Rufino Tamayo from Mexico, the muralist movement with Diego Rivera, David Siqueiros, José Orozco, Pedro Nel Gómez and Santiago Martinez Delgado and the Symbolist paintings by Frida Kahlo began a renaissance of the arts for the region, with a use of color and historic, and political messages. Frida Kahlo's Symbolist works also relate strongly to Surrealism and to the Magic Realism movement in literature. The psychological drama in many of Kahlo's self-portraits (above) underscore the vitality and relevance of her paintings to artists in the 21st century.\nIn Germany Neue Sachlichkeit (\"New Objectivity\") emerged as Max Beckmann, Otto Dix, George Grosz and others associated with the Berlin Secession politicized their paintings. The work of these artists grew out of expressionism, and was a response to the political tensions of the Weimar Republic, and was often sharply satirical.\n\nDiego Rivera is perhaps best known by the public for his 1933 mural, \"Man at the Crossroads\", in the lobby of the RCA Building at Rockefeller Center. When his patron Nelson Rockefeller discovered that the mural included a portrait of Lenin and other communist imagery, he fired Rivera, and the unfinished work was eventually destroyed by Rockefeller's staff. The film \"Cradle Will Rock\" includes a dramatization of the controversy. Frida Kahlo (Rivera's wife's) works are often characterized by their stark portrayals of pain. Of her 143 paintings 55 are self-portraits, which frequently incorporate symbolic portrayals of her physical and psychological wounds. Kahlo was deeply influenced by indigenous Mexican culture, which is apparent in her paintings' bright colors and dramatic symbolism. Christian and Jewish themes are often depicted in her work as well; she combined elements of the classic religious Mexican tradition—which were often bloody and violent—with surrealist renderings. While her paintings are not overtly Christian—she was an avowed communist—they certainly contain elements of the macabre Mexican Christian style of religious paintings.\n\nDuring the 1930s radical leftist politics characterized many of the artists connected to Surrealism, including Pablo Picasso. On 26 April 1937, during the Spanish Civil War, the Basque town of Gernika was the scene of the \"Bombing of Gernika\" by the Condor Legion of Nazi Germany's Luftwaffe. The Germans were attacking to support the efforts of Francisco Franco to overthrow the Basque Government and the Spanish Republican government. The town was devastated, though the Biscayan assembly and the Oak of Gernika survived. Pablo Picasso painted his mural sized \"Guernica\" to commemorate the horrors of the bombing.\n\nIn its final form, \"Guernica\" is an immense black and white, 3.5 metre (11 ft) tall and 7.8 metre (23 ft) wide mural painted in oil. The mural presents a scene of death, violence, brutality, suffering, and helplessness without portraying their immediate causes. The choice to paint in black and white contrasts with the intensity of the scene depicted and invokes the immediacy of a newspaper photograph.\nPicasso painted the mural sized painting called \"Guernica\" in protest of the bombing. The painting was first exhibited in Paris in 1937, then Scandinavia, then London in 1938 and finally in 1939 at Picasso's request the painting was sent to the United States in an extended loan (for safekeeping) at MoMA. The painting went on a tour of museums throughout the US until its final return to the Museum of Modern Art in New York City where it was exhibited for nearly thirty years. Finally in accord with Pablo Picasso's wish to give the painting to the people of Spain as a gift, it was sent to Spain in 1981.\n\nDuring the Great Depression of the 1930s, through the years of World War II American art was characterized by Social Realism and American Scene Painting (as seen above) in the work of Grant Wood, Edward Hopper, Ben Shahn, Thomas Hart Benton, and several others. \"Nighthawks\" (1942) is a painting by Edward Hopper that portrays people sitting in a downtown diner late at night. It is not only Hopper's most famous painting, but one of the most recognizable in American art. It is currently in the collection of the Art Institute of Chicago. The scene was inspired by a diner (since demolished) in Greenwich Village, Hopper's home neighborhood in Manhattan. Hopper began painting it immediately after the attack on Pearl Harbor. After this event there was a large feeling of gloominess over the country, a feeling that is portrayed in the painting. The urban street is empty outside the diner, and inside none of the three patrons is apparently looking or talking to the others but instead is lost in their own thoughts. This portrayal of modern urban life as empty or lonely is a common theme throughout Hopper's work.\n\n\"American Gothic\" is a painting by Grant Wood from 1930. Portraying a pitchfork-holding farmer and a younger woman in front of a house of Carpenter Gothic style, it is one of the most familiar images in 20th-century American art. Art critics had favorable opinions about the painting, like Gertrude Stein and Christopher Morley, they assumed the painting was meant to be a satire of rural small-town life. It was thus seen as part of the trend towards increasingly critical depictions of rural America, along the lines of Sherwood Anderson's \"1919 Winesburg, Ohio\", Sinclair Lewis' 1920 \"Main Street\", and Carl Van Vechten's \"The Tattooed Countess\" in literature. However, with the onset of the Great Depression, the painting came to be seen as a depiction of steadfast American pioneer spirit.\n\nThe 1940s in New York City heralded the triumph of American abstract expressionism, a modernist movement that combined lessons learned from Henri Matisse, Pablo Picasso, Surrealism, Joan Miró, Cubism, Fauvism, and early Modernism via great teachers in America like Hans Hofmann and John D. Graham. American artists benefited from the presence of Piet Mondrian, Fernand Léger, Max Ernst and the André Breton group, Pierre Matisse's gallery, and Peggy Guggenheim's The Art of This Century Gallery, as well as other factors.\n\nPost-Second World War American painting called Abstract expressionism included artists like Jackson Pollock, Willem de Kooning, Arshile Gorky, Mark Rothko, Hans Hofmann, Clyfford Still, Franz Kline, Adolph Gottlieb, Barnett Newman, Mark Tobey, James Brooks, Philip Guston, Robert Motherwell, Conrad Marca-Relli, Jack Tworkov, William Baziotes, Richard Pousette-Dart, Ad Reinhardt, Esteban Vicente, Hedda Sterne, Jimmy Ernst, Bradley Walker Tomlin, and Theodoros Stamos, among others. American Abstract expressionism got its name in 1946 from the art critic Robert Coates. It is seen as combining the emotional intensity and self-denial of the German Expressionists with the anti-figurative aesthetic of the European abstract schools such as Futurism, the Bauhaus and Synthetic Cubism. Abstract expressionism, Action painting, and Color Field painting are synonymous with the New York School.\n\nTechnically Surrealism was an important predecessor for Abstract expressionism with its emphasis on spontaneous, automatic or subconscious creation. Jackson Pollock's dripping paint onto a canvas laid on the floor is a technique that has its roots in the work of André Masson. Another important ear<nowiki>ly m</nowiki>anifestation of what came to be abstract expressionism is the work of American Northwest artist Mark Tobey, especially his \"white writing\" canvases, which, though generally not large in scale, anticipate the \"all over\" look of Pollock's drip paintings.\n\nAdditionally, abstract expressionism has an image of being rebellious, anarchic, highly idiosyncratic and, some feel, rather nihilistic. In practice, the term is applied to any number of artists working (mostly) in New York who had quite different styles, and even applied to work which is not especially abstract nor expressionist. Pollock's energetic \"action paintings\", with their \"busy\" feel, are different both technically and aesthetically, to the violent and grotesque \"Women\" series of Willem de Kooning. As seen above in the gallery \"Woman V\" is one of a series of six paintings made by de Kooning between 1950 and 1953 that depict a three-quarter-length female figure. He began the first of these paintings, \"Woman I\" collection: The Museum of Modern Art, New York City, in June 1950, repeatedly changing and painting out the image until January or February 1952, when the painting was abandoned unfinished. The art historian Meyer Schapiro saw the painting in de Kooning's studio soon afterwards and encouraged the artist to persist. De Kooning's response was to begin three other paintings on the same theme; \"Woman II\", The Museum of Modern Art, New York City, \"Woman III\", Tehran Museum of Contemporary Art, \"Woman IV\", Nelson-Atkins Museum of Art, Kansas City, Missouri. During the summer of 1952, spent at East Hampton, de Kooning further explored the theme through drawings and pastels. He may have finished work on \"Woman I\" by the end of June, or possibly as late as November 1952, and probably the other three women pictures were concluded at much the same time. The \"Woman series\" are decidedly figurative paintings. Another important artist is Franz Kline, as demonstrated by his painting \"High Street,\" 1950 as with Jackson Pollock and other Abstract Expressionists, was labelled an action painter because of his seemingly spontaneous and intense style, focusing less, or not at all, on figures or imagery, but on the actual brush strokes and use of canvas.\n\nClyfford Still, Barnett Newman, Adolph Gottlieb, and the serenely shimmering blocks of color in Mark Rothko's work (which is not what would usually be called expressionist and which Rothko denied was abstract), are classified as abstract expressionists, albeit from what Clement Greenberg termed the Color field direction of abstract expressionism. Both Hans Hofmann and Robert Motherwell (gallery) can be comfortably described as practitioners of action painting and Color field painting.\n\nAbstract expressionism has many stylistic similarities to the Russian artists of the early twentieth century such as Wassily Kandinsky. Although it is true that spontaneity or of the impression of spontaneity characterized many of the abstract expressionists works, most of these paintings involved careful planning, especially since their large size demanded it. An exception might be the drip paintings of Pollock.\nWhy this style gained mainstream acceptance in the 1950s is a matter of debate. American Social realism had been the mainstream in the 1930s. It had been influenced not only by the Great Depression but also by the Social Realists of Mexico such as David Alfaro Siqueiros and Diego Rivera. The political climate after World War II did not long tolerate the social protests of those painters. Abstract expressionism arose during World War II and began to be showcased during the early 1940s at galleries in New York like The Art of This Century Gallery. The late 1940s through the mid-1950s ushered in the McCarthy era. It was after World War II and a time of political conservatism and extreme artistic censorship in the United States. Some people have conjectured that since the subject matter was often totally abstract, Abstract expressionism became a safe strategy for artists to pursue this style. Abstract art could be seen as apolitical. Or if the art was political, the message was largely for the insiders. However those theorists are in the minority. As the first truly original school of painting in America, Abstract expressionism demonstrated the vitality and creativity of the country in the post-war years, as well as its ability (or need) to develop an aesthetic sense that was not constrained by the European standards of beauty.\n\nAlthough Abstract expressionism spread quickly throughout the United States, the major centers of this style were New York City and California, especially in the New York School, and the San Francisco Bay area. Abstract expressionist paintings share certain characteristics, including the use of large canvases, an \"all-over\" approach, in which the whole canvas is treated with equal importance (as opposed to the center being of more interest than the edges). The canvas as the \"arena\" became a credo of Action painting, while the \"integrity of the picture plane\" became a credo of the Color Field painters. Many other artists began exhibiting their abstract expressionist related paintings during the 1950s including Alfred Leslie, Sam Francis, Joan Mitchell, Helen Frankenthaler, Cy Twombly, Milton Resnick, Michael Goldberg, Norman Bluhm, Ray Parker, Nicolas Carone, Grace Hartigan, Friedel Dzubas, and Robert Goodnough among others.\n\nDuring the 1950s, Color Field painting initially referred to a particular type of abstract expressionism, especially the work of Mark Rothko, Clyfford Still, Barnett Newman, Robert Motherwell and Adolph Gottlieb. It essentially involved abstract paintings with large, flat expanses of color that expressed the sensual, and visual feelings and properties of large areas of nuanced surface. Art critic Clement Greenberg perceived Color Field painting as related to but different from Action painting. The overall expanse and gestalt of the work of the early color field painters speaks of an almost religious experience, awestruck in the face of an expanding universe of sensuality, color and surface. During the early to mid-1960s, \"Color Field painting\" came to refer to the styles of artists like Jules Olitski, Kenneth Noland, and Helen Frankenthaler, whose works were related to second-generation abstract expressionism, and to younger artists like Larry Zox, and Frank Stella – all moving in a new direction. Artists like Clyfford Still, Mark Rothko, Hans Hofmann, Morris Louis, Jules Olitski, Kenneth Noland, Helen Frankenthaler, Larry Zox, and others often used greatly reduced references to nature, and they painted with a highly articulated and psychological use of color. In general these artists eliminated recognizable imagery. In \"Mountains and Sea,\" from 1952, a seminal work of Colorfield painting by Helen Frankenthaler the artist used the stain technique for the first time.\n\nIn Europe there was the continuation of Surrealism, Cubism, Dada and the works of Matisse. Also in Europe, Tachisme (the European equivalent to Abstract expressionism) and Informalism took hold of the newest generation. Serge Poliakoff, Nicolas de Staël, Georges Mathieu, Vieira da Silva, Jean Dubuffet, Yves Klein and Pierre Soulages among others are considered important figures in post-war European painting.\n\nEventually abstract painting in America evolved into movements such as Neo-Dada, Color Field painting, Post painterly abstraction, Op art, hard-edge painting, Minimal art, shaped canvas painting, Lyrical Abstraction, Neo-expressionism and the continuation of Abstract expressionism. As a response to the tendency toward abstraction imagery emerged through various new movements, notably Pop art.\n\nDuring the 1930s through the 1960s as abstract painting in America and Europe evolved into movements such as Abstract Expressionism, Color Field painting, Post painterly abstraction, Op art, hard-edge painting, Minimal art, shaped canvas painting, and Lyrical Abstraction. Other artists reacted as a response to the tendency toward abstraction allowing imagery to continue through various new contexts like the Bay Area Figurative Movement in the 1950s and new forms of expressionism from the 1940s through the 1960s. In Italy during this time, Giorgio Morandi was the foremost still life painter, exploring a wide variety of approaches to depicting everyday bottles and kitchen implements. Throughout the 20th century many painters practiced Realism and used expressive imagery; practicing landscape and figurative painting with contemporary subjects and solid technique, and unique expressivity like Milton Avery, John D. Graham, Fairfield Porter, Edward Hopper, Andrew Wyeth, Balthus, Francis Bacon, Frank Auerbach, Lucian Freud, Leon Kossoff, Philip Pearlstein, Willem de Kooning, Arshile Gorky, Grace Hartigan, Robert De Niro, Sr., Elaine de Kooning and others. Along with Henri Matisse, Pablo Picasso, Pierre Bonnard, Georges Braque, and other 20th-century masters. The figurative work of Francis Bacon, Frida Kahlo, Edward Hopper, Lucian Freud Andrew Wyeth and others served as a kind of alternative to abstract expressionism. One of the most well-known images in 20th-century American art is Wyeth's painting, \"Christina's World\", currently in the collection of the Museum of Modern Art in New York City. It depicts a woman lying on the ground in a treeless, mostly tawny field, looking up at and crawling towards a gray house on the horizon; a barn and various other small outbuildings are adjacent to the house. This tempera work, done in a realist style, is nearly always on display at the Museum of Modern Art in New York.\n\nArshile Gorky's portrait of what may be his friend Willem de Kooning (left) is an example of the evolution of Abstract Expressionism from the context of figure painting, cubism and surrealism. Along with his friends de Kooning and John D. Graham Gorky created bio-morphically shaped and abstracted figurative compositions that by the 1940s evolved into totally abstract paintings. Gorky's work seems to be a careful analysis of memory, emotion and shape, using line and color to express feeling and nature.\n\n\"Study after Velázquez's Portrait of Pope Innocent X,\" 1953 is a painting by the Irish born artist Francis Bacon and is an example of Post World War II European Expressionism. The work shows a distorted version of the Portrait of Innocent X painted by the Spanish artist Diego Velázquez in 1650. The work is one of a series of variants of the Velázquez painting which Bacon executed throughout the 1950s and early 1960s, over a total of forty-five works. When asked why he was compelled to revisit the subject so often, Bacon replied that he had nothing against the Popes, that he merely \"wanted an excuse to use these colours, and you can't give ordinary clothes that purple colour without getting into a sort of false fauve manner.\" The Pope in this version seethes with anger and aggression, and the dark colors give the image a grotesque and nightmarish appearance. The pleated curtains of the backdrop are rendered transparent, and seem to fall through the Pope's face.\n\nAfter World War II the term School of Paris often referred to Tachisme, the European equivalent of American Abstract expressionism and those artists are also related to Cobra. In 1952 Michel Tapié authored the book \"Un Autre art\" which gave name and voice to Informalism. Important proponents being Jean Dubuffet, Pierre Soulages, Nicolas de Staël, Hans Hartung, Serge Poliakoff, and Georges Mathieu, among several others. During the early 1950s Dubuffet (who was always a figurative artist), and de Staël, abandoned abstraction, and returned to imagery via figuration and landscape. De Staël 's work was quickly recognised within the post-war art world, and he became one of the most influential artists of the 1950s. His return to representation (seascapes, footballers, jazz musicians, seagulls) during the early 1950s can be seen as an influential precedent for the American Bay Area Figurative Movement, as many of those abstract painters like Richard Diebenkorn, David Park, Elmer Bischoff, Wayne Thiebaud, Nathan Oliveira, Joan Brown and others made a similar move; returning to imagery during the mid-1950s. Much of de Staël 's late work – in particular his thinned, and diluted oil on canvas abstract landscapes of the mid-1950s predicts Color Field painting and Lyrical Abstraction of the 1960s and 1970s. Nicolas de Staël's bold and intensely vivid color in his last paintings predict the direction of much of contemporary painting that came after him including Pop art of the 1960s.\n\nPop art in America was to a large degree initially inspired by the works of Jasper Johns, Larry Rivers, and Robert Rauschenberg. Although the paintings of Gerald Murphy, Stuart Davis and Charles Demuth during the 1920s and 1930s set the table for Pop art in America. In New York City during the mid-1950s Robert Rauschenberg and Jasper Johns created works of art that at first seemed to be continuations of Abstract expressionist painting. Actually their works and the work of Larry Rivers, were radical departures from abstract expressionism especially in the use of banal and literal imagery and the inclusion and the combining of mundane materials into their work. The innovations of Johns' specific use of various images and objects like chairs, numbers, targets, beer cans and the American Flag; Rivers paintings of subjects drawn from popular culture such as George Washington crossing the Delaware, and his inclusions of images from advertisements like the camel from Camel cigarettes, and Rauschenberg's surprising constructions using inclusions of objects and pictures taken from popular culture, hardware stores, junkyards, the city streets, and taxidermy gave rise to a radical new movement in American art. Eventually by 1963 the movement came to be known worldwide as Pop art.\n\nPop art is exemplified by artists: Andy Warhol, Claes Oldenburg, Wayne Thiebaud, James Rosenquist, Jim Dine, Tom Wesselmann and Roy Lichtenstein among others. Lichtenstein used oil and Magna paint in his best known works, such as \"Drowning Girl\" (1963), which was appropriated from the lead story in DC Comics' \"Secret Hearts\" #83. (\"Drowning Girl\" now is in the collection of Museum of Modern Art, New York.) Also featuring thick outlines, bold colors and Ben-Day dots to represent certain colors, as if created by photographic reproduction. Lichtenstein would say of his own work: Abstract Expressionists \"put things down on the canvas and responded to what they had done, to the color positions and sizes. My style looks completely different, but the nature of putting down lines pretty much is the same; mine just don't come out looking calligraphic, like Pollock's or Kline's.\" Pop art merges popular and mass culture with fine art, while injecting humor, irony, and recognizable imagery and content into the mix. In October 1962 the Sidney Janis Gallery mounted \"The New Realists\" the first major Pop art group exhibition in an uptown art gallery in New York City. Sidney Janis mounted the exhibition in a 57th Street storefront near his gallery at 15 E. 57th Street. The show sent shockwaves through the New York School and reverberated worldwide. Earlier in the fall of 1962 a historically important and ground-breaking \"New Painting of Common Objects\" exhibition of Pop art, curated by Walter Hopps at the Pasadena Art Museum sent shock waves across the Western United States. \"Campbell's Soup Cans\" (sometimes referred to as \"32 Campbell's Soup Cans\") is the title of an Andy Warhol work of art that was produced in 1962. It consists of thirty-two canvases, each measuring 20 inches in height x 16 inches in width (50.8 x 40.6 cm) and each consisting of a painting of a Campbell's Soup can—one of each canned soup variety the company offered at the time. The individual paintings were produced with a semi-mechanised silkscreen process, using a non-painterly style. They helped usher in Pop art as a major art movement that relied on themes from popular culture. These works by Andy Warhol are repetitive and they are made in a non-painterly commercial manner.\n\nEarlier in England in 1956 the term \"Pop Art\" was used by Lawrence Alloway for paintings that celebrated consumerism of the post World War II era. This movement rejected Abstract expressionism and its focus on the hermeneutic and psychological interior, in favor of art which depicted, and often celebrated material consumer culture, advertising, and iconography of the mass production age. The early works of David Hockney whose paintings emerged from England during the 1960s like \"A Bigger Splash,\" and the works of Richard Hamilton, Peter Blake, and Eduardo Paolozzi, are considered seminal examples in the movement.\n\nWhile in the downtown scene in New York's East Village 10th Street galleries artists were formulating an American version of Pop art. Claes Oldenburg had his storefront, and the Green Gallery on 57th Street began to show Tom Wesselmann and James Rosenquist. Later Leo Castelli exhibited other American artists including the bulk of the careers of Andy Warhol and Roy Lichtenstein and his use of Benday dots, a technique used in commercial reproduction and seen in ordinary comic books and in paintings like \"Drowning Girl\", 1963, in the gallery above. There is a connection between the radical works of Duchamp, and Man Ray, the rebellious Dadaists – with a sense of humor; and Pop Artists like Alex Katz (who became known for his parodies of portrait photography and suburban life), Claes Oldenburg, Andy Warhol, Roy Lichtenstein and the others.\n\nDuring the 1950s and 1960s as abstract painting in America and Europe evolved into movements such as Color Field painting, Post painterly abstraction, Op art, hard-edge painting, Minimal art, shaped canvas painting, Lyrical Abstraction, and the continuation of Abstract expressionism. Other artists reacted as a response to the tendency toward abstraction with Art brut, as seen in \"Court les rues,\" 1962, by Jean Dubuffet, Fluxus, Neo-Dada, New Realism, allowing imagery to re-emerge through various new contexts like Pop art, the Bay Area Figurative Movement (a prime example is Diebenkorn's \"Cityscape I,(Landscape No. 1),\" 1963, Oil on canvas, 60 1/4 x 50 1/2 inches, collection: San Francisco Museum of Modern Art), and later in the 1970s Neo-expressionism. The Bay Area Figurative Movement of whom David Park, Elmer Bischoff, Nathan Oliveira and Richard Diebenkorn whose painting \"Cityscape 1,\" 1963 is a typical example were influential members flourished during the 1950s and 1960s in California. Although throughout the 20th-century painters continued to practice Realism and use imagery, practicing landscape and figurative painting with contemporary subjects and solid technique, and unique expressivity like Milton Avery, Edward Hopper, Jean Dubuffet, Francis Bacon, Frank Auerbach, Lucian Freud, Philip Pearlstein, and others. Younger painters practiced the use of imagery in new and radical ways. Yves Klein, Martial Raysse, Niki de Saint Phalle, David Hockney, Alex Katz, Antoni Tàpies, Malcolm Morley, Ralph Goings, Audrey Flack, Richard Estes, Chuck Close, Susan Rothenberg, Eric Fischl, John Baeder, and Vija Celmins were a few who became prominent between the 1960s and the 1980s. Fairfield Porter was largely self-taught, and produced representational work in the midst of the Abstract Expressionist movement. His subjects were primarily landscapes, domestic interiors and portraits of family, friends and fellow artists, many of them affiliated with the New York School of writers, including John Ashbery, Frank O'Hara, and James Schuyler. Many of his paintings were set in or around the family summer house on Great Spruce Head Island, Maine.\n\nNeo-Dada is a movement that started 1n the 1950s and 1960s and was related to Abstract expressionism only with imagery. Featuring the emergence of combined manufactured items, with artist materials, moving away from previous conventions of painting. This trend in art is exemplified by the work of Jasper Johns and Robert Rauschenberg, whose \"combines\" in the 1950s were forerunners of Pop Art and Installation art, and made use of the assemblage of large physical objects, including stuffed animals, birds and commercial photography. Robert Rauschenberg, Jasper Johns, Larry Rivers, John Chamberlain, Claes Oldenburg, George Segal, Jim Dine, and Edward Kienholz among others were important pioneers of both abstraction and Pop Art; creating new conventions of art-making; they made acceptable in serious contemporary art circles the radical inclusion of unlikely materials as parts of their works of art.\n\nDuring the 1960s and 1970s abstract painting continued to develop in America through varied styles. Geometric abstraction, Op art, hard-edge painting, Color Field painting and minimal painting, were some interrelated directions for advanced abstract painting as well as some other new movements. Morris Louis was an important pioneer in advanced Colorfield painting, his work can serve as a bridge between Abstract expressionism, Colorfield painting, and Minimal Art. Two influential teachers Josef Albers and Hans Hofmann introduced a new generation of American artists to their advanced theories of color and space. Josef Albers is best remembered for his work as a Geometric abstractionist painter and theorist. Most famous are the hundreds of paintings and prints that make up the series \"Homage to the Square\". In this rigorous series, begun in 1949, Albers explored chromatic interactions with flat colored squares arranged concentrically on the canvas. Albers' theories on art and education were formative for the next generation of artists. His own paintings form the foundation of both hard-edge painting and Op art.\nJosef Albers, Hans Hofmann, Ilya Bolotowsky, Burgoyne Diller, Victor Vasarely, Bridget Riley, Richard Anuszkiewicz, Frank Stella, Morris Louis, Kenneth Noland, Ellsworth Kelly, Barnett Newman, Larry Poons, Ronald Davis, John Hoyland, Larry Zox, and Al Held are artists closely associated with Geometric abstraction, Op art, Color Field painting, and in the case of Hofmann and Newman Abstract expressionism as well. Agnes Martin, Robert Mangold, Brice Marden, Jo Baer, Robert Ryman, Richard Tuttle, Neil Williams, David Novros, Paul Mogenson, are examples of artists associated with Minimalism and (exceptions of Martin, Baer and Marden) the use of the shaped canvas also during the period beginning in the early 1960s. Many Geometric abstract artists, minimalists, and Hard-edge painters elected to use the edges of the image to define the shape of the painting rather than accepting the rectangular format. In fact, the use of the shaped canvas is primarily associated with paintings of the 1960s and 1970s that are coolly abstract, formalistic, geometrical, objective, rationalistic, clean-lined, brashly sharp-edged, or minimalist in character. The Bykert Gallery, and the Park Place Gallery were important showcases for Minimalism and shaped canvas painting in New York City during the 1960s.\n\nIn 1965, an exhibition called \"The Responsive Eye\", curated by William C. Seitz, was held at the Museum of Modern Art, in New York City. The works shown were wide ranging, encompassing the Minimalism of Frank Stella, the Op art of Larry Poons, the work of Alexander Liberman, alongside the masters of the Op Art movement: Victor Vasarely, Richard Anuszkiewicz, Bridget Riley and others. The exhibition focused on the perceptual aspects of art, which result both from the illusion of movement and the interaction of color relationships. Op art, also known as optical art, is a style present in some paintings and other works of art that use optical illusions. Op art is also closely akin to geometric abstraction and hard-edge painting. Although sometimes the term used for it is perceptual abstraction.\nOp art is a method of painting concerning the interaction between illusion and picture plane, between understanding and seeing. Op art works are abstract, with many of the better known pieces made in only black and white. When the viewer looks at them, the impression is given of movement, hidden images, flashing and vibration, patterns, or alternatively, of swelling or warping.\n\nColor Field painting clearly pointed toward a new direction in American painting, away from abstract expressionism. Color Field painting is related to Post-painterly abstraction, Suprematism, Abstract Expressionism, Hard-edge painting and Lyrical Abstraction.\n\nColor Field painting sought to rid art of superfluous rhetoric. Artists like Clyfford Still, Mark Rothko, Hans Hofmann, Morris Louis, Jules Olitski, Kenneth Noland, Helen Frankenthaler, Larry Zox, and others often used greatly reduced references to nature, and they painted with a highly articulated and psychological use of color. In general these artists eliminated recognizable imagery. Certain artists made references to past or present art, but in general color field painting presents abstraction as an end in itself. In pursuing this direction of modern art, artists wanted to present each painting as one unified, cohesive, monolithic image. Gene Davis along with Kenneth Noland, Morris Louis and several others was a member of the Washington Color School painters who began to create Color Field paintings in Washington, D.C. during the 1950s and 1960s, \"Black, Grey, Beat\"is a large vertical stripe painting and typical of Gene Davis's work.\n\nFrank Stella, Kenneth Noland, Ellsworth Kelly, Barnett Newman, Ronald Davis, Neil Williams, Robert Mangold, Charles Hinman, Richard Tuttle, David Novros, and Al Loving are examples of artists associated with the use of the shaped canvas during the period beginning in the early 1960s. Many Geometric abstract artists, minimalists, and Hard-edge painters elected to use the edges of the image to define the shape of the painting rather than accepting the rectangular format. In fact, the use of the shaped canvas is primarily associated with paintings of the 1960s and 1970s that are coolly abstract, formalistic, geometrical, objective, rationalistic, clean-lined, brashly sharp-edged, or minimalist in character. From 1960 Frank Stella produced paintings in aluminum and copper paint and are his first works using shaped canvases (canvases in a shape other than the traditional rectangle or square), often being in L, N, U or T-shapes. These later developed into more elaborate designs, in the \"Irregular Polygon\" series (67), for example. Also in the 1960s, Stella began to use a wider range of colors, typically arranged in straight or curved lines. Later he began his \"Protractor Series\" (71) of paintings, in which arcs, sometimes overlapping, within square borders are arranged side-by-side to produce full and half circles painted in rings of concentric color. \"Harran II,\" 1967, is an example of the \"Protractor Series\". These paintings are named after circular cities he had visited while in the Middle East earlier in the 1960s. The Irregular Polygon canvases and Protractor series further extended the concept of the shaped canvas.\n\nThe Andre Emmerich Gallery, the Leo Castelli Gallery, the Richard Feigen Gallery, and the Park Place Gallery were important showcases for Color Field painting, shaped canvas painting and Lyrical Abstraction in New York City during the 1960s. There is a connection with post-painterly abstraction, which reacted against abstract expressionisms' mysticism, hyper-subjectivity, and emphasis on making the act of painting itself dramatically visible – as well as the solemn acceptance of the flat rectangle as an almost ritual prerequisite for serious painting. During the 1960s Color Field painting and Minimal art were often closely associated with each other. In actuality by the early 1970s both movements became decidedly diverse.\n\nAnother related movement of the late 1960s, Lyrical Abstraction (the term being coined by Larry Aldrich, the founder of the Aldrich Contemporary Art Museum, Ridgefield Connecticut), encompassed what Aldrich said he saw in the studios of many artists at that time. It is also the name of an exhibition that originated in the Aldrich Museum and traveled to the Whitney Museum of American Art and other museums throughout the United States between 1969 and 1971.\n\nLyrical Abstraction in the late 1960s is characterized by the paintings of Dan Christensen,Ronnie Landfield, Peter Young and others, and along with the Fluxus movement and Postminimalism (a term first coined by Robert Pincus-Witten in the pages of Artforum in 1969) sought to expand the boundaries of abstract painting and Minimalism by focusing on process, new materials and new ways of expression. Postminimalism often incorporating industrial materials, raw materials, fabrications, found objects, installation, serial repetition, and often with references to Dada and Surrealism is best exemplified in the sculptures of Eva Hesse. Lyrical Abstraction, Conceptual Art, Postminimalism, Earth Art, Video, Performance art, Installation art, along with the continuation of Fluxus, Abstract Expressionism, Color Field painting, Hard-edge painting, Minimal Art, Op art, Pop art, Photorealism and New Realism extended the boundaries of Contemporary Art in the mid-1960s through the 1970s. Lyrical Abstraction is a type of freewheeling abstract painting that emerged in the mid-1960s when abstract painters returned to various forms of painterly, pictorial, expressionism with a predominate focus on process, gestalt and repetitive compositional strategies in general.\n\nLyrical Abstraction shares similarities with Color Field painting and Abstract Expressionism especially in the freewheeling usage of paint – texture and surface. Direct drawing, calligraphic use of line, the effects of brushed, splattered, stained, squeegeed, poured, and splashed paint superficially resemble the effects seen in Abstract Expressionism and Color Field painting. However the styles are markedly different. Setting it apart from Abstract Expressionism and Action Painting of the 1940s and 1950s is the approach to composition and drama. As seen in Action Painting there is an emphasis on brushstrokes, high compositional drama, dynamic compositional tension. While in Lyrical Abstraction as exemplified by the 1971 Ronnie Landfield painting \"Garden of Delight\" (above), there is a sense of compositional randomness, all over composition, low key and relaxed compositional drama and an emphasis on process, repetition, and an all over sensibility.\nDuring the 1960s and 1970s artists as powerful and influential as Robert Motherwell, Adolph Gottlieb, Philip Guston, Lee Krasner, Cy Twombly, Robert Rauschenberg, Jasper Johns, Richard Diebenkorn, Josef Albers, Elmer Bischoff, Agnes Martin, Al Held, Sam Francis, Ellsworth Kelly, Morris Louis, Helen Frankenthaler, Gene Davis, Frank Stella, Kenneth Noland, Joan Mitchell, Friedel Dzubas, and younger artists like Brice Marden, Robert Mangold, Sam Gilliam, John Hoyland, Sean Scully, Blinky Palermo, Pat Steir, Elizabeth Murray, Larry Poons, Walter Darby Bannard, Larry Zox, Ronnie Landfield, Ronald Davis, Dan Christensen, Joan Snyder, Richard Tuttle, Ross Bleckner, Archie Rand, Susan Crile, Mino Argento and dozens of others produced vital and influential paintings.\n\nArtists such as Larry Poons—whose work related to Op Art with his emphasis on dots, ovals and after-images bouncing across color fields—Ellsworth Kelly, Kenneth Noland, Ralph Humphrey, Robert Motherwell and Robert Ryman had also begun to explore stripes, monochromatic and Hard-edge formats from the late 1950s through the 1960s.\n\nBecause of a tendency in Minimalism to exclude the pictorial, illusionistic and fictive in favor of the literal—as demonstrated by Robert Mangold, who understood the concept of the shape of the canvas and its relationship to \"objecthood\"—there was a movement away from painterly and toward sculptural concerns. Donald Judd had started as a painter, and ended as a creator of objects. His seminal essay, \"Specific Objects\" (published in Arts Yearbook 8, 1965), was a touchstone of theory for the formation of Minimalist aesthetics. In this essay, Judd found a starting point for a new territory for American art, and a simultaneous rejection of residual inherited European artistic values. He pointed to evidence of this development in the works of an array of artists active in New York at the time, including Jasper Johns, Dan Flavin and Lee Bontecou. Of \"preliminary\" importance for Judd was the work of George Earl Ortman, who had concretized and distilled painting's forms into blunt, tough, philosophically charged geometries. These Specific Objects inhabited a space not then comfortably classifiable as either painting or sculpture. That the categorical identity of such objects was itself in question, and that they avoided easy association with well-worn and over-familiar conventions, was a part of their value for Judd.\n\nIn a much more general sense, one might find European roots of Minimalism in the geometric abstractions painters in the Bauhaus, in the works of Piet Mondrian and other artists associated with the movement DeStijl, in Russian Constructivists and in the work of the Romanian sculptor Constantin Brâncuși. American painters such as Brice Marden and Cy Twombly show a clear European influence in their \"pure\" abstraction, minimalist painting of the 1960s. Ronald Davis polyurethane works from the late 1960s pay homage to the \"Broken Glass\" of Marcel Duchamp\".\n\nThis movement was heavily criticised by high modernist formalist art critics and historians. Some anxious critics thought Minimalist art represented a misunderstanding of the modern dialectic of painting and sculpture as defined by critic Clement Greenberg, arguably the dominant American critic of painting in the period leading up to the 1960s. The most notable critique of Minimalism was produced by Michael Fried, a Greenbergian critic, who objected to the work on the basis of its \"theatricality\". In \"Art and Objecthood\" (published in Artforum in June 1967) he declared that the Minimalist work of art, particularly Minimalist sculpture, was based on an engagement with the physicality of the spectator. He argued that work like Robert Morris's transformed the act of viewing into a type of spectacle, in which the artifice of the act observation and the viewer's participation in the work were unveiled. Fried saw this displacement of the viewer's experience from an aesthetic engagement within, to an event outside of the artwork as a failure of Minimal art.\n\nAd Reinhardt, actually an artist of the Abstract Expressionist generation, but one whose reductive all-black paintings seemed to anticipate minimalism, had this to say about the value of a reductive approach to art: \"The more stuff in it, the busier the work of art, the worse it is. More is less. Less is more. The eye is a menace to clear sight. The laying bare of oneself is obscene. Art begins with the getting rid of nature.\"\n\nStill other important innovations in abstract painting took place during the 1960s and the 1970s characterized by monochrome painting and hard-edge painting inspired by Ad Reinhardt, Barnett Newman, Milton Resnick, and Ellsworth Kelly. Artists as diverse as Agnes Martin, Al Held, Larry Zox, Frank Stella, Larry Poons, Brice Marden and others explored the power of simplification. The convergence of Color Field painting, minimal art, hard-edge painting, Lyrical Abstraction, and postminimalism blurred the distinction between movements that became more apparent in the 1980s and 1990s. The neo-expressionism movement is related to earlier developments in abstract expressionism, neo-Dada, Lyrical Abstraction and postminimal painting.\n\nIn the late 1960s the abstract expressionist painter Philip Guston helped to lead a transition from abstract expressionism to Neo-expressionism in painting, abandoning the so-called \"pure abstraction\" of abstract expressionism in favor of more cartoonish renderings of various personal symbols and objects. These works were inspirational to a new generation of painters interested in a revival of expressive imagery. His painting \"Painting, Smoking, Eating\" 1973, seen above in the gallery is an example of Guston's final and conclusive return to representation.\n\nIn the late 1970s and early 1980s, there was also a return to painting that occurred almost simultaneously in Italy, Germany, France and Britain. These movements were called Transavantguardia, Neue Wilde, Figuration Libre, Neo-expressionism, the school of London, and in the late 1990s the Stuckists, a group that emerged late in 1990s respectively. These painting were characterized by large formats, free expressive mark making, figuration, myth and imagination. All work in this genre came to be labeled neo-expressionism. Critical reaction was divided. Some critics regarded it as driven by profit motivations by large commercial galleries. This type of art continues in popularity into the 21st century, even after the art crash of the late 1980s.\n\nDuring the late 1970s in the United States painters who began working with invigorated surfaces and who returned to imagery like Susan Rothenberg gained in popularity, especially as seen above in paintings like \"Horse 2,\" 1979. During the 1980s American artists like Eric Fischl, David Salle, Jean-Michel Basquiat (who began as a graffiti artist), Julian Schnabel, and Keith Haring, and Italian painters like Mimmo Paladino, Sandro Chia, and Enzo Cucchi, among others defined the idea of Neo-expressionism in America.\n\nNeo-expressionism was a style of modern painting that became popular in the late 1970s and dominated the art market until the mid-1980s. It developed in Europe as a reaction against the conceptual and minimalistic art of the 1960s and 1970s. Neo-expressionists returned to portraying recognizable objects, such as the human body (although sometimes in a virtually abstract manner), in a rough and violently emotional way using vivid colours and banal colour harmonies. The veteran painters Philip Guston, Frank Auerbach, Leon Kossoff, Gerhard Richter, A. R. Penck and Georg Baselitz, along with slightly younger artists like Anselm Kiefer, Eric Fischl, Susan Rothenberg, Francesco Clemente, Damien Hirst, Jean-Michel Basquiat, Julian Schnabel, Keith Haring, and many others became known for working in this intense expressionist vein of painting.\n\nAt the beginning of the 21st century Contemporary painting and Contemporary art in general continues in several contiguous modes, characterized by the idea of pluralism. \nMainstream painting has been rejected by artists of the postmodern era in favor of artistic pluralism. According to art critic Arthur Danto there is an \"anything goes\" attitude that prevails; an \"everything going on\", and consequently \"nothing going on\" syndrome; this creates an aesthetic traffic jam with no firm and clear direction and with every lane on the artistic superhighway filled to capacity.\n\n\n\n"}
{"id": "693370", "url": "https://en.wikipedia.org/wiki?curid=693370", "title": "A Cyborg Manifesto", "text": "A Cyborg Manifesto\n\n\"A Cyborg Manifesto\" is an essay written by Donna Haraway and published in 1985. In it, the concept of the cyborg is a rejection of rigid boundaries, notably those separating \"human\" from \"animal\" and \"human\" from \"machine.\" She writes: \"The cyborg does not dream of community on the model of the organic family, this time without the oedipal project. The cyborg would not recognize the Garden of Eden; it is not made of mud and cannot dream of returning to dust.\"\n\nThe \"Manifesto\" criticizes traditional notions of feminism, particularly feminist focuses on identity politics, and encouraging instead coalition through affinity. She uses the metaphor of a cyborg to urge feminists to move beyond the limitations of traditional gender, feminism, and politics; consequently, the \"Manifesto\" is considered one of the milestones in the development of feminist posthumanist theory.\n\nHaraway begins the \"Manifesto\" by explaining three boundary breakdowns since the 20th Century that have allowed for her hybrid, cyborg myth: the breakdown of boundaries between human and animal, animal-human and machine, and physical and non-physical. Evolution has blurred the lines between human and animal; 20th Century machines have made ambiguous the lines between natural and artificial; and microelectronics and the political invisibility of cyborgs have confused the lines of physicality.\n\nHaraway highlights the problematic use and justification of Western traditions like patriarchy, colonialism, essentialism, and naturalism (among others). These traditions in turn allow for the problematic formations of taxonomies (and identifications of the Other) and what Haraway explains as \"antagonistic dualisms\" that order Western discourse. These dualisms, Haraway states, \"have all been systematic to the logics and practices of domination of women, people of color, nature, workers, animals... all [those] constituted as others.\" She highlights specific problematic dualisms of self/other, culture/nature, male/female, civilized/primitive, right/wrong, truth/illusion, total/partial, God/man (among others). She explains that these dualisms are in competition with one another, creating paradoxical relations of domination (especially between the One and the Other). However, high-tech culture provides a challenge to these antagonistic dualisms.\n\nHaraway's cyborg theory rejects the notions of essentialism, proposing instead a chimeric, monstrous world of fusions between animal and machine. Cyborg theory relies on writing as \"the technology of cyborgs,\" and asserts that \"cyborg politics is the struggle for language and the struggle against perfect communication, against the one code that translates all meaning perfectly, the central dogma of phallogocentrism.\" Instead, Haraway’s cyborg calls for a non-essentialized, material-semiotic metaphor capable of uniting diffuse political coalitions along the lines of affinity rather than identity. Following Lacanian feminists such as Luce Irigaray, Haraway’s work addresses the chasm between feminist discourses and the dominant language of Western patriarchy. As Haraway explains, “grammar is politics by other means,” and effective politics require speaking in the language of domination.\n\nAs she details in a chart of the paradigmatic shifts from modern to postmodern epistemology within the Manifesto, the unified human subject of identity has shifted to the hybridized posthuman of technoscience, from “representation” to “simulation,” “bourgeois novel” to “science fiction,” “reproduction” to “replication,” and “white capitalist patriarchy” to “informatics of domination.” While Haraway’s “ironic dream of a common language” is inspired by Irigaray’s argument for a discourse other than patriarchy, she rejects Irigaray’s essentializing construction of woman-as-not-male to argue for a linguistic community of situated, partial knowledges in which no one is innocent.\n\nHaraway takes issue with some traditional feminists, reflected in statements describing how \"women more than men somehow sustain daily life, and so have a privileged epistemological (relating to the theory of knowledge) position potentially.\" The views of traditional feminism operate under the totalizing assumptions that all men are one way, and women another, whereas \"a cyborg theory of wholes and parts,\" does not desire to explain things in total theory. Haraway suggests that feminists should move beyond naturalism and essentialism, criticizing feminist tactics as \"identity politics\" that victimize those excluded, and she proposes that it is better strategically to confuse identities. \nHer criticism mainly focuses on socialist and radical feminism. The former, she writes, achieves \"to expand the category of labour to what (some) women did\" Socialist feminism does not naturalize but rather builds a unity that was non-existent before -namely the woman worker.\nOn the other hand, radical feminism, according to Catherine MacKinnon, describes a world in which the woman only exists in opposition to the man. The concept of woman is socially constructed within the patriarchal structure of society and woman only exist because men have made them exist. The woman as a self does not exist. \nHaraway criticizes both when writing that \"my complaint about socialist/Marxian standpoints is their unintended erasure of polyvocal, unassimilable, radical difference made visible in anti-colonial discourse and practice\" and \"MacKinnon's intentional erasure of all difference through the device of the 'essential' non-existence of women is not reassuring\" (299). H\n\nHaraway also indirectly critiques white feminism by highlighting the struggles of women of color: she suggests that a woman of color “might be understood as a cyborg identity, a potent subjectivity synthesized from fusions of outsider identities and in the complex political-historical layerings of her ‘biomythography.’\"\n\nTo counteract the essentializing and anachronistic rhetoric of spiritual ecofeminists, who were fighting patriarchy with modernist constructions of female-as-nature and earth mothers, Haraway employs the cyborg to refigure feminism into cybernetic code.\n\nHaraway calls for a revision of the concept of gender, moving away from Western patriarchal essentialism and toward \"the utopian dream of the hope for a monstrous world without gender,\" stating that \"Cyborgs might consider more seriously the partial, fluid, sometimes aspect of sex and sexual embodiment. Gender might not be global identity after all, even if it has profound historical breadth and depth.\"\n\nHaraway also calls for a reconstruction of identity, no longer dictated by naturalism and taxonomy but instead by affinity, wherein individuals can construct their own groups by choice. In this way, groups may construct a \"post-modernist identity out of otherness, difference, and specificity\" as a way to counter Western traditions of exclusive identification.\n\nAlthough Haraway's metaphor of the cyborg has been labelled as a post-gender statement, Haraway has clarified her stance on post-genderism in some interviews. She acknowledges that her argument in the \"Manifesto \"seeks to challenge the necessity for categorization of gender, but does not correlate this argument to post-genderism. She clarifies this distinction because post-genderism is often associated with the discourse of the utopian concept of being beyond masculinity and femininity. Haraway notes that gender constructs are still prevalent and meaningful, but are troublesome and should therefore be eliminated as categories for identity.\n\nAlthough Donna Haraway intended her concept of the cyborg to be a feminist critique, she acknowledges that other scholars and popular media have taken her concept and applied it to different contexts. Haraway is aware and receptive of the different uses of her concept of the cyborg, but admits \"very few people are taking what I consider all of its parts\". \"Wired Magazine\" overlooked the feminist theory of the cyborg and instead used it to make a more literal commentary about the enmeshment of humans and technology. Despite this, Haraway also recognizes that new feminist scholars \"embrace and use the cyborg of the manifesto to do what they want for their own purposes\".\n\nPatchwork Girl, a hypertext work, makes use of elements from \"Cyborg Manifesto\". \"Patchwork Girl's\" \"thematic focus on the connections between monstrosity, subjectivity, and new reproductive technologies is apparent from its very first page, when readers, or users, open the hypertext to find a picture of a scarred and naked female body sewn together with a single dotted line...Readers enter the text by clicking on this body and following its 'limbs' or links to different sections of the text.\" In Jackson's narrative, the Patchwork Girl is an aborted female monster created by Victor Frankenstein of Mary Shelley's 1818 novel \"Frankenstein, or The Modern Prometheus, \"is an abhorrent and monstrous creature that is \"part male, part female, part animal, 175 years old, and 'razed' up through hypertext technology.\" The monster, following her destruction by Victor, is sewn back together by Mary Shelley herself, while simultaneously becoming Mary's lover; she is thus, \"a cyborg who is queer, dis-proportioned, and visibly scarred. She both facilitates and undermines preoccupations with the benefits and dangers of reproductive technologies by embracing all of the monstrosities that reproductive/fetal screenings are imagined to 'catch' and one day prevent.\" The Patchwork Girl embraces Haraway's conception of a cybernetic posthuman being in both her physical multiplicity and her challenge towards \"the images and fantasies sustaining reproductive politics.\"\n\nTurkish critical scholar Leman Giresunlu uses Haraway's cyborg as framework to examine current science fiction movies such as \" \"and Resident Evil in her essay \"Cyborg Goddesses: The Mainframe Revisited\". In this essay, she explores how her new concept of the \"cyborg goddess, \"a female figure \"capable of inflicting pain and pleasure simultaneously\",\" \" can be used to make sense of how female representation is shifting towards a more multidimensional stance. Giresunlu builds from Haraway's cyborg because the \"cyborg goddess\" goes beyond \"offering a way out from [the] duality\" and instead provides how spirituality and technology work together to form a complex and more accurate representation of women.\n\nIn her essay \"Mind Over Matter: Mental Evolution and Physical Devolution in The Incredible Shrinking Man\", American critical scholar Ruthellen Cunnally uses Haraway's cyborg to help make sense of how Robert Scott Carey, the protagonist of \"The Incredible Shrinking Man\", transforms into a cyborg in the midst of a metaphor of cold war politics in his home. As Robert continues to shrink, the gendered power dynamic between him and his wife Louise shifts from \"the realm of husband/wife into the mode of mother/son\". When Robert finds himself lost in the feminine space of the basement, an area of the house that was reserved for Louise's domestic duties of sewing and washing, he is forced to fight for his life and reclaim his masculinity. Although he is able to conquer some of his foes and regain his \"manhood\", the gender lines do not become established again because there is no one to share and implement the gendered power structure with. Robert's transformation presents \"an existence in which acceptance and meaning are released from the limitations of patriarchal dualisms\", which aligns with Haraway's cyborg.\n\nTraditional feminists have criticized \"A Cyborg Manifesto\" as anti-feminist because it denies any commonalities of the female experience. In the \"Manifesto,\" Haraway writes \"there is nothing about being 'female' that naturally binds women\", which goes against a defining characteristic of traditional feminism that calls women to join together in order to advocate for members of their gender.\n\nCriticism and controversy were built into the essay's publication history: the East Coast Collective of the \"Socialist Review\" found the piece \"a naive embrace of technology\" and advocated against its publication, while The Berkeley Collective ultimately insisted that it go to print. The essay has been described as \"controversial\" and \"viral\" in its circulation through multiple academic departments and disciplinary boundaries, contributing to the critical discourse on its claims.\nThis controversiality was matched by its omnipresence; Jackie Orr, Associate Professor of Sociology at the Maxwell School of Syracuse University, writes, \"It is hard to be a feminist graduate student in the U.S. humanities or social sciences after 1985 and not be touched in some way by the cyborg manifesto.\" The rapid adoption of the article in academic circles also increased the pace of the critical conversation surrounding the work, and in 1990, Haraway felt that the essay had \"acquired a surprise half life,\" which made it \"impossible to rewrite\" and necessitated revisiting the topic in her subsequent publications.\n\nMany critiques of \"A Cyborg Manifesto\" focus on a basic level of reader comprehension and writing style, such as Orr's observation that \"undergraduate students in a science and technology class find the cyborg manifesto curiously relevant but somewhat impenetrable to read.\" This is corroborated by Helen Merrick and Margret Grebowicz's observation that scientists who reviewed \"Primate Visions\" had similar issues, particularly as related to Haraway's use of irony. Judy Wajcman, Professor of Sociology at the London School of Economics and Political Science, suggests in \"TechnoFeminism\" that \"the openness of her writing to a variety of readings is intentional,\" which \"can sometimes make Haraway difficult to interpret;\" however, it does not seem that Wajcman critiques Haraway's tone for its capability to encompass more possibilities, rather than limit them. Wajcman concludes her chapter \"Send in the Cyborgs\" on a critical note, claiming that \"Certainly, Haraway is much stronger at providing evocative figurations of a new feminist subjectivity than she is at providing guidelines for a practical emancipatory politics.\" \n\nCritiques of Haraway have also centered on the accessibility of the thematic topics she discusses in her writing, and according to third-wave feminist readings, her work \"assumes a reader who is familiar with North American culture,\" and posits that \"readers without the appropriate cultural capital are...likely to find it infuriatingly obscure and impenetrable.\" Therefore, Haraway's symbolism is representative of North American culture symbolizing a \"non-universalizing vision for feminist strategies\" and \"has been taken up within cyberfeminism as the symbol of an essential female being.\" Considering the question of accessibility more broadly, disability studies have focused on Haraway's essay, noting the absence of \"any kind of critical engagement with disability...disabled bodies are simply presented as exemplary...requiring neither analysis nor critique\"—a gap which Alison Kafer, Professor of Feminist Studies at Southwestern University, attempts to address in \"Feminist, Queer, Crip\". Wajcman also argues that Haraway's view of technology in \"A Cyborg Manifesto\" is perhaps too totalizing, and that the binary of \"the cyborg solution and the goddess solution\" ultimately \"caricatures feminism\" by focusing too readily on a dichotomy that may in fact be a false one.\n\nIn \"Unfinished Work-From Cyborg to Cognisphere\", N. Katherine Hayles questions the validity of cyborg as a unit of analysis. She says that because of the complicated situation of technology and media, “cyborg is no longer the individual person – or for that matter, the individual cyborg – is no longer the appropriate unit of analysis, if indeed it ever was.”\n\nAs for the relationships between cyborg and religion, Robert A. Campbell argues that “in spite of Haraway's efforts to move beyond traditional Western dualisms and offer a new hope for women, and by extension a of humanity and the world, what she in fact offers is a further legitimation for buying into the not so new American civil religion of high technology.” He says that “in spite of what some may view as a radical critique of the present and a potentially frightening prescription for the future, the stark reality about Haraway's 'postmodern reality' is that there is no such thing.”\n\nBeyond its presence in academic context, \"A Cyborg Manifesto\" has also had popular traction including Wired Magazine's piece by Hari Kunzru and Mute Magazine, BuzzFeed,\n\nScholar Marilyn Maness Mehaffy writes that the \"sonographic fetus is in many ways the ultimate cyborg in that it is 'created' in a space of virtuality that straddles the conventional boundary between an organic body and a digital text.\" Yet it is this cyborg that presents a limit to Haraway's posthuman theory. The sonographic fetus, as posited by scholar Heather Latimer, \"is publicly envisioned as both independent of [its mother's] body and as independent of the sonographic equipment used to read this body. We know that fetal images are depictions, yet the sonogram invokes a documentary-like access to fetuses that makes it easy to ignore this, which in turn can limit the authority and agency of pregnant women.\" In positioning the fetus as independent, and consequently oppositional, to the pregnant mother, these reproductive technologies \"reinscribe stable meanings to the human/machine dualism they supposedly disrupt.\" Valerie Hartouni argues, \"most reproductive technologies have assimilated into the 'order of nature'\" which would make Haraway's vision of a regenerative species, unrestricted by heteronormative conceptions of reproduction, unattainable in the sonographic fetus.\n\nHaraway began writing the \"Manifesto \"in 1983 to address the \"Socialist Review \"request of American socialist feminists to ponder over the future of socialist feminism in the context of the early Reagan era and the decline of leftist politics. The first versions of the essay had a strong socialist and European connection that the \"Socialist Review \"East Coast Collective found too controversial to publish. The Berkeley \"Socialist Review\" Collective published the essay in 1985 under the editor Jeff Escoffier. The essay was most widely read as part of Haraway's 1991 book \"Simians, Cyborgs and Women\".\n\n\n"}
{"id": "1407656", "url": "https://en.wikipedia.org/wiki?curid=1407656", "title": "Action group (sociology)", "text": "Action group (sociology)\n\nIn sociology and anthropology, an action group or task group is a group of people joined temporarily to accomplish some task or take part in some organized collective action.\n\nAs the members of the action group are brought together on a single occasion and then disband, they cannot be regarded as constituting a full-fledged social group, for which they would need to interact recurrently in accordance with their social identities.\n\nAction Groups are often formed by many shareholders when they disagree with actions by the Board of Directors of a Public Company or the Government like the forced Nationalisation of Northern Rock, Railtrack with 49,000 members. Action groups are co-ordinated by private investors in shareholder associations or their legal representatives in court. Institutional investors often find loose alliances with private investor lead shareholder association actions groups useful in applying mass political pressure or to publicly embarrass Directors at Annual general meetings into making changes.\n\nThe largest established shareholder action group associations are Sveriges Aktiesparares Riksförbund (the Swedish Shareholders' Association) part of the Euroshareholders group in the UK ShareSoc and UKSA (UK Shareholder Association). American Shareholders Association, Australian Shareholders Association, and Japan Sōkaiya also have action groups.\n\nIn Cuba and elsewhere in Latin America, the word \"action group\" () was given to violent activists who gathered together to perform violent guerrilla activities e.g. (see Antonio Guiteras, Fidel Castro, Emilio Tro, Lauro Blanco, and Rolando Masferrer when young university students) .\nCommonly regarded as gang-related killing there were said to have been 200 of these killings in the Grau administration alone.\n\n(Martin, Lionel. \"The Early Fidel: Roots of Castro's Communism\". 1978. Lyle Stuart, Secaucus New Jersey; 1st ed, p. 25). .\n\n"}
{"id": "27055846", "url": "https://en.wikipedia.org/wiki?curid=27055846", "title": "Antidosis", "text": "Antidosis\n\nAntidosis (Ancient Greek ), is the title of a speech treatise by the ancient Greek rhetorician, Isocrates. The \"Antidosis\" can be viewed as a defense, an autobiography, or rhetorical treatise. However, since Isocrates wrote it when he was 82 years old, it is generally seen by some people as an autobiography. The title literally translates as “an exchange,” and was applied in ancient Greek courts as a peculiar law pertaining to an exchange of estates between two parties. If one of the 1200 wealthiest Athenians eligible was tasked with the performance of a public liturgy and financing one of the many public concerns of Athens, then he could avoid the duty by nominating a richer man who was more qualified than himself to perform it. If the supposedly richer man disagreed with the terms, then the entirety of their estates would be exchanged and the now more wealthy man would have to perform the liturgy, as originally planned. The law inspired Isocrates' \"Antidosis\", which was written in the form of a court case where Isocrates had to defend himself from a charge of corrupting the youth by teaching them how to speak well in order for them to gain an unfair advantage over their peers. Although this work is put forward by Isocrates as his imagined defense in a legal case, it is more a treatise on morality and teaching.\n\nAlthough it is assumed that he is defending himself from a charge of corrupting the youth, nowhere in his Antidosis does Isocrates mention what crime he is charged with and from which he must defend, nor does it say what the penalty would be for being found guilty. Isocrates also does not make any attempt to offer evidence in his favor, yet presents the defense that he is a good teacher to his students as the full defense for the crime that he has committed. There is no specific mention of anything that would prove him innocent in this particular case.\n\nAnother idea by scholars is that early on in the \"Antidosis\", Isocrates reads aloud that he is being indicted for corrupting his students. He is accused of educating his students to speak against the courts, question the laws, and think for themselves. Isocrates says, “Here is the indictment my accuser endeavors to vilify me, charging that I corrupt young men by teaching them to speak and gain their won advantage in the courts contrary to justice…” Isocrates is being charged by Lysimachus who has accused Isocrates of receiving money from his students. In return, Isocrates has been teaching them how to become better public speakers and leaders. Isocrates proves his innocence by showing, that through teaching his students rhetoric, he is teaching them how to become better citizens and leaders of Athens. Isocrates proved his rationale by citing the accomplishments and devotion to the state his students displayed after being educated by him. Isocrates says in his defense, “But, I beg of you, consider well whether I appear to you to corrupt the young by my words, or, on the contrary, to inspire them to a life of valor and of dangers endured for their country…” B. Keith Murphy says of Isocrates’s defense, “For Isocrates, it is through rhetoric that we can approximate truth, or at least a consensual truth. A man who is trained in rhetoric is trained in truth, and the creation of that truth through oratory.” \n\nIn the sense that he is defending himself from an onerous charge to perform a liturgy, Isocrates makes a claim that his value to the city was provided through his teaching. He states that as a lifelong teacher, the value of the education that he has provided for students is worth far more than any monetary donation he can give to Athens. His teaching has provided for civic-minded and properly educated citizens that contribute to the orderly running of the city-state. For Isocrates to be sentenced to death would be to ignore the public service that he has performed for the city, by which he has devoted his life to giving its citizens a proper rhetorical education.\n\nIsocrates was 82 years old at the time he wrote \"Antidosis\", the longest work of his life. Antidosis is used by the author to depict himself as a model citizen and a contributor to the city-state of Athens. Since he was nearing the end of his life, he presumably would have wanted to leave behind a legacy of a devoted and moderate citizen to his home town and to justify his life as a service to Athens and the citizens to whom he taught rhetoric. Isocrates presents himself as a quiet citizen who avoids the public light, where oratory is used in a way to bring many different cases of litigation against each other - which the elite believed to negatively affect the way in which the republic was run. He tries to withdraw himself from the public interfering and intrusive efforts made into democratic politics.\n\nThe \"Antidosis\" is not seen by some as an autobiography of Isocrates; it was written later in his life as a fictionalized story similar to his own court trial. While there are similarities between Isocrates’s real court case and the one in the \"Antidosis\", many aspects of the case were fictionalized. Examples are Lysimachus being the person who has brought Isocrates to trial for this crime, and the crime being different from the one with which he was actually accused. According to George Norlin, “At any rate, in the \"Antidosis\"-a title which he borrows from the actual suit to which he had just been subjected-he adopts the fiction of a capital charge brought against him by an informer named Lysimachus, and of a trial before a court with its accessories.” \n\nIt is known that in his own life Isocrates did in fact go to court in his own defense over an amount of money he was made to pay. Isocrates argued this fine was based on the fact people thought he was wealthier than he actually was. He claimed this distorted perception of his wealth was due to his popularity as a public figure. This is similar to Isocrates’s defense in the \"Antidosis\" in which he claims he has been indicted due to the fact he is a very successful person, people are jealous of him, and people want to publicly embarrass him. In the \"Antidosis\", Isocrates is charged with corrupting the youth by teaching them to become good public speakers. With this ability, the students may trick people by convincing them of false truths which could ultimately allow the students to corrupt the government. This is an obvious defense of his life’s work of teaching.\n\nIsocrates states that the teacher should be judged by his students, in that if they do good and moral works then the teacher has done a good job and should be praised. This directly opposes Gorgias’ view that it is not up to him how his students use what he teaches them. \n\nAt the end of his Antidosis, Isocrates explains how he attempts to keep his students in high moral standing by discouraging some of the more debaucherous behaviors available in the city, such as “participating in drinking-bouts and tossing dice in gambling dens.” He posits the idea that as a teacher, he can be judged as successful by the moral ways in which his students live their lives when compared to other young men who are wasting their youth in revelry. Hence he agrees to submit to the punishment if any of his students can be shown to be bad influences on society and challenges his accuser to present even one student who could be said to belong to the group of degenerate youth.\n\nAnother important topic is natural talent. Isocrates argues that native ability is necessary for becoming a good orator, along with a proper education on the subject and practical experience.\n\nD. S. Hutchinson and Monte Johnson (2010) pointed out\n\n\n\n"}
{"id": "9110963", "url": "https://en.wikipedia.org/wiki?curid=9110963", "title": "Articulatory gestures", "text": "Articulatory gestures\n\nArticulatory gestures are the actions necessary to enunciate language. Examples of articulatory gestures are the hand movements necessary to enunciate sign language and the mouth movements of speech. In semiotic terms, they are the physical embodiment (signifiers) of speech signs, which are gestural by nature (see below).\n\nThe definition of \"gesture\" varies greatly, but here it will be taken in its widest sense, namely, any meaningful action. An intentional action is meaningful if it is not strictly utilitarian: for example, sending flowers to a friend is a gesture, because this action is performed not only for the purpose of moving flowers from one place to another, but also to express some sentiment or even a conventional message in the language of flowers. Use of the broadest definition of gesture (not restricted to hand movements) allows Hockett’s “rapid fading” design feature of human language to be accommodated as a type of sign in semiotic theory.\n\nBut if an articulatory gesture is to be considered a true gesture in the above sense, it must be meaningful. Therefore, an articulatory gesture must be at least as large as the smallest meaningful unit of language, the morpheme. A morpheme corresponds roughly to a spoken word or a sign language gesture. \n\nThis definition differs from the practice, common among linguists, of referring to phonemes (meaningless mouth movements) as articulatory gestures (see articulatory phonology). In semiotics, meaningless components of spoken gestures (written as individual letters), or meaningless components of sign language gestures (such as location of hand contact) are known as figurae, the constituents of signs. \n\nIt also differs from the tradition of considering speech sounds to be the signifiers of speech signs. But this practice confuses signals with symbols. Sound and light are analogue signals, whereas mouth and hand gestures are discrete symbolic entities. A sound or light signal is subject to random noise, whereas the image of the gesture is subject to regular distortion, as when a signer’s hand is viewed from different angles. In speech, the sound of the contact of the tongue in the letter T can be distorted by surrounding mouth movements, as in the phrase “perfect memory”. When pronounced at conversational speed, the sound of the tongue contact is completely obscured by surrounding consonants even though this T movement is fully carried out.\n\nArticulatory gestures, when seen as the physical embodiment of speech and sign language symbols, provide a link between these two language types, and show how speech resembles sign language more closely than is generally presumed.\n\n\n"}
{"id": "24077317", "url": "https://en.wikipedia.org/wiki?curid=24077317", "title": "Biological functionalism", "text": "Biological functionalism\n\nBiological functionalism is an anthropological paradigm, asserting that all social institutions, beliefs, values and practices serve to address pragmatic concerns. In many ways, the theorem derives from the longer-established structural functionalism, yet the two theorems diverge from one another significantly. While both maintain the fundamental belief that a social structure is composed of many interdependent frames of reference, biological functionalists criticise the structural view that a social solidarity and collective conscience is required in a functioning system. By that fact, biological functionalism maintains that our individual survival and health is the driving provocation of actions, and that the importance of social rigidity is negligible.\n\nAlthough the actions of humans without doubt do not always engender positive results for the individual, a biological functionalist would argue that the intention was still self-preservation, albeit unsuccessful. An example of this is the belief in luck as an entity; while a disproportionately strong belief in good luck may lead to undesirable results, such as a huge loss in money from gambling, biological functionalism maintains that the newly created ability of the gambler to condemn luck will allow them to be free of individual blame, thus serving a practical and individual purpose. In this sense, biological functionalism maintains that while bad results often occur in life, which do not serve any pragmatic concerns, an entrenched cognitive psychological motivation was attempting to create a positive result, in spite of its eventual failure.\n"}
{"id": "3246877", "url": "https://en.wikipedia.org/wiki?curid=3246877", "title": "Blog fiction", "text": "Blog fiction\n\nBlog fiction is a form of blogging that can be defined as any forms of narrative written and published through online web journals such as LiveJournal. Fictional blogs are a changing phenomenon with great potential to young writers.\n\nDuring the 20th century, there were various authors who were leading the change to bring about the end of the print book. They intend to replace it with a mode of expression that was nonlinear and free to cross as many boundaries as the ever-increasing powers of the computers were making possible. Eventually, blog fiction was developed in 1965; however, it was referred to as a hypertext or web. Well-known philosopher and sociologist Ted Nelson coined the term hypertext. He defined hypertext as \"a body of pictorial material interconnected in such a complex way that is could not conveniently be presented or represented on paper.\" Although Ted Nelson was given credit for combining the term and defining hypertext, many people were confused on what tasks it could complete. Most people assumed that it involved linking, like the World Wide Web. After much confusion and speculation about hypertext, the first Hypertext Conference was held in 1985.\n\nFictional blogging is all about expressing ideas and creativity. Blog fiction or fictional blogging help readers find who they are. \n\nFake blogs, also known as flogs, are misleading web sites that launch scareware and post ads and comments to a real blog site to lure readers to the page. The con artists who run these sites currently make an estimated $750 million a year by selling products of miracle cures to phony anti-virus software and items of questionable value. Most of them have one article posing as impartial new report commenting on a miracle product.\n\nFictional blogging can be a personal journal or diary that is posted online.\n\nThough many critics and literary scholars dismiss blog fiction as an inferior and faddish literary form, there is a trend towards the recognition of blogs as a legitimate arena of fiction production. For instance, self-publishing provider Lulu sponsors the Lulu Blooker Prize, which began in 2006. The Blooker prize is an award given to the best \"blook\" of the year: a work of fiction begun as blog fiction and then transformed into a printed publication. Thus, even despite the radical and democratizing potential of blog fiction, printed works still maintain greater authority and \"official\" status in the world of fiction and academia. \n\nSome people say blog fiction is not true blogging and that blogs should not contain fake content. However, blog fiction and regular blogging are all the same. In fictional blogging, writers talk about fictional subject matter and also write about real life events but with fictional characteristics. Blog fiction is what a blogger makes of it.\n\n\n"}
{"id": "19011098", "url": "https://en.wikipedia.org/wiki?curid=19011098", "title": "CINOA Prize", "text": "CINOA Prize\n\nThe CINOA Prize is a prize awarded yearly to art historians by CINOA, the international confederation of art dealers, \"in recognition of an academic publication or a remarkable contribution to furthering the cultural preservation through art works in a CINOA member country\".\n\n"}
{"id": "6499010", "url": "https://en.wikipedia.org/wiki?curid=6499010", "title": "Classificatory kinship", "text": "Classificatory kinship\n\nClassificatory kinship systems, as defined by Lewis Henry Morgan, put people into society-wide kinship classes on the basis of abstract relationship rules. These may have to do with genealogical relations locally (e.g., son to father, daughter to mother, daughter to father) but the classes bear no overall relation to genetic closeness. If a total stranger marries into the society, for example, they may simply be placed in the appropriate class opposite to their spouse.\nIt uses kinship terms that merge or equate relatives who are genealogically distinct from one another. Here, the same term is used for different kin.\n\nThe Dravidian kinship-term system, discovered in 1964, is an example of a classificatory kin-term logic.\n"}
{"id": "42102863", "url": "https://en.wikipedia.org/wiki?curid=42102863", "title": "Committee to Ratify the Massachusetts State Equal Rights Amendment", "text": "Committee to Ratify the Massachusetts State Equal Rights Amendment\n\nFounded in 1975, the Committee to Ratify the Massachusetts State Equal Rights Amendment sought to replace the Massachusetts State ERA Coalition and carry out the mission of ratifying the Equal Rights Amendment. The Committee to Ratify the Massachusetts State Equal Rights Amendment replaced the Massachusetts State ERA Coalition because, as a coalition, the organization could not legally raise funds for any cause. The campaign created by the Committee ended with success at the ratification of the Equal Rights Amendment.\n\nThe Committee then became the ERA Implementation Project, which lobbied for reconciliation of existing laws with the ERA vis a vis legislation.\n"}
{"id": "25376009", "url": "https://en.wikipedia.org/wiki?curid=25376009", "title": "Copperplate script", "text": "Copperplate script\n\nCopperplate is a style of calligraphic writing most commonly associated with English Roundhand. Although often used as an umbrella term for various forms of pointed pen calligraphy, Copperplate most accurately refers to script styles represented in copybooks created using the Intaglio printmaking method.\n\nThe first use of the term \"Copperplate\" was in Sir Ambrose Heal's 1931 work \"The English Writing-Masters and Their Copy-Books 1570-1800\", although the term may have been used earlier. The term derives from the practice of engraving English roundhand onto a copper plate, also known as intaglio printing.\n\nCopperplate is typically written at a 55-degree angle, and written at a slow speed.\n\n"}
{"id": "161227", "url": "https://en.wikipedia.org/wiki?curid=161227", "title": "Critic", "text": "Critic\n\nA critic is a professional who communicates an assessment and an opinion of various forms of creative works such as art, literature, music, cinema, theater, fashion, architecture, and food. Critics may also take as their subject social or government policy. Critical judgments, whether derived from critical thinking or not, weigh up a range of factors, including an assessment of the extent to which the item under review achieves its purpose and its creator's intention and a knowledge of its context. They may also include a positive or negative personal response.\n\nCharacteristics of a good critic are articulateness, preferably having the ability to use language with a high level of appeal and skill. Sympathy, sensitivity and insight are important too. Form, style and medium are all considered by the critic. In architecture and food criticism, the item's function, value and cost may be added components.\n\nCritics are publicly accepted and, to a significant degree, followed because of the quality of their assessments or their reputation. Influential critics of art, music, theatre and architecture often present their arguments in complete books. One very famous example is John Ruskin's \"Seven Lamps of Architecture\" and \"The Stones of Venice\". Critics may base their assessment on a range of theoretical positions. For instance, they may take a Feminist or Freudian perspective.\n\nUnlike other individuals who may editorialize on subjects via websites or letters written to publications, professional critics are paid to produce their assessment and opinions for print, radio, magazine, television, or Internet companies. When their personal opinion outweighs considered judgment, people who give opinions, whether on current events, public affairs, sports, media or art are often referred to as \"pundits\" instead of critics.\n\nCritics are themselves subject to competing critics, since the final critical judgment always entails some subjectivity. An established critic can play a powerful role as a public arbiter of taste or opinion. Also, critics or a coordinated group of critics, may award symbols of recognition.\n\nThe word \"critic\" comes , which is a Greek derivation of the word \"(krités)\", meaning a person who offers reasoned judgment or analysis, value judgment, interpretation or observation. Early English meaning of criticism was based mainly on the criticism of literature and it was in the 17th century that more general forms of criticism began.\n\nCultural critic Clement Greenberg wrote that a good critic excels through \"insights into the evidence … and by … loyalty to the relevant\"; poet and critic T.S. Eliot wrote \"a critic must have a very highly developed sense of fact\".\n\nIn 1971, Harold C. Schonberg, chief music critic of \"The New York Times\" from 1960 to 1980, said that he wrote for himself, <nowiki>\"</nowiki>not necessarily for readers, not for musicians. ...It's not a critic's job to be right or wrong; it's his job to express an opinion in readable English.<nowiki>\"</nowiki> Schonberg was the first music critic to receive the Pulitzer Prize for criticism.\n\nDaniel Mendelsohn described the equation of criticism for critics as knowledge + taste = meaningful judgement.\n\nRestaurant critic Terry Durack explained that from a critic \"you hope for a thorough, objective and legitimate discussion\" that puts \"opera, art or book into context, so that it adds to your own body of knowledge\"; in the context of a restaurant criticism, this means it is \"not about me liking it or not; it's about me helping you decide whether you are going to like it or not.\"\n\nSocial and political critics have used various forms of art to express their criticism, including literature and music. Pierre Beaumarchais, for example, prior to the French Revolution, used his play \"The Marriage of Figaro\" to denounce aristocratic privilege. A critic's influence is enhanced by subsequent reworkings such as the operatic versions of Beaumarchais's play (\"The Barber of Seville\") by Rossini and (\"The Marriage of Figaro\") by Mozart. Among the most famous social/political criticism in literary form are Jonathan Swift's satire \"Gulliver's Travels\" and George Orwell's satire \"Animal Farm\". Some political critics, such as Ai Weiwei use visual art as their medium. Throughout history, political critics have faced higher risks, including the risk of imprisonment or death.\n\nSeveral websites have developed for the purpose of compiling or publishing original critical reviews. Examples include \"BlogCritics Magazine\", Rotten Tomatoes, and Yelp. According to A. O. Scott, chief film critic for \"The New York Times\", everyone on the Internet is a critic. Some critics like Roger Ebert achieve iconic status in pop culture and become well regarded.\n\nPeople whose work is the subject of criticism have a full range of responses to it. For example, they may be appreciative, offended, distressed, encouraged, amused or nonplussed.\n"}
{"id": "7352745", "url": "https://en.wikipedia.org/wiki?curid=7352745", "title": "Cybertext", "text": "Cybertext\n\nCybertext is the organization of text in order to analyze the influence of the medium as an integral part of the literary dynamic, as defined by Espen Aarseth in 1997. Aarseth defined it as a type of ergodic literature.\n\nThe term cybertext was coined by speculative fiction poetry author Bruce Boston. It is derived from the word cybernetics, which was coined by Norbert Wiener in his book \"Cybernetics, or Control and Communication in the Animal and the Machine\" (1948), which in turn comes from the Greek word \"kybernetes\" – helmsman. Cybertexts are pieces of literature where the medium matters. Each user obtains a different outcome based on the choices they make. Cybertexts may be equated to the transition between a linear piece of literature, such as a novel, and a game. In a novel, the reader has no choice, the plot and the characters are all chosen by the author, there is no 'user', just a 'reader', this is important because it entails that the person working their way through the novel is not an active participant. In a game, the person makes decisions and decides what to do, what punches to punch, or when to jump. The difference between a game and a cybertext is that cybertexts usually have more depth, there is a method to the madness, the piece usually has a point, or message that is translated to the reader as they work their way through the piece.\n\nCybertext is based on the idea that getting to the message is just as important as the message itself. In order to obtain the message work on the part of the user is required. This may also be referred to as nontrivial work on the part of the user.\n\nThe fundamental idea in the development of the theory of cybernetics is the concept of feedback: a portion of information produced by the system that is taken, total or partially, as input. Cybernetics is the science that studies control and regulation in systems in which there exists flow and feedback of information. Though first used by science fiction poet Bruce Boston, the term cybertext was brought to the literary world's attention by Espen Aarseth in 1997.\n\nAarseth's concept of cybertext focuses on the organization of the text in order to analyze the influence of the medium as an integral part of the literary dynamic. According to Aarseth, cybertext is not a genre in itself; in order to classify traditions, literary genres and aesthetic value, we should inspect texts at a much more local level.\n\nThe concept of cybertext offers a way to expand the reach of literary studies to include phenomena that are perceived today as foreign or marginal. In Aarseth's work, cybertext denotes the general set of text machines which, operated by readers, yield different texts for reading.\n\nFor example, with a book like Raymond Queneau's \"Hundred Thousand Billion Poems\", each reader will encounter not just poems arranged in a different order, but \"different poems\" depending on the precise way in which they turn the sections of page.\n\nAn example of a cybertext is \"12 Blue\" by Michael Joyce. Depending on what link you choose or what portion of the diagram on the side you pick you will be transferred to a different portion of the text. So in the end, you do not really finish reading the entire story or 'novel' you go through random pages and try piecing the story together yourself. You may never really 'finish' the story. But, because it is a cybertext the 'finishing' of the story is not as important as its impact on the reader, or on the conveyance. \"Stir Fry Texts\", by Jim Andrews, is a cybertext where there are many layers of text, and as you move your mouse over the words, the layers beneath them are 'dug' through.\n\"The House\" is another example of a cybertext where one might assume a description of the piece as follows:\nIt is an unruly text, the words don't listen, you are not supreme. You are guided through the piece. This is a cybertext with minimal control. You watch as something unfolds before you, \"a crumbling mania\", you must be able to go with the flow, to read texts upside down, to piece together a reflection of words, to be okay with texts half read disappearing or moving so far away so continuously that you can not make out those very important words.\n\n\n"}
{"id": "607519", "url": "https://en.wikipedia.org/wiki?curid=607519", "title": "Evolutionary anthropology", "text": "Evolutionary anthropology\n\nEvolutionary anthropology is the interdisciplinary study of the evolution of human physiology and human behaviour and the relation between hominids and non-hominid primates. Evolutionary anthropology is based in natural science and social science. Various fields and disciplines of evolutionary anthropology are:\n\n\nEvolutionary anthropology is concerned with both biological and cultural evolution of humans, past and present. It is based on a scientific approach, and brings together fields such as archaeology, behavioral ecology, psychology, primatology, and genetics. It is a dynamic and interdisciplinary field, drawing on many lines of evidence to understand the human experience, past and present.\n\nStudies of biological evolution generally concern the evolution of the human form. Cultural evolution involves the study of cultural change over time and space and frequently incorporate cultural transmission models. Note that cultural evolution is not the same as biological evolution, and that human culture involves the transmission of cultural information, which behaves in ways quite distinct from human biology and genetics. The study of cultural change is increasingly performed through cladistics and genetic models.\n\n"}
{"id": "57494486", "url": "https://en.wikipedia.org/wiki?curid=57494486", "title": "Feminist rhetoric", "text": "Feminist rhetoric\n\nFeminist rhetoric is seen as the act of producing or the study of feminist discourses. According to Jacqueline Jones Royster, it is a fairly new field that works to bring the narrative of all demographics of women into the pedagogy of rhetoric. Scholars of feminist rhetoric seek to bring women and their stories into the history of rhetoric; combine issues in feminist and rhetoric theory; and produce rhetorical criticism from feminist perspectives, with the ultimate goal of elevating historically marginalized voices.\n\nIn 1973, feminist scholar Karlyn Kohrs Campbell wrote an influential essay, \"The Rhetoric of Women’s Liberation: An Oxymoron\", that drew feminists' attention to women's role as communicators within rhetorical frameworks, and to the relevance of feminist theory to the study of rhetoric. Feminist scholar Patricia Bizzell noted in 1992 that classical canon of rhetoric consists almost entirely of well-educated male authors. In addition, feminist rhetoric scholars argued that the world of rhetoric was suffused with patriarchal values. To address this perceived problem, they made efforts to include women authors in the history of rhetoric; pointed out connections between feminist issues and theories of rhetoric; and wrote rhetorical criticism from feminist perspectives. These academics were initially inspired by feminist scholarship outside of rhetoric and composition studies, but eventually developed a distinctive school within this tradition.\n\nFollowing the initial feminist rhetoric movement, the Coalition of Women (later Feminist) Scholars in the History of Rhetoric and Composition was formed in 1988. According to its mission statement, this coalition “fosters inquiry in the histories, theories, and pedagogues of rhetoric and composition” for the \"advancement of feminist research\". It is composed of teachers and scholars dedicated to promoting the intersectionality of communication and collaboration within feminist rhetoric and research methods. Founding members included Winifred Horner, Jan Swearingen, Nan Johnson, Marjone Curry Woods, and Kathleen Welch. It continues with Andrea Lunsford, Jackie Royster, Cheryl Glenn, Shirley Logan, and other contemporary feminist writing scholars. 1996 brought the publication of \"Peitho\", the coalition’s newspaper, published by Susan Jarratt. In present day feminist rhetoric, a point of emphasis is changing research methods and methodologies to include the discourse of marginalized groups.\n\nFeminist rhetoric works to expand the rhetorical canon. The rhetorical canon is traditionally composed of five parts: Invention, Arrangement, Style, Memory, and Delivery. They were introduced by the Roman orator Cicero in his treatise \"De Inventione\" around 50 BC. The idea was expanded by the Roman rhetorician Quintilian 150 years later in \"Institutio Oratoria.\" The rhetorical canon has been used in rhetorical education since its creation. Rhetorical treatises have based their purpose off these canons. Rhetoric is integrated in the education system, which means that students are taught the rhetorical canon as well. Feminist rhetoric scholars argue that this canon, and its methods of persuasion or argument, exclude some valuable forms of public discourse and narrative, and seek to expand it accordingly.\n\nScholars, such as Jessica Enoch and Jaqueline Jones Royster, introduced the idea of changing the way research is recognized and constructed. Scholars in rhetoric studies agree there is a plethora of voices and demographics to draw upon for data necessary for research in the field. Researchers suggest this is achieved by asking questions that have never been asked before, recognizing the wealth of materials (or lack thereof) in archives, and expanding the idea of an archive. In recent years, archives have been deconstructed, or critically analyzed, outside of scholarly articles. Feminist rhetorical academics work to develop research methods and methodologies by including new types of archival research such as- yearbooks, small town newspapers, and community contributing archival websites.\n\nAnother main idea in feminist rhetoric is making the point of view, and the rhetorical canon, more expansive than what lies within the United States' domain of discourse. Feminist rhetoric scholars ask the questions of how rhetoric, writing studies, and social change intersect, or may be influenced by politics, the economy, religions, cultures, and education. A key term used in this field is \"transnationality\", defined as the culture of one nation moving through borders to another nation. It is used with the terms cultural hybridity and intertextuality, which continue the theory of cultures, texts, and ideas mixing with one another.\n\nFeminist rhetoric works to represent the voices and discourse of genders that go beyond the binary of male and female. Transgender discourse is another main point of focus in feminist rhetoric, which is recognized by scholars as a lack of privilege some authors have. Royster and others have called for research focused on how gender dynamics affect communication, including rhetoric.\n\nRace and ethnicity is an area of focus for several scholars in feminist rhetoric. They have changed research methods to include international races and ethnicity outside the typical rhetoric canon. Feminist rhetoric focuses on how archiving cultural rhetoric, such as that of Mexican-American women, can create a better understanding of the pedagogy of research methods. An issue that has arisen in feminist rhetoric is the discourse of colored women. Some female scholars of color have written about their perceived need to mask their identity when sharing their voices and opinions. Feminist rhetoric theory works to legitimize the ethnic discourse of women and give it a platform in the academia of rhetoric and writing studies.\n\nFeminist rhetoric scholars have noted the difficulty in including diverse voices in the rhetorical canon. Scholars have argued that changes in research methods may be needed to better include the voices of those who are disabled, trans, and queer in rhetoric studies.\n\nDespite longstanding feminist opposition to processes of canonization as inherently imposing limits and excluding perspectives, feminist rhetoric has begun to develop its own canon of commonly referenced texts. Accordingly, feminist rhetoric scholars try to include the works of unknown feminist rhetors in their theorization processes, and develop a gendered analysis or approach that actively includes rhetors who are traditionally excluded from rhetorical canons, such as women of color or low socioeconomic status.\n\nRhetors, along with expanding the feminist rhetorical canon, work to make feminist rhetoric applicable in pedagogy and education. Scholars discuss the importance of research, whether that be changing research methods or looking further into textual research. Some suggest this can be done by theorizing, others want to employ critical imagination. Theory as a research method approaches discourse from different communities as a generalized idea that allows people to participate in the world through rhetoric. Feminist rhetoric sees theory in this sense as a form for people to speak out and be included in the rhetorical canon. Critical imagination is using the silence, or lack of work from feminist rhetors, to extrapolate. Scholars discuss how this involves understanding that there is more to not only feminist rhetoric, but feminist practice in theory, than what is written down in textbooks or history. Scholars, such as Royster and Kirsch, acknowledge that feminist rhetoric needs to draw from the silence to help set a new precedent for rhetorical practices in the future. A part of critical imagination is knowing that the documentation of rhetoric thus far isn’t the only important rhetoric that should contribute to pedagogy.\n\nA goal of feminist rhetoric is to be viewed as a rhetorical theory of writing as opposed to a social theory. Feminist rhetoric seeks to influence the pedagogy of writing in high school and other levels of academia. Scholars in the field of feminist rhetoric seek to open academic discourse and pedagogy of rhetoric to all types of people.\n\nRoyster & Kirsch (2012) recognize, among others, the following works as of particular interest:\n"}
{"id": "13269942", "url": "https://en.wikipedia.org/wiki?curid=13269942", "title": "First-year composition", "text": "First-year composition\n\nFirst-year composition (sometimes known as freshman composition or freshman writing) is an introductory core curriculum writing course in American colleges. This course focuses on improving students' abilities to write in a university setting and introduces students to writing practices in the disciplines and professions. These courses are traditionally required of incoming students, thus the previous name, \"Freshman Composition\". Scholars working within the field of Composition-Rhetoric often have teaching First-year composition (FYC) courses as the practical focus of their scholarly work. \n\nFYC courses are structured in a variety of ways. Some institutions of higher education require only one term of FYC, while others require two or three courses. There are a number of identifiable pedagogies associated with FYC, including: current-traditional, expressivist, social-epistemic, process, post-process and Writing about Writing (WAW). Each of these pedagogies can generate a multitude of curricula. \n\nComposition professionals, including those with degrees in Writing Studies and Rhetoric and Composition, often focus on a rhetorical approach to help students learn how to apply an understanding of audience, purpose, context, invention, and style to their writing processes. This rhetorical approach has shown that real writing, rather than existing as isolated modes, has more to do with a writer choosing from among many approaches to perform rhetorical tasks. In addition to a focus on rhetoric, many first year composition courses also emphasize writing process, where students are encouraged to interact with classmates and receive feedback to be used for revision. These practices can take the form of essay peer review or workshopping. Portfolios are a common way of assessing revised student work.\n\nSince the late nineteenth century, college courses on composition have become increasingly common in American higher education. Although a longstanding course offering at many colleges, first-year composition remains controversial and marginalized.\n\nThe requirement for a first-year composition course has been debated in composition studies. This debate centers around how effective the first-year composition course is and the changes that need to be made to develop the field of composition. While most schools do require some form of the first-year composition course, there are some schools that have decided to abolish the first-year composition requirement. Some scholars, such as Sharon Crowley in \"Composition in the University: Historical and Polemical Essays,\" argue that this requirement should be abolished. Crowley does not suggest the course itself be removed, only the requirement that all freshmen take the course. She states that students would still be interested in the course if the requirement was abolished and that removing the requirement would strengthen the field of composition. She implies that composition studies is marginalized within the university because of the view of the first-year composition course as a skill course. Removing the requirement, she states, would remove the association of composition studies with introductory courses, giving more acknowledgement to the field. Crowley's opinion initiated a debate in the composition field, but she is not the only critic who advocates for the removal of this requirement. Scholars Douglas Downs and Elizabeth Wardle also dislike the requirement and instead argue for a writing studies curriculum. However, there are scholars who do not agree with this stance, argue that the first-year composition course is needed in the university, and believe that the requirement should remain. Scholar David Smit is one critic who favors keeping the requirement. He suggests that the requirement can be kept and the curriculum and structure of the first-year composition course altered for improvement. Smit explains that many of the developmental goals of those who favor abolishing the requirement can still be achieved by offering more writing experiences. He proposes more genre writing in composition courses with a \"scaffolding\" progression of discipline writing. If this was done, he suggests, the concerns over the status of composition studies in the university would still be solved, as the course would no longer be seen as skills based. There has been no consensus reached in composition studies regarding the status of the first-year composition course requirement. The benefits of the course, as well as the drawbacks, continue to be debated and the scholars noted above are only a few of the voices and perspectives involved in this discussion. Despite the debate about the requirement, it remains in effect at a majority of universities.\n\nFirst-year composition is designed to meet the goals for successful completion set forth by the Council of Writing Program Administrators. To reach these goals, students must learn rhetorical conventions, critical thinking skills, information literacy, and the process of writing an academic paper. While there is no American standard curriculum for first-year composition, curriculum is developed at several levels, including the state, institution, department, and writing program.\n\nWith the publication of James Kinneavy's \"Theory of Discourse\" in 1971, English departments began incorporating rhetoric into their composition classrooms. In doing this, composition instructors have placed more emphasis on teaching audience analysis, Aristotle's three appeals (ethos, pathos, and logos), and teaching Kinneavy's modes of discourse.\nAccording to Brian Sutton in \"Writing in the Disciplines, First-Year Composition, and the Research Paper,\" since 1980, there has been an increasing debate in academic circles as to whether the \"generic\" approach to writing in first year composition is useful for students whose future writing will be discipline specific (46).\n\nFirst described by Mina Shaughnessy in the 1970s, Basic writing is a division of composition studies that strives to bring disadvantaged students entering college to a more complete understanding of the rhetorical aspects of the writing process.\n\n\n"}
{"id": "1041756", "url": "https://en.wikipedia.org/wiki?curid=1041756", "title": "For all practical purposes", "text": "For all practical purposes\n\nFor all practical purposes (FAPP) is a pragmatic approach towards the problem of incompleteness of every scientific theory and the usage of asymptotical approximations.\n\nWhen a physicist makes an approximation - which can not be justified on rigorous grounds - he or she may attempt to justify it by saying the results obtained are good for all practical purposes (FAPP). Meaning they agree with our experience and approximation errors cannot be detected in practical measurements (for instance, if the error is smaller than the measurement resolution).\n\nFAPP theories are incomplete or lackly-based theories that nevertheless have very high agreement with experiments and tend to be very useful for all practical purposes.\n\n"}
{"id": "21078734", "url": "https://en.wikipedia.org/wiki?curid=21078734", "title": "Futures of American Studies", "text": "Futures of American Studies\n\nThe Futures of American Studies is a weeklong academic summer institute dedicated to presenting new work and critiquing the field of American Studies held at Dartmouth College in Hanover, New Hampshire. The first Futures of American Studies Institute was held in the summer of 1997. Donald E. Pease, Professor of English at Dartmouth College, founded, organizes, and directs the annual Institute.\n\nAfter the School of Criticism and Theory left Dartmouth for Cornell University in 1995, Dartmouth faculty member Donald E. Pease started the Futures Institute as an alternative summer program for faculty and graduate students. In 2017 the Futures Institute celebrated the twentieth anniversary of its founding.\n\nThe Institute is divided into two daily plenary sessions, which feature current work from Institute faculty, and multiple three-hour research seminars in which all participants present and discuss their own work-in-progress. Speakers in the plenary sessions typically examine the relation between emergent and residual practices in the field of American Studies from a variety of interdisciplinary perspectives. Participants come from a variety of programs and schools and represent fields as diverse as American history, social geography, American literature, gender studies, and the digital humanities. The Institute welcomes participants who are involved in a range of disciplinary and interdisciplinary fields and who are interested in current critical debates in American Studies.\n\nAlong with Institute Director Donald E. Pease, the institute co-directors were 2016 are:\n\nAlong with Institute Director Donald E. Pease, the institute co-directors were 2014 are:\n\nAlong with Institute Director Donald E. Pease, the institute co-directors were 2012 are:\n\nEach year the Futures Institute brings practicing Americanists and well-known critics and theorists working outside American studies. Notable plenary speakers from the Institute's history include: \n\n\n\n"}
{"id": "402119", "url": "https://en.wikipedia.org/wiki?curid=402119", "title": "Game studies", "text": "Game studies\n\nGame studies, or ludology, is the study of games, the act of playing them, and the players and cultures surrounding them. It is a discipline of cultural studies that deals with all types of games throughout history. This field of research utilizes the tactics of, at least, anthropology, sociology and psychology, while examining aspects of the design of the game, the players in the game, and finally, the role the game plays in its society or culture. Game studies is oftentimes confused with the study of video games, but this is only one area of focus; in reality game studies encompasses all types of gaming, including sports, board games, etc.\n\nBefore video games, game studies often only included anthropological work, studying the games of past societies. However, once video games were introduced and became mainstream, game studies were updated to perform sociological and psychological observations; to observe the effects of gaming on an individual, his or her interactions with society, and the way it could impact the world around us.\n\nThere are three main approaches to game studies: the social science approach asks itself how games affect people and uses tools such as surveys and controlled lab experiments. The humanities approach asks itself what meanings are expressed through games, and uses tools such as ethnography and patient observation. The industrial and engineering approach applies mostly to video games and less to games in general, and examines things such as computer graphics, artificial intelligence, and networking. Like other media disciplines, such as television and film studies, game studies often involves textual analysis and audience theory.\n\nIt wasn’t until Irving Finkel organized a colloquium in 1990 that grew into the International Board Game Studies Association, Gonzalo Frasca popularized the term \"ludology\" (from the Latin word for game, ludus) in 1999, the publication of the first issues of academic journals like \"Board Game Studies\" in 1998 and \"Game Studies\" in 2001, and the creation of the Digital Games Research Association in 2003, that scholars began to get the sense that the study of games could (and should) be considered a field in its own right. As a young field, it gathers scholars from different disciplines that had been broadly studying games, such as psychology, anthropology, economy, education, and sociology. The earliest known use of the term \"ludology\" occurred in 1982, in Mihaly Csikszentmihalyi's “Does Being Human Matter – On Some Interpretive Problems of Comparative Ludology.”\n\nOne of the earliest social science theories (1971) about the role of video games in society involved violence in video games, later becoming known as the catharsis theory. The theory suggests that playing video games in which you perform violent acts might actually channel latent aggression, resulting in less aggression in the players real lives. However, a meta-study performed by Craig A. Anderson and Brad J. Bushman, in 2001, examined data starting from the 1980s up until the article was published. The purpose of this study was to examine whether or not playing violent video games led to an increase in aggressive behaviors. They concluded that exposure to violence in video games did indeed cause an increase in aggression. However, it has been pointed out, and even stressed, by psychologist Jonathan Freedman that this research was very limited and even problematic since overly strong claims were made and the authors themselves seemed extremely biased in their writings. More recent studies, such as the one performed by Christopher J. Ferguson at Texas A&M International University have come to drastically different conclusions. In this study, individuals were either randomly assigned a game, or allowed to choose a game, in both the randomized and the choice conditions exposure to violent video games caused no difference in aggression. A later study (performed by the same people) looked for correlations between trait aggression, violent crimes, and exposure to both real life violence and violence in video games, this study suggests that while family violence and trait aggression are highly correlated with violent crime, exposure to video game violence was not a good predictor of violent crime, having little to no correlation, unless also paired with the above traits that had a much higher correlation. Over the past 15 years, a large number of meta-studies have been applied to this issue, each coming to its own conclusion, resulting in little consensus in the ludology community. It is also thought that even nonviolent video games may lead to aggressive and violent behaviour. Anderson and Dill seem to believe that it may be due to the frustration of playing video games that could in turn result in violent, aggressive behaviour.\n\nGame designers Amy Jo Kim and Jane McGonigal have suggested that platforms which leverage the powerful qualities of video games in non-game contexts can maximize learning. Known as the gamification of learning, using game elements in non-game contexts extracts the properties of games from within the game context, and applies them to a learning context such as the classroom.\n\nAnother positive aspect of video games is its conducive character towards the involvement of a person in other cultural activities. The probability of game playing increases with the consumption of other cultural goods (e.g., listening to music or watching television) or active involvement in artistic activities (e.g., writing or visual arts production). Video games by being complementary towards more traditional forms of cultural consumption, inhibit thus value from a cultural perspective.\n\nMore sociologically-informed research has sought to move away from simplistic ideas of gaming as either 'negative' or 'positive', but rather seeking to understand its role and location in the complexities of everyday life.\n\nFor example, it has been suggested that the very popular MMO \"World of Warcraft\" could be used to study the dissemination of infectious diseases because of the accidental spread of a plague-like disease in the gameworld.\n\nA major focus in game studies is the debate surrounding narratology and ludology. Many ludologists believe that the two are unable to exist together, while others believe that the two fields are similar but should be studied separately. Many narratologists believe that games should be looked at for their stories, like movies or novels. The ludological perspective says that games are not like these other mediums due to the fact that a player is actively taking part in the experience and should therefore be understood on their own terms. The idea that a videogame is \"radically different to narratives as a cognitive and communicative structure\" has led the development of new approaches to criticism that are focused on videogames as well as adapting, repurposing and proposing new ways of studying and theorizing about videogames. A recent approach towards game studies starts with an analysis of interface structures and challenges the keyboard-mouse paradigm with what is called a \"ludic interface\".\n\nAcademics across both fields provide scholarly insight into the different sides of this debate. Gonzalo Frasca, a notable ludologist due to his many publications regarding game studies, argues that while games share many similar elements with narrative stories, that should not prevent games to be studied as games. He seeks not \"to replace the narratologic approach, but to complement it.\"\n\nJesper Juul, another notable ludologist, argues for a stricter separation of ludology and narratology. Juul argues that games \"for all practicality \"can not\" tell stories.\" This argument holds that narratology and ludology cannot exist together because they are inherently different. Juul claims that the most significant difference between the two is that in a narrative, events \"have to\" follow each other, whereas in a game the player has control over what happens.\n\nGarry Crawford and Victoria K. Gosling argue in favor of narratives being an essential part of games as \"it is impossible to isolate play from the social influences of everyday life, and in turn, play will have both intended and unintended consequences for the individual and society.\" \"The Last of Us\" is a video game released in 2013 that has been referred to as a narrative \"masterpiece.\" Proponents of the narratology side of game studies argue that \"The Last of Us\" and similar games that have followed it and preceded it, serve as examples that games can in fact tell stories.\n\nAs is common with most academic disciplines, there are a number of more specialized areas or sub-domains of study.\n\nAn emerging field of study looks at the \"pre-history\" of video games, suggesting that the origins of modern digital games lie in: fairground attractions and sideshows such as shooting games; early \"Coney Island\"-style pleasure parks with elements such as large roller-coasters and \"haunted house\" simulations; nineteenth century landscape simulations such as dioramas, panoramas, planetariums, and stereographs; and amusement arcades that had mechanical game machines and also peep-show film machines.\n\nIn light of population ageing, there has been an interest into the use of games to improve the overall health and social connectedness of ageing players. For example, Adam Gazzaley and his team have designed NeuroRacer (a game that improves cognitive tasks outside of the game among its 60+ year old participants), while the AARP has organized a game jam to improve older people's social connections. Researchers such as Sarah Mosberg Iversen have argued that most of the academic work on games and ageing has been informed by notions of economical productivity, while Bob De Schutter and Vero Vanden Abeele have suggested a game design approach that is not focused on age-related decline but instead is rooted in the positive aspects of older age.\n\nMassive multiplayer online games can give economists clues about the real world. Markets based on digital information can be fully tracked as they are used by players, and thus real problems in the economy, such as inflation, deflation and even recession. The solutions the game designers come up with can therefore be studied with full information, and experiments can be performed where the economy can be studied as a whole. These games allow the economists to be omniscient, they can find every piece of information they need to study the economy, while in the real world they have to work with presumptions.\n\nFormer Finance Minister of Greece and Valve's in-house economist Yanis Varoufakis studied \"EVE Online\" and argued that video game communities give economists a venue for experimenting and simulating the economies of the future. Edward Castronova has studied virtual economies within a variety of games including Everquest and World of Warcraft.\n\nThe psychological research into games has yielded theories on how playing video games may be advantageous for both children and for adults. Some theories claim that video games in fact help improve cognitive abilities rather than impede their development. These improvement theories include the improvement of visual contrast sensitivity. Other developments include the ability to locate something specific among various impediments. This is primarily done in first-person shooter games where the protagonist must look at everything in a first person view while playing. By doing this they increase their spatial attention due to having to locate something among an area of diversions. These games place the player in a high intensity environment where the player must remain observant of their surroundings in order to achieve their goal, e.g., shooting an enemy player, while impediments obstruct their gameplay in the virtual world.\n\nAnother cognitive enhancement provided by playing video games would be the improvement of brain functioning speed. This happens as the player is immersed in an unendingly changing environment where they are required to constantly think and problem solve while playing in order to do well in the game. This constant problem solving forces the brain to constantly run and so the speed of thought is sharpened greatly, because the need to think quickly is required to succeed. The attention span of the player is also benefited. High action video games, such as fighting or racing games, require the user’s constant attention and in the process the skill of concentration is sharpened.\n\nThe overcoming of the condition known as dyslexia is also considered an improvement due to the continuous utilization of controllers for the video games. This continuous process helps to train the users to overcome their condition which impedes in their abilities of interpretation. The ability of hand-eye coordination is also improved thanks in part to video games, due to the need to operate the controller and view the screen displaying the content all at the same time. The coordination of the player is enhanced due the playing and continuous observation of a video game since the game gives high mental stimulation and coordination is important and therefore enhanced due to the constant visual and physical movement that is produced from the playing of the video game.\n\nThe playing of video games can also help increase a player's social skills. This is done by playing online multiplayer games which can require constant communication, this leads to socialization between players in order to achieve the goal within the game they may be playing. In addition it can help the users to meet new friends over their online games and at the same time communicate with friends they have already made in the past; those playing together online would only strengthen their already established bond through constant cooperation. Some video games are specifically designed to aid in learning, because of this another benefit of playing video games could be the educational value provided with the entertainment. Some video games present problem solving questions that the player must think on in order to properly solve, while action orientated video games require strategy in order to successfully complete. This process of being forced to think critically helps to sharpen the mind of the player.\n\nGame Culture\n\nOne aspect of game studies is the study of gaming culture. People who play video games are a subculture of their own. Gamers will often form communities with their own languages, attend conventions where they will dress up as their favorite characters, and have gaming competitions. One of these conventions, Gamescon 2017, had a record attendance with over 350,000 attendees.\n\nDemographics of Gamers \n\nSee also\n\n"}
{"id": "14520306", "url": "https://en.wikipedia.org/wiki?curid=14520306", "title": "Gender inequality", "text": "Gender inequality\n\nGender inequality is the idea and situation that men and women are not equal. Gender inequality refers to unequal treatment or perceptions of individuals wholly or partly due to their gender. It arises from differences in gender roles. Gender systems are often dichotomous and hierarchical. Gender inequality stems from distinctions, whether empirically grounded or socially constructed. Women lag behind men in many domains, including education, labor market opportunities and political representation and in pay.\n\nNatural differences exist between the sexes base on biological and anatomic factors, most notably differing reproductive roles. Biological differences include chromosomes and hormonal differences. There is a natural difference also in the relative physical strengths (on average) of the sexes, both in the lower body and more pronouncedly in the upper-body, though this does not mean that any given man is stronger than any given woman. Men, on average, are taller, which provides both advantages and disadvantages. Women live significantly longer than men, though it is not clear to what extent this is a biological difference - see Life expectancy. Men have larger lung volumes and more circulating blood cells and clotting factors, while women have more circulating white blood cells and produce antibodies faster. Differences such as these are hypothesized to be an adaption allowing for sexual specialization.\n\nPrenatal hormone exposure influences to what extent one exhibits traditional masculine or feminine behavior. No differences between males and females exist in general intelligence. Men are significantly more likely to take risks than women. Men are also more likely than women to be aggressive, a trait influenced by prenatal and possibly current androgen exposure. It has been theorized that these differences combined with physical differences are an adaption representing sexual division of labor. A second theory proposes sex differences in intergroup aggression represent adaptions in male aggression to allow for territory, resource and mate acquisition. Females are (on average) more empathetic than males, though this does not mean that any given woman is more empathetic than any given man. Men and women have better visuospatial and verbal memory, respectively. These changes are influenced by the male sex hormone testosterone, which increases visuospatial memory in both genders when administered.\n\nFrom birth males and females are raised differently and experience different environments throughout their lives. In the eyes of society, gender has a huge role to play in many major milestones or characteristics in life; like personality. Males and females are lead on different paths before they are able to choose their own. The colour blue is most commonly associated with boys and they get toys like monster trucks or more sport related things to play with from the time that they are babies. Girls are more commonly introduced to the colour pink, dolls, dresses, and playing house where they are taking care of the dolls as if they were children. The norm of blue is for boys and pink is for girls is cultural and has not always historically been around. These paths set by parents or other adult figures in the child's life set them on certain paths. This leads to a difference in personality, career paths, or relationships. Throughout life males and females are seen as two very different species who have very different personalities and should stay on separate paths.\n\nThe gender pay gap is the average difference between men's and women's aggregate wages or salaries. The gap is due to a variety of factors, including differences in education choices, differences in preferred job and industry, differences in the types of positions held by men and women, differences in the type of jobs men typically go into as opposed to women (especially highly paid high risk jobs), differences in amount of work experiences, difference in length of the work week, and breaks in employment. These factors resolve 60% to 75% of the pay gap, depending on the source. Various explanations for the remaining 25% to 40% have been suggested, including women's lower willingness and ability to negotiate salary and sexual discrimination. According to the European Commission direct discrimination only explains a small part of gender wage differences.\n\nIn the United States, the \"average\" female's \"unadjusted\" annual salary has been cited as 78% of that of the \"average\" male. However, multiple studies from OECD, AAUW, and the US Department of Labor have found that pay rates between males and females varied by 5–6.6% or, females earning 94 cents to every dollar earned by their male counterparts, when wages were adjusted to different individual choices made by male and female workers in college major, occupation, working hours, and maternal/parental leave. The remaining 6% of the gap has been speculated to originate from deficiency in salary negotiating skills and sexual discrimination.\n\nHuman capital theories refer to the education, knowledge, training, experience, or skill of a person which makes them potentially valuable to an employer. This has historically been understood as a cause of the gendered wage gap but is no longer a predominant cause as women and men in certain occupations tend to have similar education levels or other credentials. Even when such characteristics of jobs and workers are controlled for, the presence of women within a certain occupation leads to lower wages. This earnings discrimination is considered to be a part of pollution theory. This theory suggests that jobs which are predominated by women offer lower wages than do jobs simply because of the presence of women within the occupation. As women enter an occupation, this reduces the amount of prestige associated with the job and men subsequently leave these occupations. The entering of women into specific occupations suggests that less competent workers have begun to be hired or that the occupation is becoming deskilled. Men are reluctant to enter female-dominated occupations because of this and similarly resist the entrance of women into male-dominated occupations. \n\nThe gendered income disparity can also be attributed in part to occupational segregation, where groups of people are distributed across occupations according to ascribed characteristics; in this case, gender. Occupational gender segregation can be understood to contain two components or dimensions; horizontal segregation and vertical segregation. With horizontal segregation, occupational sex segregation occurs as men and women are thought to possess different physical, emotional, and mental capabilities. These different capabilities make the genders vary in the types of jobs they are suited for. This can be specifically viewed with the gendered division between manual and non-manual labor. With vertical segregation, occupational sex segregation occurs as occupations are stratified according to the power, authority, income, and prestige associated with the occupation and women are excluded from holding such jobs.\n\nAs women entered the workforce in larger numbers since the 1960s, occupations have become segregated based on the amount femininity or masculinity presupposed to be associated with each occupation. Census data suggests that while some occupations have become more gender integrated (mail carriers, bartenders, bus drivers, and real estate agents), occupations including teachers, nurses, secretaries, and librarians have become female-dominated while occupations including architects, electrical engineers, and airplane pilots remain predominately male in composition. Based on the census data, women occupy the service sector jobs at higher rates than men. Women’s overrepresentation in service sector jobs, as opposed to jobs that require managerial work acts as a reinforcement of women and men into traditional gender roles that causes gender inequality.\n“The gender wage gap is an indicator of women’s earnings compared with men’s. It is figured by dividing the average annual earnings for women by the average annual earnings for men.” (Higgins et al., 2014) \nScholars disagree about how much of the male-female wage gap depends on factors such as experience, education, occupation, and other job-relevant characteristics. Sociologist Douglas Massey found that 41% remains unexplained, while CONSAD analysts found that these factors explain between 65.1 and 76.4 percent of the raw wage gap. CONSAD also noted that other factors such as benefits and overtime explain \"additional portions of the raw gender wage gap\".\n\nThe glass ceiling effect is also considered a possible contributor to the gender wage gap or income disparity. This effect suggests that gender provides significant disadvantages towards the top of job hierarchies which become worse as a person’s career goes on. The term glass ceiling implies that invisible or artificial barriers exist which prevent women from advancing within their jobs or receiving promotions. These barriers exist in spite of the achievements or qualifications of the women and still exist when other characteristics that are job-relevant such as experience, education, and abilities are controlled for. The inequality effects of the glass ceiling are more prevalent within higher-powered or higher income occupations, with fewer women holding these types of occupations. The glass ceiling effect also indicates the limited chances of women for income raises and promotion or advancement to more prestigious positions or jobs. As women are prevented by these artificial barriers, from either receiving job promotions or income raises, the effects of the inequality of the glass ceiling increase over the course of a woman’s career.\n\nStatistical discrimination is also cited as a cause for income disparities and gendered inequality in the workplace. Statistical discrimination indicates the likelihood of employers to deny women access to certain occupational tracks because women are more likely than men to leave their job or the labor force when they become married or pregnant. Women are instead given positions that dead-end or jobs that have very little mobility.\n\nIn Third World countries such as the Dominican Republic, female entrepreneurs are statistically more prone to failure in business. In the event of a business failure women often return to their domestic lifestyle despite the absence of income. On the other hand, men tend to search for other employment as the household is not a priority.\n\nThe gender earnings ratio suggests that there has been an increase in women’s earnings comparative to men. Men’s plateau in earnings began after the 1970s, allowing for the increase in women’s wages to close the ratio between incomes. Despite the smaller ratio between men and women’s wages, disparity still exists. Census data suggests that women’s earnings are 71 percent of men's earnings in 1999.\n\nThe gendered wage gap varies in its width among different races. Whites comparatively have the greatest wage gap between the genders. With whites, women earn 78% of the wages that white men do. With African Americans, women earn 90% of the wages that African American men do.\n\nThere are some exceptions where women earn more than men: According to a survey on gender pay inequality by the International Trade Union Confederation, female workers in the Gulf state of Bahrain earn 40 percent more than male workers.\n\nIn 2014, a report by the International Labor Organization (ILO) reveals the wage gap between Cambodian women factory workers and other male counterparts. There was a $25 USD monthly pay difference conveying that women have a much lower power and being devalued not only at home but also in the workplace.\n\nThe gender gap also appeared to narrow considerably beginning in the mid-1960s. Where some 5% of first-year students in professional programs were female in 1965, by 1985 this number had jumped to 40% in law and medicine, and over 30% in dentistry and business school. Before the highly effective birth control pill was available, women planning professional careers, which required a long-term, expensive commitment, had to \"pay the penalty of abstinence or cope with considerable uncertainty regarding pregnancy.\" This control over their reproductive decisions allowed women to more easily make long-term decisions about their education and professional opportunities. Women are highly underrepresented on boards of directors and in senior positions in the private sector.\n\nAdditionally, with reliable birth control, young men and women had more reason to delay marriage. This meant that the marriage market available to any women who \"delay[ed] marriage to pursue a career... would not be as depleted. Thus the Pill \"could\" have influenced women's careers, college majors, professional degrees, and the age at marriage.\"\n\nStudies on sexism in science and technology fields have produced conflicting results. Corinne et al. found that science faculty of both sexes rated a male applicant as significantly more competent and hireable than an identical female applicant. These participants also selected a higher starting salary and offered more career mentoring to the male applicant. Williams and Ceci, however, found that science and technology faculty of both sexes \"preferred female applicants 2:1 over identically qualified males with matching lifestyles\" for tenure-track positions. Studies show parents are more likely to expect their sons, rather than their daughters, to work in a science, technology, engineering or mathematics field – even when their 15-year-old boys and girls perform at the same level in mathematics.\n\nA survey by the U.K. Office for National Statistics in 2016 showed that in the health sector 56% of roles are held by women, while in teaching it is 68%. However equality is less evident in other area; only 30% of M.P.'s are women and only 32% of finance and investment analysts. In the natural and social sciences 43% of employees are women, and in the environmental sector 42%.\n\nA 2010 study conducted by David R. Hekman and colleagues found that customers who viewed videos featuring a black male, a white female, or a white male actor playing the role of an employee helping a customer were 19 percent more satisfied with the white male employee's performance.\n\nThis discrepancy with race can be found as early as 1947, when Kenneth Clark conducted a study in which black children were asked to choose between white and black dolls. White male dolls were the ones children preferred to play with.\n\nAlthough the disparities between men and women are decreasing in the medical field, gender inequalities still exist as social problems. From 1999 to 2008, recently qualified female doctors in the US made almost $170,000,000 less than their male counterparts. The pay discrepancy could not be explained by specialty choice, practice setting, work hours, or other characteristics. A case study carried out on Swedish medical doctors showed that the gender wage gap among physicians was greater in 2007 than in 1975.\n\nGender roles are heavily influenced by biology, with male-female play styles correlating with sex hormones, sexual orientation, aggressive traits, and pain. Furthermore, females with congenital adrenal hyperplasia demonstrate increased masculinity and it has been shown that rhesus macaque children exhibit preferences for stereotypically male and female toys.\n\nGender equality in relationships has been growing over the years but for the majority of relationships, the power lies with the male. Even now men and women present themselves as divided along gender lines. A study done by Szymanowicz and Furnham, looked at the cultural stereotypes of intelligence in men and women, showing the gender inequality in self-presentation. This study showed that females thought if they revealed their intelligence to a potential partner, then it would diminish their chance with him. Men however would much more readily discuss their own intelligence with a potential partner. Also, women are aware of people’s negative reactions to IQ, so they limit its disclosure to only trusted friends. Females would disclose IQ more often than men with the expectation that a real true friend would respond in a positive way. Intelligence continues to be viewed as a more masculine trait, than feminine trait. The article suggested that men might think women with a high IQ would lack traits that were desirable in a mate such as warmth, nurturance, sensitivity, or kindness. Another discovery was that females thought that friends should be told about one’s IQ more so than males. However, males expressed doubts about the test’s reliability and the importance of IQ in real life more so than women. The inequality is highlighted when a couple starts to decide who is in charge of family issues and who is primarily responsible for earning income. For example, in Londa Schiebinger’s book, \"Has Feminism Changed Science?\", she claims that \"Married men with families on average earn more money, live longer and happier, and progress faster in their careers,\" while \"for a working woman, a family is a liability, extra baggage threatening to drag down her career.\" Furthermore, statistics had shown that \"only 17 percent of the women who are full professors of engineering have children, while 82 percent of the men do.\"\n\nDespite the increase in women in the labour force since the mid-1900s, traditional gender roles are still prevalent in American society. Women may be expected to put their educational and career goals on hold in order to raise children, while their husbands work. However, women who choose to work as well as fulfill a perceived gender role of cleaning the house and taking care of the children. Despite the fact that different households may divide chores more evenly, there is evidence that supports that women have retained the primary caregiver role within familial life despite contributions economically. This evidence suggest that women who work outside the home often put an extra 18 hours a week doing household or childcare related chores as opposed to men who average 12 minutes a day in childcare activities. One study by van Hooff showed that modern couples, do not necessarily purposefully divide things like household chores along gender lines, but instead may rationalize it and make excuses. One excuse used is that women are more competent at household chores and have more motivation to do them. Another is that some say the demands of the males’ jobs is higher.\n\nThere was a study conducted at an \"urban comprehensive school\". They were asked questions regarding their views in sexual inequality. Many parents were for the equal pay for men and women. They also were in favor for men to help with the housework. In this study, the majority of the people who were interviewed wanted gender equality and more people wants a change in gender roles. Where men stay home, cleans, and cooks while the women can work and help support the family.\n\nGender roles have changed drastically over the past few decades. In the article, it says that in 1920-1966, there was data recorded that women spent the most time care-tending with the home and family. There was a study made with the gender roles with the males and females, The results showed that as women spend less time in the house, men have taken over the role as the mother. The article also said that women who work spend less time within the house and with their children if they have any. Furthermore, men are taking the roles of women in the homes and its changing as time goes on. Robin A. Douthitt, the author of the article, \"The Division of Labor Within the Home: Have Gender Roles Changed?\" concluded by saying, \"(1) men do not spend significnatly more time with chil- dren when their wives are employed and (2) employed women spend signifi- cantly less time in child care than their full-time homemaker counterparts, over a 10-year period both mothers and fathers are spending more total time with children.\" (703).\n\nOne survey showed that men rate their technological skills in activities such as basic computer functions and online participatory communication higher than women. However, it should be noted that this study was a self-reporting study, where men evaluate themselves on their own perceived capabilities. It thus is not data based on actual ability, but merely perceived ability, as participants' ability was not assessed. Additionally, this study is inevitably subject to the significant bias associated with self-reported data.\n\nIn contrary to such findings, a carefully controlled study that analyzed data sets from 25 developing countries led to the consistent finding that the reason why fewer women access and use digital technology is a direct result of their unfavorable conditions and ongoing discrimination with respect to employment, education and income. When controlling for these variables, women turn out to be more active users of digital tools than men. This turns the alleged digital gender divide into an opportunity: given women's affinity for ICT, and given that digital technologies are tools that can improve living conditions, ICT represents a concrete and tangible opportunity to tackle longstanding challenges of gender inequalities in developing countries, including access to employment, income, education and health services.\n\nMany countries have laws that give less inheritance of ancestral property for women compared to men.\n\nGender inequalities often stem from social structures that have institutionalized conceptions of gender differences.\n\nMarginalization occurs on an individual level when someone feels as if they are on the fringes or margins of their respective society. This is a social process and displays how current policies in place can affect people. For example, media advertisements display young girls with easy bake ovens (promoting being a housewife) as well as with dolls that they can feed and change the diaper of (promoting being a mother).\n\nCultural stereotypes, which can dictate specific roles, are engrained in both men and women and these stereotypes are a possible explanation for gender inequality and the resulting gendered wage disparity. Women have traditionally been viewed as being caring and nurturing and are designated to occupations which require such skills. While these skills are culturally valued, they were typically associated with domesticity, so occupations requiring these same skills are not economically valued. Men have traditionally been viewed as the main worker in the home, so jobs held by men have been historically economically valued and occupations predominated by men continue to be economically valued and earn higher wages.\n\nGender Stereotypes influenced greatly by gender expectations, different expectations on gender influence how people determine their roles, appearance, behaviors, etc. When expectations of gender roles deeply rooted in people's mind, people' values and ideas started to be influenced and leading to situation of stereotypes, which actualize their ideas into actions and perform different standards labelling the behaviors of people. Gender stereotypes limit opportunities of different gender when their performance or abilities were standardizing according to their gender-at-birth, that women and men may encounter limitations and difficulties when challenging the society through performing behaviors that their gender \"not supposed\" to perform. For example, men may receive judgments when they trying to stay at home and finish housework and allow their wives to go out and work instead, as men are expected to be work outside for earning money for the family. The traditional concepts of gender stereotypes are being challenged nowadays in different societies and improvement could be observed that men could also be responsible for housework, women could also be construction worker in some societies. It is still a long process when traditional concepts and values have deep-rooted in people's mind, that higher acceptance towards gender roles and characteristics is homely to be gradually developed.\n\nBonnie Spanier coined the term hereditary inequality. Her opinion is that some scientific publications depict human fertilization such that sperms seem to actively compete for the \"passive\" egg, even though in reality it is complicated (e.g. the egg has specific active membrane proteins that select sperm etc.)\n\nGender inequality can further be understood through the mechanisms of sexism. Discrimination takes place in this manner as men and women are subject to prejudicial treatment on the basis of gender alone. Sexism occurs when men and women are framed within two dimensions of social cognition.\n\nDiscrimination also plays out with networking and in preferential treatment within the economic market. Men typically occupy positions of power within the job economy. Due to taste or preference for other men because they share similar characteristics, men in these positions of power are more likely to hire or promote other men, thus discriminating against women.\n\nSonja B. Starr conducted a study in the US that found that the prison sentences that men serve are on average 63% longer than those that women serve when controlling for arrest offense and criminal history. However, the study does not purport to explain why this is the case. Starr does not believe that men are disadvantaged generally. Men's rights advocates have argued that men being over-represented in both those who commit murder and the victims of murder is evidence that men are being harmed by outmoded cultural attitudes.\n\nThe New York Film Academy took a closer look at the women in Hollywood and gathered statistics from the top 500 films from 2007 to 2012, for their history and achievements, or lack of.\n\nThere was a 5:1 ratio of men to women working in films. 30.8% of women having speaking characters, who may or may not have been a part of the 28.8% of women who were written to wear revealing clothing compared to the 7% of men who did, or the 26.2% of women who wore little to no clothing opposed to the 9.4% of men who did the same. A study analyzing five years of text from over 2,000 news sources found a similar 5:1 ratio of male to female names overall, and 3:1 for names in entertainment.\n\nHollywood actresses are paid less than actors. Topping \"Forbes\" highest paid actors list of 2013 was Robert Downey Jr. with $75 million, yet Angelina Jolie topped the highest paid actresses list with only $33 million, which tied with Denzel Washington ($33 million) and Liam Neeson ($32 million), who were the last two on the top ten highest paid actors list.\n\nIn the 2013 Academy Awards, 140 men were nominated for an award, but only 35 women were nominated. No woman was nominated for directing, cinematography, film editing, writing (original screenplay), or original score that year. Since the Academy Awards began in 1929, only seven women producers have won the Best Picture category (all of whom were co-producers with men), and only eight women have been nominated for Best Original Screenplay. Lina Wertmuller (1976), Jane Campion (1994), Sofia Coppola (2004), and Kathryn Bigelow (2012) were the only four women to be nominated for Best Director, with Bigelow being the first woman to win for her film \"The Hurt Locker\". 77% of the Academy Awards' voters are male.\n\nA group of Hollywood actors have launched their own social movement titled #AskMoreOfHim. This movement is built on the basis of men speaking out against sexual misconduct against females.   A number of male activists, specifically in the film industry, have signed an open letter explaining their responsibility in the ownership of their actions, as well as calling out the actions of others. The letter has been signed and supported by Friends actor David Schwimmer, shown above, among many others. The Hollywood Reporter published their support saying, “We applaud the courage and pledge our support to the courageous women — and men, and gender non-conforming individuals — who have come forward to recount their experiences of harassment, abuse and violence at the hands of men in our country. As men, we have a special responsibility to prevent abuse from happening in the first place... After all, the vast majority of sexual harassment, abuse and violence is perpetrated by men, whether in Hollywood or not.” This accountability is set to change the way women are seen and treated in the film and television industry, hopefully ending in the closing of the gap women are experiencing in pay, promotion, and overall respect. This initiative was created in response to the #MeToo movement. The #MeToo movement, started by a single tweet, asked women to share their stories of sexual assault against men in a professional setting. Within one day, 30,000 women had used the hashtag sharing their stories. Many women feel as if they have more power in their voices than they ever had and are choosing to make personal claims that may have been brushed under the rug prior to the internet culture we’re now living in. According to Time Magazine, 95% of women in the film and entertainment industry admit to being sexually harassed by men in their industry. In addition to the #MeToo movement, women in industry are using #TimesUp, with the goal of aiming to help prevent sexual harassment in the workplace for victims who cannot afford their own resources. \n\nGender inequality and discrimination are argued to cause and perpetuate poverty and vulnerability in society as a whole. Household and intra-household knowledge and resources are key influences in individuals' abilities to take advantage of external livelihood opportunities or respond appropriately to threats. High education levels and social integration significantly improve the productivity of all members of the household and improve equity throughout society. Gender Equity Indices seek to provide the tools to demonstrate this feature of poverty.\n\nPoverty has many different factors, one of which is the gender wage gap. Women are more likely to be living in poverty and the wage gap is one of the causes.\n\nThere are many difficulties in creating a comprehensive response. It is argued that the Millennium Development Goals (MDGs) fail to acknowledge gender inequality as a cross-cutting issue. Gender is mentioned in MDG3 and MDG5: MDG3 measures gender parity in education, the share of women in wage employment and the proportion women in national legislatures. MDG5 focuses on maternal mortality and on universal access to reproductive health. These targets are significantly off-track.\n\nAddressing gender inequality through social protection programmes designed to increase equity would be an effective way of reducing gender inequality, according to the Overseas Development Institute (ODI). Researchers at the ODI argue for the need to develop the following in social protection in order to reduce gender inequality and increase growth:\nThe ODI maintains that society limits governments' ability to act on economic incentives.\n\nNGOs tend to protect women against gender inequality and structural violence.\n\nDuring war, combatants primarily target men. Both sexes die however, due to disease, malnutrition and incidental crime and violence, as well as the battlefield injuries which predominately affect men. A 2009 review of papers and data covering war related deaths disaggregated by gender concluded \"It appears to be difficult to say whether more men or women die from conflict conditions overall.\" The ratio also depends on the type of war, for example in the Falklands War 904 of the 907 dead were men. Conversely figures for war deaths in 1990, almost all relating to civil war, gave ratios in the order of 1.3 males per female.\n\nAnother opportunity to tackle gender inequality is presented by modern information and communication technologies. In a carefully controlled study, it has been shown that women embrace digital technology more than men. Given that digital information and communication technologies have the potential to provide access to employment, education, income, health services, participation, protection, and safety, among others (ICT4D), the natural affinity of women with these new communication tools provide women with a tangible bootstrapping opportunity to tackle social discrimination.\n\nGender inequality is a result of the persistent discrimination of one group of people based upon gender and it manifests itself differently according to race, culture, politics, country, and economic situation. It is furthermore considered a causal factor of violence against women. While gender discrimination happens to both men and women in individual situations, discrimination against women is an entrenched, global pandemic. In the Democratic Republic of the Congo, rape and violence against women and girls is used as a tool of war. In Afghanistan, girls have had acid thrown in their faces for attending school. Considerable focus has been given to the issue of gender inequality at the international level by organizations such as the United Nations (UN), the Organisation for Economic Co-operation and Development (OECD), and the World Bank, particularly in developing countries. The causes and effects of gender inequality vary geographically, as do methods for combating it.\n\nOne example of the continued existence of gender inequality in Asia is the \"missing girls\" phenomenon. \"Many families desire male children in order to ensure an extra source of income. In China, females are perceived as less valuable for labor and unable to provide sustenance.\" Moreover, gender inequality also reflected in educational aspect in rural China . Gender inequality existed because of gender stereotypes in rural China, families may consider that is useless for girls to acquire knowledge at school because they will marry someone one day and their major responsibility is to take care of housework. When people have expectations on the gender roles, that considering marriage is the major goals of a girl's life in rural China, gender inequality easily existed to limit the rights and opportunities of women.\n\nA Cambodian said, \"Men are gold, women are white cloth\", emphasizing that women had a lower value and importance compared to men. In Cambodia, approximately 15% (485,000 hectares) of land was owned by women. In Asian culture, there is a stereotype that women usually have lower status than men because males carry on the family name and hold the responsibilities to take care of the family. Females have a less important role, mainly to carry out domestic chores, and taking care of husbands and children. Women are also the main victims of poverty as they have little or no access to education, low pay and low chances owning assets such as lands, homes or even basic items.\n\nIn Cambodia, the Ministry of Women's Affairs (MoWA) was formed in 1998 with the role of improving women's overall power and status in the country.\n\nThe Global Gender Gap Report put out by the World Economic Forum (WEF) in 2013 ranks nations on a scale of 0 to 1, with a score of 1.0 indicating full gender equality. A nation with 35 women and 65 men in political office would get a score of 0.538 as the WEF is measuring the gap between the two figures and not the actual percentage of women in a given category. While Europe holds the top four spots for gender equality, with Iceland, Finland, Norway and Sweden ranking 1st through 4th respectively, it also contains two nations ranked in the bottom 30 countries, Albania at 108 and Turkey at 120. The Nordic Countries, for several years, have been at the forefront of bridging the gap in gender inequality. Every Nordic country, aside from Denmark who is at 0.778, has reached above a 0.800 score. In contrast to the Nordic nations, the countries of Albania and Turkey continue to struggle with gender inequality. Albania and Turkey failed to break the top 100 nations in 2 of 4 and 3 of 4 factors, respectively. However, despite the disparity, European nations continue to make advances in the many factors that are used to determine a nation's gender gap score.\n\nWestern Europe, a region most often described as comprising the non-communist members of post-WWII Europe, has, for the most part been doing well in eliminating the gender gap. Western Europe holds 12 of the top 20 spots on the Global Gender Gap Report for overall score. While remaining mostly in the top 50 nations, four Western European nations fall below that benchmark. Portugal is just outside of the top 50 at number 51 with score of 0.706 while Italy (71), Greece (81) and Malta (84) received scores of 0.689, 0.678 and 0.676, respectively.\n\nA large portion of Eastern Europe, a region most often described as the former communist members of post-WWII Europe, resides between 40th and 100th place in the Global Gender Gap Report. A few outlier countries include Lithuania, which jumped nine places (37th to 28th) from 2011 to 2013, Latvia, which has held the 12th spot for two consecutive years, Albania and Turkey.\n\nIndia ranking remains low in gender equality measures by the World Economic Forum, although the rank has been improving in recent years. When broken down into components that contribute the rank, India performs well on political empowerment, but is scored near the bottom with China on sex selective abortion. India also scores poorly on overall female to male literacy and health rankings. India with a 2013 ranking of 101 out of 136 countries had an overall score of 0.6551, while Iceland, the nation that topped the list, had an overall score of 0.8731 (no gender gap would yield a score of 1.0). Gender inequalities impact India's sex ratio, women's health over their lifetimes, their educational attainment, and economic conditions. It is a multifaceted issue that concerns men and women alike.\n\nThe labor force participation rate of women was 80.7% in 2013. Nancy Lockwood of the Society for Human Resource Management, the world's largest human resources association with members in 140 countries, in a 2009 report wrote that female labor participation is lower than men, but has been rapidly increasing since the 1990s. Out of India's 397 million workers in 2001, 124 million were women, states Lockwood.\n\nIndia is on target to meet its Millennium Development Goal of gender parity in education before 2016. UNICEF's measures of attendance rate and Gender Equality in Education Index (GEEI) attempt to capture the quality of education. Despite some gains, India needs to triple its rate of improvement to reach GEEI score of 95% by 2015 under the Millennium Development Goals. A 1998 report stated that rural India girls continue to be less educated than the boys.\n\nThe World Economic Forum measures gender equity through a series of economic, educational, and political benchmarks. It has ranked the United States as 19th (up from 31st in 2009) in terms of achieving gender equity. The US Department of Labor has indicated that in 2009, \"the median weekly earnings of women who were full-time wage and salary workers was... 80 percent of men's\". The Department of Justice found that in 2009, \"the percentage of female victims (26%) of intimate partner violence was about 5 times that of male victims (5%)\". \"The United States ranks 41st in a ranking of 184 countries on maternal deaths during pregnancy and childbirth, below all other industrialized nations and a number of developing countries\" and women only represent 20% of members of Congress.\n\nExisting research on the topic of gender/sex and politics has found differences in political affiliation, beliefs, and voting behavior between men and women, although these differences vary across cultures. Gender is omnipresent in every culture, and while there are many factors to consider when labeling people \"Democrat\" or \"Republican\"—such as race and religion—gender is especially prominent in politics. Studying gender and political behavior poses challenges, as it can be difficult to determine if men and women actually differ in substantial ways in their political views and voting behavior, or if biases and stereotypes about gender cause people to make assumptions. However, trends in voting behavior among men and women have been proven through research.\n\nResearch shows that women in postindustrial countries like the United States, Canada, and Germany primarily identified as conservative before the 1960s; however, as time has progressed and new waves of feminism have occurred, women have become more left-wing due to shared beliefs and values between women and parties more on the left. Women in these countries typically oppose war and the death penalty, favor gun control, support environment protection, and are more supportive of programs that help people of lower socioeconomic statuses. Voting behaviors of men have not experienced as drastic of a shift over the last fifty years as women in their voting behavior and political affiliations. These behaviors tend to consistently be more conservative than women overall. These trends change with every generation, and factors such as culture, race, and religion also must be considered when discussing political affiliation. These factors make the connection between gender and political affiliation complex due to intersectionality.\n\nCandidate gender also plays a role in voting behavior. Women candidates are far more likely than male candidates to be scrutinized and have their competence questioned by both men and women when they are seeking information on candidates in the beginning stages of election campaigns. Democrat male voters tend to seek more information about female Democrat candidates over male Democrat candidates. Female Republican voters tend to seek more information about female Republican candidates. For this reason, female candidates in either party typically need to work harder to prove themselves competent more than their male counterparts.\n\nOverall, politics in the United States are dominated by men, which can pose many challenges to women who decide to enter the political sphere. As the number of women participants in politics continue to increase around the world, the gender of female candidates serves as both a benefit and a hindrance within their campaign themes and advertising practices. The overarching challenge seems to be that—no matter their actions—women are unable to win in the political sphere as different standards are used to judge them when compared to their male counterparts.\n\nOne area in particular that exemplifies varying perceptions between male and female candidates is the way female candidates decide to dress and how their choice is evaluated. When women decide to dress more masculine, they are perceived as being \"conspicuous.\" When they decide to dress more feminine, they are perceived as \"deficient.\" At the same time, however, women in politics are generally expected to adhere to the masculine standard, thereby validating the idea that gender is binary and that power is associated with masculinity. As illustrated by the points above, these simultaneous, mixed messages create a \"double-bind\" for women. Some scholars go on to claim that this masculine standard represents symbolic violence against women in politics.\n\nPolitical knowledge is a second area where male and female candidates are evaluated differently and where political science research has consistently shown women with a lower level of knowledge than their male counterparts. One reason for this finding is the argument that there are different areas of political knowledge that different groups consider. Due to this line of thought, scholars are advocating the replacement of traditional political knowledge with gender-relevant political knowledge because women are not as politically disadvantaged as it may appear.\n\nA third area that affects women's engagement in politics is their low level of political interest and perception of politics as a \"men's game.\" Despite female candidates' political contributions being equal to that of male candidates, research has shown that women perceive more barriers to office in the form of rigorous campaigns, less overall recruitment, inability to balance office and family commitments, hesitancy to enter competitive environments, and a general lack of belief in their own merit and competence. Male candidates are evaluated most heavily on their achievements, while female candidates are evaluated on their appearance, voice, verbal dexterity, and facial features in addition to their achievements.\n\nSeveral forms of action have been taken to combat institutionalized sexism. People are beginning to speak up or \"talk back\" in a constructive way to expose gender inequality in politics, as well as gender inequality and under-representation in other institutions. Researchers who have delved into the topic of institutionalized sexism in politics have introduced the term \"undoing gender.\" This term focuses on education and an overarching understanding of gender by encouraging \"social interactions that reduce gender difference.\" Some feminists argue that \"undoing gender\" is problematic because it is context-dependent and may actually reinforce gender. For this reason, researchers suggest \"doing gender differently\" by dismantling gender norms and expectations in politics, but this can also depend on culture and level of government (e.g. local versus federal).\n\nAnother key to combating institutionalized sexism in politics is to diffuse gender norms through \"gender-balanced decision-making,\" particularly at the international level, which \"establishes expectations about appropriate levels of women in decision-making positions.\" In conjunction with this solution, scholars have started placing emphasis on \"the value of the individual and the importance of capturing individual experience.\" This is done throughout a candidate's political career—whether that candidate is male or female—instead of the collective male or female candidate experience. Five recommended areas of further study for examining the role of gender in U.S. political participation are (1) realizing the \"intersection between gender and perceptions\"; (2) investigating the influence of \"local electoral politics\"; (3) examining \"gender socialization\"; (4) discerning the connection \"between gender and political conservatism\"; and (5) recognizing the influence of female political role models in recent years. Due to the fact that gender is intricately entwined in every societal institution, gender in politics can only change once gender norms in other institutions change, as well.\n\n\nHiggins, M. and Reagan, M. (n.d). The gender wage gap, 9th ed. North Mankato: Abdo Publishing, pp. 9–11\n"}
{"id": "23668992", "url": "https://en.wikipedia.org/wiki?curid=23668992", "title": "Geocriticism", "text": "Geocriticism\n\nGeocriticism is a method of literary analysis and literary theory that incorporates the study of geographic space. The term designates a number of different critical practices. In France, Bertrand Westphal has elaborated the concept of \"géocritique\" in several works. In the United States, Robert Tally has argued for a geocriticism as a critical practice suited to the analysis of what he has termed \"literary cartography\".\n\nSome of the first expressly \"geocritical\" writings emerged from symposia organized by Westphal at the University of Limoges. Westphal's foundational essay, \"Pour une approche géocritique des textes\" constitutes a manifesto for geocriticism. Westphal's theory is elaborated in greater detail in his \"Geocriticism: Real and Fictional Spaces\", translated by Tally, who also provides a brief introduction. But there are also many works addressing similar themes and using similar methods that might be considered geocritical, even if the term \"geocriticism\" is not used.\n\nIn Westphal's theory, geocriticism is based on three theoretical concepts: spatio-temporality, transgressivity, and referentiality. \n\nThe idea that space and time form a continuum (space-time) is a tenet of modern physics. In the field of literary theory, geocriticism is an interdisciplinary method of literary analysis that focuses not only on such temporal data as relations between the life and times of the author (as in biographical criticism), the history of the text (as in textual criticism), or the story (as studied by narratology), but also on spatial data. Geocriticism therefore has affinities with geography, architecture, urban studies, and so on; it also correlates to philosophical concepts such as deterritorialization.\nFollowing the work of Michel Foucault, Gilles Deleuze, Henri Lefebvre and Mikhail Bakhtin, among others, a geocritical approach to literature recognizes that representations of space are often transgressive, crossing the boundaries of established norms while also reestablishing new relations among people, places, and things. Cartography is no longer seen as the exclusive province of the state or the government; rather, various agents or groups may be responsible for representing the geographic spaces at the same time and with different effects. In practice, therefore, geocriticism is multifocal, examining a variety of topics at once, thus differentiating itself from practices that focus on the singular point of view of the traveler or protagonist. \n\nGeocriticism also assumes a literary referentiality between world and text, or, in other words, between the referent and its representation. By questioning the relations between a given space's nature and its actually existing condition, the geocritical approach allows for a study of fiction that points also to the theory of possible worlds, such as may be seen in the work on third space by the American geographer Edward Soja (\"Thirdspace\"). Tally's book \"Spatiality\", an introduction to spatiality studies in literature and critical theory, includes a chapter on geocriticism.\n\nGeocriticism frequently involves the study of places described in the literature by various authors, but it can also study the effects of literary representations of a given space. An example of the range of geocritical practices can be found in Tally's collection \"Geocritical Explorations: Space, Place, and Mapping in Literary and Cultural Studies\".\n\nGeocriticism derives some of its practices from precursors whose theoretical work helped establish space as a valid topic for literary analysis. For example, in \"The Poetics of Space\" and elsewhere, Gaston Bachelard studied literary works to develop a typology of places according to their connotations. Maurice Blanchot's writings have legitimized the idea of literary space, an imaginary place for the creation of the work of literature. One might also look at the developments of cultural studies and especially postcolonial studies, such as Raymond Williams's \"The Country and the City\" or Edward Said's \"Culture and Imperialism\", which employ what Said has called a \"geographical inquiry into historical experience.\" Fredric Jameson's concept of cognitive mapping and his theoretical engagement with the postmodern condition also highlight the importance of spatial representation and aesthetic productions, including literature, film, architecture, and design. In \"The Atlas of European Novel, 1800-1900\", Franco Moretti has examined the diffusion of literary spaces in Europe, focusing on the complex relationship between the text and space. Moretti has also promulgated a theory of literary history, or literary geography, that would use maps to bring to light new connections between the texts studied and their social spaces. And, in his study of Herman Melville's literary cartography, Robert Tally has offered a geocritical approach to certain texts. \n\nGeocriticism has intellectual and methodological affiliations with such fields as Literature and the Environment or ecocriticism, regional literature, urban studies, sociological and philosophical approaches to literature, and utopian studies.\n\nNotes\nFurther reading\n"}
{"id": "21021107", "url": "https://en.wikipedia.org/wiki?curid=21021107", "title": "Health communication", "text": "Health communication\n\nHealth communication is the study and practice of communicating promotional health information, such as in public health campaigns, health education, and between doctor and patient. The purpose of disseminating health information is to influence personal health choices by improving health literacy.\n\nBecause effective health communication must be tailored for the audience and the situation, research into health communication seeks to refine communication strategies to inform people about ways to enhance health or to avoid specific health risks. Academically, health communication is a discipline within communication studies.\n\nHealth communication may variously seek to:\n\nThe term was generated when members of an ICA, International Communication Association, interest group adopted the term. Interdisciplinary marriage between health and communication was certainly a common-law relationship long before the term \"health communication\" was introduced. (Stacks & Salween, page 489)\nThe research of health communication surrounds the development of effective messages about health, the dissemination of health-related information through broadcast, print, and electronic media, and the role of inter personal relationships in health communities. At the core of all of the communication is the idea of health and the emphasis of health. The goal of health communication research is to identify and provide better and more effective communication strategies that will improve the overall health of society. (Stacks & Salween, page 489)\n\nThere are many purposes and reasons why health communication research is important and how it betters the health care field. The training programs of Health Care Professionals, or HCP, can be adapted and developed based on health communication research. (Stacks & Salween, 495) Due to there being a diverse culture that makes up the group of patients within the health care field, communication to other cultures has been taught and has been made a focus in health care training classes. Research suggests that nonverbal and verbal communication between health care professionals and patient can lead to improved patient outcomes. According to Stacks and Salween on page 496 some health care facilities, like hospitals are providing training and education materials to patients. The goal of hospitals doing this is to allow for patients to have a better outcome due to better communication skills.\nOver the years, there has been much research done on health communication. For example, researchers want to know if people are more effectively motivated by a positive message versus a negative message. Researchers examine ideas like, are people better motivated by ideas of wealth and safety or an idea of illness and death. Researchers are examining which dimensions of persuasive incentives are most influential: physical health versus economic, versus psychological, versus moral, versus social. (Stacks & Salween, 497)\nImpact of the Health Campaign-After research has been conducted and analyzed on the effects of health communication, it can be concluded that a health communication campaign that is requiring a behavior change causes the desired behavior change in about 7%-10% or more in the people who are in the campaign site than those who are in the control group. Also, the effects are stronger for adoption of a new behavior than cessation of a current behavior, about 12% higher.\n\nWhen assessing how affective a health campaign is, the key determinant is the degree of audience reception, the quality and quantity of the message, the dissemination channels, and the larger communication environment. It is possible that an audience can be more receptive to some messages than others. The media channel and how the message is reached by the audience can affect the effectiveness of the health campaign. (Stacks & Salween page 498)\n\nThe efforts and effects of health messages and communication are often counter affected by alcohol and tobacco commercials. The advertisement for these items is often made to be glamorous and will contradict what was said in the health campaign. This can lead to the efforts of the health communication seem to be pointless.\n\nHealth communication professionals are specifically trained in methods and strategies for effective communication of public health messages, with qualifications in research, strategic development, and evaluating effectiveness. Health communication is taught in master's and doctoral programs. The Coalition for Health Communication maintains a list of such programs.\n\nScholars and practitioners in health communication are often trained in disciplines such as communication studies, sociology, psychology, public health, or medicine and then focus within their field on either health or communication. Practitioners are pragmatic and draw from social-scientific scholarship, theories from the humanities, and professional fields such as education, management, law and marketing (Maibach 2008). Professionals trained in health communication encounter a wide range of employment opportunities spanning between the public, private, and volunteer sectors and have the opportunity for a large amount of career mobility. Examples of jobs in each of these categories include federal, state, and local health departments in the public sector, Pharmaceutical companies and large corporations in the private sector, and various non-profit organizations such as the American Cancer Society and the American Heart Association in the volunteer sector.\n\nInternational Communication Association officially recognized health communication in 1975; in 1997, the American Public Health Association categorised health communication as a discipline of Public Health Education and Health Promotion.\n\nCareers in the field of health communication range widely between the public, private, and volunteer sectors and professionals of health communication are distinctively trained to conduct communication research, develop successful and repeatable campaigns for health promotion and advocacy, and to evaluate how effective these strategies have been for future campaigns.\n\nClear communication is essential to successful public health practice at every level of the ecological model: intrapersonal, interpersonal, group, organizational, and societal. In each instance of health communication, there must be careful deliberation concerning the appropriate channel for messages to best reach the target audience, ranging from face-to-face interactions to television, Internet, and other forms of mass media. The recent explosion of new Internet communication technologies, particularly through the development of health websites (such as MedlinePlus, Healthfinder, and WebMD), online support groups (such as the Association for Cancer Online Resources), web portals, tailored information systems, telehealth programs, electronic health records, social networking, and mobile devices (cell phones, PDAs, etc.) means that the potential media are ever changing.\n\nThe social and cultural contexts in which health communication occurs are also widely diverse and can include (but are not limited to) homes, schools, doctor’s offices, and workplaces, and messages must consider the variant levels of health literacy and education of their audience, as well as demographics, values, socioeconomic issues, and many other factors that may influence effective communication.\n\nCritical Health Communication refers to scholarship that interrogates \"how meanings and enactments of health are tied to issues of power through the systematic construction and maintenance of inequalities.\" It examines links with culture, resources, and other social structures. It is distinct from mainstream Health Communication in its emphasis on qualitative and interpretive methods, and its attention to the ideological processes that underpin shared understandings of health. Unlike much mainstream Health Communication, most Critical Health Communication holds that simply circulating better quality, or more visible message about health is not enough to meaningfully influence health outcomes or correct health care disparities. The first comprehensive review of Critical Health Communication was published in 2008, and since then the volume of Health Communication research taking a critical approach has steadily increased.\n\nTailoring a health message is one strategy for persuasive health communication. For messages of health communication to reach selected audiences accurately and quickly, health communication professionals must assemble a collection of superior and audience appropriate information that target population segments. Understanding the audience for the information is critical to effective delivery.\n\nCommunication is an enigma that is detrimental to the healthcare world and to the resulting health of a patient. Communication is an activity that involves oral speech, voice, tone, nonverbal body language, listening and more. It is a process for a mutual understanding to come at hand during interpersonal connections. A patient’s communication with their healthcare team and vice versa, affects the outcome of their health. Strong, clear, and positive relationships with physicians can chronically improve and increase the condition of a certain patient. Through two approaches, the biomedical model and the biopsychosocial model; this can be successfully achieved. Evidence has shown that communication and its traditions have altered throughout the years. With the use of many new discoveries and the changes within our technology market, communication has severely improved and become instantaneous.\n\nCommunicators need to continually synthesize knowledge from a range of other scholarly disciplines including marketing, psychology, and behavioural sciences. Once this information has been collected, professionals can choose from a variety of methods and strategies of communication that they believe would best convey their message. These methods include campaigns, entertainment advocacy, media advocacy, new technologies, and interpersonal communication.\n\nHealth Communication campaigns are arguably the most utilized and effective method for spreading public health messages, especially in endorsing disease prevention (e.g. cancer, HIV/AIDS) and in general health promotion and wellness (e.g. family planning, reproductive health). The Institute of Medicine argues that health communication campaigns tend to organize their message for a diverse audience in one of three ways:\nBoth the Centers for Disease Control and Prevention and scholars of health communication emphasize the importance of strategic planning throughout a campaign. This includes a variety of steps to ensure a well-developed message is being communicated:\n\nIn 1721, health communication was used to mitigate the smallpox epidemic in Boston. Cotton Mather, a political leader, used pamphlets and speeches to promote inoculation of smallpox.Alcohol abuse has been a problem within society for about as long as alcohol has been around. In the 19th century, the Women’s Christian Temperance Union led a movement against alcohol abuse. They utilized mass communication to communicate the desired message. News papers and magazines allowed for the promotion of the anti-alcohol movement. \n\nThree-community study and the five-city project were experimental campaigns to inform middle-aged men about the causes of cardiovascular disease. Health messages were communicated via television, radio, newspaper, cookbooks, booklets, and bus cards. The three \"communities\" comprised three experimental communication strategies: a media-only campaign, a media campaign supplemented with face-to-face communication, and a no-intervention control group. The experimented revealed that after one year, the most informed at-risk men were those in the second experimental group: they men consumed the media campaign and were attended by a health care provider.\n\nUsing the entertainment industry as a platform for advocating health information and education is a communication strategy that has become increasingly popular. The most utilized strategy is for health communication professionals to create partnerships with storyline creators so that public health information can be incorporated into within the plot of a television show. The Centers for Disease Control and Prevention has formed a strong partnership with Hollywood, Health, and Society, at the University of Southern California Norman Lear Center to continue to produce new storylines on television and in film studios that will help to promote public health information. Some of the resources provided with this partnership include comprehensive \"tip sheets\" to provide writers with easy to access and trustworthy information on health issues, and meetings and panels to discuss new information and resources. Some of the most notable examples of this method of communication in recent years have been with the films \"Contagion\" and \"I Am Legend\" in understanding the spread of disease, \"NBC's\" series \"Parenthood\" in Asperger's Syndrome, and with the \"CW's\" series \"90210\" and spreading cancer awareness. More recently, film festivals and competitions focused specifically on health films have been organised by the American Public Health Association, the International Health Film Festival, the Global Health Film Initiative of the Royal Society of Medicine and the Public Health Film Society.\n\nWriters and storyline developers have an increased motivation to continue to incorporate public health information into their scripts with the creation of the Sentinel for Health Awards in 2000, which honors storylines that effectively promote health topics and audience awareness of public health issues. Surveys conducted by Porter Novelli in 2001 reported many interesting statistics on the effectiveness of this strategy, such as that over half of regular prime time and daytime drama viewers have reported that they have learned something about health promotion or disease prevention from a TV show. Amongst this data, minority groups are significantly represented with well over half of African American and Hispanic viewers stating that they had either taken some form of preventative action after hearing about a health issue on TV, or that a TV storyline helped them to provide vital health information to a friend or family member.\n\nMedia advocacy use strategic mass media tools combined with widespread organization in order to advocate for healthy public policies or lifestyles. This can include the use of text messaging and email to spread messages from person to person, and using social networking venues to promote health information to a wide-ranging audience. As technologies expand, the platforms for health communication through media advocacy will undoubtedly expand as well.\n\nHealth communication relies on strong interpersonal communications in order to influence health decisions and behaviours. The most important of these relationships are the connection and interaction between an individual and their health care provider (e.g., physician, therapist, pharmacist) and an individual's social support system (family, friends, community). These connections can positively influence the individual's decision to make healthy choices. Patients are more prone to listen when they feel invested emotionally into the situation. If they feel as if they understand what is being said, they are more prone to make objective decisions based on the information heard.\n\nHealth communication has become essential in promoting the general public health in myriad situations. One of health communication's most important applications has been throughout major Environmental events (e.g. hurricanes, flooding, tornados) and addressing the affected audience's questions and needs quickly and efficiently, keeping the protection of public health and the forefront of their message. Health communication professionals are constantly working to improve this type of risk communication in order to be prepared in the case of an emergency.\n\nAnother increasingly important application of health communication has been in reaching students in the college community. The National College Health Assessment has measured that 92.5% of college students reported being in \"good, very good, or excellent health\", however college students seem to battle serious problems with stress, depression, substance abuse, and a general lack of nutrition in comparison to other age groups and audiences. Professionals in health communication are actively striving for new ways to reach this at-risk audience in order to raise standards of public health in the college setting and to promote a healthier life style amongst students.\n\nThere are many challenges in communicating information about health to individuals. Some of the most essential issues have to do with the gap between individual health literacy and healthcare workers and institutions, as well as flaws in communicating health information through mass media.\n\nOne problem that health communication seeks to address is the gap that has formed between health literacy and the use of health communication. While the goal is that health communication will effectively \"lead\" to health literacy, issues such as the use of unexplained medical jargon, ill-formed messages, and often a general educational gap have created a disparity. Specifically, studies have been done amongst elderly populations in America to illustrate a common audience who is left at a disadvantage due to this issue. The older adults comprise an age group that generally suffers from the most chronic health conditions in comparison to other age groups, however studies have shown that even this group have difficulty understanding written health materials, understanding health care and policies, and generally do not comprehend medical jargon. Such shortcomings of health communication may lead to increased hospitalizations, the inability to respond to and manage a disease or medical condition, and a generally declining health status.\n\nIn some populations, health-related websites (e.g., WebMD) and online support groups (e.g., Association for Cancer Online Resources) have increased access to health information.\n\nMass communication is used to promote beneficial changes in behavior among members of populations. A major criticism of the use of mass media as a method of health communication is the unfortunate ability for false and misinformed messages to spread quickly through the mass media, before they have the chance to be disputed by professionals. This issue may generate unwarranted panic amongst those who receive the messages and be an issue as technology continues to advance. An example of this may be observed in the ongoing distrust of vaccinations due to the publication of numerous messages that wrongly link the childhood measles-mumps-rubella (MMR) vaccination with the development and onset of Autism. The speed with which this message spread due to new social networking technologies caused many parents to distrust vaccinations and therefore forgo having their children receive the vaccine. Although this panic has been ferociously labeled as fictitious, many still harbor a lingering suspicion towards vaccinations and refuse them, which has caused an immediate public health concern.\n\nThe following are some of the key events in the development of health communication as a formal discipline since the 1970s.\n\n\n"}
{"id": "7485789", "url": "https://en.wikipedia.org/wiki?curid=7485789", "title": "Historical particularism", "text": "Historical particularism\n\nHistorical particularism (coined by Marvin Harris in 1968) is widely considered the first American anthropological school of thought.\n\nClosely associated with Franz Boas and the Boasian approach to anthropology, historical particularism rejected the cultural evolutionary model that had dominated anthropology until Boas. It argued that each society is a collective representation of its unique historical past. Boas rejected parallel evolutionism, the idea that all societies are on the same path and have reached their specific level of development the same way all other societies have. Instead, historical particularism showed that societies could reach the same level of cultural development through different paths.\n\nBoas suggested that diffusion, trade, corresponding environment, and historical accident may create similar cultural traits. Three traits, as suggested by Boas, are used to explain cultural customs: environmental conditions, psychological factors, and historical connections, history being the most important (hence the school's name).\n\nCritics of historical particularism argue that it is anti theoretical because it doesn't seek to make universal theories, applicable to all the world's cultures. Boas believed that theories would arise spontaneously once enough data was collected. This school of anthropological thought was the first to be uniquely American and Boas (his school of thought included) was, arguably, the most influential anthropological thinker in American history.\n\n"}
{"id": "6163435", "url": "https://en.wikipedia.org/wiki?curid=6163435", "title": "Human Traces", "text": "Human Traces\n\nHuman Traces is a 2005 novel by Sebastian Faulks, best known as the British author of \"Birdsong\" and \"Charlotte Gray\". The novel took Faulks five years to write. It tells of two friends who set up a pioneering asylum in 19th-century Austria, in tandem with the evolution of psychiatry and the start of the First World War.\n\nTracing the intertwined lives of Doctors Thomas Midwinter, who is English, and Jacques Rebière, from Brittany, France, \"Human Traces\" explores the development of psychiatry and psychoanalysis in the late 19th century, by way of excursions into first alienism then metaphysics, human evolution and neuroscience, before the search for what it means to be human takes us into a brief foray into the First World War. Central to the plot is the theory of bicameralism.\n\nWhilst some have criticised \"Human Traces\" as excessively expository, detailed and didactic, it has also been considered wide-ranging, ambitious and well written. It has enjoyed commercial success, having been a bestseller in the United Kingdom.\n\nFaulks himself says of his novel:\n\n'Human Traces was a Sisyphean task. After spending five years in libraries reading up on madness, psychiatry and psychoanalysis (my office had charts and timelines and things plastered all over the walls), the act of finishing it felt like a bereavement.\n"}
{"id": "10326", "url": "https://en.wikipedia.org/wiki?curid=10326", "title": "Human evolution", "text": "Human evolution\n\nHuman evolution is the evolutionary process that led to the emergence of anatomically modern humans, beginning with the evolutionary history of primates – in particular genus \"Homo\" – and leading to the emergence of \"Homo sapiens\" as a distinct species of the hominid family, the great apes. This process involved the gradual development of traits such as human bipedalism and language, as well as interbreeding with other hominins, which indicate that human evolution was not linear but a web.\n\nThe study of human evolution involves several scientific disciplines, including physical anthropology, primatology, archaeology, paleontology, neurobiology, ethology, linguistics, evolutionary psychology, embryology and genetics. Genetic studies show that primates diverged from other mammals about , in the Late Cretaceous period, and the earliest fossils appear in the Paleocene, around .\n\nWithin the Hominoidea (apes) superfamily, the Hominidae family diverged from the Hylobatidae (gibbon) family some 15–20 million years ago; African great apes (subfamily Homininae) diverged from orangutans (Ponginae) about ; the Hominini tribe (humans, \"Australopithecines\" and other extinct biped genera, and chimpanzee) parted from the Gorillini tribe (gorillas) between 8-9 million years ago; and, in turn, the subtribes Hominina (humans and biped ancestors) and Panina (chimps) separated 4-7.5 million years ago.\n\nHuman evolution from its first separation from the last common ancestor of humans and chimpanzees is characterized by a number of morphological, developmental, physiological, and behavioral changes.\nThe most significant of these adaptations are bipedalism, increased brain size, lengthened ontogeny (gestation and infancy), and decreased sexual dimorphism. The relationship between these changes is the subject of ongoing debate. Other significant morphological changes included the evolution of a power and precision grip, a change first occurring in \"H. erectus\".\n\nBipedalism is the basic adaptation of the hominid and is considered the main cause behind a suite of skeletal changes shared by all bipedal hominids. The earliest hominin, of presumably primitive bipedalism, is considered to be either \"Sahelanthropus\" or \"Orrorin\", both of which arose some 6 to 7 million years ago. The non-bipedal knuckle-walkers, the gorilla and chimpanzee, diverged from the hominin line over a period covering the same time, so either of \"Sahelanthropus\" or \"Orrorin\" may be our last shared ancestor. \"Ardipithecus\", a full biped, arose somewhat later. \n\nThe early bipeds eventually evolved into the australopithecines and still later into the genus \"Homo\". There are several theories of the adaptation value of bipedalism. It is possible that bipedalism was favored because it freed the hands for reaching and carrying food, saved energy during locomotion, enabled long distance running and hunting, provided an enhanced field of vision, and helped avoid hyperthermia by reducing the surface area exposed to direct sun; features all advantageous for thriving in the new savanna and woodland environment created as a result of the East African Rift Valley uplift versus the previous closed forest habitat. A new study provides support for the hypothesis that walking on two legs, or bipedalism, evolved because it used less energy than quadrupedal knuckle-walking. However, recent studies suggest that bipedality without the ability to use fire would not have allowed global dispersal. This change in gait saw a lengthening of the legs proportionately when compared to the length of the arms, which were shortened through the removal of the need for brachiation. Another change is the shape of the big toe. Recent studies suggest that Australopithecines still lived part of the time in trees as a result of maintaining a grasping big toe. This was progressively lost in Habilines.\n\nAnatomically, the evolution of bipedalism has been accompanied by a large number of skeletal changes, not just to the legs and pelvis, but also to the vertebral column, feet and ankles, and skull. The femur evolved into a slightly more angular position to move the center of gravity toward the geometric center of the body. The knee and ankle joints became increasingly robust to better support increased weight. To support the increased weight on each vertebra in the upright position, the human vertebral column became S-shaped and the lumbar vertebrae became shorter and wider. In the feet the big toe moved into alignment with the other toes to help in forward locomotion. The arms and forearms shortened relative to the legs making it easier to run. The foramen magnum migrated under the skull and more anterior.\n\nThe most significant changes occurred in the pelvic region, where the long downward facing iliac blade was shortened and widened as a requirement for keeping the center of gravity stable while walking; bipedal hominids have a shorter but broader, bowl-like pelvis due to this. A drawback is that the birth canal of bipedal apes is smaller than in knuckle-walking apes, though there has been a widening of it in comparison to that of australopithecine and modern humans, permitting the passage of newborns due to the increase in cranial size but this is limited to the upper portion, since further increase can hinder normal bipedal movement.\n\nThe shortening of the pelvis and smaller birth canal evolved as a requirement for bipedalism and had significant effects on the process of human birth which is much more difficult in modern humans than in other primates. During human birth, because of the variation in size of the pelvic region, the fetal head must be in a transverse position (compared to the mother) during entry into the birth canal and rotate about 90 degrees upon exit. The smaller birth canal became a limiting factor to brain size increases in early humans and prompted a shorter gestation period leading to the relative immaturity of human offspring, who are unable to walk much before 12 months and have greater neoteny, compared to other primates, who are mobile at a much earlier age. The increased brain growth after birth and the increased dependency of children on mothers had a big effect upon the female reproductive cycle, and the more frequent appearance of alloparenting in humans when compared with other hominids. Delayed human sexual maturity also led to the evolution of menopause with one explanation providing that elderly women could better pass on their genes by taking care of their daughter's offspring, as compared to having more children of their own.\n\nThe human species eventually developed a much larger brain than that of other primates—typically in modern humans, nearly three times the size of a chimpanzee or gorilla brain. After a period of stasis with \"Australopithecus anamensis\" and \"Ardipithecus\", species which had smaller brains as a result of their bipedal locomotion, the pattern of encephalization started with \"Homo habilis\", whose brain was slightly larger than that of chimpanzees. This evolution continued in \"Homo erectus\" with , and reached a maximum in Neanderthals with , larger even than modern \"Homo sapiens\". This brain increase manifested during postnatal brain growth, far exceeding that of other apes (heterochrony). It also allowed for extended periods of social learning and language acquisition in juvenile humans, beginning as much as 2 million years ago.\n\nFurthermore, the changes in the structure of human brains may be even more significant than the increase in size.\n\nThe increase in volume of the neocortex also included a rapid increase in size of the cerebellum. Its function has traditionally been associated with balance and fine motor control, but more recently with speech and cognition. The great apes, including hominids, had a more pronounced cerebellum relative to the neocortex than other primates. It has been suggested that because of its function of sensory-motor control and learning complex muscular actions, the cerebellum may have underpinned human technological adaptations, including the preconditions of speech.\n\nThe immediate survival advantage of encephalization is difficult to discern, as the major brain changes from \"Homo erectus\" to \"Homo heidelbergensis\" were not accompanied by major changes in technology. It has been suggested that the changes were mainly social and behavioural, including increased empathic abilities, increases in size of social groups, and increased behavioural plasticity \n\nThe reduced degree of sexual dimorphism in humans is visible primarily in the reduction of the male canine tooth relative to other ape species (except gibbons) and reduced brow ridges and general robustness of males. Another important physiological change related to sexuality in humans was the evolution of hidden estrus. Humans are the only hominoids in which the female is fertile year round and in which no special signals of fertility are produced by the body (such as genital swelling or overt changes in proceptivity during estrus).\n\nNonetheless, humans retain a degree of sexual dimorphism in the distribution of body hair and subcutaneous fat, and in the overall size, males being around 15% larger than females. These changes taken together have been interpreted as a result of an increased emphasis on pair bonding as a possible solution to the requirement for increased parental investment due to the prolonged infancy of offspring.\n\nThe ulnar opposition – the contact between the thumb and the tip of the little finger of the same hand – is unique to anatomically modern humans. In other primates the thumb is short and unable to touch the little finger. The ulnar opposition facilitates the precision grip and power grip of the human hand, underlying all the skilled manipulations.\n\nA number of other changes have also characterized the evolution of humans, among them an increased importance on vision rather than smell; a longer juvenile developmental period and higher infant dependency; a smaller gut; faster basal metabolism; loss of body hair; evolution of sweat glands; a change in the shape of the dental arcade from being u-shaped to being parabolic; development of a chin (found in \"Homo sapiens\" alone); development of styloid processes; and the development of a descended larynx.\n\nThe word \"homo\", the name of the biological genus to which humans belong, is Latin for \"human\". It was chosen originally by Carl Linnaeus in his classification system. The word \"human\" is from the Latin \"humanus\", the adjectival form of \"homo\". The Latin \"homo\" derives from the Indo-European root *\"dhghem\", or \"earth\". Linnaeus and other scientists of his time also considered the great apes to be the closest relatives of humans based on morphological and anatomical similarities.\n\nThe possibility of linking humans with earlier apes by descent became clear only after 1859 with the publication of Charles Darwin's \"On the Origin of Species\", in which he argued for the idea of the evolution of new species from earlier ones. Darwin's book did not address the question of human evolution, saying only that \"Light will be thrown on the origin of man and his history.\"\n\nThe first debates about the nature of human evolution arose between Thomas Henry Huxley and Richard Owen. Huxley argued for human evolution from apes by illustrating many of the similarities and differences between humans and apes, and did so particularly in his 1863 book \"Evidence as to Man's Place in Nature\". However, many of Darwin's early supporters (such as Alfred Russel Wallace and Charles Lyell) did not initially agree that the origin of the mental capacities and the moral sensibilities of humans could be explained by natural selection, though this later changed. Darwin applied the theory of evolution and sexual selection to humans when he published \"The Descent of Man\" in 1871.\n\nA major problem in the 19th century was the lack of fossil intermediaries. Neanderthal remains were discovered in a limestone quarry in 1856, three years before the publication of \"On the Origin of Species\", and Neanderthal fossils had been discovered in Gibraltar even earlier, but it was originally claimed that these were human remains of a creature suffering some kind of illness. Despite the 1891 discovery by Eugène Dubois of what is now called \"Homo erectus\" at Trinil, Java, it was only in the 1920s when such fossils were discovered in Africa, that intermediate species began to accumulate. In 1925, Raymond Dart described \"Australopithecus africanus\". The type specimen was the Taung Child, an australopithecine infant which was discovered in a cave. The child's remains were a remarkably well-preserved tiny skull and an endocast of the brain.\n\nAlthough the brain was small (410 cm), its shape was rounded, unlike that of chimpanzees and gorillas, and more like a modern human brain. Also, the specimen showed short canine teeth, and the position of the foramen magnum (the hole in the skull where the spine enters) was evidence of bipedal locomotion. All of these traits convinced Dart that the Taung Child was a bipedal human ancestor, a transitional form between apes and humans.\n\nDuring the 1960s and 1970s, hundreds of fossils were found in East Africa in the regions of the Olduvai Gorge and Lake Turkana. The driving force of these searches was the Leakey family, with Louis Leakey and his wife Mary Leakey, and later their son Richard and daughter-in-law Meave—all successful and world-renowned fossil hunters and paleoanthropologists. From the fossil beds of Olduvai and Lake Turkana they amassed specimens of the early hominins: the australopithecines and \"Homo\" species, and even \"Homo erectus\".\n\nThese finds cemented Africa as the cradle of humankind. In the late 1970s and the 1980s, Ethiopia emerged as the new hot spot of paleoanthropology after \"Lucy\", the most complete fossil member of the species \"Australopithecus afarensis\", was found in 1974 by Donald Johanson near Hadar in the desertic Afar Triangle region of northern Ethiopia. Although the specimen had a small brain, the pelvis and leg bones were almost identical in function to those of modern humans, showing with certainty that these hominins had walked erect. Lucy was classified as a new species, \"Australopithecus afarensis\", which is thought to be more closely related to the genus \"Homo\" as a direct ancestor, or as a close relative of an unknown ancestor, than any other known hominid or hominin from this early time range; \"see\" terms \"hominid\" and \"hominin\". (The specimen was nicknamed \"Lucy\" after the Beatles' song \"Lucy in the Sky with Diamonds\", which was played loudly and repeatedly in the camp during the excavations.) The Afar Triangle area would later yield discovery of many more hominin fossils, particularly those uncovered or described by teams headed by Tim D. White in the 1990s, including \"Ardipithecus ramidus\" and \"Ardipithecus kadabba\".\n\nIn 2013, fossil skeletons of \"Homo naledi\", an extinct species of hominin assigned (provisionally) to the genus \"Homo\", were found in the Rising Star Cave system, a site in South Africa's Cradle of Humankind region in Gauteng province near Johannesburg. , fossils of at least fifteen individuals, amounting to 1550 specimens, have been excavated from the cave. The species is characterized by a body mass and stature similar to small-bodied human populations, a smaller endocranial volume similar to \"Australopithecus\", and a cranial morphology (skull shape) similar to early \"Homo\" species. The skeletal anatomy combines primitive features known from australopithecines with features known from early hominins. The individuals show signs of having been deliberately disposed of within the cave near the time of death. The fossils were dated close to 250,000 years ago, and thus are not a direct ancestor but a contemporary with the first appearance of larger-brained anatomically modern humans.\n\nThe genetic revolution in studies of human evolution started when Vincent Sarich and Allan Wilson measured the strength of immunological cross-reactions of blood serum albumin between pairs of creatures, including humans and African apes (chimpanzees and gorillas). The strength of the reaction could be expressed numerically as an immunological distance, which was in turn proportional to the number of amino acid differences between homologous proteins in different species. By constructing a calibration curve of the ID of species' pairs with known divergence times in the fossil record, the data could be used as a molecular clock to estimate the times of divergence of pairs with poorer or unknown fossil records.\n\nIn their seminal 1967 paper in \"Science\", Sarich and Wilson estimated the divergence time of humans and apes as four to five million years ago, at a time when standard interpretations of the fossil record gave this divergence as at least 10 to as much as 30 million years. Subsequent fossil discoveries, notably \"Lucy\", and reinterpretation of older fossil materials, notably \"Ramapithecus\", showed the younger estimates to be correct and validated the albumin method.\n\nProgress in DNA sequencing, specifically mitochondrial DNA (mtDNA) and then Y-chromosome DNA (Y-DNA) advanced the understanding of human origins. Application of the molecular clock principle revolutionized the study of molecular evolution.\n\nOn the basis of a separation from the orangutan between 10 and 20 million years ago, earlier studies of the molecular clock suggested that there were about 76 mutations per generation that were not inherited by human children from their parents; this evidence supported the divergence time between hominins and chimps noted above. However, a 2012 study in Iceland of 78 children and their parents suggests a mutation rate of only 36 mutations per generation; this datum extends the separation between humans and chimps to an earlier period greater than 7 million years ago (Ma). Additional research with 226 offspring of wild chimp populations in 8 locations suggests that chimps reproduce at age 26.5 years, on average; which suggests the human divergence from chimps occurred between 7 and 13 million years ago. And these data suggest that \"Ardipithecus\" (4.5 Ma), \"Orrorin\" (6 Ma) and \"Sahelanthropus\" (7 Ma) all may be on the hominid lineage, and even that the separation may have occurred outside the East African Rift region.\n\nFurthermore, analysis of the two species' genes in 2006 provides evidence that after human ancestors had started to diverge from chimpanzees, interspecies mating between \"proto-human\" and \"proto-chimps\" nonetheless occurred regularly enough to change certain genes in the new gene pool:\nThe research suggests:\n\nIn the 1990s, several teams of paleoanthropologists were working throughout Africa looking for evidence of the earliest divergence of the hominin lineage from the great apes. In 1994, Meave Leakey discovered \"Australopithecus anamensis\". The find was overshadowed by Tim D. White's 1995 discovery of \"Ardipithecus ramidus\", which pushed back the fossil record to .\n\nIn 2000, Martin Pickford and Brigitte Senut discovered, in the Tugen Hills of Kenya, a 6-million-year-old bipedal hominin which they named \"Orrorin tugenensis\". And in 2001, a team led by Michel Brunet discovered the skull of \"Sahelanthropus tchadensis\" which was dated as , and which Brunet argued was a bipedal, and therefore a hominid—that is, a hominin ( Hominidae; terms \"hominids\" and hominins).\n\nAnthropologists in the 1980s were divided regarding some details of reproductive barriers and migratory dispersals of the genus \"Homo\". Subsequently, genetics has been used to investigate and resolve these issues. According to the Sahara pump theory evidence suggests that genus \"Homo\" have migrated out of Africa at least three and possibly four times (e.g. \"Homo erectus\", \"Homo heidelbergensis\" and two or three times for \"Homo sapiens\"). Recent evidence suggests these dispersals are closely related to fluctuating periods of climate change.\n\nRecent evidence suggests that humans may have left Africa half a million years earlier than previously thought. A joint Franco-Indian team has found human artifacts in the Siwalk Hills north of New Delhi dating back at least 2.6 million years. This is earlier than the previous earliest finding of genus \"Homo\" at Dmanisi, in Georgia, dating to 1.85 million years. Although controversial, tools found at a Chinese cave strengthen the case that humans used tools as far back as 2.48 million years ago. This suggests that the Asian \"Chopper\" tool tradition, found in Java and northern China may have left Africa before the appearance of the Acheulian hand axe.\n\nUp until the genetic evidence became available there were two dominant models for the dispersal of modern humans. The multiregional hypothesis proposed that the genus \"Homo\" contained only a single interconnected population as it does today (not separate species), and that its evolution took place worldwide continuously over the last couple of million years. This model was proposed in 1988 by Milford H. Wolpoff. In contrast the \"out of Africa\" model proposed that modern \"H. sapiens\" speciated in Africa recently (that is, approximately 200,000 years ago) and the subsequent migration through Eurasia resulted in nearly complete replacement of other \"Homo\" species. This model has been developed by Chris B. Stringer and Peter Andrews.\n\nSequencing mtDNA and Y-DNA sampled from a wide range of indigenous populations revealed ancestral information relating to both male and female genetic heritage, and strengthened the Out of Africa theory and weakened the views of Multiregional Evolutionism. Aligned in genetic tree differences were interpreted as supportive of a recent single origin. Analyses have shown a greater diversity of DNA patterns throughout Africa, consistent with the idea that Africa is the ancestral home of mitochondrial Eve and Y-chromosomal Adam, and that modern human dispersal out of Africa has only occurred over the last 55,000 years.\n\n\"Out of Africa\" has thus gained much support from research using female mitochondrial DNA and the male Y chromosome. After analysing genealogy trees constructed using 133 types of mtDNA, researchers concluded that all were descended from a female African progenitor, dubbed Mitochondrial Eve. \"Out of Africa\" is also supported by the fact that mitochondrial genetic diversity is highest among African populations.\n\nA broad study of African genetic diversity, headed by Sarah Tishkoff, found the San people had the greatest genetic diversity among the 113 distinct populations sampled, making them one of 14 \"ancestral population clusters\". The research also located a possible origin of modern human migration in south-western Africa, near the coastal border of Namibia and Angola. The fossil evidence was insufficient for archaeologist Richard Leakey to resolve the debate about exactly where in Africa modern humans first appeared. Studies of haplogroups in Y-chromosomal DNA and mitochondrial DNA have largely supported a recent African origin. All the evidence from autosomal DNA also predominantly supports a Recent African origin. However, evidence for archaic admixture in modern humans, both in Africa and later, throughout Eurasia has recently been suggested by a number of studies.\n\nRecent sequencing of Neanderthal and Denisovan genomes shows that some admixture with these populations has occurred. Modern humans outside Africa have 2–4% Neanderthal alleles in their genome, and some Melanesians have an additional 4–6% of Denisovan alleles. These new results do not contradict the \"out of Africa\" model, except in its strictest interpretation, although they make the situation more complex. After recovery from a genetic bottleneck that could possibly be due to the Toba supervolcano catastrophe, a fairly small group left Africa and later briefly interbred on three separate occasions with Neanderthals, probably in the middle-east, on the Eurasian steppe or even in North Africa before their departure. Their still predominantly African descendants spread to populate the world. A fraction in turn interbred with Denisovans, probably in south-east Asia, before populating Melanesia. HLA haplotypes of Neanderthal and Denisova origin have been identified in modern Eurasian and Oceanian populations. The Denisovan EPAS1 gene has also been found in Tibetan populations.\n\nThere are still differing theories on whether there was a single exodus from Africa or several. A multiple dispersal model involves the Southern Dispersal theory, which has gained support in recent years from genetic, linguistic and archaeological evidence. In this theory, there was a coastal dispersal of modern humans from the Horn of Africa crossing the Bab el Mandib to Yemen at a lower sea level around 70,000 years ago. This group helped to populate Southeast Asia and Oceania, explaining the discovery of early human sites in these areas much earlier than those in the Levant. This group seems to have been dependent upon marine resources for their survival.\n\nStephen Oppenheimer has proposed a second wave of humans may have later dispersed through the Persian Gulf oases, and the Zagros mountains into the Middle East. Alternatively it may have come across the Sinai Peninsula into Asia, from shortly after 50,000 yrs BP, resulting in the bulk of the human populations of Eurasia. It has been suggested that this second group possibly possessed a more sophisticated \"big game hunting\" tool technology and was less dependent on coastal food sources than the original group. Much of the evidence for the first group's expansion would have been destroyed by the rising sea levels at the end of each glacial maximum. The multiple dispersal model is contradicted by studies indicating that the populations of Eurasia and the populations of Southeast Asia and Oceania are all descended from the same mitochondrial DNA L3 lineages, which support a single migration out of Africa that gave rise to all non-African populations.\n\nStephen Oppenheimer, on the basis of the early date of Badoshan Iranian Aurignacian, suggests that this second dispersal, may have occurred with a pluvial period about 50,000 years before the present, with modern human big-game hunting cultures spreading up the Zagros Mountains, carrying modern human genomes from Oman, throughout the Persian Gulf, northward into Armenia and Anatolia, with a variant travelling south into Israel and to Cyrenicia.\n\nThe evidence on which scientific accounts of human evolution are based comes from many fields of natural science. The main source of knowledge about the evolutionary process has traditionally been the fossil record, but since the development of genetics beginning in the 1970s, DNA analysis has come to occupy a place of comparable importance. The studies of ontogeny, phylogeny and especially evolutionary developmental biology of both vertebrates and invertebrates offer considerable insight into the evolution of all life, including how humans evolved. The specific study of the origin and life of humans is anthropology, particularly paleoanthropology which focuses on the study of human prehistory.\n\nThe closest living relatives of humans are bonobos and chimpanzees (both genus \"Pan\") and gorillas (genus \"Gorilla\"). With the sequencing of both the human and chimpanzee genome, estimates of the similarity between their DNA sequences range between 95% and 99%. By using the technique called the molecular clock which estimates the time required for the number of divergent mutations to accumulate between two lineages, the approximate date for the split between lineages can be calculated.\n\nThe gibbons (family Hylobatidae) and then orangutans (genus \"Pongo\") were the first groups to split from the line leading to the hominins, including humans—followed by gorillas, and, ultimately, by the chimpanzees (genus \"Pan\"). The splitting date between hominin and chimpanzee lineages is placed by some between , that is, during the Late Miocene. Speciation, however, appears to have been unusually drawn-out. Initial divergence occurred sometime between , but ongoing hybridization blurred the separation and delayed complete separation during several millions of years. Patterson (2006) dated the final divergence at .\n\nGenetic evidence has also been employed to resolve the question of whether there was any gene flow between early modern humans and Neanderthals, and to enhance our understanding of the early human migration patterns and splitting dates. By comparing the parts of the genome that are not under natural selection and which therefore accumulate mutations at a fairly steady rate, it is possible to reconstruct a genetic tree incorporating the entire human species since the last shared ancestor.\n\nEach time a certain mutation (single-nucleotide polymorphism) appears in an individual and is passed on to his or her descendants a haplogroup is formed including all of the descendants of the individual who will also carry that mutation. By comparing mitochondrial DNA which is inherited only from the mother, geneticists have concluded that the last female common ancestor whose genetic marker is found in all modern humans, the so-called mitochondrial Eve, must have lived around 200,000 years ago.\n\nHuman evolutionary genetics studies how one human genome differs from the other, the evolutionary past that gave rise to it, and its current effects. Differences between genomes have anthropological, medical and forensic implications and applications. Genetic data can provide important insight into human evolution.\n\nThere is little fossil evidence for the divergence of the gorilla, chimpanzee and hominin lineages. The earliest fossils that have been proposed as members of the hominin lineage are \"Sahelanthropus tchadensis\" dating from , \"Orrorin tugenensis\" dating from , and \"Ardipithecus kadabba\" dating to . Each of these have been argued to be a bipedal ancestor of later hominins but, in each case, the claims have been contested. It is also possible that one or more of these species are ancestors of another branch of African apes, or that they represent a shared ancestor between hominins and other apes.\n\nThe question then of the relationship between these early fossil species and the hominin lineage is still to be resolved. From these early species, the australopithecines arose around and diverged into robust (also called \"Paranthropus\") and gracile branches, one of which (possibly \"A. garhi\") probably went on to become ancestors of the genus \"Homo\". The australopithecine species that is best represented in the fossil record is \"Australopithecus afarensis\" with more than one hundred fossil individuals represented, found from Northern Ethiopia (such as the famous \"Lucy\"), to Kenya, and South Africa. Fossils of robust australopithecines such as \"Au. robustus\" (or alternatively \"Paranthropus robustus\") and \"Au./P. boisei\" are particularly abundant in South Africa at sites such as Kromdraai and Swartkrans, and around Lake Turkana in Kenya.\n\nThe earliest member of the genus \"Homo\" is \"Homo habilis\" which evolved around . \"Homo habilis\" is the first species for which we have positive evidence of the use of stone tools. They developed the Oldowan lithic technology, named after the Olduvai Gorge in which the first specimens were found. Some scientists consider \"Homo rudolfensis\", a larger bodied group of fossils with similar morphology to the original \"H. habilis\" fossils, to be a separate species while others consider them to be part of \"H. habilis\"—simply representing intraspecies variation, or perhaps even sexual dimorphism. The brains of these early hominins were about the same size as that of a chimpanzee, and their main adaptation was bipedalism as an adaptation to terrestrial living.\n\nDuring the next million years, a process of encephalization began and, by the arrival (about ) of \"Homo erectus\" in the fossil record, cranial capacity had doubled. \"Homo erectus\" were the first of the hominins to emigrate from Africa, and, from , this species spread through Africa, Asia, and Europe. One population of \"H. erectus\", also sometimes classified as a separate species \"Homo ergaster\", remained in Africa and evolved into \"Homo sapiens\". It is believed that these species, \"H. erectus\" and \"H. ergaster\", were the first to use fire and complex tools.\n\nThe earliest transitional fossils between \"H. ergaster/erectus\" and archaic \"H. sapiens\" are from Africa, such as \"Homo rhodesiensis\", but seemingly transitional forms were also found at Dmanisi, Georgia. These descendants of African \"H. erectus\" spread through Eurasia from ca. 500,000 years ago evolving into \"H. antecessor\", \"H. heidelbergensis\" and \"H. neanderthalensis\". The earliest fossils of anatomically modern humans are from the Middle Paleolithic, about 200,000 years ago such as the Omo remains of Ethiopia; later fossils from Es Skhul cave in Israel and Southern Europe begin around 90,000 years ago ().\n\nAs modern humans spread out from Africa, they encountered other hominins such as \"Homo neanderthalensis\" and the so-called Denisovans, who may have evolved from populations of \"Homo erectus\" that had left Africa around . The nature of interaction between early humans and these sister species has been a long-standing source of controversy, the question being whether humans replaced these earlier species or whether they were in fact similar enough to interbreed, in which case these earlier populations may have contributed genetic material to modern humans.\n\nThis migration out of Africa is estimated to have begun about 70,000 years BP and modern humans subsequently spread globally, replacing earlier hominins either through competition or hybridization. They inhabited Eurasia and Oceania by 40,000 years BP, and the Americas by at least 14,500 years BP.\n\nThe hypothesis of interbreeding, also known as hybridization, admixture or hybrid-origin theory, has been discussed ever since the discovery of Neanderthal remains in the 19th century. The linear view of human evolution began to be abandoned in the 1970s as different species of humans were discovered that made the linear concept increasingly unlikely. In the 21st century with the advent of molecular biology techniques and computerization, whole-genome sequencing of Neanderthal and human genome were performed, confirming recent admixture between different human species. In 2010, evidence based on molecular biology was published, revealing unambiguous examples of interbreeding between archaic and modern humans during the Middle Paleolithic and early Upper Paleolithic. It has been demonstrated that interbreeding happened in several independent events that included Neanderthals, Denisovans, as well as several unidentified hominins. Today, approximately 2% of DNA from most Europeans and Asians is Neanderthal, with traces of Denisovan heritage. Also, 4-6% of modern Melanesian genetics are Denisovan. Comparisons of the human genome to the genomes of Neandertals, Denisovans and apes can help identify features that set modern humans apart from other hominin species. In a 2016 comparative genomics study, a Harvard Medical School/UCLA research team made a world map on the distribution and made some predictions about where Denisovan and Neanderthal genes may be impacting modern human biology.\n\nFor example, comparative studies in the mid-2010s found several traits related to neurological, immunological, developmental, and metabolic phenotypes, that were developed by archaic humans to European and Asian environments and inherited to modern humans through admixture with local hominins.\nAlthough the narratives of human evolution are often contentious, several discoveries since 2010 show that human evolution should not be seen as a simple linear or branched progression, but a mix of related species. In fact, genomic research has shown that hybridization between substantially diverged lineages is the rule, not the exception, in human evolution. Furthermore, it is argued that hybridization was an essential creative force in the emergence of modern humans.\n\nEvolutionary history of the primates can be traced back 65 million years. One of the oldest known primate-like mammal species, the \"Plesiadapis\", came from North America; another, \"Archicebus\", came from China. Other similar basal primates were widespread in Eurasia and Africa during the tropical conditions of the Paleocene and Eocene.\n\nDavid R. Begun concluded that early primates flourished in Eurasia and that a lineage leading to the African apes and humans, including to \"Dryopithecus\", migrated south from Europe or Western Asia into Africa. The surviving tropical population of primates—which is seen most completely in the Upper Eocene and lowermost Oligocene fossil beds of the Faiyum depression southwest of Cairo—gave rise to all extant primate species, including the lemurs of Madagascar, lorises of Southeast Asia, galagos or \"bush babies\" of Africa, and to the anthropoids, which are the Platyrrhines or New World monkeys, the Catarrhines or Old World monkeys, and the great apes, including humans and other hominids.\n\nThe earliest known catarrhine is \"Kamoyapithecus\" from uppermost Oligocene at Eragaleit in the northern Great Rift Valley in Kenya, dated to 24 million years ago. Its ancestry is thought to be species related to \"Aegyptopithecus\", \"Propliopithecus\", and \"Parapithecus\" from the Faiyum, at around 35 million years ago. In 2010, \"Saadanius\" was described as a close relative of the last common ancestor of the crown catarrhines, and tentatively dated to 29–28 million years ago, helping to fill an 11-million-year gap in the fossil record.\n\nIn the Early Miocene, about 22 million years ago, the many kinds of arboreally adapted primitive catarrhines from East Africa suggest a long history of prior diversification. Fossils at 20 million years ago include fragments attributed to \"Victoriapithecus\", the earliest Old World monkey. Among the genera thought to be in the ape lineage leading up to 13 million years ago are \"Proconsul\", \"Rangwapithecus\", \"Dendropithecus\", \"Limnopithecus\", \"Nacholapithecus\", \"Equatorius\", \"Nyanzapithecus\", \"Afropithecus\", \"Heliopithecus\", and \"Kenyapithecus\", all from East Africa.\n\nThe presence of other generalized non-cercopithecids of Middle Miocene from sites far distant—\"Otavipithecus\" from cave deposits in Namibia, and \"Pierolapithecus\" and \"Dryopithecus\" from France, Spain and Austria—is evidence of a wide diversity of forms across Africa and the Mediterranean basin during the relatively warm and equable climatic regimes of the Early and Middle Miocene. The youngest of the Miocene hominoids, \"Oreopithecus\", is from coal beds in Italy that have been dated to 9 million years ago.\n\nMolecular evidence indicates that the lineage of gibbons (family Hylobatidae) diverged from the line of great apes some 18–12 million years ago, and that of orangutans (subfamily Ponginae) diverged from the other great apes at about 12 million years; there are no fossils that clearly document the ancestry of gibbons, which may have originated in a so-far-unknown South East Asian hominoid population, but fossil proto-orangutans may be represented by \"Sivapithecus\" from India and \"Griphopithecus\" from Turkey, dated to around 10 million years ago.\n\nSpecies close to the last common ancestor of gorillas, chimpanzees and humans may be represented by \"Nakalipithecus\" fossils found in Kenya and \"Ouranopithecus\" found in Greece. Molecular evidence suggests that between 8 and 4 million years ago, first the gorillas, and then the chimpanzees (genus \"Pan\") split off from the line leading to the humans. Human DNA is approximately 98.4% identical to that of chimpanzees when comparing single nucleotide polymorphisms (see human evolutionary genetics). The fossil record, however, of gorillas and chimpanzees is limited; both poor preservation—rain forest soils tend to be acidic and dissolve bone—and sampling bias probably contribute to this problem.\n\nOther hominins probably adapted to the drier environments outside the equatorial belt; and there they encountered antelope, hyenas, dogs, pigs, elephants, horses, and others. The equatorial belt contracted after about 8 million years ago, and there is very little fossil evidence for the split—thought to have occurred around that time—of the hominin lineage from the lineages of gorillas and chimpanzees. The earliest fossils argued by some to belong to the human lineage are \"Sahelanthropus tchadensis\" (7 Ma) and \"Orrorin tugenensis\" (6 Ma), followed by \"Ardipithecus\" (5.5–4.4 Ma), with species \"Ar. kadabba\" and \"Ar. ramidus\".\n\nIt has been argued in a study of the life history of \"Ar. ramidus\" that the species provides evidence for a suite of anatomical and behavioral adaptations in very early hominins unlike any species of extant great ape. This study demonstrated affinities between the skull morphology of \"Ar. ramidus\" and that of infant and juvenile chimpanzees, suggesting the species evolved a juvenalised or paedomorphic craniofacial morphology via heterochronic dissociation of growth trajectories. It was also argued that the species provides support for the notion that very early hominins, akin to bonobos (\"Pan paniscus\") the less aggressive species of chimpanzee, may have evolved via the process of self-domestication. Consequently, arguing against the so-called \"chimpanzee referential model\" the authors suggest it is no longer tenable to use common chimpanzee (\"Pan troglodytes\") social and mating behaviors in models of early hominin social evolution. When commenting on the absence of aggressive canine morphology in \"Ar. ramidus\" and the implications this has for the evolution of hominin social psychology, they wrote:\n\nThe authors argue that many of the basic human adaptations evolved in the ancient forest and woodland ecosystems of late Miocene and early Pliocene Africa. Consequently, they argue that humans may not represent evolution from a chimpanzee-like ancestor as has traditionally been supposed. This suggests many modern human adaptations represent phylogenetically deep traits and that the behavior and morphology of chimpanzees may have evolved subsequent to the split with the common ancestor they share with humans.\n\nThe genus \"Australopithecus\" evolved in eastern Africa around 4 million years ago before spreading throughout the continent and eventually becoming extinct 2 million years ago. During this time period various forms of australopiths existed, including \"Australopithecus anamensis\", \"Au. afarensis\", \"Au. sediba\", and \"Au. africanus\". There is still some debate among academics whether certain African hominid species of this time, such as \"Au. robustus\" and \"Au. boisei\", constitute members of the same genus; if so, they would be considered to be \"Au. robust australopiths\" whilst the others would be considered \"Au. gracile australopiths\". However, if these species do indeed constitute their own genus, then they may be given their own name, the \"Paranthropus\".\nA new proposed species \"Australopithecus deyiremeda\" is claimed to have been discovered living at the same time period of \"Au. afarensis\". There is debate if Au. deyiremeda is a new species or is \"Au. afarensis.\" \"Australopithecus prometheus\", otherwise known as Little Foot has recently been dated at 3.67 million years old through a new dating technique, making the genus \"Australopithecus\" as old as \"afarensis\". Given the opposable big toe found on Little Foot, it seems that he was a good climber, and it is thought given the night predators of the region, he probably, like gorillas and chimpanzees, built a nesting platform at night, in the trees.\n\nThe earliest documented representative of the genus \"Homo\" is \"Homo habilis\", which evolved around , and is arguably the earliest species for which there is positive evidence of the use of stone tools. The brains of these early hominins were about the same size as that of a chimpanzee, although it has been suggested that this was the time in which the human SRGAP2 gene doubled, producing a more rapid wiring of the frontal cortex. During the next million years a process of rapid encephalization occurred, and with the arrival of \"Homo erectus\" and \"Homo ergaster\" in the fossil record, cranial capacity had doubled to 850 cm. (Such an increase in human brain size is equivalent to each generation having 125,000 more neurons than their parents.) It is believed that \"Homo erectus\" and \"Homo ergaster\" were the first to use fire and complex tools, and were the first of the hominin line to leave Africa, spreading throughout Africa, Asia, and Europe between .\nAccording to the recent African origin of modern humans theory, modern humans evolved in Africa possibly from \"Homo heidelbergensis\", \"Homo rhodesiensis\" or \"Homo antecessor\" and migrated out of the continent some 50,000 to 100,000 years ago, gradually replacing local populations of \"Homo erectus\", Denisova hominins, \"Homo floresiensis\" and \"Homo neanderthalensis\". Archaic \"Homo sapiens\", the forerunner of anatomically modern humans, evolved in the Middle Paleolithic between 400,000 and 250,000 years ago. Recent DNA evidence suggests that several haplotypes of Neanderthal origin are present among all non-African populations, and Neanderthals and other hominins, such as Denisovans, may have contributed up to 6% of their genome to present-day humans, suggestive of a limited inter-breeding between these species.<ref name=\"10.1126/science.1209202\"></ref> The transition to behavioral modernity with the development of symbolic culture, language, and specialized lithic technology happened around 50,000 years ago according to some anthropologists although others point to evidence that suggests that a gradual change in behavior took place over a longer time span.\n\n\"Homo sapiens\" is the only extant species of its genus, \"Homo\". While some (extinct) \"Homo\" species might have been ancestors of \"Homo sapiens\", many, perhaps most, were likely \"cousins\", having speciated away from the ancestral hominin line. There is yet no consensus as to which of these groups should be considered a separate species and which should be a subspecies; this may be due to the dearth of fossils or to the slight differences used to classify species in the genus \"Homo\". The Sahara pump theory (describing an occasionally passable \"wet\" Sahara desert) provides one possible explanation of the early variation in the genus \"Homo\".\n\nBased on archaeological and paleontological evidence, it has been possible to infer, to some extent, the ancient dietary practices of various \"Homo\" species and to study the role of diet in physical and behavioral evolution within \"Homo\".\n\nSome anthropologists and archaeologists subscribe to the Toba catastrophe theory, which posits that the supereruption of Lake Toba on Sumatran island in Indonesia some 70,000 years ago caused global consequences, killing the majority of humans and creating a population bottleneck that affected the genetic inheritance of all humans today.\n\n\"Homo habilis\" lived from about 2.8 to 1.4 Ma. The species evolved in South and East Africa in the Late Pliocene or Early Pleistocene, 2.5–2 Ma, when it diverged from the australopithecines. \"Homo habilis\" had smaller molars and larger brains than the australopithecines, and made tools from stone and perhaps animal bones. One of the first known hominins was nicknamed 'handy man' by discoverer Louis Leakey due to its association with stone tools. Some scientists have proposed moving this species out of \"Homo\" and into \"Australopithecus\" due to the morphology of its skeleton being more adapted to living on trees rather than to moving on two legs like \"Homo sapiens\".\n\nIn May 2010, a new species, \"Homo gautengensis\", was discovered in South Africa.\n\nThese are proposed species names for fossils from about 1.9–1.6 Ma, whose relation to \"Homo habilis\" is not yet clear.\n\nThe first fossils of \"Homo erectus\" were discovered by Dutch physician Eugene Dubois in 1891 on the Indonesian island of Java. He originally named the material \"Anthropopithecus erectus\" (1892-1893, considered at this point as a chimpanzee-like fossil primate) and \"Pithecanthropus erectus\" (1893-1894, changing his mind as of based on its morphology, which he considered to be intermediate between that of humans and apes). Years later, in the 20th century, the German physician and paleoanthropologist Franz Weidenreich (1873-1948) compared in detail the characters of Dubois' Java Man, then named \"Pithecanthropus erectus\", with the characters of the Peking Man, then named \"Sinanthropus pekinensis\". Weidenreich concluded in 1940 that because of their anatomical similarity with modern humans it was necessary to gather all these specimens of Java and China in a single species of the genus \"Homo\", the species \"Homo erectus\". \"Homo erectus\" lived from about 1.8 Ma to about 70,000 years ago—which would indicate that they were probably wiped out by the Toba catastrophe; however, nearby \"Homo floresiensis\" survived it. The early phase of \"Homo erectus\", from 1.8 to 1.25 Ma, is considered by some to be a separate species, \"Homo ergaster\", or as \"Homo erectus ergaster\", a subspecies of \"Homo erectus\".\n\nIn Africa in the Early Pleistocene, 1.5–1 Ma, some populations of \"Homo habilis\" are thought to have evolved larger brains and to have made more elaborate stone tools; these differences and others are sufficient for anthropologists to classify them as a new species, \"Homo erectus\"—in Africa. The evolution of locking knees and the movement of the foramen magnum are thought to be likely drivers of the larger population changes. This species also may have used fire to cook meat. suggests that the fact that Homo seems to have been ground dwelling, with reduced intestinal length, smaller dentition, \"and swelled our brains to their current, horrendously fuel-inefficient size\", suggest that control of fire and releasing increased nutritional value through cooking was the key adaptation that separated Homo from tree-sleeping Australopithecines.\n\nA famous example of \"Homo erectus\" is Peking Man; others were found in Asia (notably in Indonesia), Africa, and Europe. Many paleoanthropologists now use the term \"Homo ergaster\" for the non-Asian forms of this group, and reserve \"Homo erectus\" only for those fossils that are found in Asia and meet certain skeletal and dental requirements which differ slightly from \"H. ergaster\".\n\nThese are proposed as species that may be intermediate between \"H. erectus\" and \"H. heidelbergensis\".\n\n\"H. heidelbergensis\" (\"Heidelberg Man\") lived from about 800,000 to about 300,000 years ago. Also proposed as \"Homo sapiens heidelbergensis\" or \"Homo sapiens paleohungaricus\".\n\n\n\"Homo neanderthalensis\", alternatively designated as \"Homo sapiens neanderthalensis\", lived in Europe and Asia from 400,000 to about 28,000 years ago.\nThere are a number of clear anatomical differences between anatomically modern humans (AMH) and Neanderthal populations. Many of these relate to the superior adaptation to cold environments possessed by the Neanderthal populations. Their surface to volume ratio is an extreme version of that found amongst Inuit populations, indicating that they were less inclined to lose body heat than were AMH. From brain Endocasts, Neanderthals also had significantly larger brains. This would seem to indicate that the intellectual superiority of AMH populations may be questionable. More recent research by Eiluned Pearce, Chris Stringer, R. I. M. Dunbar, however, have shown important differences in Brain architecture. For example, in both the orbital chamber size and in the size of the occipital lobe, the larger size suggests that the Neanderthal had a better visual acuity than modern humans. This would give a superior vision in the inferior light conditions found in Glacial Europe. It also seems that the higher body mass of Neanderthals had a correspondingly larger brain mass required for body care and control.\n\nThe Neanderthal populations seem to have been physically superior to AMH populations. These differences may have been sufficient to give Neanderthal populations an environmental superiority to AMH populations from 75,000 to 45,000 years BP. With these differences, Neanderthal brains show a smaller area was available for social functioning. Plotting group size possible from endocrainial volume, suggests that AMH populations (minus occipital lobe size), had a Dunbars number of 144 possible relationships. Neanderthal populations seem to have been limited to about 120 individuals. This would show up in a larger number of possible mates for AMH humans, with increased risks of inbreeding amongst Neanderthal populations. It also suggests that humans had larger trade catchment areas than Neanderthals (confirmed in the distribution of stone tools). With larger populations, social and technological innovations were easier to fix in human populations, which may have all contributed to the fact that modern Homo sapiens replaced the Neanderthal populations by 28,000 BP.\n\nEarlier evidence from sequencing mitochondrial DNA suggested that no significant gene flow occurred between \"H. neanderthalensis\" and \"H. sapiens\", and that the two were separate species that shared a common ancestor about 660,000 years ago. However, a sequencing of the Neanderthal genome in 2010 indicated that Neanderthals did indeed interbreed with anatomically modern humans \"circa\" 45,000 to 80,000 years ago (at the approximate time that modern humans migrated out from Africa, but before they dispersed into Europe, Asia and elsewhere). The genetic sequencing of a 40,000 year old human skeleton from Romania showed that 11% of its genome was Neanderthal, and it was estimated that the individual had a Neanderthal ancestor 4-6 generations previously, in addition to a contribution from earlier interbreeding in the Middle East. Though this interbred Romanian population seems not to have been ancestral to modern humans, the finding indicates that interbreeding happened repeatedly.\n\nNearly all modern non-African humans have 1% to 4% of their DNA derived from Neanderthal DNA, and this finding is consistent with recent studies indicating that the divergence of some human alleles dates to one Ma, although the interpretation of these studies has been questioned. Neanderthals and \"Homo sapiens\" could have co-existed in Europe for as long as 10,000 years, during which human populations exploded vastly outnumbering Neanderthals, possibly outcompeting them by sheer numerical strength.\n\nIn 2008, archaeologists working at the site of Denisova Cave in the Altai Mountains of Siberia uncovered a small bone fragment from the fifth finger of a juvenile member of Denisovans. Artifacts, including a bracelet, excavated in the cave at the same level were carbon dated to around 40,000 BP. As DNA had survived in the fossil fragment due to the cool climate of the Denisova Cave, both mtDNA and nuclear DNA were sequenced.\n\nWhile the divergence point of the mtDNA was unexpectedly deep in time, the full genomic sequence suggested the Denisovans belonged to the same lineage as Neanderthals, with the two diverging shortly after their line split from the lineage that gave rise to modern humans. Modern humans are known to have overlapped with Neanderthals in Europe and the Near East for possibly more than 40,000 years, and the discovery raises the possibility that Neanderthals, Denisovans, and modern humans may have co-existed and interbred. The existence of this distant branch creates a much more complex picture of humankind during the Late Pleistocene than previously thought. Evidence has also been found that as much as 6% of the DNA of some modern Melanesians derive from Denisovans, indicating limited interbreeding in Southeast Asia.\n\nAlleles thought to have originated in Neanderthals and Denisovans have been identified at several genetic loci in the genomes of modern humans outside of Africa. HLA haplotypes from Denisovans and Neanderthal represent more than half the HLA alleles of modern Eurasians, indicating strong positive selection for these introgressed alleles. Corinne Simoneti at Vanderbilt University, in Nashville and her team have found from medical records of 28,000 people of European descent that the presence of Neanderthal DNA segments may be associated with a likelihood to suffer depression more frequently.\n\nThe flow of genes from Neanderthal populations to modern human was not all one way. Sergi Castellano of the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany, has in 2016 reported that while Denisovan and Neanderthal genomes are more related to each other than they are to us, Siberian Neanderthal genomes show similarity to the modern human gene pool, more so than to European Neanderthal populations. The evidence suggests that the Neanderthal populations interbred with modern humans possibly 100,000 years ago, probably somewhere in the Near East.\n\nStudies of a Neanderthal child at Gibraltar show from brain development and teeth eruption that Neanderthal children may have matured more rapidly than is the case for Homo sapiens.\n\n\"H. floresiensis\", which lived from approximately 190,000 to 50,000 years before present, has been nicknamed \"hobbit\" for its small size, possibly a result of insular dwarfism. \"H. floresiensis\" is intriguing both for its size and its age, being an example of a recent species of the genus \"Homo\" that exhibits derived traits not shared with modern humans. In other words, \"H. floresiensis\" shares a common ancestor with modern humans, but split from the modern human lineage and followed a distinct evolutionary path. The main find was a skeleton believed to be a woman of about 30 years of age. Found in 2003, it has been dated to approximately 18,000 years old. The living woman was estimated to be one meter in height, with a brain volume of just 380 cm (considered small for a chimpanzee and less than a third of the \"H. sapiens\" average of 1400 cm). \n\nHowever, there is an ongoing debate over whether \"H. floresiensis\" is indeed a separate species. Some scientists hold that \"H. floresiensis\" was a modern \"H. sapiens\" with pathological dwarfism. This hypothesis is supported in part, because some modern humans who live on Flores, the Indonesian island where the skeleton was found, are pygmies. This, coupled with pathological dwarfism, could have resulted in a significantly diminutive human. The other major attack on \"H. floresiensis\" as a separate species is that it was found with tools only associated with \"H. sapiens\".\n\nThe hypothesis of pathological dwarfism, however, fails to explain additional anatomical features that are unlike those of modern humans (diseased or not) but much like those of ancient members of our genus. Aside from cranial features, these features include the form of bones in the wrist, forearm, shoulder, knees, and feet. Additionally, this hypothesis fails to explain the find of multiple examples of individuals with these same characteristics, indicating they were common to a large population, and not limited to one individual. \n\n\"H. sapiens\" (the adjective \"sapiens\" is Latin for \"wise\" or \"intelligent\") emerged around 300,000 years ago, likely derived from \"Homo heidelbergensis\". Between 400,000 years ago and the second interglacial period in the Middle Pleistocene, around 250,000 years ago, the trend in intra-cranial volume expansion and the elaboration of stone tool technologies developed, providing evidence for a transition from \"H. erectus\" to \"H. sapiens\". The direct evidence suggests there was a migration of \"H. erectus\" out of Africa, then a further speciation of \"H. sapiens\" from \"H. erectus\" in Africa. A subsequent migration (both within and out of Africa) eventually replaced the earlier dispersed \"H. erectus\". This migration and origin theory is usually referred to as the \"recent single-origin hypothesis\" or \"out of Africa\" theory. \n\"H. sapiens\" interbred with archaic humans both in Africa and in Eurasia, in Eurasia notably with Neanderthals and Denisovans.\n\nThe Toba catastrophe theory, which postulates a population bottleneck for \"H. sapiens\" about 70,000 years ago, was controversial from its first proposal in the 1990s and by the 2010s had very little support.\nDistinctive human genetic variability has arisen as the result of the founder effect, by archaic admixture and by recent evolutionary pressures.\n\nThe use of tools has been interpreted as a sign of intelligence, and it has been theorized that tool use may have stimulated certain aspects of human evolution, especially the continued expansion of the human brain. Paleontology has yet to explain the expansion of this organ over millions of years despite being extremely demanding in terms of energy consumption. The brain of a modern human consumes about 13 watts (260 kilocalories per day), a fifth of the body's resting power consumption. Increased tool use would allow hunting for energy-rich meat products, and would enable processing more energy-rich plant products. Researchers have suggested that early hominins were thus under evolutionary pressure to increase their capacity to create and use tools.\n\nPrecisely when early humans started to use tools is difficult to determine, because the more primitive these tools are (for example, sharp-edged stones) the more difficult it is to decide whether they are natural objects or human artifacts. There is some evidence that the australopithecines (4 Ma) may have used broken bones as tools, but this is debated.\n\nMany species make and use tools, but it is the human genus that dominates the areas of making and using more complex tools. The oldest known tools are flakes from West Turkana, Kenya, which date to 3.3 million years ago. The next oldest stone tools are from Gona, Ethiopia, and are considered the beginning of the Oldowan technology. These tools date to about 2.6 million years ago. A \"Homo\" fossil was found near some Oldowan tools, and its age was noted at 2.3 million years old, suggesting that maybe the \"Homo\" species did indeed create and use these tools. It is a possibility but does not yet represent solid evidence. The third metacarpal styloid process enables the hand bone to lock into the wrist bones, allowing for greater amounts of pressure to be applied to the wrist and hand from a grasping thumb and fingers. It allows humans the dexterity and strength to make and use complex tools. This unique anatomical feature separates humans from apes and other nonhuman primates, and is not seen in human fossils older than 1.8 million years.\n\nBernard Wood noted that \"Paranthropus\" co-existed with the early \"Homo\" species in the area of the \"Oldowan Industrial Complex\" over roughly the same span of time. Although there is no direct evidence which identifies \"Paranthropus\" as the tool makers, their anatomy lends to indirect evidence of their capabilities in this area. Most paleoanthropologists agree that the early \"Homo\" species were indeed responsible for most of the Oldowan tools found. They argue that when most of the Oldowan tools were found in association with human fossils, \"Homo\" was always present, but \"Paranthropus\" was not.\n\nIn 1994, Randall Susman used the anatomy of opposable thumbs as the basis for his argument that both the \"Homo\" and \"Paranthropus\" species were toolmakers. He compared bones and muscles of human and chimpanzee thumbs, finding that humans have 3 muscles which are lacking in chimpanzees. Humans also have thicker metacarpals with broader heads, allowing more precise grasping than the chimpanzee hand can perform. Susman posited that modern anatomy of the human opposable thumb is an evolutionary response to the requirements associated with making and handling tools and that both species were indeed toolmakers.\n\nStone tools are first attested around 2.6 Million years ago, when hominins in Eastern Africa used so-called core tools, choppers made out of round cores that had been split by simple strikes. This marks the beginning of the Paleolithic, or Old Stone Age; its end is taken to be the end of the last Ice Age, around 10,000 years ago. The Paleolithic is subdivided into the Lower Paleolithic (Early Stone Age), ending around 350,000–300,000 years ago, the Middle Paleolithic (Middle Stone Age), until 50,000–30,000 years ago, and the Upper Paleolithic, (Late Stone Age), 50,000-10,000 years ago.\n\nArchaeologists working in the Great Rift Valley in Kenya have discovered the oldest known stone tools in the world. Dated to around 3.3 million years ago, the implements are some 700,000 years older than stone tools from Ethiopia that previously held this distinction.\n\nThe period from 700,000–300,000 years ago is also known as the Acheulean, when \"H. ergaster\" (or \"erectus\") made large stone hand axes out of flint and quartzite, at first quite rough (Early Acheulian), later \"retouched\" by additional, more-subtle strikes at the sides of the flakes. After 350,000 BP the more refined so-called Levallois technique was developed, a series of consecutive strikes, by which scrapers, slicers (\"racloirs\"), needles, and flattened needles were made. Finally, after about 50,000 BP, ever more refined and specialized flint tools were made by the Neanderthals and the immigrant Cro-Magnons (knives, blades, skimmers). In this period they also started to make tools out of bone.\n\nUntil about 50,000–40,000 years ago, the use of stone tools seems to have progressed stepwise. Each phase (\"H. habilis\", \"H. ergaster\", \"H. neanderthalensis\") started at a higher level than the previous one, but after each phase started, further development was slow. Currently paleoanthropologists are debating whether these \"Homo\" species possessed some or many of the cultural and behavioral traits associated with modern humans such as language, complex symbolic thinking, technological creativity etc. It seems that they were culturally conservative maintaining simple technologies and foraging patterns over very long periods.\n\nAround 50,000 BP, modern human culture started to evolve more rapidly. The transition to behavioral modernity has been characterized by most as a Eurasian \"Great Leap Forward\", or as the \"Upper Palaeolithic Revolution\", due to the sudden appearance of distinctive signs of modern behavior and big game hunting in the archaeological record. Some other scholars consider the transition to have been more gradual, noting that some features had already appeared among archaic African \"Homo sapiens\" since 200,000 years ago. Recent evidence suggests that the Australian Aboriginal population separated from the African population 75,000 years ago, and that they made a sea journey of up to 160 km 60,000 years ago, which may diminish the evidence of the Upper Paleolithic Revolution.\n\nModern humans started burying their dead, using animal hides to make clothing, hunting with more sophisticated techniques (such as using trapping pits or driving animals off cliffs), and engaging in cave painting. As human culture advanced, different populations of humans introduced novelty to existing technologies: artifacts such as fish hooks, buttons, and bone needles show signs of variation among different populations of humans, something that had not been seen in human cultures prior to 50,000 BP. Typically, \"H. neanderthalensis\" populations do not vary in their technologies, although the Chatelperronian assemblages have been found to be Neanderthal innovations produced as a result of exposure to the Homo sapiens Aurignacian technologies.\n\nAmong concrete examples of modern human behavior, anthropologists include specialization of tools, use of jewellery and images (such as cave drawings), organization of living space, rituals (for example, burials with grave gifts), specialized hunting techniques, exploration of less hospitable geographical areas, and barter trade networks. Debate continues as to whether a \"revolution\" led to modern humans (\"the big bang of human consciousness\"), or whether the evolution was more \"gradual\".\n\nEvolution has continued in anatomically modern human populations, which are affected by both natural selection and genetic drift. Although selection pressure on some traits, such as resistance to smallpox, has decreased in modern human life, humans are still undergoing natural selection for many other traits. Some of these are due to specific environmental pressures, while others are related to lifestyle changes since the development of agriculture (10,000 years ago), urban civilization (5,000), and industrialization (250 years ago). It has been argued that human evolution has accelerated since the development of agriculture 10,000 years ago and civilization some 5,000 years ago, resulting, it is claimed, in substantial genetic differences between different current human populations.\n\nParticularly conspicuous is variation in superficial characteristics, such as Afro-textured hair, or the recent evolution of light skin and blond hair in some populations, which are attributed to differences in climate. Particularly strong selective pressures have resulted in high-altitude adaptation in humans, with different ones in different isolated populations. Studies of the genetic basis show that some developed very recently, with Tibetans evolving over 3,000 years to have high proportions of an allele of EPAS1 that is adaptive to high altitudes.\n\nOther evolution is related to endemic diseases: the presence of malaria selected for sickle cell trait (the heterozygote form of sickle cell gene), while the absence of malaria and the health effects of sickle-cell anemia select against this trait. For example, the population at risk of the severe debilitating disease kuru has significant over-representation of an immune variant of the prion protein gene G127V versus non-immune alleles. The frequency of this genetic variant is due to the survival of immune persons.\n\nRecent human evolution related to agriculture includes genetic resistance to infectious disease that has appeared in human populations by crossing the species barrier from domesticated animals, as well as changes in metabolism due to changes in diet, such as lactase persistence.\n\nIn contemporary times, since industrialization, some trends have been observed: for instance, menopause is evolving to occur later. Other reported trends appear to include lengthening of the human reproductive period and reduction in cholesterol levels, blood glucose and blood pressure in some populations.<ref name=\"doi10.1073/pnas.0906199106\"></ref>\n\nThis list is in chronological order across the table by genus. Some species/subspecies names are well-established, and some are less established – especially in genus \"Homo\". Please see articles for more information.\n\n\n"}
{"id": "48648074", "url": "https://en.wikipedia.org/wiki?curid=48648074", "title": "Institute for Social Anthropology", "text": "Institute for Social Anthropology\n\nThe Institute for Social Anthropology (ISA) is a research institute of the Austrian Academy of Sciences (AAS) in Vienna, Austria.\n\nThe Institute for Social Anthropology is an Asia-specialized research institute at the AAS. Its long-term research focus lies on \"Consensus and Conflict in Asia and the Eastern Mediterranean\". Special emphasis is placed on transnational developments in Asia, such as regional integration and cooperation, transformations of family and kinship relations, as well as regional and internal migration.\n\nISA’s medium-term research program (2012-2017) is entitled \"Engaging with Crisis, Mobility and Transformations in Asia and the Eastern Mediterranean\". It focuses on systemic and biographic crises in Asia’s past and present, on the relationships between crises and mobility, and on the role of major social, economic and political transformations in connection with this. In short, ISA’s research problematizes the ways in which Asian societies engage with crises, mobility and transformations.\n\nProf. Ulf Hannerz is the current chair and Prof. Regina Bendix the current deputy chair of ISA’s Scientific Advisory Board for the term 2014-2016.\n\nThe roots of the current Institute for Social Anthropology can be traced back to March 2, 1938, when the erstwhile Commissions for \"Research on Illiterate Languages of Non-European Peoples\" and for \"Publishing Songs and Texts Recorded in Prisoner of War Camps\" at the Austrian Academy of Sciences were merged into the new \"Commission for Research on Primitive Cultures and Languages.\" As the Commission’s research focus increasingly turned to complex societies, in line with developments within anthropology at the time, it was renamed \"Ethnological Commission\" on November 22, 1961. In the years between 1955 and 1965, Prof. Robert Heine-Geldern established a focus on the region of Southeast Asia for the first time in the history of AAS.\n\nFrom 1980 to 2000 Prof. Walter Dostal substantially expanded the already existing focus on Southeast Asia by two additional core fields – South-Western Arabia and Tibet.\nA second institutional merger took place under Prof. Walter Dostal on January 1, 1993, when the \"Arabic Commission\" was integrated into the Ethnological Commission, resulting both in an interdisciplinary approach that provided a strong foundation for research on intercultural relations, and in a special emphasis on language competency in its anthropological work.\n\nAs it became clear that the term \"ethnos\" did not adequately capture anthropology’s research focus on the human anymore, the Commission for Ethnology was renamed, on January 18, 1995, to \"Commission for Social Anthropology.\" Since 1995, this socio-anthropological research approach follows an interdisciplinary program by conducting empirical and ethnographic investigations in connection with philological and historical analyses and intercultural comparative studies of socio-cultural phenomena. The award of the Wittgenstein prize to Prof. Andre Gingrich in the year 2000 marked the start of a new era at the Commission.\n\nOn January 1, 2007, the \"Commission for Social Anthropology\" was transformed into a Research Unit as part of the \"Center for Studies in Asian Cultures and Social Anthropology,\" and awarded temporary institutional status as the \"Institute for Social Anthropology\" on January 1, 2010. Following a successful external evaluation, it was granted permanent status on September 15, 2011.\n\nToday, the Institute for Social Anthropology constitutes one of the Austrian Academy of Sciences’ flagship institutes in the humanities and social sciences, and counts among the top European research institutions in anthropology.\n\nISA’s regional foci and expertise encompass: 1) the Islamic Middle East and North Africa, 2) primarily Buddhist Central Asia and the Tibetan language-speaking Himalayan regions, as well as 3) Southeast Asia and the islands of the Indian Ocean.\n\nResearchers on the Middle East examine, inter alia, social transformations and conflicts of South-Western Arabia past and present and the memory of crimes against humanity among Kurdish people. In Central Asia, Tibet and the Himalayan region, staff carry out research on imperial-time and early Buddhist Tibet, Tibetan medicine, and nomadic artefacts. \nOngoing projects in the region of Southeast Asia and the islands of the Indian Ocean include research on health-related mobility and tourism in the Republic of Maldives, on cognitive concepts among the Maniq-Semang in Southern Thailand, and on social media and Islamic practices in Indonesia.\n\nMethodologically, ISA pursues gender-sensitive ethnographic fieldwork carried out in local languages and systematic cross-cultural comparison in its analyses and interpretations of socio-cultural processes in the past and present.\n\nSince 2002, ISA co-organizes the international Eric Wolf Lectures in collaboration with the Department of Social and Cultural Anthropology of the University of Vienna and the International Research Center for Cultural Studies (IFK). Every year, a prominent representative of the discipline is invited to hold a public lecture. Since 2004, the Eric Wolf Lectures are published in Current Anthropology.\n\nAbout eight times a year, the institute organizes its ISA International Guest Lectures, for which ISA invites researchers from all relevant fields and well-established lecturers. These lectures – mainly in English – are published in the institute’s working papers in Social Anthropology.\n\nEvery two years, ISA organizes an \"International Anthropological Atelier\" – a special workshop together with another European anthropological institute in order to share, discuss, and develop preliminary results of ongoing research. The outcomes of the workshop are published as an edited volume or a special focus section in a leading journal of the field.\n\nISA’s Research Forum is meant primarily as an internal forum for staff members of ISA or partner institutions to present new projects as well as the results of ongoing or already completed research.\n\nOccasionally, ISA organizes or co-organizes domestic as well as international conferences and seminars on theoretical-methodological debates, special topics, or regional agendas. In 2004, ISA co-organized the 8th EASA Biennial Conference, in 2011 the GAA Conference 2011, and in 2015 the 8th EuroSEAS Conference as well as the 11th CHaGS Conference in Vienna.\n\nSince 1996, ISA publishes the book series \"Veröffentlichungen zur Sozialanthropologie\" (\"Publications in Social Anthropology\"). This series is available from the Austrian Academy of Sciences Press.\n\nSince 2008, AAS Working Papers in Social Anthropology appear several times a year. This series is available online and \"open access\" on the institute’s homepage.\n\nAnother publication is the book series \"Sammlung Eduard Glaser\", which is also available from the Austrian Academy of Sciences Press.\n\n"}
{"id": "53977347", "url": "https://en.wikipedia.org/wiki?curid=53977347", "title": "Joan Gero", "text": "Joan Gero\n\nJoan M. Gero (26 May 1944 – 14 July 2016) was an American archaeologist and pioneer of feminist archaeology. Her research focused on gender and power issues in prehistory, particularly in the Andean regions of Argentina and Peru.\n\nGero was born in New York City on 26 May 1944. She graduated from the University of Pennsylvania with a BA in English Literature in 1968, before receiving an M.Ed from Boston College in 1970. The next two years were spent teaching socioeconomically disadvantaged groups with the Teacher Corps. In 1972 Gero studied archaeology during a summer course in Oxford, excavating at an Iron Age site in Wiltshire.\n\nIn 1974 Gero began graduate studies at the University of Massachusetts Amherst, with Martin Wobst, gaining a Phd in Anthropology in 1983. Gero taught at the University of South Carolina from 1983 to 1997.\n\nWith Margaret Conkey she co-edited the groundbreaking 1991 volume \"Engendering Archaeology: Women and Prehistory\", reprinted six times, which stemmed from a 1988 conference “Women and Production in Prehistory\".\n\nGero has held visiting professorships at Cambridge University, the Universidad Nacional de Catamarca, the University of Umeå, University of Uppsala, and the Universidad Nacional del Centro de Buenos Aires, Olavarría, Argentina. In 1998 Gero was appointed Assistant Professor at American University in Washington, D.C., where she taught courses in archaeology, anthropology and women's studies. Gero was also a research associate at the Department of Anthropology at the Smithsonian Institution. At the time of her death she was a professor emerita.\n\nGero received funding from Fulbright, the National Science Foundation, the Wenner-Gren Foundation, the Heintz foundation, and the National Endowment for the Humanities.\n\nThroughout her career Gero was heavily involved in the World Archaeological Congress, serving as the senior North American representative from 1999–2008, organising WAC-5 in 2003, acting as Head Series Editor for the One World Book Series 2003-2008, and serving on the Standing Committee for Ethics from 2007. Gero was a Lifetime Fellow of Clare Hall, University of Cambridge.\n\nGero died on July 14, 2016.\n\nGero was awarded the Squeaky Wheel by the American Anthropological Association’s Committee on the Status of Women in Anthropology in 2007.\n\n"}
{"id": "25430653", "url": "https://en.wikipedia.org/wiki?curid=25430653", "title": "Joke (sketch)", "text": "Joke (sketch)\n\n\"Joke\" is a comedy sketch written and performed by English comedians Rowan Atkinson and Richard Curtis. It was performed live during Atkinson's 1980 tour of the United Kingdom. A live recording was made at the Grand Opera House in Belfast, Northern Ireland, on 19 or 20 September 1980 and released as the last track on Atkinson's live comedy album, \"Live in Belfast\".\n\nAt the start of the sketch, Rowan Atkinson's character tells Richard Curtis' character he's \"got a joke\". He goes on to explain, \"It's one of those ones where I ask a question and you say 'I don't know, dot, dot, dot, dot, dot, dot'.\" Rowan begins the joke by saying, \"I say, I say, I say, what is the secret of great comedy?\" Richard replies by literally saying, \"I don't know, dot, dot, dot, dot, dot, dot.\" After laughing at his own wit, Richard tells the annoyed Rowan that he'll \"do it again.\" Rowan repeats the question and Richard replies, \"I don't know. What is the secret to great comedy?\" However, before Richard can finish his reply, Rowan interrupts, \"timing.\" The point of the joke is that Rowan deliberately got his timing wrong when saying that timing is the secret to great comedy; however, Richard does not understand the joke. Bewildered by Richard's stupidity, Rowan enquires, \"But didn't you see?\" \"Sorry, was it a visual joke?\" Richard replies, still confused. After trying the joke a third time, Richard still does not understand the irony of the joke. Annoyed by Richard, Rowan asks, \"Don't you think that's a clever joke?\" to which Richard replies, \"Clever... no. Joke... no.\" Disheartened by his joke falling flat with Richard, he claims \"it was very funny when Michael told it,\" then makes the excuse that his timing might be out to which Richard replies, \"It could be other factors like the complete absence of anything funny in the joke.\" Rowan proclaims, \"It's so difficult telling jokes,\" before suddenly exclaiming that he has another joke that is \"far more straightforward\" to which Richard replies, \"As long as it's funny, I don't mind.\"\n\nThe second joke begins with Rowan saying, \"Knock! Knock! Who's there? Death...\" but he is interrupted by Richard who complains that knock-knock jokes \"are meant to be two-handers\". Rowan argues that this joke \"doesn't work that way\". It becomes apparent that Rowan is attempting to tell the same joke told by Toby, the Devil, during a previous sketch. The joke is meant to go \"Knock, Knock. Who's there? Death. Death wh...\" at which point the person telling the joke pretends to die. If a second person was replying in the joke, they would not know to pretend to die when saying \"Death who?\". Richard protests and Rowan gives in and the pair attempt the knock-knock joke in the \"old way\". Richard, ignorant to the fact he is meant to say the punch line, replies \"Death who?\" and Rowan replies \"Ah, now, that's where it goes wrong.\" Rowan explains to Richard how he is meant to say the punch-line but after another attempt at the joke, Richard replies wrong. Rowan, getting increasingly more annoyed as the sketch goes on, says he has another joke and this time he will be \"third time lucky\".\n\nRowan explains that the third joke is \"a very old joke but a very good joke\". Rowan begins to tell the classic joke, which begins \"I say, I say, I say, my wife's gone to Jamaica\". The person replying is meant to say, \"Jamaica?\" which sounds like \"D'you make 'er?\" or \"Did you make her?\". The joke-teller then replies, \"No, of her own accord.\" However, when Rowan says, \"I say, I say, I say, my wife's gone to Jamaica,\" Richard replies, \"Of her own accord?\" which ruins the whole joke and causes Rowan to swear loudly and give up telling jokes to Richard.\n\n\nSide one\nSide two\n"}
{"id": "5791408", "url": "https://en.wikipedia.org/wiki?curid=5791408", "title": "Kaligrafos", "text": "Kaligrafos\n\nKaligrafos is a non-profit organization which was founded in 1980 to promote the lettering arts. The guild is based in the Dallas / Fort Worth, Texas area, where monthly meetings are held. A variety of calligraphy workshops and classes are offered to novices and masters alike. Membership fees are currently $25 per year.\n\n"}
{"id": "1136479", "url": "https://en.wikipedia.org/wiki?curid=1136479", "title": "Kinesics", "text": "Kinesics\n\nKinesics is the interpretation of body motion communication such as facial expressions and gestures, nonverbal behavior related to movement of any part of the body or the body as a whole. The equivalent popular culture term is body language, a term Ray Birdwhistell, considered the founder of this area of study, neither used nor liked (on the grounds that what can be conveyed with the body does not meet the linguist's definition of language).\n\nKinesics was first used in 1952 by an anthropologist named Ray Birdwhistell. Birdwhistell wished to study how people communicate through posture, gesture, stance and movement. His ideas over several decades were synthesized and resulted in the book \"Kinesics and Context.\" Interest in kinesics specifically and nonverbal behavior generally was popularized in the late 1960s and early 1970s by such popular mass market (nonacademic) publications as \"How to Read a Person Like a Book\". Part of Birdwhistell's work involved filming people in social situations and analyzing them to show elements of communication that were not clearly seen otherwise. One of his most important projects was \"The Natural History of an Interview,\" a long-term interdisciplinary collaboration including Gregory Bateson, Frieda Fromm-Reichmann, Norman A. McQuown, Henry W. Brosin and others.\n\nDrawing heavily on descriptive linguistics, Birdwhistell argued that all movements of the body have meaning and that nonverbal behavior has a grammar that can be analyzed in similar terms to spoken language. Thus, a \"kineme\" is \"similar to a phoneme because it consists of a group of movements which are not identical, but which may be used interchangeably without affecting social meaning.\"\n\nBirdwhistell estimated that no more than 30 to 35 percent of the social meaning of a conversation or an interaction is carried by the words. He also concluded that there were no universals in these kinesic displays, a claim that was disputed by Paul Ekman, who was interested in analysis of universals, especially in facial expression.\n\nIn a current application, kinesic behaviors are sometimes used as signs of deception by interviewers looking for clusters of movements to determine the veracity of the statement being uttered, although kinesics can be equally applied in any context and type of setting to construe innocuous messages whose carriers are indolent or unable to express verbally.\n\nRelevant concepts include:\n\nKinesic behaviors are an important part of nonverbal communication. Body movements convey information, but interpretations vary by culture. As many movements are carried out at a subconscious or at least a low-awareness level, kinesic movements carry a significant risk of being misinterpreted in an intercultural communication situation.\n\n\n"}
{"id": "13503257", "url": "https://en.wikipedia.org/wiki?curid=13503257", "title": "Language survey", "text": "Language survey\n\nA language survey is conducted around the world for a variety of reasons.\n\n\nMethods used in language surveys depend on the questions that the survey is trying to answer. Methods used include collecting word lists (Bender 1971), playing recorded texts to assess comprehension (Casad 1974), sentence repetition tests (Radloff 1991), questionnaires (Hochstetler and Tillinghast 1996), group and individual interviews, retelling of stories (McKinnies and Priestly 2004), direct observation (Cooper and Carpenter 1976), and even internet surveys (tafesilafai.org).\n\nAs with any form of research, the methods used depend on the questions that the researchers are trying to answer. Also, the reliability of the results varies according to the method and the rigor with which it is applied, proper sampling technique, etc.\n\nThe results of language surveys are use for a variety of purposes. One of the most common is in making decisions for implementing educational programs. The results have also been used for making decision for language development work (Holbrook, 2001). And of course, academics are always interested in the results of any language survey.\n\nSurveys have also been conducted by ethnic associations (Saskatchewan 1991), government agencies (Statistics Canada 1993), NGO's (Toba, et al. 2002), foundations (Pew Hispanic Center 2004), etc. Often such groups work together (Clifton 2002). Some large and notable surveys include the Language Survey of India which was begun by George Abraham Grierson late in the 19th century (Sociolinguistics research in India) and the Survey of Language Use and Language Teaching in East Africa, sponsored by the Ford Foundation from the 1960s. Both resulted in a number of volumes describing locations of languages, patterns of multilingualism, language classification, and also included descriptions of languages, such as \"Language in Ethiopia\" (Bender, Bowen, Cooper, and Ferguson 1976). The single agency conducting the most language surveys around the world is SIL International (Summer Institute of Linguistics). The results of many of their surveys are posted on the web: http://www.sil.org/silesr.\n\nSurveys have usually been conducted among spoken languages. However, surveys have also been done among users of sign languages (Vasishta, Woodward, and Wilson 1978, Woodward 1991, 1993, 1996, Parkhurst & Parkhurst 1998, Al-Fityani & Padden 2008). As with surveys among spoken languages, surveys among sign languages have studied multilingualism, attitudes about various languages both spoken and signed (Ciupek-Reed 2012), differences and similarities between signed varieties (Aldersson and McEntee-Atalianis 2007, Bickford 1991, 2005, Parks 2011), and assessing the vitality of signed languages, and initial descriptions of undocumented sign languages.\n\n\n\n"}
{"id": "6719326", "url": "https://en.wikipedia.org/wiki?curid=6719326", "title": "Legal tests", "text": "Legal tests\n\nLegal tests are various kinds of commonly applied methods of evaluation used to resolve matters of jurisprudence. In the context of a trial, a hearing, discovery, or other kinds of legal proceedings, the resolution of certain questions of fact or law may hinge on the application of one or more legal tests.\n\nLegal tests are often formulated from the logical analysis of a judicial decision or a court order where it appears that a finder of fact or the court made a particular decision after contemplating a well-defined set of circumstances. It is assumed that evaluating any given set of circumstances under a legal test will lead to an unambiguous and repeatable result. \n\n\n\n\n\n"}
{"id": "4741829", "url": "https://en.wikipedia.org/wiki?curid=4741829", "title": "List of female rhetoricians", "text": "List of female rhetoricians\n\nWithin the field of rhetoric, the contributions of female rhetoricians have often been overlooked. Anthologies comprising the history of rhetoric or rhetoricians often leave the impression there were none. Throughout history, however, there have been a significant number of women rhetoricians.\n\n\"Re∙Vision—the act of looking back, of seeing with fresh eyes, of entering an old text from a new critical direction—is for women more than a chapter in cultural history: it is an act of survival.\" -Adrienne Rich\n\nThe following is a timeline of contributions made to the field of rhetoric by women.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "44475530", "url": "https://en.wikipedia.org/wiki?curid=44475530", "title": "Mammalian assemblage zone", "text": "Mammalian assemblage zone\n\nIn Pleistocene palaeontology, a mammalian assemblage zone (MAZ) is a collection of fossil bones of mammals.\n"}
{"id": "1684561", "url": "https://en.wikipedia.org/wiki?curid=1684561", "title": "Method of loci", "text": "Method of loci\n\nThe method of loci (\"loci\" being Latin for \"places\") is a method of memory enhancement which uses visualizations with the use of spatial memory, familiar information about one's environment, to quickly and efficiently recall information. The method of loci is also known as the memory journey, memory palace, or mind palace technique. This method is a mnemonic device adopted in ancient Roman and Greek rhetorical treatises (in the anonymous \"Rhetorica ad Herennium\", Cicero's \"De Oratore\", and Quintilian's \"Institutio Oratoria\"). Many memory contest champions claim to use this technique to recall faces, digits, and lists of words.\n\nThe term is most often found in specialised works on psychology, neurobiology, and memory, though it was used in the same general way at least as early as the first half of the nineteenth century in works on rhetoric, logic, and philosophy. John O'Keefe and Lynn Nadel refer to:'the method of loci', an imaginal technique known to the ancient Greeks and Romans and described by Yates (1966) in her book \"The Art of Memory\" as well as by Luria (1969). In this technique the subject memorizes the layout of some building, or the arrangement of shops on a street, or any geographical entity which is composed of a number of discrete loci. When desiring to remember a set of items the subject 'walks' through these loci in their imagination and commits an item to each one by forming an image between the item and any feature of that locus. Retrieval of items is achieved by 'walking' through the loci, allowing the latter to activate the desired items. The efficacy of this technique has been well established (Ross and Lawrence 1968, Crovitz 1969, 1971, Briggs, Hawkins and Crovitz 1970, Lea 1975), as is the minimal interference seen with its use.\n\nThe items to be remembered in this mnemonic system are mentally associated with specific physical locations. The method relies on memorized spatial relationships to establish order and recollect memorial content. It is also known as the \"Journey Method\", used for storing lists of related items, or the \"Roman Room\" technique, which is most effective for storing unrelated information.\n\nMany effective memorisers today use the \"method of loci\" to some degree. Contemporary memory competition, in particular the World Memory Championship, was initiated in 1991 and the first United States championship was held in 1997. Part of the competition requires committing to memory and recalling a sequence of digits, two-digit numbers, alphabetic letters, or playing cards. In a simple method of doing this, contestants, using various strategies well before competing, commit to long-term memory a unique vivid image associated with each item. They have also committed to long-term memory a familiar route with firmly established stop-points or loci. Then in the competition they need only deposit the image that they have associated with each item at the loci. To recall, they retrace the route, \"stop\" at each locus, and \"observe\" the image. They then translate this back to the associated item. For example, Ed Cooke, a World Memory Champion Competitor, describes to Josh Foer in his book \"Moonwalking with Einstein\" how he uses the method of loci. First, he describes a very familiar location where he can clearly remember many different smaller locations like his sink in his childhood home or his dog's bed. Cooke also advises that the more outlandish and vulgar the symbol used to memorize the material, the more likely it will stick.\n\nMemory champions elaborate on this by combining images. Eight-time World Memory Champion Dominic O'Brien uses this technique. The 2006 World Memory Champion, Clemens Mayer, used a 300-point-long journey through his house for his world record in \"number half marathon\", memorising 1040 random digits in a half-hour. Gary Shang has used the method of loci to memorise pi to over 65,536 (2) digits.\n\nUsing this technique a person with ordinary memorisation capabilities, after establishing the route stop-points and committing the associated images to long-term memory, with less than an hour of practice, can remember the sequence of a shuffled deck of cards. The world record for this is held by Simon Reinhard at 21.19 seconds.\n\nThe technique is taught as a metacognitive technique in learning-to-learn courses. It is generally applied to encoding the key ideas of a subject. Two approaches are:\n\nThe method of loci has also been shown to help sufferers of depression remember positive, self-affirming memories.\n\nA study at the University of Maryland evaluated participants ability to accurately recall two sets of familiar faces, using a traditional desktop, and with a head-mounted display. The study was designed to leverage the method of loci technique, with virtual environments resembling memory palaces. The study found an 8.8% recall improvement in favor of the head-mounted display, in part due to participants being able to leverage their vestibular and proprioceptive sensations.\n\nThe \"Rhetorica ad Herennium\" and most other sources recommend that the method of loci should be integrated with elaborative encoding (i.e., adding visual, auditory, or other details) to strengthen memory. However, due to the strength of spatial memory, simply mentally placing objects in real or imagined locations without further elaboration can be effective for simple associations.\n\nA variation of the \"method of loci\" involves creating imaginary locations (houses, palaces, roads, and cities) to which the same procedure is applied. It is accepted that there is a greater cost involved in the initial setup, but thereafter the performance is in line with the standard loci method. The purported advantage is to create towns and cities that each represent a topic or an area of study, thus offering an efficient filing of the information and an easy path for the regular review necessary for long term memory storage.\n\nSomething that is likely a reference to the \"method of loci\" techniques survives to this day in the common English phrases \"in the first place\", \"in the second place\", and so forth.\n\nThe technique is also used for second language vocabulary learning, as polyglot Timothy Doner described in his 2014 TED talk. The method is further described in Anthony Metiver's book \"How to learn and memorise German vocabulary\". What the author suggests is creating a memory palace for each letter of the German alphabet. Each memory palace then shall include a number of loci where an entry (a word or a phrase) can be stored and recalled whenever you need it.\n\nThe designation is not used with strict consistency. In some cases it refers broadly to what is otherwise known as the art of memory, the origins of which are related, according to tradition, in the story of Simonides of Ceos and the collapsing banquet hall. For example, after relating the story of how Simonides relied on remembered seating arrangements to call to mind the faces of recently deceased guests, Stephen M. Kosslyn remarks \"[t]his insight led to the development of a technique the Greeks called the method of loci, which is a systematic way of improving one's memory by using imagery.\" Skoyles and Sagan indicate that \"an ancient technique of memorization called Method of Loci, by which memories are referenced directly onto spatial maps\" originated with the story of Simonides. Referring to mnemonic methods, Verlee Williams mentions, \"One such strategy is the 'loci' method, which was developed by Simonides, a Greek poet of the fifth and sixth centuries BC.\" Loftus cites the foundation story of Simonides (more or less taken from Frances Yates) and describes some of the most basic aspects of the use of space in the art of memory. She states, \"This particular mnemonic technique has come to be called the \"method of loci\". While place or position certainly figured prominently in ancient mnemonic techniques, no designation equivalent to \"method of loci\" was used exclusively to refer to mnemonic schemes relying upon space for organization.\n\nIn other cases the designation is generally consistent, but more specific: \"The Method of Loci is a Mnemonic Device involving the creation of a Visual Map of one's house.\"\n\nThis term can be misleading: the ancient principles and techniques of the art of memory, hastily glossed in some of the works, cited above, depended equally upon images \"and\" places. The designator \"method of loci\" does not convey the equal weight placed on both elements. Training in the art or arts of memory as a whole, as attested in classical antiquity, was far more inclusive and comprehensive in the treatment of this subject.\n\nBrain scans of \"superior memorizers\", 90% of whom use the method of loci technique, have shown that it involves activation of regions of the brain involved in spatial awareness, such as the medial parietal cortex, retrosplenial cortex, and the right posterior hippocampus. The medial parietal cortex is most associated with encoding and retrieving of information. Patients who have medial parietal cortex damage have trouble linking landmarks with certain locations; many of these patients are unable to give or follow directions and often get lost. The retrosplenial cortex is also linked to memory and navigation. In one study on the effects of selective granular retrosplenial cortex lesions in rats, the researcher found that damage to the retrosplenial cortex led to impaired spatial learning abilities. Rats with damage to this area failed to recall which areas of the maze they had already visited, rarely explored different arms of the maze, almost never recalled the maze in future trials, and took longer to reach the end of the maze, as compared to rats with a fully working retrosplenial cortex.\n\nIn a classic study in cognitive neuroscience, O'Keefe and Nadel proposed \"that the hippocampus is the core of a neural memory system providing an objective spatial framework within which the items and events of an organism's experience are located and interrelated.\"\n\nIn a more recent study, memory champions during resting periods did not exhibit specific regional brain differences, but distributed functional brain network connectivity changes compared to control subjects. When volunteers trained use of the method of loci for six weeks, the training-induced changes in brain connectivity were similar to the brain network organization that distinguished memory champions from controls.\n\nFictional portrayals of the method of loci extend as far back as ancient Greek myths. The method of loci also features prominently in the BBC series \"Sherlock\", in which the titular main character uses a \"mind palace\" to store information. In the original Arthur Conan Doyle stories, Sherlock Holmes referred to his brain as an attic. In \"Hannibal Rising\" by Thomas Harris, a detailed description of Hannibal Lecter's memory palace is provided.\n"}
{"id": "47888989", "url": "https://en.wikipedia.org/wiki?curid=47888989", "title": "OTSEM", "text": "OTSEM\n\nOTSEM (\"Old Testament Studies: Epistemologies and Methods\") is a research network of several Northern European universities. Originally Nordic-German in nature, it now includes British universities as well, which gives it a Nordic-German-Anglo profile. The organization focuses on research into the Hebrew Bible and promotes the development of young scholars in particular, especially through annual seminars and special lectures. Though encompassing theological faculties and using the term Old Testament, OTSEM is non-confessional, with members from Christian (both Protestant and Catholic), Jewish, Muslim, and secular backgrounds.\n\nCurrently, OTSEM consists of sixteen institutions from eight different countries:\n\n\nThe history of OTSEM divides into three distinct periods: Proto-OTSEM, OTSEM I, and OTSEM II (the present iteration).\n\nOTSEM's antecedents reach back to the 1970s. At that time, Helsinki and Göttingen had a student and professor exchange between their theological faculties. Beginning in the 1990s, bi- and trilateral doctoral seminars took place among the various institutions that would later become members of OTSEM.\n\n\nIn addition to arranging doctoral seminars, future members also organised professorial exchanges.\n\n\nFrom 2004 through 2008, the Nordic Council (\"NordForsk\") co-funded the research network. In this period, OTSEM saw formalization and expansion. This expansion included an increase in not only the number of members and member institutions but also meetings and individual exchanges. In particular, the now formal organization commenced its annual conferences (Oslo in 2004, Göttingen in 2005, Aarhus in 2006, Helsinki in 2008, and Lund in 2008). With the end of funding from the Nordic Council came the end of OTSEM I and start of OTSEM II.\n\nSince 2009, OTSEM has been financed solely by its associated institutions. Nevertheless, the research network has continued its annual conference and individual exchanges.\n\nOTSEM members with a high international profile have included the following:\n\n\n"}
{"id": "53445996", "url": "https://en.wikipedia.org/wiki?curid=53445996", "title": "Optimot", "text": "Optimot\n\nOptimot, linguistic inquiries, is a service provided by the Directorate - General of Linguistic Policy of the Catalan Government in collaboration with the Institute for Catalan Studies and the Terminology Center TERMCAT. It consists of a search engine for linguistic information that helps to clarify doubts about the Catalan language. With Optimot different sources can be checked at the same time in an integrated way. When the search options provided by Optimot do not manage to answer the linguistic question, a personalized inquiry service can be accessed.\n\nUp to 2007, the Consortium for Linguistic Normalization, the TERMCAT Terminology Centre, the Institute of Catalan Studies, and the directorate-general of Linguistics Politics dealt with linguistic inqueries that came from general population, companies, organizations, and language professionals. In order to avoid decentralization of the linguistic enquiries and to offer a unified service, Optimot was implemented. The search engine started working in October 2007 and the personalized service began in February 2008. Mainly, Optimot was to improve quality in linguistic-inquiry service by unifying criteria, and to promote linguistic autonomy.\n\nThe sources used in the searches performed by Optimot are the following:\n\n\n"}
{"id": "3641629", "url": "https://en.wikipedia.org/wiki?curid=3641629", "title": "Overview (debate)", "text": "Overview (debate)\n\nAn overview in policy debate is part of a speech which is flagged as not responding to the line-by-line arguments on the flow. An overview may be \"global\" if presented at the beginning of a speech or \"local\" if presented at the beginning of a position.\n\nOverviews typically list the order a debater's speech will be given in. For instance, On-Case, Co-op Disadvantage, Spending Disadvantage, Weapon Disadvantage would be an expected overview.\n\nDebaters will usually inform the judge where they will be giving an overview before they start a speech because it can make it harder to flow the speech. A small minority of judges dislike this practice and will start speech time when a debater starts giving this order.\n\nMany judges dislike overviews because, since many are scripted before the round begins, they tend to be non-responsive or repetitive and are often long. However, most judges and coaches support the practice for arguments which cannot be placed anywhere on the line-by-line or that need to be flagged for their importance.\n\nThe above is also informally known as a \"road map\"\n\nAnother form of an overview is of specifically in the section of the speech(i.e. Topicality, Kritik, Disadvantage, Counter-Plan, etc). This is commonly announced before the debater's speech and will signpost an overview.\n\nThe overview usually consists of star/main arguments being made and more than likely tend to contain extended evidence or authors. This practice is used to elaborate an argument and/or bring it back up in order for the judge to be aware of the main arguments being made by a team.\n"}
{"id": "676361", "url": "https://en.wikipedia.org/wiki?curid=676361", "title": "Paraprosdokian", "text": "Paraprosdokian\n\nA paraprosdokian () is a figure of speech in which the latter part of a sentence, phrase, or larger discourse is surprising or unexpected in a way that causes the reader or listener to reframe or reinterpret the first part. It is frequently used for humorous or dramatic effect, sometimes producing an anticlimax. For this reason, it is extremely popular among comedians and satirists. Some paraprosdokians not only change the meaning of an early phrase, but they also play on the double meaning of a particular word, creating a form of syllepsis.\n\n\"Paraprosdokian\" comes from the Greek \"παρά\", meaning \"against\" and \"προσδοκία\", meaning \"expectation\". The term \"prosdokia\" (\"expectation\") occurs with the preposition \"para\" in Greek rhetorical writers of the 1st century BCE and the 1st and 2nd centuries CE, with the meaning \"contrary to expectation\" or \"unexpectedly.\" These four sources are cited under \"prosdokia\" in Liddell-Scott-Jones, Greek Lexicon. Canadian linguist and etymology author William Gordon Casselman argues that, while the word is now in wide circulation, \"paraprosdokian\" (or \"paraprosdokia\") is not a term of classical (or medieval) Greek or Latin rhetoric, but a late 20th-century neologism, citing the fact that the word does not yet appear in the Oxford English Dictionary as evidence of its late coinage. However, the word appeared in print as early as 1891 in a humorous article in \"Punch\".\n\n\n"}
{"id": "5807798", "url": "https://en.wikipedia.org/wiki?curid=5807798", "title": "Philogyny", "text": "Philogyny\n\nPhilogyny is fondness, love, or admiration towards women. Its antonym is \"misogyny\". Philogyny is not be confused with gynephilia, which is sexual attraction to women or femininity (and whose antonym is \"gynophobia\").\n\nCicero reports the word could be used in Greek philosophy to denote being overly fond of women, which was considered a disease along with misogyny.\n\nChristian Groes-Green has argued that the conceptual content of philogyny must be developed as an alternative to the concept of misogyny. Criticizing R.W. Connell's theory of hegemonic masculinities he shows how philogynous masculinities play out among youth in Maputo, Mozambique.\n\n\"Philogyny\" comes from \"philo-\" (loving) and Greek \"gynē\" (woman). The parallel Greek-based terms with respect to men (males) are philandry for \"fondness towards men\" and misandry for \"hatred of men\". Parallel terms for humanity generally are philanthropy and misanthropy.\n\n"}
{"id": "29976943", "url": "https://en.wikipedia.org/wiki?curid=29976943", "title": "Political climate", "text": "Political climate\n\nThe political climate is the aggregate mood and opinions of a political society at a particular time. It is generally used to describe when the state of mood and opinion is changing or unstable rather than in a state of equilibrium. The phrase has origins from both ancient Greece and medieval-era France.\n\nWhile the concept of a political climate has been used historically to describe both politics and public reactions to political actions in various forms, the naming of the concept by the addition of the modifier “political” to the base “climate” has been fairly recent. Public opinion is also widely used incorrectly as a synonym for political climate.\n\nAs for judging what the climate is at any given time, there is no way to know an entire country's views on certain subjects. So, polls are used to estimate what the political climate \"feels\" like on a regular basis. However, this only works to some degree as polls cannot involve the entire population at once.\n\nAccording to the Oxford English Dictionary, the base climate comes from the Middle French \"climat\", which was first used to describe a region's prevailing weather conditions around 1314. One of the first recorded uses of climate as a description of prevailing political attitudes was in The Vanity of Dogmatizing by Joseph Glanvill in 1661 where he mentions “divers Climates of Opinions”.\n\nThe modifier (\"politic\" with the \"–al\" suffix) comes originally from the ancient Greek noun \"polis\" which referred to both a Greek city state, and the ideal state or government. Over time, this evolved through the Latin noun \"politicus\" which is defined as the civil government, to the Middle French adjective \"politique\" which is the state of government, or relating to government.\n\nFrom 431 B.C.E to 404 B.C.E., ancient Greece was torn apart by the Peloponnesian War between Athens and Sparta. The war concluded with an Athenian defeat and several years of oppression by pro-Spartan rulers. By 399 B.C.E, Athens had returned to self-rule through revolution. At this time, Athens was undergoing social turmoil due to the apparent failure of democracy as an effective form of government, which created a public backlash against anything anti-democratic. \n\nSocrates—the self-described \"gadfly\" of Athens because of his practice of \"elenchos\" (critical interrogation)—was seen by many as anti-democratic and thus a traitor to Athens due to his associations with Critias and Alcibiades (the former a Spartan supported tyrant, the latter a deserter to Sparta) and his frequent praises of the Spartan and Creten governments because of their similarity to many of his philosophical opinions on government. The prevailing political climate of distrusting anything remotely anti-Athens or anti-democracy coupled with attacks from Socrates's personal enemies led to the philosopher's execution by poison in 399 B.C.E.\n\nThe phrase originates from the French term \"opinion publique\", which was first attributed to Montaigne, the father of modern Skepticism and a major figure of the French Renaissance, around 1588 C.E.\nIt is generally used to describe the overall opinion of the public body about a certain issue. The phrase is commonly used interchangeably with political climate but the two actually refer to separate concepts. Public opinion is the aggregate logical thoughts that the public thinks and expresses about an issue (which does not have to be political in nature), while political climate is what the public's emotional reaction to those logical thoughts are.\n\nDepending on the nature of the thoughts (if they are considered controversial or extreme), the emotional reaction can range from nothing to a highly violent state. Accordingly, controversial issues in the public eye are usually accompanied by or can even produce a polarizing political climate. For example, the introduction, passing, and court fight over Proposition 8 in California brought a controversial issue into the public sphere, which resulted in such a drastic change in the political climate of the United States as to produce many protests throughout the nation, some of them violent.\n\nAn opinion poll is a survey of public opinion from a particular group of people or sample. For determining the political climate, this usually would be a cross-section of the population in question. Opinion polls conduct series of questions and then extrapolate the average opinion of the sample according to their answers. However, opinion polls generally have appreciative margins of error because of the inability to survey the entire population and the improbability of surveying a perfectly random cross-section of the population. For example, the Wall Street Journal estimated in 2006 that the average margin of error is about 3-5% in opinion polls because of a wide variety of potential inaccuracies such as response bias and selection bias.\n\nOpinion polls are also known to be entirely incorrect when predicting the outcome of certain events. The best-known example of this is the 1948 US presidential election, in which the prediction was that Thomas Dewey would easily defeat Harry Truman. Major polling organizations, including Gallup and Roper, indicated a landslide victory for Dewey when, in fact, Truman was the victor in a close election and kept the presidency. 2016 saw the validity of opinion polls enter the debate once more, as polls in both the UK \"Brexit\" referendum and the US presidential election were ultimately shown to be largely incorrect. Polls widely predicted a win for Democratic candidate Hillary Clinton, while instead Republican candidate Donald J. Trump won the Electoral College and, therefore, the presidency.\n\n\n"}
{"id": "508376", "url": "https://en.wikipedia.org/wiki?curid=508376", "title": "Proto-Indo-European mythology", "text": "Proto-Indo-European mythology\n\n</noinclude>\nProto-Indo-European mythology is the body of myths and stories associated with the Proto-Indo-Europeans. Although these stories are not directly attested, they have been reconstructed by scholars of comparative mythology based on the similarities in the belief systems of various Indo-European peoples.\n\nVarious schools of thought exist regarding the precise nature of Proto-Indo-European mythology, which do not always agree with each other. Vedic mythology, Roman mythology, and Norse mythology are the main mythologies normally used for comparative reconstruction, though they are often supplemented with supporting evidence from the Baltic, Celtic, Greek, Slavic, and Hittite traditions as well.\n\nThe Proto-Indo-European pantheon includes well-attested deities such as *\"Dyḗus Pḥtḗr\", the god of the daylit skies, his daughter *\"Héusōs\", the goddess of the dawn, the divine twins, and the storm god *\"Perkunos\". Other probable deities include \"*Péhusōn\", a pastoral god, and \"*Sehul\", a female solar deity.\n\nWell-attested myths of the Proto-Indo-Europeans include a myth involving a storm god who slays a multi-headed serpent that dwells in water and a creation story involving two brothers, one of whom sacrifices the other to create the world. The Proto-Indo-Europeans may have believed that the Otherworld was guarded by a watchdog and could only be reached by crossing a river. They also may have believed in a world tree, bearing fruit of immortality, either guarded by or gnawed on by a serpent or dragon, and tended by three goddesses who spun the thread of life.\n\nThe mythology of the Proto-Indo-Europeans is not directly attested and it is difficult to match their language to archaeological findings related to any specific culture from the Chalcolithic. Nonetheless, scholars of comparative mythology have attempted to reconstruct aspects of Proto-Indo-European mythologies based on the existence of similarities among the deities, religious practices, and myths of various Indo-European peoples. This method is known as the comparative method. Different schools of thought have approached the subject of Proto-Indo-European mythology from different angles. The Meteorological School holds that Proto-Indo-European mythology was largely centered around deified natural phenomena such as the sky, the Sun, the Moon, and the dawn. This meteorological interpretation was popular among early scholars, but has lost a considerable degree of scholarly support in recent years. The Ritual School, on the other hand, holds that Proto-Indo-European myths are best understood as stories invented to explain various rituals and religious practices. Bruce Lincoln, a member of the Ritual School, argues that the Proto-Indo-Europeans believed that every sacrifice was a reenactment of the original sacrifice performed by the founder of the human race on his twin brother. The Functionalist School holds that Proto-Indo-European society and, consequently, their mythology, was largely centered around the trifunctional system proposed by Georges Dumézil, which holds that Proto-Indo-European society was divided into three distinct social classes: farmers, warriors, and priests. The Structuralist School, by contrast, argues that Proto-Indo-European mythology was largely centered around the concept of dualistic opposition. This approach generally tends to focus on cultural universals within the realm of mythology, rather than the genetic origins of those myths, but it also offers refinements of the Dumézilian trifunctional system by highlighting the oppositional elements present within each function, such as the creative and destructive elements both found within the role of the warrior.\n\nOne of the earliest attested and thus most important of all Indo-European mythologies is Vedic mythology, especially the mythology of the Rigveda, the oldest of the Vedas. Early scholars of comparative mythology such as Max Müller stressed the importance of Vedic mythology to such an extent that they practically equated it with Proto-Indo-European myth. Modern researchers have been much more cautious, recognizing that, although Vedic mythology is still central, other mythologies must also be taken into account.\n\nAnother of the most important source mythologies for comparative research is Roman mythology. Contrary to the frequent erroneous statement made by some authors that \"Rome has no myth\", the Romans possessed a very complex mythological system, parts of which have been preserved through the characteristic Roman tendency to rationalize their myths into historical accounts. Despite its relatively late attestation, Norse mythology is still considered one of the three most important of the Indo-European mythologies for comparative research, simply due to the vast bulk of surviving Icelandic material.\n\nBaltic mythology has also received a great deal of scholarly attention, but has so far remained frustrating to researchers because the sources are so comparatively late. Nonetheless, Latvian folk songs are seen as a major source of information in the process of reconstructing Proto-Indo-European myth. Despite the popularity of Greek mythology in western culture, Greek mythology is generally seen as having little importance in comparative mythology due to the heavy influence of Pre-Greek and Near Eastern cultures, which overwhelms what little Indo-European material can be extracted from it. Consequently, Greek mythology received minimal scholarly attention until the mid 2000s.\n\nAlthough Scythians are considered relatively conservative in regards to Proto-Indo-European cultures, retaining a similar lifestyle and culture, their mythology has very rarely been examined in an Indo-European context and infrequently discussed in regards to the nature of the ancestral Indo-European mythology. At least three deities, Tabiti, Papaios and Api, are generally interpreted as having Indo-European origins, while the remaining have seen more disparate interpretations. Influence from Siberian, Turkic and even Near Eastern beliefs, on the other hand, are more widely discussed in literature.\n\nLinguists are able to reconstruct the names of some deities in the Proto-Indo-European language (PIE) from many types of sources. Some of the proposed deity names are more readily accepted among scholars than others.\n\nThe term for \"a god\" was \"*deiwos\", reflected in Hittite, \"sius\"; Latin, \"deus\", \"divus\"; Sanskrit, \"deva\"; Avestan, \"daeva\" (later, Persian, \"div\"); Welsh, \"duw\"; Irish, \"dia\"; Old Norse, \"tívurr\"; Lithuanian, \"Dievas\"; Latvian, \"Dievs\".\n\nThe head deity of the Proto-Indo-European pantheon was the god *\"Dyḗus Pḥtḗr\", whose name literally means \"Sky Father\". He is believed to have been regarded as the god of the daylit skies. He is, by far, the most well-attested of all the Proto-Indo-European deities. The Greek god Zeus, the Roman god Jupiter, and the Illyrian god Dei-Pátrous all appear as the head gods of their respective pantheons. The Norse god Týr, however, seems to have been demoted to the role of a minor war-deity prior to the composition of the earliest Germanic texts. *\"Dyḗus Pḥtḗr\" is also attested in the Rigveda as Dyáus Pitā, a minor ancestor figure mentioned in only a few hymns. The names of the Latvian god Dievs and the Hittite god Attas Isanus do not preserve the exact literal translation of the name *\"Dyḗus Pḥtḗr\", but do preserve the general meaning of it.\n\n\"*Dyḗus Pḥtḗr\" may have had a consort who was an earth goddess. This possibility is attested in the Vedic pairing of Dyáus Pitā and Prithvi Mater, the Roman pairing of Jupiter and Tellus Mater from Macrobius's \"Saturnalia\", and the Norse pairing of Odin and Jörð. Odin is not a reflex of *\"Dyḗus Pḥtḗr\", but his cult may have subsumed aspects of an earlier chief deity who was. This pairing may also be further attested in an Old English ploughing prayer and in the Greek pairings of Ouranos and Gaia and Zeus and Demeter.\n\n\n\"*Sehul\" and \"*Mehnot\" are reconstructed as the Proto-Indo-European goddess of the Sun and god of the Moon respectively. \"*Sehul\" is reconstructed based on the Greek god Helios, the Roman god Sol, the Celtic goddess Sul/Suil, the North Germanic goddess Sól, the Continental Germanic goddess *Sowilō, the Hittite goddess \"UTU-liya\", the Zoroastrian Hvare-khshaeta and the Vedic god Surya.\n\n\"*Mehnot-\" is reconstructed based on the Norse god Máni, the Slavic god Myesyats, and the Lithuanian god *Meno, or Mėnuo (Mėnulis). They are often seen as the twin children of various deities, but in fact the sun and moon were deified several times and are often found in competing forms within the same language.\n\nThe usual scheme is that one of these celestial deities is male and the other female, though the exact gender of the Sun or Moon tends to vary among subsequent Indo-European mythologies. The original Indo-European solar deity appears to have been female, a characteristic not only supported by the higher number of sun goddesses in subsequent derivations (feminine Sól, Saule, Sulis, Solntse—not directly attested as a goddess, but feminine in gender — Étaín, Grían, Aimend, Áine, and Catha versus masculine Helios, Surya, Savitr, Usil, and Sol) (Hvare-khshaeta is of neutral gender), but also by vestiges in mythologies with male solar deities (Usil in Etruscan art is depicted occasionally as a goddess, while solar characteristics in Athena and Helen of Troy still remain in Greek mythology). The original Indo-European lunar deity appears to have been masculine, with feminine lunar deities like Selene, Minerva, and Luna being a development exclusive to the eastern Mediterranean. Even in these traditions, remnants of male lunar deities, like Menelaus, remain.\n\nAlthough the sun was personified as an independent, female deity, the Proto-Indo-Europeans also visualized the sun as the eye of *\"Dyḗus Pḥtḗr\", as seen in various reflexes: Helios as the eye of Zeus, Hvare-khshaeta as the eye of Ahura Mazda, and the sun as \"God's eye\" in Romanian folklore. The names of Celtic sun goddesses like Sulis and Grian may also allude to this association; the words for \"eye\" and \"sun\" are switched in these languages, hence the name of the goddesses.\n\nThe Horse Twins are a set of twin brothers found throughout nearly every Indo-European pantheon who usually have a name that means 'horse' \"*ekwa-\", but the names are not always cognate and no Proto-Indo-European name for them can be reconstructed. In most Indo-European pantheons, the Horse Twins are brothers of the Sun Maiden or Dawn goddess, and sons of the sky god.\n\nThey are reconstructed based on the Vedic Ashvins, the Lithuanian Ašvieniai, the Latvian Dieva deli, the Greek Dioskouroi (Kastor and Polydeukes), the Roman Dioscuri (Castor and Pollux), and the Old English Hengist and Horsa (whose names mean \"stallion\" and \"horse\"). References from the Greek writer Timaeus indicate that the Celts may have had a set of horse twins as well. The Welsh Brân and Manawydan may also be related. The horse twins may have been based on the morning and evening star (the planet Venus) and they often have stories about them in which they \"accompany\" the Sun goddess, because of the close orbit of the planet Venus to the sun.\n\nThe Proto-Indo-European Creation myth seems to have involved two key figures: *\"Manu-\" (\"Man\"; Indic Manu; Germanic Mannus) and his twin brother *\"Yemo-\" (\"Twin\"; Indic Yama; Germanic Ymir). Reflexes of these two figures usually fulfill the respective roles of founder of the human race and first human to die.\n\n\nSome authors have proposed \"*Neptonos\" or *\"Hepom Nepōts\" as the Proto-Indo-European god of the waters. The name literally means \"Grandson [or \"Nephew\"] of the Waters.\" Philologists reconstruct his name from that of the Vedic god Apám Nápát, the Roman god Neptūnus, and the Old Irish god Nechtain. Although such a god has been solidly reconstructed in Proto-Indo-Iranian religion, Mallory and Adams nonetheless still reject him as a Proto-Indo-European deity on linguistic grounds.\n\nA river goddess *\"Dehnu-\" has been proposed based on the Vedic goddess Dānu, the Irish goddess Danu, the Welsh goddess Don and the names of the rivers Danube, Don, Dnieper, and Dniester. Mallory and Adams, however, dismiss this reconstruction, commenting that it does not have any evidence to support it.\n\nSome have also proposed the reconstruction of a sea god named *\"Trihtōn\" based on the Greek god Triton and the Old Irish word \"trïath\", meaning \"sea.\" Mallory and Adams reject this reconstruction as having no basis, asserting that the \"lexical correspondence is only just possible and with no evidence of a cognate sea god in Irish.\"\n\n\"*Péhusōn\", a pastoral deity, is reconstructed based on the Greek god Pan and the Vedic god Pūshān. Both deities are closely affiliated with goats and were worshipped as pastoral deities. The minor discrepancies between the two deities can be easily explained by the possibility that many attributes originally associated with Pan may have been transferred over to his father Hermes. The association between Pan and Pūshān was first identified in 1924 by the German scholar Hermann Collitz.\n\nIn 1855, Adalbert Kuhn suggested that the Proto-Indo-Europeans may have believed in a set of helper deities, whom he reconstructed based on the Germanic elves and the Hindu ribhus. Though this proposal is often mentioned in academic writings, very few scholars actually accept it. There may also have been a female cognate akin to the Greco-Roman nymphs, Slavic vilas, the Huldra of Germanic folklore, and the Hindu Apsaras.\n\nIt is highly probable that the Proto-Indo-Europeans believed in three fate goddesses who spun the destinies of mankind. Although such fate goddesses are not directly attested in the Indo-Aryan tradition, the Atharvaveda does contain an allusion comparing fate to a warp. Furthermore, the three Fates appear in nearly every other Indo-European mythology. The earliest attested set of fate goddesses are the Gulses in Hittite mythology, who were said to preside over the individual destinies of human beings. They often appear in mythical narratives alongside the goddesses Papaya and Istustaya, who, in a ritual text for the foundation of a new temple, are described sitting holding mirrors and spindles, spinning the king's thread of life. In the Greek tradition, the Moirai (\"Apportioners\") are mentioned dispensing destiny in both the \"Iliad\" and the \"Odyssey\", in which they are given the epithet Κλῶθες (\"Klothes\", meaning \"Spinners\"). In Hesiod's \"Theogony\", the Moirai are said to \"give mortal men both good and ill\" and their names are listed as Klotho (\"Spinner\"), Lachesis (\"Apportioner\"), and Atropos (\"Inflexible\"). In his \"Republic\", Plato records that Klotho sings of the past, Lachesis of the present, and Atropos of the future. In Roman legend, the Parcae were three goddesses who presided over the births of children and whose names were Nona (\"Ninth\"), Decuma (\"Tenth\"), and Morta (\"Death\"). They too were said to spin destinies, although this may have been due to influence from Greek literature.\n\nIn the Old Norse \"Völuspá\" and \"Gylfaginning\", the Norns are three cosmic goddesses of fate who are described sitting by the well of Urðr at the foot of the world tree Yggdrasil. In Old Norse texts, the Norns are frequently conflated with Valkyries, who are sometimes also described as spinning. Old English texts, such as \"Rhyme Poem\" 70, and \"Guthlac\" 1350 f., reference Wyrd as a singular power that \"weaves\" destinies. Later texts mention the Wyrds as a group, with Geoffrey Chaucer referring to them as \"the Werdys that we clepyn Destiné\" in \"The Legend of Good Women\". A goddess spinning appears in a bracteate from southwest Germany and a relief from Trier shows three mother goddesses, with two of them holding distaffs. Tenth-century German ecclesiastical writings denounce the popular belief in three sisters who determined the course of a man's life at his birth. An Old Irish hymn attests to seven goddesses who were believed to weave the thread of destiny, which demonstrates that these spinster fate-goddesses were present in Celtic mythology as well. A Lithuanian folktale recorded in 1839 recounts that a man's fate is spun at his birth by seven goddesses known as the \"deivės valdytojos\" and used to hang a star in the sky; when he dies, his thread snaps and his star falls as a meteor. In Latvian folk songs, a goddess called the Láima is described as weaving a child's fate at its birth. Although she is usually only one goddess, the Láima sometimes appears as three. The three spinning fate goddesses appear in Slavic traditions in the forms of the Russian Rožanicy, the Czech Sudičky, the Bulgarian Narenčnice or Urisnice, the Polish Rodzanice, the Croatian Rodjenice, the Serbian Sudjenice, and the Slovene Rojenice. Albanian folk tales speak of the Fatit, three old women who appear three days after a child is born and determine its fate, using language reminiscent of spinning.\n\nAlthough the name of a particular Proto-Indo-European smith god cannot be linguistically reconstructed, it is highly probable that the Proto-Indo-Europeans had a smith deity of some kind, since smith gods occur in nearly every Indo-European culture, with examples including the Hittite god Hasammili, the Vedic god Tvastr, the Greek god Hephaestus, the Germanic villain Wayland the Smith, and the Ossetian culture figure Kurdalagon. Many of these smith figures share certain characteristics in common. Hephaestus, the Greek god of blacksmiths, and Wayland the Smith, a nefarious blacksmith from Germanic mythology, are both described as lame. Additionally, Wayland the Smith and the Greek mythical inventor Daedalus both escape imprisonment on an island by fashioning sets of mechanical wings from feathers and wax and using them to fly away.\n\nThe Proto-Indo-Europeans may have had a goddess who presided over the trifunctional organization of society. Various epithets of the Iranian goddess Anahita and the Roman goddess Juno provide sufficient evidence to solidly attest that she was probably worshipped, but no specific name for her can be lexically reconstructed. Vague remnants of this goddess may also be preserved in the Greek goddess Athena.\n\nSome scholars have proposed a war god *\"Māwort-\" based on the Roman god Mars and the Vedic Marutás, companions of the war-god Indra. Mallory and Adams, however, reject this reconstruction on linguistic grounds. Likewise, some researchers have found it more plausible that Mars was originally a storm deity, while this cannot be said for Ares.\n\nOne common myth found in nearly all Indo-European mythologies is a battle ending with a hero or god slaying a serpent or dragon of some sort. Although the details of story often vary widely, in all iterations, several features remain remarkably the same. In iterations of the story, the serpent is usually associated with water in some way. The hero of the story is usually a thunder-god or a hero who is somehow associated with thunder. The serpent is usually multi-headed, or else \"multiple\" in some other way.\n\nIn Hittite mythology, the storm god Tarhunt slays the giant serpent Illuyanka. In the Rigveda, the god Indra slays the multi-headed serpent Vritra, which had been causing a drought. In the \"Bhagavata Purana\", Krishna slays the serpent Kāliyā.\n\nSeveral variations of the story are also found in Greek mythology as well. The story is attested in the legend of Zeus slaying the hundred-headed Typhon from Hesiod's \"Theogony\", but it is also in the myths of the slaying of the nine-headed Lernaean Hydra by Heracles and the slaying of Python by Apollo. The story of Heracles's theft of the cattle of Geryon is probably also related. Although Heracles is not usually thought of as a storm deity in the conventional sense, he bears many attributes held by other Indo-European storm deities, including physical strength and a knack for violence and gluttony.\n\nThe original Proto-Indo-European myth is also reflected in Germanic mythology. In Norse mythology, Thor, the god of thunder, slays the giant serpent Jörmungandr, which lived in the waters surrounding the realm of Midgard. Other dragon-slaying myths are also found in the Germanic tradition. In the \"Völsunga saga\", Sigurd slays the dragon Fafnir and, in \"Beowulf\", the eponymous hero slays a different dragon.\n\nReflexes of the Proto-Indo-European dragon-slaying myth are found throughout other branches of the language family as well. In Zoroastrianism and Persian mythology, Fereydun, and later Garshasp, slays Zahhak. In Slavic mythology, Perun, the god of storms, slays Veles and Dobrynya Nikitich slays the three-headed dragon Zmey. In Armenian mythology, the god Vahagn slays the dragon Vishap. In Romanian folklore, Făt-Frumos slays the fire-spitting monster Zmeu. In Celtic mythology, Dian Cecht slays Meichi. The myth is believed to have symbolized a clash between forces of order and chaos. In every version of the story, the dragon or serpent always loses, although in some mythologies, such as the Norse Ragnarök myth, the hero or god dies as well.\n\nThe analysis of different Indo-European tales indicates that the Proto-Indo-Europeans believed there were two progenitors of mankind: *' (\"Man\") and *' (\"Twin\"), his twin brother. A reconstructed creation myth involving the two is given by David W. Anthony, attributed in part to Bruce Lincoln: Manu and Yemo traverse the cosmos, accompanied by the primordial cow, and finally decide to create the world. To do so, Manu sacrifices either Yemo or the cow, and with help from the sky father, the storm god and the divine twins, forges the earth from the remains. Manu thus becomes the first priest and establishes the practice of sacrifice. The sky gods then present cattle to the third man, *', who loses it to the three-headed serpent *', but eventually overcomes this monster either alone or aided by the sky father. Trito is now the first warrior and ensures that the cycle of mutual giving between gods and humans may continue. Reflexes of *Manu include Indic Manu, Germanic Mannus; of Yemo, Indic Yama, Avestan Yima, Norse Ymir, possibly Roman Remus (< earlier Old Latin \"*Yemos\").\n\nThe early \"history\" of Rome is widely recognized as a historicized retelling of various old myths. Romulus and Remus are twin brothers from Roman mythology who both have stories in which they are killed. The Roman writer Livy reports that Remus was believed to have been killed by his brother Romulus at the founding of Rome when they entered into a disagreement about which hill to build the city on. Later, Romulus himself is said to have been torn limb-from-limb by a group of senators. Both of these myths are widely recognized as historicized remnants of the Proto-Indo-European creation story.\n\nThe Germanic languages have information about both Ymir and Mannus (reflexes of \"*Yemo-\" and \"*Manu-\" respectively), but they never appear together in the same myth. Instead, they only occur in myths widely separated by both time and circumstances. In chapter two of his book \"Germania\", which was written in Latin in around 98 A.D., the Roman writer Tacitus claims that Mannus, the son of Tuisto, was the ancestor of the Germanic peoples. This name never recurs anywhere in later Germanic literature, but one proposed meaning of the continental Germanic tribal name \"Alamanni\" is \"Mannus' own people\" (\"all-men\" being another scholarly etymology).\n\nAnother important possible myth is the myth of the fire in the waters, a myth which centers around the possible deity *\"Hepom Nepōts\", a fiery deity who dwells in water. In the Rigveda, the god Apám Nápát is envisioned as a form of fire residing in the waters. In Celtic mythology, a well belonging to the god Nechtain is said to blind all those who gaze into it. In an old Armenian poem, a small reed in the middle of the sea spontaneously catches fire and the hero Vahagn springs forth from it with fiery hair and a fiery beard and eyes that blaze as suns. In a ninth-century Norwegian poem by the poet Thiodolf, the name \"sǣvar niþr\", meaning \"grandson of the sea,\" is used as a kenning for fire. Even the Greek tradition contains possible allusions to the myth of a fire-god dwelling deep beneath the sea. The phrase \"\"νέποδες καλῆς Ἁλοσύδνης\",\" meaning \"descendants of the beautiful seas,\" is used in \"The Odyssey\" 4.404 as an epithet for the seals of Proteus.\n\nJaan Puhvel notes similarities between the Norse myth in which the god Týr inserts his hand into the wolf Fenrir's mouth while the other gods bind him with Gleipnir, only for Fenrir to bite off Týr's hand when he discovers he cannot break his bindings, and the Iranian myth in which Jamshid rescues his brother's corpse from Ahriman's bowels by reaching his hand up Ahriman's anus and pulling out his brother's corpse, only for his hand to become infected with leprosy. In both accounts, an authority figure forces the evil entity into submission by inserting his hand into the being's orifice (in Fenrir's case the mouth, in Ahriman's the anus) and losing it. Fenrir and Ahriman fulfill different roles in their own mythological traditions and are unlikely to be remnants of a Proto-Indo-European \"evil god\"; nonetheless, it is clear that the \"binding myth\" is of Proto-Indo-European origin.\n\nIn the cosmogonic myths of many Indo-European cultures a Cosmic Egg symbolizes the primordial state from which the universe arises.\n\nMost Indo-European traditions contain some kind of Underworld or Afterlife. It is possible that the Proto-Indo-Europeans may have believed that, in order to reach the Underworld, one needed to cross a river, guided by an old man (\"*ĝerhont-\"). The Greek tradition of the dead being ferried across the river Styx by Charon is probably a reflex of this belief. The idea of crossing a river to reach the Underworld is also present throughout Celtic mythologies. Several Vedic texts contain references to crossing a river in order to reach the land of the dead and the Latin word \"tarentum\" meaning \"tomb\" originally meant \"crossing point.\" In Norse mythology, Hermóðr must cross a bridge over the river Giöll in order to reach Hel. In Latvian folk songs, the dead must cross a marsh rather than a river. Traditions of placing coins on the bodies of the deceased in order to pay the ferryman are attested in both ancient Greek and early modern Slavic funerary practices. It is also possible that the Proto-Indo-Europeans may have believed that the Underworld was guarded by some kind of watchdog, similar to the Greek Cerberus, the Hindu Śárvara, or the Norse Garmr.\n\nThe Proto-Indo-Europeans may have believed in some kind of world tree. It is also possible that they may have believed that this tree was either guarded by or under constant attack from some kind of dragon or serpent. In Norse mythology, the cosmic tree Yggdrasil is tended by the three Norns while the dragon Nidhogg gnaws at its roots. In Greek mythology, the tree of the golden apples in the Garden of the Hesperides is tended by the three Hesperides and guarded by the hundred-headed dragon Ladon. In Indo-Iranian texts, there is a mythical tree dripping with Soma, the immortal drink of the gods and, in later Pahlavi sources, a malicious lizard is said to lurk at the bottom of it.\n\n\n"}
{"id": "34531625", "url": "https://en.wikipedia.org/wiki?curid=34531625", "title": "Religious Research Association", "text": "Religious Research Association\n\nThe Religious Research Association is a scholarly association of researchers and religious professionals.\n\nIt was created in 1951 as the Religious Research Fellowship, although it existed informally as far back as the 1920s as a partnership between the Institute of Social and Religious Research and the Federal Council of Churches. Since 1958, it has held an annual lecture series in the name of H. Paul Douglass. Since the 1970s, it has met annually with the Society for the Scientific Study of Religion.\n\nIt publishes the \"Review of Religious Research\" four times a year (September, December, March and June). It contains articles, book reviews and reports on research projects.\n"}
{"id": "181885", "url": "https://en.wikipedia.org/wiki?curid=181885", "title": "Ribaldry", "text": "Ribaldry\n\nRibaldry, or blue comedy, is humorous entertainment that ranges from bordering on indelicacy to gross indecency. It is also referred to as \"bawdiness\", \"gaminess\" or \"bawdy\".\n\nSex is presented in ribald material more for the purpose of poking fun at the foibles and weaknesses that manifest themselves in human sexuality, rather than to present sexual stimulation either excitingly or artistically. Also, ribaldry may use sex as a metaphor to illustrate some non-sexual concern, in which case ribaldry may verge on the territory of satire.\n\nLike any humour, ribaldry may be read as conventional or subversive. Ribaldry typically depends on a shared background of sexual conventions and values, and its comedy generally depends on seeing those conventions broken.\n\nThe ritual taboo-breaking that is a usual counterpart of ribaldry underlies its controversial nature and explains why ribaldry is sometimes a subject of censorship. Ribaldry, whose usual aim is \"not\" \"merely\" to be sexually stimulating, often does address larger concerns than mere sexual appetite. However, being presented in the form of comedy, these larger concerns may be overlooked by censors.\n\nRibaldry differs from black comedy (or gallows humor) in that black comedy deals with topics which would normally considered painful or frightening whereas ribaldry deals with topics that would only be considered offensive.\n\nRibaldry is present to some degree in every culture and has likely been around for all of human history. Works like \"Lysistrata\" by Aristophanes, \"Menaechmi\" by Plautus, \"Cena Trimalchionis\" by Petronius, and \"The Golden Ass\" of Apuleius are ribald classics from ancient Greece and Rome. Geoffrey Chaucer's \"The Miller's Tale\" from his \"Canterbury Tales\" and \"The Crabfish\", one of the oldest English traditional ballads, are classic examples. The Frenchman François Rabelais showed himself to be a master of ribaldry (technically called grotesque body) in his \"Gargantua\" and other works. \"The Life and Opinions of Tristram Shandy, Gentleman\" by Laurence Sterne and \"The Lady's Dressing Room\" by Jonathan Swift are also in this genre; as is Mark Twain's long-suppressed \"1601\".\n\nAnother example of ribaldry is \"De Brevitate Vitae\", a song which in many European-influenced universities is both a student beer-drinking song and an anthem sung by official university choirs at public graduation ceremonies. The private and public versions of the song contain vastly different words. More recent works like \"Candy\", \"Barbarella\", \"L'Infermiera\", the comedic works of Russ Meyer, \"Little Annie Fanny\" and John Barth's \"The Sot-Weed Factor\" are probably better classified as ribaldry than as either pornography or erotica.\n\nA bawdy song is a humorous song that emphasises sexual themes and is often rich with innuendo. Historically these songs tend to be confined to groups of young males, either as students or in an environment where alcohol is flowing freely. An early collection was \"Wit and Mirth, or Pills to Purge Melancholy\", edited by Thomas D'Urfey and published between 1698 and 1720. Selected songs from \"Wit and Mirth\" have been recorded by the City Waites and other singers. Sailor's songs tend to be quite frank about the exploitative nature of the relationship between men and women. There are many examples of folk songs in which a man encounters a woman in the countryside. This is followed by a short conversation, and then sexual intercourse, e.g. \"The Game of All Fours\". Neither side demonstrates any shame or regret. If the woman becomes pregnant, the man will not be there anyway. Rugby songs are often bawdy. Examples of bawdy folk songs are: \"Seventeen Come Sunday\" and \"The Ballad of Eskimo Nell\". Robert Burns compiled \"The Merry Muses of Caledonia\" (the title is not Burns's), a collection of bawdy lyrics that were popular in the music halls of Scotland as late as the 20th century. In modern times Hash House Harriers have taken on the role of tradition-bearers for this kind of song. \"The Unexpurgated Folk Songs of Men\" (Arhoolie 4006) is a gramophone record containing a collection of American bawdy songs recorded in 1959.\n\nBlue comedy is comedy that is off-color, risqué, indecent or profane, largely about sex. It often contains profanity or sexual imagery that may shock and offend some audience members.\n\n\"Working blue\" refers to the act of using curse words and discussing things that people do not discuss in \"polite society\". A \"blue comedian\" or \"blue comic\" is a comedian who usually performs risqué routines layered with curse words. \n\nThere is a common belief that comedian Max Miller (1894–1963) coined the phrase, after his stage act which involved telling jokes from either a white book or a blue book, chosen by audience preference (the blue book contained ribald jokes). This is not so, as the \"Oxford English Dictionary\" contains earlier references to the use of blue to mean ribald: 1890 \"Sporting Times\" 25 Jan. 1/1 \"\"Shifter wondered whether the damsel knew any novel blue stories.\" and 1900 \"Bulletin\" (Sydney) 20 Oct. 12/4 \"Let someone propose to celebrate Chaucer by publicly reading some of his bluest productions unexpurgated. The reader would probably be locked up.\"\"\n\nPrivate events at show business clubs such as the Bob Saget Club and The Masquers often showed this blue side of otherwise cleancut Bob Saget; a recording survives of one Masquers roast from the 1950s with Jack Benny, George Jessel, George Burns, and Art Linkletter all using highly risqué material and obscenities. Many comedians who are normally family-friendly might choose to work blue when off-camera or in an adult-oriented environment; Bob Saget exemplifies this dichotomy. Bill Cosby's 1969 record album records both his family-friendly evening standup comedy show, and his blue midnight show, which included a joke about impregnating his wife \"right through the old midnight trampoline\" (her diaphragm) and other sexual references.\n\nSome comedians build their careers on blue comedy. Among the best known of these are Redd Foxx, Lawanda Page, and the team of Leroy and Skillet, all of whom later performed on the family-friendly television show Sanford and Son. Page, Leroy, and Skillet specialized in a particular African American form of blue spoken word recitation called signifying or toasting. Dave Attell has also been described by his peers as one of the greatest modern-day blue comics. \n\nOn talk radio in the United States, many commentators use blue comedy in their political programs. Examples include Neal Boortz, Eric Von Haessler, Phil Hendrie and Steve Morrison.\n\n\n\n"}
{"id": "1495838", "url": "https://en.wikipedia.org/wiki?curid=1495838", "title": "Romance studies", "text": "Romance studies\n\nRomance studies is an academic discipline that covers the study of the languages, literatures, and cultures of areas that speak a Romance language. Romance studies departments usually include the study of Spanish, French, Italian, and Portuguese. Additional languages of study include Romanian and Catalan, on one hand, and culture, history, and politics on the other hand.\n\nBecause most places in Latin America speak a Romance language, Latin America is also studied in Romance studies departments. As a result, non-Romance languages in use in Latin America, such as Quechua, are sometimes also taught in Romance studies departments.\n\nRomance studies departments differ from single- or two-language departments in that they attempt to break down the barriers in scholarship among the various languages, through interdisciplinary or comparative work. These departments differ from Romance \"language\" departments in that they place a heavier emphasis on connections between language and literature, among others.\n\n"}
{"id": "28100194", "url": "https://en.wikipedia.org/wiki?curid=28100194", "title": "Society for Disability Studies", "text": "Society for Disability Studies\n\nThe Society for Disability Studies is an international academic network of disability studies practitioners. It often abbreviates its name to SDS, though that abbreviation continues to be used by academics and political scientists to describe the Students for a Democratic Society organization in the United States. The society's overall goal is to promote disability studies as a serious academic discipline on par with philosophy, the social sciences, and similar fields.\n\nIn 1993 the society adopted an official definition of \"Disability Studies\":\n\"... examines the policies and practices of all societies to understand the social, rather than the physical or psychological determinants of the experience of disability. Disability Studies has been developed to disentangle impairments from the myths, ideology and stigma that influence social interaction and social policy. The scholarship challenges the idea that the economic and social statuses and the assigned roles of people with disabilities are the inevitable outcomes of their condition.\"\n\nThe organization was founded in 1982 first as the Section for the Study of Chronic Illness, Impairment, and Disability (SSCIID), and renamed Society for Disability Studies in 1986. Its founders are Daryl Evans, Nora Groce, Steve Hey, Gary Kiger, John Seidel, Jessica Scheer and Irving Kenneth Zola (1935-1994). The Society for Disability Studies is a 501(c)(3) not-for-profit organization.\n\nThe Society maintains affiliation status with the Western Social Science Association (WSSA) through its Chronic Disease and Disability section. Currently, the SDS has hundreds of members both nationally and internationally who continue to make disability studies a part of academic conversations.\n\nThe Society for Disability Studies holds an annual conference in June and publishes a quarterly peer-reviewed journal, the \"Disability Studies Quarterly\". The journal is published exclusively online. SDS has created a good model to follow when approaching publishers about their accessibility.\n\nIn 2015, Adam Newman organized the \"Digital Access Facilitation Team\" (DAFT) to make the 2015 annual conference of the Society for Disability Studies more accessible for a wider range of attendees. DAFT is coordinated by the Society's Student Caucus, whose members are a group of 25-30 students of SDS. Working in teams of two, members of DAFT were live-tweeting every session, contingent upon the consent of presenters. Live-tweeting all sessions and following standards for that emerging media, allowed a new way of producing accessibility for the disability community. In the Society for Disability Studies, there are a number of caucuses which “designate groups that are under-represented within society or SDS as an organization.” DAFT is composed entirely of students (undergraduate, graduate, professional) who work on behalf of the interest and needs of students. \n\nThere are several options for membership opportunities, even if someone is unable to pay the membership fees they will not be turned away; \"No one is denied membership in SDS due to an inability to pay an established membership fee.\"\n\nTwo awards have been established by the society \"to honor individuals who have shown dedication to Disability studies\": the \"Senior Scholar Award\" and the \"Irving K. Zola Award for Emerging Scholars in Disability Studies\". The \"Senior Scholar Award\" is awarded to individuals who have made significant contributions to the field of disability studies. Past award winners: Devva Kasnitz (2014), Richard Scotch (2013), Carol Gill (2012), Tobin Siebers (2011), Rosemarie Garland Thomson (2010), Elizabeth Depoy and Stephen Gilson (2009), and Steven J. Taylor (2008). On the contrary, the \"Irving K. Zola Award for Emerging Scholars in Disability Studies\" is awarded to an up and coming individual who also has made significant contributions to the field of disability studies.\n\n\nBelow is a list of the current and past presidents of the SDS.\n\n"}
{"id": "44744681", "url": "https://en.wikipedia.org/wiki?curid=44744681", "title": "Special measures for gender equality in the United Nations", "text": "Special measures for gender equality in the United Nations\n\nThe United Nations Secretariat, in September 1999, promulgated Administrative Instruction (AI) on \"Special Measures for the Achievement Of Gender Equality\" (ST/AI/1999/9 also Gender Equality A/I), to strengthen and expedite measures to achieve gender equality, especially in posts in the Professional category.<ref name=\"ST /AI/1999/9-1\"></ref> Gender Equality A/I (ST/AI/1999/9), which superseded ST/AI/412 of 5 January 1996, came into effect on 1 October 1999. In 2012, Ban Ki Moon, the Secretary General of the United Nations, in his Annual Reports to the General Assembly, titled \"Improvement of the status of women in the United Nations system\" stated that Special Measures are \"procedures designed to accelerate the achievement of gender parity at the Professional levels and above\"<ref name=\"A /67/347\"></ref> and that the aim of these procedures was to ensure \"gender balance in recruitment and promotion\" and rectify \"past and current forms and effects of discrimination against women\" The Secretary General reiterated that Special measures for gender equality would remain in effect until the \"goal of gender parity is achieved, and would be sustained for a period of time\".\n\nGender Equality A/I echoes the goals of, and is in conformity with, mandate of Articles 8 and 101 of the Charter of the United Nations, and Article 4 paragraph 1 of the \"Convention on the Elimination of All Forms of Discrimination against Women (CEDAW)\".\n\nThe CEDAW or the Convention, also known as the international Bill of Women's Rights, is a legally binding international treaty ratified by 187 States Parties, that entered into force in 1981. All parties to CEDAW, including the UN, are bound to honor it. The UN Doctrine of Gender Equality, and 'Special Measures for Gender Equality' are inspired by and rooted in the CEDAW, specifically in Article 4 paragraph 1, which states:\n\n\"Article 4, paragraph 1 : Adoption by States Parties of temporary special measures aimed at accelerating de facto equality between men and women shall not be considered discrimination as defined in the present Convention, but shall in no way entail as a consequence the maintenance of unequal or separate standards; these measures shall be discontinued when the objectives of equality of opportunity and treatment have been achieved. [emphasis added].\"\n\nThe United Nations Committee on the Elimination of Discrimination against Women (CEDAW), an expert body composed of 23 experts on women's issues established in 1982 to monitor the progress of the CEDAW's implementation, in 2004, adopted \"General Recommendation 25, on Temporary Special Measures, on Article 4 paragraph 1 of the Convention on the Elimination of All Forms of Discrimination against Women\".<ref name=\"(A/59/38)\"></ref> The general recommendation 'clarifies the nature and meaning of article 4, paragraph 1, in order to facilitate and ensure the implementation of Temporary special measures and accelerate progress in achieving gender equality in 'employment and professional fields'.\n\nThe purpose of the \"special measures\", which are 'temporary' is \" to accelerate the improvement of the position of women to achieve their \"substantive equality with men, and to effect the structural, social and cultural changes necessary to correct past and current forms and effects of discrimination against women, as well as to provide them with compensation.\" These are not an exception 'to the norm of non-discrimination', but rather 'part of a necessary strategy' to achieve 'substantive equality of women'. The 'temporary special measures', includes 'preferential treatment; targeted recruitment, hiring and promotion; numerical goals connected with time frames; and quota systems'.\n\nThe UN Committee on CEDAW in its general recommendations number 25 commended the Secretary General on his initiative to implement 'temporary special measures', noting \"The use of temporary special measures by the Secretary-General of the United Nations is a practical example in the area of women's employment, including through administrative instructions on the recruitment, promotion and placement of women in the Secretariat. These measures aim at achieving the goal of 50/50 gender distribution at all levels, but at the higher echelons in particular\".\n\nIn 2012, the UN Secretary General drawing attention to Article 4 paragraph 1 of the \"Convention on the Elimination of All Forms of Discrimination against Women (CEDAW)\" assured the General Assembly that Special Measures were temporary, and will \"be discontinued when gender parity has been achieved and sustained for a period of time\".\n\nGender equality in the United Nations (UN), particularly at managerial and decision-making positions at the D-1 level and above level, has been a United Nations General Assembly goal since 1970 and a recurring concern since then.<ref name=\"A/RES/45/239\"></ref> In 1984, the UN Secretariat, in order to better focus efforts to wards achieving Gender Equality, prepared its first five-year \"Action plan to improve the status of women for the period 1985-90\".<ref name=\"40/258. 18 December 1985\"></ref> Since then there have been several Five Year Plans, including Strategic Plans, to Improve the Status of Women in the UN. In 1990, the United Nations General Assembly after years of slow progress, urged the Secretary General to \"accord priority\" to increase \"the participation of women at the D-1 level and above to 25 per cent \" by 1995.\nIn December 1994, the UN General Assembly \"disappointment\" that the targets set in 1990 were not achieved and that women in posts at the D-1 level were \"still well below the 25 per cent goal\", urged the Secretary General to prioritize the recruitment and promotion of women to reach to 50/50 representation in D1 and above posts by 2000.<ref name=\"A/RES/49/167 - 23/dec\"></ref> In Feb 2004, in the context of the continuing sluggish progress in achieving the target of gender parity the General Assembly revised the target for achieving 50/50 goal to 2015. Gender parity in the UN has become a distant goal, and since 2005 the UN has stopped indicating the target year when gender parity will be achieved.\n\nWomen's representation in the UN secretariat, at the D1 level, in 2000, was 30.3 percent.<ref name=\"A /65/334\"></ref> By 2009, the representation of women in the secretariat at D1 level decreased to 26.7 percent; by way comparison the representation in the entire UN system in 2009 was 29.2 percent.\n\nThe representation of women in 2011 in the secretariat at D2 level was 24.4 percent. In December 2011, the representation of women in the Secretariat at the D1 level was 27.4 percent, an increase of .6 percent over a two-year period. At the current rate of progress, according to the secretary general's report, gender parity at the D-1 to higher levels will be achieved after 102 years.\n\nReasons most frequently cited for failure to meet the General Assembly targets for Gender Equality, and slide in women's representation at the D1 and above level, according to the Secretary General, are: [a] failure by some \"entities\" in the \"implementation of special measures for gender equality\", and [b] tardy implementation of five year Actions Plans [para 143]. In 2010-2011 only two out of thirty departments in the UN applied the \"target of gender parity\"[A/67/347, p 46, paragraph 110(b)]. The UN Secretariat, the largest entity in the United Nations system, in which 38.7 percent of the staff is women, according to the report, claimed to be one of them. .\n\nThe Special Measures in ST/AI/1999/9, which has four sections, apply throughout the Secretariat \"for the filling of all vacant posts in the Professional category and above\", including temporary posts. Sections 1.6 and 1.8 of the ST/AI makes clear that special measures \"shall apply to the selection of staff for posts in all categories where women are under-represented\".\n\nThe Goal of the \"Special Measures\" as set by the General Assembly is to achieve a 50/50 gender distribution in the entire organization and all departments, and in all \"posts in the professional category and above, including posts at the D1 level and above. This goal applies throughout the Organization, and in every department, office or regional commission, overall and at each level. It applies not only to posts subject to geographical distribution but to all categories of posts, irrespective of the type or duration of the appointment ..., the series of the Staff Rules under which the appointment is to be given ... or the source of funding. ...\"\n\nSection 1.6 of ST/AI/1999/9 mandates that \"Cumulative seniority shall be taken into account when considering women candidates for promotion, except when it would adversely affect their situation. Cumulative seniority shall be calculated as the average of the years of the staff member accrued in her present Professional grade and in her immediately preceding Professional grade\".\n\nSection 1.8 of ST/AI/1999/9 mandates \"Vacancies in the Professional category and above shall be filled, when there are one or more women candidates, by one of those candidates\" when \"Her qualifications meet the requirements for the vacant post\"; and \"her qualifications are substantially equal (emphasis added) or superior to those of competing male candidates\" (Section 1.8 (i)(ii)).\n\nHead of Departments(HODs) of UN entities are responsible, and accountable, for the implementation of the special measures, and the Office of Human Resources(OHRM), under the Department of Management (DM), is responsible for monitoring the implementation of the special measures .Steering Committee for the Improvement of the Status of Women in the Secretariat (ST/SGB/1999/9), is responsible for monitoring and implementation of policies of gender equality, including \"Special Measures\".<ref name=\"ST/IC/2014/1*\"></ref><ref name=\"ST /SGB/1999/9\"></ref>\n\nThe current Policies for ' appointments, placement and promotion' to obtain gender equality in the United Nations system are outlined in ST/SGB/282 (Policies to achieve Gender Equality in the United nations),<ref name=\"ST/SGB/282\"></ref> and ST/AI/1999/9(Special measures for Achievement of gender Equality), which entered into force on 1 October 1999. Gender Equality A/I is listed under the heading 'Appointments, placement and promotion\" in the Index to Administrative Issuance in effect as on 31 December 2013. The Gender Equality A/I is in force, and will remain in effect, as the UN secretariat has made clear, \"until specifically amended or abolished\".\n\nThe United Nations Entity for Gender Equality and the Empowerment of Women, known as UN-Women, the United Nations entity responsible for 'gender equality', and holding \"the UN system accountable for its own commitments on gender equality\" has also promulgated that the Gender Parity A/I will \"remain in effect until the Secretary-General is satisfied that substantial progress towards the goal of gender balance has been made\".\n\nSpecial Measures have found regular affirmation by the Secretary General in his reports to the General Assembly. In 2011, Secretary-General's Policy Committee, of which all heads of departments (HOD) are members, including HOD of management, decided on \"specific actions\" to \"accelerate progress towards the goal of gender parity\". An important measure identified for action was measures for more effective implementations of Special Measures. The Secretary General in the \"Report of the Secretary General\" to the General Assembly, in September 2012, to accelerate the goal of gender parity, affirmed the following:\n\nThe United Nations Secretariat, in April 2010, superseded the old Staff selection system (ST/AI/ 2006/3/Rev.1) with a new staff selection system (ST/AI 2010/3).<ref name=\"2010/3\"></ref> The new Staff selection system was listed in 2014 with Gender Affirmative A/I ((ST/AI 1999/9)) as applicable law for Appointments, placement and promotion.<ref name=\"2ST /AI/1999/9-1\"></ref>\n\nThe 'final provision' of the new staffing system which entered into force on 22 April 2010 in section 13.3 notes that \"The provisions of the present administrative instruction shall prevail over any inconsistent provisions contained in other administrative instructions and information circulars currently in force\" There was a similar provision in the earlier Administrative instruction Staff selection system (ST /AI/2006/3) as well.<ref name=\"ST /AI/2006/3\"></ref> The UN Secretariat since 2010, citing section 13.3 of the new Staff Selection System (ST/AI 2010/3), has, some times, contended that provision of section 13.3 supersede Gender Equality A/I(ST/AI 1999/9), especially section 1.8 on affirmative action, as Gender A/I is inconsistent with 2010/3. UN Women, the agency designated to implement UN Gender Policy, however, continues to assert, including on its web site, that affirmative action for women remain in effect, and continues to be list it as applicable law. The Secretary General has also affirmed to the general assembly, as noted above, that Gender A/I, remains in effect. As a result of the secretariat's reservation, and reluctance to implement gender A/I, it has been implemented with diminishing enthusiasm, and become a frequent cause of litigation.\n\nUNAT jurisprudence has, in its consideration of unsuccessful male candidates, as well as claims of women under the Organization's affirmative action policies, repeatedly upheld affirmative action provisions in ST/AI/412 and its successor ST/AI 1999/9, as lawful and in accordance with the UN Charter. ST/AI 1999/9. ST/AI 1999/9 continues to be cited as an applicable instrument by applicants, as well as the respondent, in cases before the UN Dispute Tribunal(UNDT) and the United Nations Appeals Tribunal (UNAT).\n\nUnited Nations Administrative tribunal with Judge Sumar Sen as president, in a seminal Judgment on affirmative action, held : \"that the various resolutions for Improvement of the status of women in the Secretariat which have been referred to and statements of the Secretary-General have conceded the existence of an unsatisfactory history with respect to the recruitment and promotion of women that does not accord with Article 8 of the Charter. In such circumstances, the Tribunal considers that Article 8 of the Charter must be regarded as a source of authority for reasonable efforts to improve the status of women. It would be anomalous indeed if this unsatisfactory history had to remain unremedied for an unduly long period. Unless affirmative action measures are taken towards ameliorating the effects of this past history, they will, without doubt, be perpetuated for many years. This is incompatible with the objectives of Article 8, as recognized by the General Assembly. Hence, the Tribunal concludes that Article 8 permits the adoption of reasonable affirmative action measures for improvement of the status of women.\" The Tribunal added that, \"as long as affirmative action is required to redress the gender imbalance with which the Secretary-General and the General Assembly have been concerned, Article 8 of the Charter would permit, as a reasonable measure, preferential treatment to women candidates where their qualifications are substantially equal to the qualifications of competing male candidates; obviously such a preference is not needed if a woman's qualifications are superior\". The Tribunal, however, cautioned that affirmative action, however laudable, did not justify any weakening of the requirements of Article 101 (3) of the Charter that officials of the Organization be of \"the highest standards of efficiency, competence and integrity\": \"In evaluating the reasonableness of affirmative action measures, pertinent provisions of the Charter may not be ignored. The Tribunal considers that, with respect to affirmative action measures, it would be impermissible to view Article 8 of the Charter as overriding Article 101 (3) ... This language unequivocally establishes a standard under which less qualified persons are not entitled to preferential treatment based on gender. The fundamental principle reflected in Article 101 (3) may not be diluted by a desire, however commendable, to overcome past problems.” The Tribunal concluded that, \"as long as affirmative action is required to redress the gender imbalance with which the Secretary-General and the General Assembly have been concerned, Article 8 of the Charter would permit, as a reasonable measure, preferential treatment to women candidates where their qualifications are substantially equal to the qualifications of competing male candidates(emphasis added); obviously such a preference is not needed if a woman's qualifications are superior\".\n\nUnited Nations Administrative Tribunal, with Mr. Luis de Posadas Montero, Vice-President, Presiding, reaffirmed Grinblat(1992), and held that \"the affirmative action measure establishes a right to preferential treatment for women whose qualifications 'are substantially equal to the qualifications of competing male candidates'\", and ruled that \"as the Applicant was the only woman short-listed for the post, and as she was equally, if not more, qualified for the post, she had a right to promotion, in the light of ST/SGB/237\". \n\nUnited Nations Administrative Tribunal (Mr. Mayer Gabay, President; Ms. Marsha A.Echols; and Mr. Omer Yousif Bireedo) in the case of an unsuccessful male candidate who challenged the promotion of a woman as unlawful and in violation of his rights of due process, citing UNAT Judgements No. 411 (1988) and No. 958 (2000), recalled its “established jurisprudence ... that appointments and promotions are within the discretionary authority of the Secretary-General”,<ref name=\"AT/Dec/1056\"></ref> and that \"qualifications, experience, favourable performance reports and seniority are appraised freely by the Secretary-General and therefore cannot be considered by staff members as giving rise to any expectancy\". The tribunal held that these \"discretionary powers of the Secretary-General are not absolute. They are governed by the relevant provisions of the Charter and by General Assembly resolutions. In this context the Tribunal notes that ST/AI/412 provides that special measures for the achievement of gender equality within the Secretariat must be instituted towards achieving the goal of '50-50 parity between men and women both overall and for positions at the D-1 level and above by the year 2000. It also provides for flexibility in various promotion requirements in order to increase the number of women considered for promotions, for example flexibility regarding seniority.\"<ref name=\"AT/Dec/1056\"></ref> \n\nThe Tribunal concluded that the male candidate had received \"full and fair consideration\" for the post and that the Organization was not bound by the recommendation of the hiring department. \"The Tribunal held that the Secretary-General \"acted within his discretionary authority in deciding to promote a substantially equally qualified female candidate to the D-1 level post\"\"\n\nUnited Nations Administrative Tribunal (Mr. Spyridon Flogaitis, President; Mr. Kevin Haugh; Ms. Brigitte Stern) held that ST/AI/1999/9 required that where there are both male and female candidates with substantially equal qualifications, \"the female candidate should be appointed unless the qualifications of the male candidate are in some demonstrable and measurable way superior to those of the best qualified female candidate. The Tribunal has chosen to use the words 'demonstrable' and 'measurable' advisedly for, since appointment and promotion decisions are amenable to administrative review under the Organization's internal justice system and under the Statute of the Tribunal, and since the body carrying out such review ... should act on identifiable facts and on evidence, there should be evidence which can be examined and assessed by the body carrying out the review from which it can be determined if the affirmative action requirements have been fully complied with. This is an essential requirement if there is to be a meaningful review and if the policy of ST/AI/1999/9 or other affirmative action policy is to be enforced as an imperative. Otherwise, such policies become mere shibboleths or mere set pieces of lip service invoked in the pretence of achieving their objectives.\" The tribunal concluded that although as a general principle, the party making an allegation bears the burden of proving it, but \"this general proposition must require modification where the relevant evidence is solely in the hands of the Administration and not available to an Applicant, the onus of proof in certain matters should be viewed as neutral rather than as resting on the Applicant\". The Tribunal, while agreeing with the conclusions of the Joint Appeals Board (JAB) that \"since there was no demonstrable or measurable evidence to support a conclusion that the successful male candidate enjoyed substantially superior qualifications when compared with those of the Applicant, a breach of ST/AI/1999/9 [had] been established\" Accordingly, the Tribunal ordered the Respondent to pay the Applicant the difference between her salary at the P-3 level and the P-4 salary that she would have received had she been appointed to the post in question, from October 2002, for the lesser of either two years or until her promotion to the P-4 level, and ordered the Respondent either to place the Applicant on the \"Galaxy roster\" until she secures a suitable post at the P-4 level or to pay her two months' net base salary . \n\nIn August 2012, the Organization acknowledged ST/AI/1999/9 (Special measures for achieving Gender Equality) as applicable law on gender balance, noting that \"The general provisions to give women preferential treatment for appointments are found in ST/AI/1999/9\" and that \"selection, vacancies shall be filled by a woman if her qualifications: (a) meet the requirements for the vacant post and (b) are substantially equal or superior to those of competing male candidates (sec. 1.8)\" The Tribunal Judge Coral Shaw confirmed the mandatory character of the \"rules in ST/AI/1999/9 concerning preferential treatment of women in selection processes\".\n\nIn August 2014, the UN administration conceded in a United Nations Dispute Tribunal hearing that administrative instruction ST/AI/1999/9 (Special measures for the achievement of Gender Equality), was valid in a selection process.<ref name=\"Judgment No.: Undt/2014/068\"></ref> While examining \"whether the relevant sections of that administrative instruction have been correctly applied to the case at hand\", the Tribunal Judge Thomas Laker concluded that the Administration failed to respect provision of the ST/AI 1999/9and rescinded the selection decision, for this and other infractions.\"<ref name=\"Judgment No.: Undt/2014/068\"></ref>\n\nGender A/I, as already mentioned, has often been a subject of litigation. In cases brought before the tribunal, the respondent, ie, the Secretary General, has, so far, not contested the applicability of the gender AI as applicable law. The tribunal in several cases has upheld the validity of Gender AI and cited it in its judgment. A tabulation of the salient cases in which Gender A/I has been a subject of litigation, and judgments by the tribunal, is below: \n\n\n"}
{"id": "1264097", "url": "https://en.wikipedia.org/wiki?curid=1264097", "title": "Theodorus of Gadara", "text": "Theodorus of Gadara\n\nTheodorus of Gadara () was a Greek rhetorician of the 1st century BC who founded a rhetorical school in Gadara (present-day Um Qais, Jordan), where he taught future Roman emperor Tiberius the art of rhetoric. It was written of Tiberius that:\n\nHis other well-known pupil was Greek rhetorician Hermagoras of Temnos, who later taught oratory in Rome.\n\nTheodorus was one of the two most famous rhetoric teachers of the time, the other being Apollodorus of Pergamon. Students of Apollodorus were commonly referred to as \"Apollodoreans\", while students of Theodorus were known as \"Theodoreans\".\n\nAccording to the Suda, Theodorus wrote the following books, among others: \n"}
{"id": "6904987", "url": "https://en.wikipedia.org/wiki?curid=6904987", "title": "Virtual heritage", "text": "Virtual heritage\n\nVirtual heritage or cultural heritage and technology is the body of works dealing with information and communication technologies (ICT) and their application to cultural heritage, such as \"virtual archaeology\". Virtual heritage and cultural heritage have independent meanings: \"cultural heritage\" refers to sites, monuments, buildings and objects \"with historical, aesthetic, archaeological, scientific, ethnological or anthropological value\", whereas \"virtual heritage\" refers to instances of these within a technological domain, usually involving computer visualization of artefacts or Virtual Reality environments.\n\nOne technology that is frequently employed in virtual heritage applications is augmented reality, which is used to provide on-site reconstructions of archaeological sites or artefacts.\n\nMany virtual heritage projects focus on the tangible aspects of cultural heritage, for example 3D modelling, graphics and animation. In doing so they often overlook the intangible aspects of cultural heritage associated with objects and sites, such as stories, performances and dances. The tangible aspects of cultural heritage are not inseparable from the intangible and one method for combining them is the use of virtual heritage serious games, such as the 'Digital Songlines' and 'Virtual Songlines' which modified computer game technology to preserve, protect and present the cultural heritage of Australian Aborigines.\n\nThe first use of virtual heritage as a museum exhibit, and the derivation of the name virtual tour, was in 1994 as a museum visitor interpretation, providing a 'walk-through' of a 3D reconstruction of Dudley Castle in England as it was in 1550.\nThis consisted of a computer controlled laserdisc based system designed by British-based engineer Colin Johnson. It is a little-known fact that one of the first users of Virtual Heritage was Queen Elizabeth II, when she officially opened the visitor centre in June 1994.\nBecause the Queen's officials had requested titles, descriptions and instructions of all activities, the system was named 'Virtual Tour', being a cross between virtual reality and Royal Tour.\n\n\n"}
