{"id": "199226", "url": "https://en.wikipedia.org/wiki?curid=199226", "title": "Bernoulli family", "text": "Bernoulli family\n\nThe Bernoulli family () of Basel is a patrician family, notable for having produced eight mathematically gifted academics who, between them, contributed to the foundations of applied mathematics and physics during the early modern period. Originally from Antwerp, Belgium, they moved to Basel and gained Swiss citizenship in 1620.\nThe family was related by marriage to the prominent French academic dynasty, Curie family through Johann Bernoulli (1667–1748).\nWhile their origin in Antwerp is certain, proposed connections with the Dutch family \"Bornouilla, Bernoullie\" or the Castilian family \"de Bernuy\" (\"Bernoille, Bernouille\") are uncertain.\n\nThe first known member of the family was Leon Bernoulli (d. 1561), a doctor in Antwerp, at that time, part of the Spanish Netherlands. His son, Jacob, emigrated to Frankfurt am Main in 1570 to escape from the Spanish persecution of the Protestants. \nJacob's grandson, a spice trader, also named Jacob, moved to Basel, Switzerland in 1620, and was granted Basel citizenship in 1622. His son, Niklaus (Nicolaus, 1623–1708), Leon's great-great-grandson, married Margarethe Schönauer. \nNiklaus had four sons, of which Johann and Hieronyus became the progenitors of the \"greater\" and the \"lesser\" branch of the family, respectively.\nThe four sons of Niklaus were:\n\nIn addition to those mentioned above, the Bernoulli family produced many notable artists and scientists, in particular, a number of famous mathematicians in the 18th century:\n\nThe surname survives in Switzerland, with ten entries in the white pages for the city of Basel as of 2018.\n\n\n"}
{"id": "58559351", "url": "https://en.wikipedia.org/wiki?curid=58559351", "title": "Brown measure", "text": "Brown measure\n\nIn mathematics, the Brown measure of an operator in a finite factor is a probability measure on the complex plane which may be viewed as an analog of the spectral counting measure (based on algebraic multiplicity) of matrices. \n\nLet formula_1 be a finite factor with the canonical normalized trace formula_2. For every operator formula_3, the function\n\nis subharmonic and its Laplacian in the distributional sense is a probability measure on formula_5 which is called the Brown measure of formula_6.\n\n"}
{"id": "17910805", "url": "https://en.wikipedia.org/wiki?curid=17910805", "title": "Counting board", "text": "Counting board\n\nThe counting board is the precursor of the abacus, and the earliest known form of a counting device (excluding fingers and other very simple methods). Counting boards were made of stone or wood, and the counting was done on the board with beads, or pebbles etc. Not many boards survive because of the perishable materials used in their construction. \n\nThe oldest known counting board, the Salamis Tablet (\"c.\" 300 BC) was discovered on the Greek island of Salamis in 1899. It is thought to have been used by the Babylonians in about 300 BC and is more of a gaming board than a calculating device. It is marble, about 150 x 75 x 4.5 cm, and is in the Greek National museum in Athens. It has carved Greek letters and parallel grooves. \n\nThe German mathematicican Adam Ries described the use of counting boards in \"Rechenbuch auf Linien und Ziphren in allerlei Handthierung / geschäfften und Kaufmanschafft\". In the novel \"Wolf Hall\", Hilary Mantel refers to Thomas Cromwell using a counting board in 16th-century England.\n\n"}
{"id": "18062300", "url": "https://en.wikipedia.org/wiki?curid=18062300", "title": "Cyclic (mathematics)", "text": "Cyclic (mathematics)\n\nThere are many terms in mathematics that begin with cyclic:\n\n"}
{"id": "690759", "url": "https://en.wikipedia.org/wiki?curid=690759", "title": "Differential analyser", "text": "Differential analyser\n\nThe differential analyser is a mechanical analogue computer designed to solve differential equations by integration, using wheel-and-disc mechanisms to perform the integration. It was one of the first advanced computing devices to be used operationally.\nThe original machines could not add, but then it was noticed that if the two wheels of a rear differential are turned, the drive shaft will compute the average of the left and right wheels. A simple gear ratio of 1:2 then enables multiplication by two, so addition (and subtraction) are achieved. Multiplication is just a special case of integration, namely integrating a constant function.\n\nResearch on solutions for differential equations using mechanical devices, discounting planimeters, started at least as early as 1836, when the French physicist Gaspard-Gustave Coriolis designed a mechanical device to integrate differential equations of the first order.\n\nThe first description of a device which could integrate differential equations of any order was published in 1876 by James Thomson, who was born in Belfast in 1822, but lived in Scotland from the age of 10. Though Thomson called his device an \"integrating machine\", it is his description of the device, together with the additional publication in 1876 of two further descriptions by his younger brother, Lord Kelvin, which represents the invention of the differential analyser.\n\nOne of the earliest practical uses of Thomson's concepts was a tide-predicting machine built by Kelvin starting in 1872-3. On Lord Kelvin's advice, Thomson's integrating machine was later incorporated into a fire-control system for naval gunnery being developed by Arthur Pollen, resulting in an electrically driven, mechanical analogue computer, which was completed by about 1912. Italian mathematician Ernesto Pascal also developed integraphs for the mechanical integration of differential equations and published details in 1914.\n\nHowever, the first widely practical general purpose differential analyser was constructed by Harold Locke Hazen and Vannevar Bush at MIT, 1928–1931, comprising six mechanical integrators. In the same year, Bush described this machine in a journal article as a \"continuous integraph\". When he published a further article on the device in 1931, he called it a \"differential analyzer\". In this article, Bush stated that \"[the] present device incorporates the same basic idea of interconnection of integrating units as did [Lord Kelvin's]. In detail, however, there is little resemblance to the earlier model.\" According to his 1970 autobiography, Bush was \"unaware of Kelvin’s work until after the first differential analyzer was operational.\" Claude Shannon was hired as a research assistant in 1936 to run the differential analyzer in Bush's lab.\n\nDouglas Hartree of Manchester University brought Bush's design to England, where he constructed his first \"proof of concept\" model with his student, Arthur Porter, during 1934: as a result of this, the university acquired a full scale machine incorporating four mechanical integrators in March 1935, which was built by Metropolitan-Vickers, and was, according to Hartree, \"[the] first machine of its kind in operation outside the United States\". During the next five years three more were added, at Cambridge University, Queen's University Belfast, and the Royal Aircraft Establishment in Farnborough. One of the integrators from this proof of concept is on display in the History of Computing section of the Science Museum (London) alongside a complete Manchester machine.\n\nIn Norway, the locally built Oslo Analyser was finished during 1938, based on the same principles as the MIT machine. This machine had 12 integrators, and was the largest analyser built for a period of four years.\n\nIn the United States, further differential analysers were built at the Ballistic Research Laboratory in Maryland and in the basement of the Moore School of Electrical Engineering at the University of Pennsylvania during the early 1940s. The latter was used extensively in the computation of artillery firing tables prior to the invention of the ENIAC, which, in many ways, was modelled on the differential analyser. Also in the early 1940s, with Samuel H. Caldwell, one of the initial contributors during the early 1930s, Bush attempted an electrical, rather than mechanical, variation, but the digital computer built elsewhere had much greater promise and the project ceased. In 1947, UCLA installed a differential analyser built for them by General Electric at a cost of $125,000. By 1950, this machine had been joined by three more. The UCLA differential analyzer appeared in 1951's When Worlds Collide, where it was called, euphemistically, \"DA\". \nAt Osaka Imperial University (present-day Osaka University) around 1944, \na complete differential analyser machine was developed (illustrated) to calculate the movement of an object and other problems with mechanical components, and then draws graphs on paper with a pen. It was later transferred to the Tokyo University of Science and has been displayed at the school’s Museum of Science in Shinjuku Ward. Restored in 2014 is one of only two still operational differential analyzers produced before the end of World War II.\n\nIn Canada, a differential analyser was constructed at the University of Toronto in 1948 by Beatrice Helen Worsley, but it appears to have had little or no use.\n\nA differential analyser may have been used in the development of the bouncing bomb, used to attack German hydroelectric dams during World War II. Differential analysers have also been used in the calculation of soil erosion by river control authorities.\n\nThe differential analyser was eventually rendered obsolete by electronic analogue computers and, later, digital computers.\n\nThe model differential analyser built at Manchester University in 1934 by Douglas Hartree and Arthur Porter made extensive use of Meccano parts: this meant that the machine was less costly to build, and it proved \"accurate enough for the solution of many scientific problems\". A similar machine built by J.B. Bratt at Cambridge University in 1935 is now in the Museum of Transport and Technology (MOTAT) collection in Auckland, New Zealand. A memorandum written for the British military's Armament Research Department in 1944 describes how this machine had been modified during World War II for improved reliability and enhanced capability, and identifies its wartime applications as including research on the flow of heat, explosive detonations, and simulations of transmission lines. \nIn 1948, this machine was bought by Professor Harry Whale of Auckland, for 100 pounds sterling, and he then took it to Auckland for use at the Seagrave Radio Research Centre.\n\nIt is estimated that \"about 15 Meccano model Differential Analysers were built for serious work by scientists and researchers around the world\". More recently, building differential analysers with Meccano parts has become a popular project among serious Meccano hobbyists. An example is the differential analyser built at Marshall University, which is now used for educational purposes, in that a student not only solves a differential equation but also becomes the \"calculator\" by operating the machine, and so develops a better understanding of what a differential equation is.\n\n\n\n\n"}
{"id": "53252845", "url": "https://en.wikipedia.org/wiki?curid=53252845", "title": "Glossary of calculus", "text": "Glossary of calculus\n\n\"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.\"\nThis glossary of calculus is a list of definitions about calculus, its sub-disciplines, and related fields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "37822732", "url": "https://en.wikipedia.org/wiki?curid=37822732", "title": "History of network traffic models", "text": "History of network traffic models\n\nDesign of robust and reliable networks and network services relies on an understanding of the traffic characteristics of the network. Throughout history, different models of network traffic have been developed and used for evaluating existing and proposed networks and services.\n\nDemands on computer networks are not entirely predictable. Performance modeling is necessary for deciding the quality of service (QoS) level. Performance models in turn, require accurate traffic models that have the ability to capture the statistical characteristics of the actual traffic on the network. Many traffic models have been developed based on traffic measurement data. If the underlying traffic models do not efficiently capture the characteristics of the actual traffic, the result may be the under-estimation or over-estimation of the performance of the network. This impairs the design of the network. Traffic models are hence, a core component of any performance evaluation of networks and they need to be very accurate.\n\n“Teletraffic theory is the application of mathematics to the measurement, modeling, and control of traffic in telecommunications networks. The aim of traffic modeling is to find stochastic processes to represent the behavior of traffic. Working at the Copenhagen Telephone Company in the 1910s, A. K. Erlang famously characterized telephone traffic at the call level by certain probability distributions for arrivals of new calls and their holding times. Erlang applied the traffic models to estimate the telephone switch capacity needed to achieve a given call blocking probability. The Erlang blocking formulas had tremendous practical interest for public carriers because telephone facilities (switching and transmission) involved considerable investments. Over several decades, Erlang’s work stimulated the use of queuing theory, and applied probability in general, to engineer the public switched telephone network. Teletraffic theory for packet networks has seen considerable progress in recent decades. Significant advances have been made in long-range dependence, wavelet, and multifractal approaches. At the same time, traffic modeling continues to be challenged by evolving network technologies and new multimedia applications. For example, wireless technologies allow greater mobility of users. Mobility must be an additional consideration for modeling traffic in wireless networks. Traffic modeling is an ongoing process without a real end. Traffic models represent our best current understanding of traffic behavior, but our understanding will change and grow over time.”\n\nMeasurements are useful and necessary for verifying the actual network performance. However, measurements do not have the level of abstraction that makes traffic models useful. Traffic models can be used for hypothetical problem solving whereas traffic measurements only reflect current reality. In probabilistic terms, a traffic trace is a realization of a random process, whereas a traffic model is a random process. Thus, traffic models have universality. A traffic trace gives insight about a particular traffic source, but a traffic model gives insight about all traffic sources of that type. Traffic models have three major uses. One important use of traffic models is to properly dimension network resources for a target level of QoS. It was mentioned earlier that Erlang developed models of voice calls to estimate telephone switch capacity to achieve a target call blocking probability. Similarly, models of packet traffic are needed to estimate the bandwidth and buffer resources to provide acceptable packet delays and packet loss probability. Knowledge of the average traffic rate is not sufficient. It is known from queuing theory that queue lengths increase with the variability of traffic. Hence, an understanding of traffic burstiness or variability is needed to determine sufficient buffer sizes at nodes and link capacities. A second important use of traffic models is to verify network performance under specific traffic controls. For example, given a packet scheduling algorithm, it would be possible to evaluate the network performance resulting from different traffic scenarios. For another example, a popular area of research is new improvements to the TCP congestion avoidance algorithm. It is critical that any algorithm is stable and allows multiple hosts to share bandwidth fairly, while sustaining a high throughput. Effective evaluation of the stability, fairness, and throughput of new algorithms would not be possible without realistic source models. A third important use of traffic models is admission control. In particular, connection oriented networks such as ATM depends on admission control to block new connections to maintain QOS guarantees. A simple admission strategy could be based on the peak rate of a new connection; a new connection is admitted if the available bandwidth is greater than the peak rate. However, that strategy would be overly conservative because a variable bit-rate connection may need significantly less bandwidth than its peak rate. A more sophisticated admission strategy is based on effective bandwidths. The source traffic behavior is translated into an effective bandwidth between the peak rate and average rate, which is the specific amount of bandwidth required to meet a given QoS constraint. The effective bandwidth depends on the variability of the source.\n\nTraffic modeling consists of three steps:\nParameter estimation is based on a set of statistics (e.g. mean, variance, density function or auto covariance function, multifractal characteristics) that are measured or calculated from observed data. The set of statistics used in the inference process depends on the impact they may have in the main performance metrics of interest.\n\nIn recent years several types of traffic behavior, that can have significant impact on network performance, were discovered: long-range dependence, self-similarity and, more recently, multifractality.\nThere are two major parameters generated by network traffic models: packet length distributions and packet inter-arrival distributions. Other parameters, such as routes, distribution of destinations, etc., are of less importance. Simulations that use traces generated by network traffic models usually examine a single node in the network, such as a router or switch; factors that depend on specific network topologies or routing information are specific to those topologies and simulations. The problem of packet size distribution is fairly well-understood today. Existing models of packet sizes have proven to be valid and simple. Most packet size models do not consider the problem of order in packet sizes. For example, a TCP datagram in one direction is likely to be followed by a tiny ACK in the other direction about half of one Round-Trip Time (RTT) later. The problem of packet inter-arrival distribution is much more difficult. Understanding of network traffic has evolved significantly over the years, leading to a series of evolutions in network traffic models.\n\nOne of the earliest objections to self-similar traffic models was the difficulty in mathematical analysis. Existing self-similar models could not be used in conventional queuing models. This limitation was rapidly overturned and workable models were constructed. Once basic self-similar models became feasible, the traffic modeling community settled into the “detail” concerns. TCP’s congestion control algorithm complicated the matter of modeling traffic, so solutions needed to be created. Parameter estimation of self-similar models was always difficult, and recent research addresses ways to model network traffic without fully understanding it.\n\nWhen self-similar traffic models were first introduced, there were no efficient, analytically tractable processes to generate the models. Ilkka Norros devised a stochastic process for a storage model with self-similar input and constant bit-rate output. While this initial model was continuous rather than discrete, the model was effective, simple, and attractive.\nAll self-similar traffic models suffer from one significant drawback: estimating the self-similarity parameters from real network traffic requires huge amounts of data and takes extended computation. The most modern method, wavelet multi-resolution analysis, is more efficient, but still very costly. This is undesirable in a traffic model. SWING uses a surprisingly simple model for the network traffic analysis and generation. The model examines characteristics of users, Request-Response Exchanges (RREs), connections, individual packets, and the overall network. No attempt is made to analyze self-similarity characteristics; any self-similarity in the generated traffic comes naturally from the aggregation of many ON/OFF sources.\nThe Pareto distribution process produces independent and identically distributed (IID) inter-arrival times. In general if X is a random variable with a Pareto distribution, then the probability that X is greater than some number x is given by P(X > x) = (x/x_m)-k for all x ≥ x_m where k is a positive parameter and x_m is the minimum possible value of Xi The probability distribution and the density functions are represented as:\nF(t) = 1 – (α/t)β where α,β ≥ 0 & t ≥ α\nf(t) = βαβ t-β-1\nThe parameters β and α are the shape and location parameters, respectively. The Pareto distribution is applied to model self-similar arrival in packet traffic. It is also referred to as double exponential, power law distribution. Other important characteristics of the model are that the Pareto distribution has infinite variance, when β ≥ 2 and achieves infinite mean, when β ≤ 1.\nThe Weibull distributed process is heavy-tailed and can model the fixed rate in ON period and ON/OFF period lengths, when producing self-similar traffic by multiplexing ON/OFF sources. The distribution function in this case is given by:\nF(t) = 1 – e-(t/β)α t > 0\nand the density function of the weibull distribution is given as:\nf(t) = αβ-α tα-1 e -(t/β)α t > 0\nwhere parameters β ≥ 0 and α > 0 are the scale and location parameters respectively.\nThe Weibull distribution is close to a normal distribution. For β ≤ 1 the density function of the distribution is L shaped and for values of β > 1, it is bell shaped. This distribution gives a failure rate increasing with time. For β > 1, the failure rate decreases with time. At, β = 1, the failure rate is constant and the lifetimes are exponentially distributed.\nThe Autoregressive model is one of a group of linear prediction formulas that attempt to predict an output y_n of a system based on previous set of outputs {y_k} where k < n and inputs x_n and {x_k} where k < n. There exist minor changes in the way the predictions are computed based on which, several variations of the model are developed. Basically, when the model depends only on the previous outputs of the system, it is referred to as an auto-regressive model. It is referred to as a Moving Average Model (MAM), if it depends on only the inputs to the system. Finally, Autoregressive-Moving Average models are those that depend both on the inputs and the outputs, for prediction of current output. Autoregressive model of order p, denoted as AR(p), has the following form:\nXt = R1 Xt-1 + R2 Xt-2 + ... + Rp Xt-p + Wt\nwhere Wt is the white noise, Ri are real numbers and Xt are prescribed correlated random numbers. The auto-correlation function of the AR(p) process consists of damped sine waves depending on whether the roots (solutions) of the model are real or imaginary. Discrete Autoregressive Model of order p, denoted as DAR(p), generates a stationary sequence of discrete random variables with a probability distribution and with an auto-correlation structure similar to that of the Autoregressive model of order p.[3]\nRegression models define explicitly the next random variable in the sequence by previous ones within a specified time window and a moving average of a white noise.[5]\nTransform-expand-sample (TES) models are non-linear regression models with modulo-1 arithmetic. They aim to capture both auto-correlation and marginal distribution of empirical data. TES models consist of two major TES processes: TES+ and TES–. TES+ produces a sequence which has positive correlation at lag 1, while TES– produces a negative correlation at lag 1.\n\nEarly traffic models were derived from telecommunications models and focused on simplicity of analysis. They generally operated under the assumption that aggregating traffic from a large number of sources tended to smooth out bursts; that burstiness decreased as the number of traffic sources increased.\nOne of the most widely used and oldest traffic models is the Poisson Model. The memoryless Poisson distribution is the predominant model used for analyzing traffic in traditional telephony networks. The Poisson process is characterized as a renewal process. In a Poisson process the inter-arrival times are exponentially distributed with a rate parameter λ: P{An ≤ t} = 1 – exp(-λt). The Poisson distribution is appropriate if the arrivals are from a large number of independent sources, referred to as Poisson sources. The distribution has a mean and variance equal to the parameter λ.\nThe Poisson distribution can be visualized as a limiting form of the binomial distribution, and is also used widely in queuing models. There are a number of interesting mathematical properties exhibited by Poisson processes. Primarily, superposition of independent Poisson processes results in a new Poisson process whose rate is the sum of the rates of the independent Poisson processes. Further, the independent increment property renders a Poisson process memoryless. Poisson processes are common in traffic applications scenarios that consist of a large number of independent traffic streams. The reason behind the usage stems from Palm's Theorem which states that under suitable conditions, such large number of independent multiplexed streams approach a Poisson process as the number of processes grows, but the individual rates decrease in order to keep the aggregate rate constant. Nevertheless, it is to be noted that traffic aggregation need not always result in a Poisson process. The two primary assumptions that the Poisson model makes are:\n1. The number of sources is infinite\n2. The traffic arrival pattern is random.\nIn the compound Poisson model, the base Poisson model is extended to deliver batches of packets at once. The inter-batch arrival times are exponentially distributed, while the batch size is geometric Mathematically, this model has two parameters, λ, the arrival rate, and ρ in (0,1), the batch parameter. Thus, the mean number of packets in a batch is 1/ ρ, while the mean inter-batch arrival time is 1/ λ. Mean packet arrivals over time period t are tλ/ ρ.\nThe compound Poisson model shares some of the analytical benefits of the pure Poisson model: the model is still memoryless, aggregation of streams is still (compound) Poisson, and the steady-state equation is still reasonably simple to calculate, although varying batch parameters for differing flows would complicate the derivation.\nMarkov models attempt to model the activities of a traffic source on a network, by a finite number of states. The accuracy of the model increases linearly with the number of states used in the model. However, the complexity of the model also increases proportionally with increasing number of states. An important aspect of the Markov model - the Markov Property, states that the next (future) state depends only on the current state. In other words, the probability of the next state, denoted by some random variable Xn+1, depends only on the current state, indicated by Xn, and not on any other state Xi, where i<n. The set of random variables referring to different states {Xn} is referred to as a Discrete Markov Chain.\nAnother attempt at providing a bursty traffic model is found in Jain and Routhier’s Packet Trains model. This model was principally designed to recognize that address locality applies to routing decisions; that is, packets that arrive near each other in time are frequently going to the same destination. In generating a traffic model that allows for easier analysis of locality, the authors created the notion of packet trains, a sequence of packets from the same source, traveling to the same destination (with replies in the opposite direction). Packet trains are optionally sub-divided into tandem trailers. Traffic between a source and a destination usually consists of a series of messages back and forth. Thus, a series of packets go one direction, followed by one or more reply packets, followed by a new series in the initial direction. Traffic quantity is then a superposition of packet trains, which generates substantial bursty behavior. This refines the general conception of the compound Poisson model, which recognized that packets arrived in groups, by analyzing why they arrive in groups, and better characterizing the attributes of the group. Finally, the authors demonstrate that packet arrival times are not Poisson distributed, which led to a model that departs from variations on the Poisson theme. The packet train model is characterized by the following parameters and their associated probability distributions:\nThe train model is designed for analyzing and categorizing real traffic, not for generating synthetic loads for simulation. Thus, little claim has been made about the feasibility of packet trains for generating synthetic traffic. Given accurate parameters and distributions, generation should be straightforward, but derivation of these parameters is not addressed.\n\nNS-2 is a popular network simulator; PackMimeHTTP is a web traffic generator for NS-2, published in 2004. It does take long-range dependencies into account, and uses the Weibull distribution. Thus, it relies on heavy tails to emulate true self-similarity. Over most time scales, the effort is a success; only a long-running simulation would allow a distinction to be drawn. This follows suggestions from where it is suggested that self-similar processes can be represented as a superposition of many sources each individually modeled with a heavy-tailed distribution. It is clear that self-similar traffic models are in the mainstream.\n\n\n"}
{"id": "20375066", "url": "https://en.wikipedia.org/wiki?curid=20375066", "title": "Iatromathematicians", "text": "Iatromathematicians\n\nIatromathematicians (from Greek ἰατρική \"medicine\" and μαθηματικά \"mathematics\") were a school of physicians in 17th-century Italy who tried to apply the laws of mathematics and mechanics in order to understand the functioning of the human body. They were also keen students of anatomy. These iantromathematicians made an effort to prove that applying a purely mechanical conception to the study of the human body is futile. The mechanical conceptions that they had referred to was Leonardo da Vinci’s studies of the human body, and the writings of Aristotle about the motion of animals related to geometric analysis. Iantromathematicians considered the bodies functioning to be measured by quantifiable numbers, weights, and measures.\n\nThe field of iatromathematics is allied to science; however, it lacks the applicability of the proper scientific method and is therefore considered a form of pseudoscience. It applies the study of astrology to medicine.\n\nIatromathematicians viewed the human body through astrological reasoning as well as mechanics. They associate various stars, or zodiac signs with the functioning of the human body. The twelve astrological signs contribute to each part of the body from head to toe. Moreover, planets and existing cosmos in space are correlated with certain parts of the body. Through examining a natal chart, iatromathematicians attempt to predict biological setbacks in an individual.\n\nIatromathematicians examine the active and energetic temperament of the human body. Moreover, they explore the causes of various health problems and attempt to find ways to treat certain detrimental diseases. In iatromathematics, there is a particular assumption that there is an impact of various energetic fields caused on the star bodies. The star body of an individual is often referred to by astrologists as an energetic matrix and is believed to be spawned by heavenly bodies such as the sun, moon, planets, and several other astrological signs.\n\nIatromathematicians study these conceptions and try to regulate the path of the star body of individuals so that it will give a positive, rather than a negative result. By doing so, they believe that it will contribute to a healthier lifestyle. Its doctrine is based on cosmobiology in which several emotional and physiological dilemmas in the body are associated with the positioning of celestial bodies in outer space.\n\nIatromathematics is closely correlated with biomechanics because the field of biomechanics investigates macrobiotic bodies to a macroscopic degree through the appliance of several engineering principles. The perspective of iatromathematicians differed from that of iatrophysicists and iatrochemists in terms of the way human bodies function. Iatrophysicists predicted the deviations from the biological norm of the body through the appliance of physics, while iatrochemists measured the detrimental problems of the body by chemical means.\n\nSeveral individuals contributed to this field study of iatromathematics. For example, Ibn Ezra (Rabbi Avraham Ben Meir Ibn Ezra) wrote nine different astrological treatises. He covered all the subsections of astrology which include the branches of natal, medicinal, horary, electional, and mundane. Ibn Ezra’s best known work was known as \"The Beginning of Wisdom\". Over time, various individuals studied his works comprehensively. One such person was George Sarton, who is the founder of the History of Science Society.\n\nRecently, Archibald Pitcairne was mentioned as the \"forgotten father of mathematical medicine\" and his contribution praised to creating the bases of iatromathematics.\n\n\n"}
{"id": "23624339", "url": "https://en.wikipedia.org/wiki?curid=23624339", "title": "Indexed family", "text": "Indexed family\n\nIn mathematics, an indexed family is informally a collection of objects, each associated with an index from some index set. For example, a \"family of real numbers, indexed by the set of integers\" is a collection of real numbers, where a given function selects for each integer one real number (possibly the same).\n\nMore formally, an indexed family is a mathematical function together with its domain \"I\" and image \"X\". Often the elements of the set \"X\" are referred to as making up the family. In this view indexed families are interpreted as collections instead of as functions. The set \"I\" is called the \"index (set)\" of the family, and \"X\" is the \"indexed set\".\n\nDefinition. Let \"I\" and \"X\" be sets and formula_1 a surjective function, such that\nthen this establishes a family of elements in \"X\" indexed by \"I\" , which is denoted by (\"x\") or simply (\"x\"), when the index set is assumed to be known. Sometimes angle brackets or braces are used instead of parentheses, the latter with the risk of mixing-up families with sets.\n\nAn indexed family can be turned into a set by considering the set formula_3, that is, the image of \"I\" under \"x\". Since the mapping x is not required to be injective, there may exist formula_4 with formula_5 such that formula_6. Thus, formula_7 where |\"A\"| denotes the cardinality of the set \"A\".\n\nThe index set is not restricted to be countable, and, of course, a subset of a powerset may be indexed, resulting in an indexed family of sets. For the important differences in sets and families see below.\n\nWhenever index notation is used the indexed objects form a family. For example, consider the following sentence.\nHere (\"v\") denotes a family of vectors. The \"i\"-th vector \"v\" only makes sense with respect to this family, as sets are unordered and there is no \"i\"-th vector of a set. Furthermore, linear independence is only defined as the property of a collection; it therefore is important if those vectors are linearly independent as a set or as a family. \n\nIf we consider \"n\" = 2 and \"v\" = \"v\" = (1, 0), the \"set\" of them consists of only one element and is linearly independent, but the family contains the same element twice and is linearly dependent.\n\nSuppose a text states the following:\n\nAs in the previous example it is important that the rows of \"A\" are linearly independent as a family, not as a set. For Example, consider the matrix\nThe \"set\" of rows only consists of a single element (1, 1) and is linearly independent, but the matrix is not invertible. The \"family\" of rows contains two elements and is linearly dependent. The statement is therefore correct if it refers to the family of rows, but wrong if it refers to the set of rows. (The statement is also correct when \"the rows\" is interpreted as referring to a multiset, in which the elements are also kept distinct but which lacks some of the structure of an indexed family.)\n\nSurjective functions and families are formally equivalent, as any function \"f\" with domain \"I\" induces a family (\"f\"(\"i\")). In practice, however, a family is viewed as a collection, not as a function: being \"an element of a family\" is equivalent with being in the range of the corresponding function. A family contains any element exactly once, if and only if the corresponding function is injective. \n\nLike a set, a family is a container and any set \"X\" gives rise to a family (\"x\"). Thus any set naturally becomes a family. For any family (\"A\") there is the set of all elements {\"A\" | \"i\"∈\"I\"}, but this does not carry any information on multiple containment or the structure given by \"I\". Hence, by using a set instead of the family, some information might be lost.\n\nLet n be the finite set {1, 2, …, \"n\"}, where \"n\" is a positive integer.\n\nIndex sets are often used in sums and other similar operations. For example, if (\"a\") is a family of numbers, the sum of all those numbers is denoted by\n\nWhen (\"A\") is a family of sets, the union of all those sets is denoted by\n\nLikewise for intersections and cartesian products.\n\nA family (\"B\") is a subfamily of a family (\"A\"), if and only if \"J\" is a subset of \"I\" and for all \"i\" in \"J\" \n\nThe analogous concept in category theory is called a diagram. A diagram is a functor giving rise to an indexed family of objects in a category C, indexed by another category J, and related by morphisms depending on two indices.\n\n\n"}
{"id": "8298904", "url": "https://en.wikipedia.org/wiki?curid=8298904", "title": "International Centre for Mathematical Sciences", "text": "International Centre for Mathematical Sciences\n\nThe International Centre for Mathematical Sciences (ICMS) is a mathematical research centre based in Edinburgh. According to its website, the Centre is \"designed to bring together mathematicians and practitioners in science, industry and commerce for research workshops and other meetings.\" \n\nThe Centre was jointly established in 1990 by the University of Edinburgh and Heriot-Watt University, under the supervision of Professor Elmer Rees, with initial support from Edinburgh District Council, the Scottish Development Agency and the International Centre for Theoretical Physics. In April 1994 the Centre moved to 14 India Street, Edinburgh, the birthplace of James Clerk Maxwell and home of the James Clerk Maxwell Foundation. In 2010 it relocated to 15 South College Street to accommodate larger events. The current scientific director (appointed in 2016) is Professor Paul Glendinning.\n\n\n"}
{"id": "34969712", "url": "https://en.wikipedia.org/wiki?curid=34969712", "title": "Jorge Luis Borges and mathematics", "text": "Jorge Luis Borges and mathematics\n\nJorge Luis Borges and mathematics concerns several modern mathematical concepts found in certain essays and short stories of Argentinian author Jorge Luis Borges (1899-1986), including concepts such as set theory, recursion, chaos theory, and infinite sequences, although Borges' strongest links to mathematics are through Georg Cantor's theory of infinite sets, outlined in \"The Doctrine of Cycles\" (\"La doctrina de los ciclos\"). Some of Borges' most popular works such as \"The Library of Babel\" (\"La Biblioteca de Babel\"), \"The Garden of Forking Paths\" (\"El Jardín de Senderos que se Bifurcan\"), \"The Aleph\" (\"El Aleph\"), an allusion to Cantor's use of the Hebrew letter aleph (formula_1) to denote cardinality of transfinite sets, and \"The Approach to Al-Mu'tasim\" (\"El acercamiento a Almotásim\") illustrate his use of mathematics.\n\nAccording to Argentinian mathematician Guillermo Martínez, Borges at least had a knowledge of mathematics at the level of first courses in algebra and analysis at a university – covering logic, paradoxes, infinity, topology and probability theory. He was also aware of the contemporary debates on the foundations of mathematics. \n\nHis 1939 essay \"Avatars of the Tortoise\" (\"Avatares de la Tortuga\") is about infinity, and he opens by describing the book he would like to write on infinity: “five or seven years of metaphysical, theological, and mathematical training would prepare me (perhaps) for properly planning that book.” \n\nIn Borges' 1941 story, \"The Library of Babel\", the narrator declares that the collection of books of a fixed number of orthographic symbols and pages is unending. However, since the permutations of twenty-five orthographic symbols is finite, the library has to be periodic and self-repeating. \n\nIn his 1975 short story \"The Book of Sand\" (\"El Libro de Arena\"), he deals with another form of infinity; one whose elements are a dense set, that is, for any two elements, we can always find another between them. This concept was also used in the physical book the short-story came from, \"The Book of Sand\" book. The narrator describes the book as having pages that are \"infinitely thin\", which can be interpreted either as referring to a set of measure zero, or of having infinitesimal length, in the sense of second order logic.\n\nIn his 1936 essay \"The Doctrine of Cycles\" (\"La doctrina de los ciclos\"), published in his essay anthology of the same year Historia de la eternidad, Borges speculated about a universe with infinite time and finite mass: \"The number of all the atoms that compose the world is immense but finite, and as such only capable of a finite (though also immense) number of permutations. In an infinite stretch of time, the number of possible permutations must be run through, and the universe has to repeat itself. Once again you will be born from a belly, once again your skeleton will grow, once again this same page will reach your identical hands, once again you will follow the course of all the hours of your life until that of your incredible death.\" As usual with many of Borges' ideas and constructions, this line of thought was received as metaphysical speculation, a language and philosophical game. Yet almost one century later theoretical physicists are crossing the same paths, this time as a possible consequence of string theory: \"“Well, if the universe is really accelerating its expansion, then we know that it’s going to get infinitely large, and that things will happen over and over and over.” And if you have infinitely many tries at something, then every possible outcome is going to happen infinitely many times, no matter how unlikely it is.\".\n\nBorges in \"The Library of Babel\" states that \"The Library is a sphere whose exact center is any hexagon and whose circumference is unattainable\". The library can then be visualized as being a 3-manifold, and if the only restriction is that of being locally euclidean, it can equally well be visualized as a topologically non-trivial manifold such as a torus or a Klein bottle. \n\nIn his 1951 essay \"Pascal's sphere\" (\"La esfera de Pascal\"), Borges writes about a \"sphere with center everywhere and circumference nowhere\". A realization of this concept can be given by a sequence of spheres with contained centres and increasingly large radii, which eventually encompasses the entire space. This can be compared to the special point in \"The Aleph\" by the process of inversion.\n\nIn \"The Garden of Forking Paths\", Borges describes a novel by the fictional Chinese scholar Ts'ui Pên, whose plot bifurcates at every point in time. The idea of the flow of time branching can be compared to the many-worlds interpretation of quantum mechanics and the notion of multiverses present in some versions of string theory. Similarly, the infinitude of diverging, infinite universes in mathematical cosmology is reflected Borges' rejection of linear, absolute time. Borges' writings address the nature of entity and the possibility of infinite \"realities\", as in his essay \"New Time Refutations\" (1946).\n\nBifurcation theory is a model in chaos theory of order appearing from a disordered system, and is a local theory that describes behavior of systems at local points. Borges anticipated the development of bifurcation theory in mathematics, through \"The Garden of Forking Paths\" in 1941. In \"Garden\", Borges captured the idea of a system splitting into multiple, uncorrelated states. For example, if a leaf floating in a river comes across a rock, it must flow across either side of the rock, and the two possibilities are statistically uncorrelated.\n"}
{"id": "9212913", "url": "https://en.wikipedia.org/wiki?curid=9212913", "title": "List of impossible puzzles", "text": "List of impossible puzzles\n\nThis is a list of puzzles that cannot be solved.\n\n\n"}
{"id": "42908722", "url": "https://en.wikipedia.org/wiki?curid=42908722", "title": "Model order reduction", "text": "Model order reduction\n\nModel order reduction (MOR) is a technique for reducing the computational complexity of mathematical models in numerical simulations. As such it is closely related to the concept of metamodeling with applications in all areas of mathematical modelling.\n\nMany modern mathematical models of real-life processes pose challenges when used in numerical simulations, due to complexity and large size (dimension). Model order reduction aims to lower the computational complexity of such problems, for example, in simulations of large-scale dynamical systems and control systems. By a reduction of the model's associated state space dimension or degrees of freedom, an approximation to the original model is computed which is commonly referred to as a reduced order model.\n\nReduced order models are useful in settings where it is often unfeasible to perform numerical simulations using the complete full order model. This can be due to limitations in computational resources or the requirements of the simulations setting, for instance real-time simulation settings or many-query settings in which a large number of simulations needs to be performed. Examples of Real-time simulation settings include control systems in electronics and visualization of model results while examples for a many-query setting can include optimization problems and design exploration. In order to be applicable to real-world problems, often the requirements of a reduced order model are:\n\nModel order reduction techniques used most commonly nowadays can be broadly classified into 4 classes:\n\n\nThe simplified physics approach can be described to be analogous to the traditional Mathematical modelling approach, in which a less complex description of a system is constructed based on assumptions and simplifications using physical insight or otherwise derived information. However, this approach is not often the topic of discussion in the context of model order reduction as it is a general method in science, engineering and mathematics and is not the subject of the current article.\n\nThe remaining listed methods fall into the category of projection-based reduction. Projection-based reduction relies on the projection of either the model equations or the solution onto a basis of reduced dimensionality compared to the original solution space. Methods that also fall into this class but are perhaps less commonly found are:\n\n\nModel order reduction finds application within all fields involving mathematical modelling and many reviews exist for the topics of electronics, fluid- and structural mechanics.\n\nCurrent Problems in fluid mechanics involve large dynamical systems representing many effects on many different scales. Computational fluid dynamics studies often involve models solving the navier-stokes with a number of degrees of freedom in the order of magnitude upwards of formula_1. The first usage of model order reduction techniques dates back to the work of Lumley in 1967 where it was used to gain insight into the mechanisms and intensity of turbulence and large coherent structures present in fluid flow problems. Model order reduction also finds modern applications in Aeronautics to model the flow over the body of aircraft. An example can be found in Lieu et al in which the full order model of an F16 fighter-aircraft with over 2.1 million degrees of freedom, was reduced to a model of just 90 degrees of freedom. Additionally reduced order modelling has been applied to study rheology in Hemodynamics and the Fluid–structure interactionbetween the blood flowing through the vascular system and the vascular walls.\n\n\n"}
{"id": "37252648", "url": "https://en.wikipedia.org/wiki?curid=37252648", "title": "Moderne Algebra", "text": "Moderne Algebra\n\nModerne Algebra is a two-volume German textbook on graduate abstract algebra by , originally based on lectures given by Emil Artin in 1926 and by from 1924 to 1928. The English translation of 1949–1950 had the title Modern algebra, though a later, extensively revised edition in 1970 had the title Algebra.\n\nThe book was one of the first textbooks to use an abstract axiomatic approach to groups, rings, and fields, and was by far the most successful, becoming the standard reference for graduate algebra for several decades. It \"had a tremendous impact, and is widely considered to be the major text on algebra in the twentieth century.\"\n\nIn 1975 van der Waerden described the sources he drew upon to write the book.\n\nIn 1997 Saunders Mac Lane recollected the book's influence: \n\nModerne Algebra has a rather confusing publication history, because it went through many different editions, several of which were extensively rewritten with chapters and major topics added, deleted, or rearranged. In addition the new editions of first and second volumes were issued almost independently and at different times, and the numbering of the English editions does not correspond to the numbering of the German editions. In 1955 the title was changed from \"Moderne Algebra\" to \"Algebra\" following a suggestion of Brandt, with the result that the two volumes of the third German edition do not even have the same title.\n\nFor volume 1, the first German edition was published in 1930, the second in 1937 (with the axiom of choice removed), the third in 1951 (with the axiom of choice reinstated, and with more on valuations). The fourth edition appeared in 1955 (with the title changed to \"Algebra\"), the fifth in 1960, the sixth in 1964, the seventh in 1966, the eighth in 1971, the ninth in 1993. For volume 2, the first edition was published in 1931, the second in 1940, the third in 1955 (with the title changed to \"Algebra\"), the fourth in 1959 (extensively rewritten, with elimination theory replaced by algebraic functions of 1 variable), the fifth in 1967, and the sixth in 1993. The German editions were all published by Springer.\n\nThe first English edition was published in 1949–1950 and was a translation of the second German edition. There was a second edition in 1953, and a third edition under the new title Algebra in 1970 translated from the 7th German edition of volume 1 and the 5th German edition of volume 2. The three English editions were originally published by Ungar, though the 3rd English edition was later reprinted by Springer.\n\nThere were also Russian editions published in 1976 and 1979, and Japanese editions published in 1959 and 1967–1971.\n\n"}
{"id": "54225729", "url": "https://en.wikipedia.org/wiki?curid=54225729", "title": "Moschovakis coding lemma", "text": "Moschovakis coding lemma\n\nThe Moschovakis coding lemma is a lemma from descriptive set theory involving sets of real numbers in the axiom of determinacy (the principle that every two-player integer game is determined). The lemma was developed and named after the mathematician, Yiannis N. Moschovakis.\n\nThe lemma may be expressed generally as follows:\n\nLet Γ be a non-selfdual point-class closed under ∃ ω ω and ∧ , and ≺ a Γ well-founded relation on ω ω of rank θ ∈ ON. Let R ⊆ dom( ≺ ) × ω ω be such that ∀ x ∈ dom( ≺ ) ∃ y R ( x,y ) . Then there is a Γ set A ⊆ dom( ≺ ) × ω ω which is a choice set for R , that is: 1. ∀ α < θ ∃ x ∈ dom( ≺ ) ∃ y [ | x | ≺ = α ∧ A ( x,y )] . 2. ∀ x ∀ y A ( x,y ) → [ x ∈ dom( ≺ ) ∧ R ( x,y )] . Proof. We may assume θ is minimal so that the theorem fails, and fix ≺ , R , and a good universal set U ⊆ ( ω ω ) 3 for the Γ subsets of ( ω ω ) 2 . Easily, θ is a limit ordinal. For δ < θ , say u ∈ ω ω codes a δ -choice set provided (1) holds for α ≤ δ using A = U u , and (2) holds for A = U u where we replace x ∈ dom( ≺ ) with x ∈ dom( ≺ ) ∧| x | ≺ ≤ δ . By minimality of θ , for all δ < θ there are δ -choice sets. Play the game where I, II play out u,v ∈ ω ω , and II wins provided that if u codes a δ 1 -choice set for some δ 1 < θ , then v codes a δ 2 -choice set for some δ 2 > δ 1 . If I has a winning strategy, we get a Σ 1 1 set B of reals coding δ -choice sets for arbitrarily large δ < θ . Define then A ( x,y ) ↔∃ w ∈ B U ( w,x,y ), which easily works. Suppose now τ is a winning strategy for II. From the s - m - n theorem, let s : ( ω ω ) 2 → ω ω be continuous such that for all ,x,t,w, U ( s (,x ) ,t,w ) ↔ ∃ y ∃ z [ y ≺ x ∧ U (,y,z ) ∧ U ( z,t,w )]. By the recursion theorem, let 0 be such that U (0 ,x,z ) ↔ z = τ ( s (0 ,x )). A straightforward induction on | x | ≺ for x ∈ dom( ≺ ) shows that ∀ x ∈ dom( ≺ ) ∃ ! z U ( 0 ,x,z ), and ∀ x ∈ dom( ≺ ) ∀ z [ U ( 0 ,x,z ) → z codes a ≥| x | ≺ -choice set]. Let A ( x,y ) ↔∃ z ∈ dom( ≺ ) ∃ w [ U ( 0 ,z,w ) ∧ U ( w,x,y )]., \n\n"}
{"id": "837995", "url": "https://en.wikipedia.org/wiki?curid=837995", "title": "Protractor", "text": "Protractor\n\nA protractor is a measuring instrument, typically made of transparent plastic or glass, for measuring angles. Most protractors measure angles in\ndegrees (°). Radian-scale protractors measure angles in radians. Most protractors are divided into 180 equal parts.\n\nThey are used for a variety of mechanical and engineering-related applications, but perhaps the most common use is in geometry lessons in schools.\n\nSome protractors are simple half-discs. More advanced protractors, such as the bevel protractor, have one or two swinging arms, which can be used to help measure the angle.\n\nA bevel protractor is a graduated circular protractor with one pivoted arm; used for measuring or marking off angles. Sometimes Vernier scales are attached to give more precise readings. It has wide application in architectural and mechanical drawing, although its use is decreasing with the availability of modern drawing software or CAD.\n\nUniversal bevel protractors are also used by toolmakers; as they measure angles by mechanical contact they are classed as mechanical protractors.\n\nThe bevel protractor is used to establish and test angles to very close tolerances. It reads to 5 minutes or 1/12° and can measure any angle from 0° to 360°.\nThe bevel protractor consists of a beam, a graduated dial and a blade which is connected to a swivel plate (with Vernier scale) by thumb nut and clamp. When the edges of the beam and blade are parallel, a small mark on the swivel plate coincides with the zero line on the graduated dial. To measure an angle between the beam and the blade of 90° or less, the reading may be obtained direct from the graduation number on the dial indicated by the mark on the swivel plate. To measure an angle of over 90°, subtract the number of degrees as indicated on the dial from 180°, as the dial is graduated from opposite zero marks to 90° each way.\nSince the spaces, both on the main scale and the Vernier scale, are numbered both to the right and to the left from zero, any angle can be measured. The readings can be taken either to the right or to the left, according to the direction in which the zero on the main scale is moved.\n\nThe above picture illustrates a variety of uses of the bevel protractor.\nReading the Vernier scale:\n\nThe bevel protractor Vernier scale may have graduations of 5′ (minutes) or 1/12°. Each space on the Vernier scale is 5′ less than two spaces on the main scale. Twenty four spaces on the Vernier scale equal in extreme length twenty three double degrees. Thus the difference between the space occupied by 2° on a main scale and the space of the Vernier scale is equal to one twenty-fourth of 2°, or 5′.\n\nRead off directly from the main scale the number of whole degrees between 0 on this scale and the 0 of the Vernier scale. Then count, in the same direction, the number of spaces from the zero on the Vernier scale to a line that coincides with a line on the main scale; multiply this number by 5 and the product will be the number of minutes to be added to the whole number of degrees.\nFor example: Zero on the vernier scale has moved 28 whole degrees to the right of the 0 on the main scale and the 3rd line on the vernier scale coincides with a line upon the main scale as indicated. Multiplying 3 by 5, the product, 15, is the number of minutes to be added to the whole number of degrees, thus indicating a setting of 28 degrees and 15 minutes.\n\nProtractors have traditionally been one-sided. This is thought to be because early manufacturing methods set the tone of future production. Unfortunately, the two number scales on a one-sided protractor often confuse learners when first learning to measure and draw angles. However, in 2009, Jake Adamson, a maths teacher working at Musselburgh Grammar School invented and patented the first two sided protractor trademarked \"The Angler\". This was a double sided protractor with one scale on each side, avoiding the confusion of having two scales together and enabling easier measuring and drawing of angles. \"The Angler\" protractor has been widely adopted by schools in the UK.\n\n\n"}
{"id": "11496902", "url": "https://en.wikipedia.org/wiki?curid=11496902", "title": "Secondary polynomials", "text": "Secondary polynomials\n\nIn mathematics, the secondary polynomials formula_1 associated with a sequence formula_2 of polynomials orthogonal with respect to a density formula_3 are defined by\n\nTo see that the functions formula_5 are indeed polynomials, consider the simple example of formula_6 Then,\n\nwhich is a polynomial formula_8 provided that the three integrals in formula_9 (the moments of the density formula_10) are convergent.\n\n"}
{"id": "58684510", "url": "https://en.wikipedia.org/wiki?curid=58684510", "title": "Seneca effect", "text": "Seneca effect\n\nThe Seneca effect, or Seneca cliff or Seneca collapse, is a mathematical model, proposed by Ugo Bardi. This model is aimed to describe a class of problems in nature in which the decline is faster than growth, under the condition of some constraints. This model is closely related to the work \"The Limits to Growth\" issued by the Club of Rome in the Seventies and its main application is to describe various kind of economics under the condition of shortage of fossil fuels, e.g. in relation to the Hubbert curve.\n\nThe reference to the Latin philosopher and writer Seneca of the model's name lies in the verse: \"the raise is gradual, the ruin is precipitous\" (Lucius Anneus Seneca, \"Letters to Lucilius\", 91–63).\n\n\n"}
{"id": "220089", "url": "https://en.wikipedia.org/wiki?curid=220089", "title": "Set-builder notation", "text": "Set-builder notation\n\nIn set theory and its applications to logic, mathematics, and computer science, set-builder notation is a mathematical notation for describing a set by enumerating its elements or stating the properties that its members must satisfy.\n\nDefining sets by properties is also known as set comprehension, set abstraction or as defining a set's intension.\n\nSet-builder notation is sometimes simply referred to as \"set notation\", although this phrase may be better reserved for the broader class of means of denoting sets.\n\nA set is an unordered collection of \"elements\". (An \"element\" may also be referred to as a \"member\".) An element may be any mathematical entity.\n\nA set can be described directly by enumerating all of its elements between curly brackets, as in the following two examples:\n\n\nThis is sometimes called the \"roster method\" for specifying a set.\n\nWhen it is desired to denote a set that contains elements from a regular sequence an ellipses notation may be employed, as shown in the next examples:\n\n\nThere is no order among the elements of a set (this explains and validates the equality of the last example), but with the ellipses notation we show an ordered sequence before (or after) the ellipsis as a convenient notational vehicle for explaining to a reader which elements are in a set. The first few elements of the sequence are shown then the ellipses indicate that the simplest interpretation should be applied for continuing the sequence. Should no terminating value appear to the right of the ellipses then the sequence is considered to be unbounded.\n\nIn each preceding example, each set is described by enumerating its elements. Not all sets can be described in this way, or if they can, their enumeration may be too long or too complicated to be useful. Therefore, many sets are defined by a property that characterizes their elements. This characterization may be done informally using general prose, as in the following example.\n\n\nHowever, the prose approach may lack accuracy or be ambiguous. Thus, set builder notation is often used with a predicate characterizing the elements of the set being defined, as described in the following section.\n\nSet builder notation can be used to describe sets that are defined by a predicate, rather than explicitly enumerated. In this form, set builder notation has three parts: a variable, a colon or vertical bar separator, and a logical predicate. Thus there is a variable on the left of the separator, and a rule on the right of it. These three parts are contained in curly brackets:\nor\nThe vertical bar, which can also be written as a colon, is a separator that can be read as \"such that\", \"for which\", or \"with the property that\". The formula is said to be the \"rule\" or the \"predicate\". All values of \"x\" for which the predicate holds (is true) belong to the set being defined. All values of for which the predicate does not hold do not belong to the set. Thus formula_8 is the set of all values of that satisfy the formula . It may be the empty set, if no value of satisfies the formula.\n\nA domain can appear on the left of the vertical bar: \nor by adjoining it to the predicate:\nThe ∈ symbol here denotes set membership, while the formula_13 symbol denotes the logical \"and\" operator, known as logical conjunction. This notation represents the set of all values of that belong to some given set for which the predicate is true. (see \"Set existence axiom\" below)\n\nIn general, it is not a good idea to consider sets without defining a domain, as this would represent the subset of \"all possible things that may exist\" for which the predicate is true. This can easily lead to contradictions and paradoxes. For example, Russell's paradox shows that the expression formula_14 although seemingly well formed as a set builder expression, can not define a set without producing a contradiction.\n\nIn cases where the set \"E\" is clear from context, it may be not explicitly specified. It is common in the literature for an author to state the domain ahead of time, and then not specify it in the set builder notation. For example, an author may say something such as, \"Unless otherwise stated, variables are to be taken to be natural numbers.\"\n\nThe following examples illustrate particular sets defined by set builder notation via predicates. In each case, the domain is specified on the left side of the vertical bar, while the rule is specified on the right side. \n\n\n\n\n\n\nAn extension of set-builder notation replaces the single variable with a term that may include one or more variables, combined with functions acting on them. So instead of formula_30, we may have formula_31 which should be read\n\nFor example:\n\nWhen inverse functions can be explicitly stated, the expression on the left can be eliminated through simple substitution. Consider the example set formula_37. Make the substitution formula_40, which is to say formula_41, then replace \"t\" in the set builder notation to find \n\nTwo sets are equal if and only if they have the same elements. Sets defined by set builder notation are equal if and only if their set builder rules, including the domain specifiers, are equivalent. That is \nif and only if \nTherefore, in order to prove the equality of two sets defined by set builder notation, it suffices to prove the equivalence of their predicates, including the domain qualifiers.\n\nFor example,\nbecause the two rule predicates are logically equivalent:\nThis equivalence holds because, for any real number \"x\", we have formula_47 if and only if \"x\" is a rational number with formula_48. In particular, both sets are equal to the set formula_49.\n\nIn many formal set theories, such as Zermelo–Fraenkel set theory, set builder notation is not part of the formal syntax of the theory. Instead, there is a set existence axiom scheme, which states that if \"E\" is a set and Φ(x) is a formula in the language of set theory, then there is a set \"Y\" whose members are exactly the elements of \"E\" that satisfy Φ:\nThe set \"Y\" obtained from this axiom is exactly the set described in set builder notation as formula_51.\n\nIn Z notation, the set of all \"x\" in a universe of discourse \"A\" that satisfy the condition \"P\"(\"x\") is written\nIn Z, the set membership of an element \"x\" in set \"A\" is written as formula_53 instead of formula_54. Versions of set builder notation are also available in Z which allow for terms more complicated than a single variable, using a bullet to indicate the form of members of the set. For example,\ndenotes the set of all values \"F\"(\"x\"), where \"x\" is in \"A\" and \"P\"(\"x\") holds.\n\nA similar notation available in a number of programming languages (notably Python and Haskell) is the list comprehension, which combines map and filter operations over one or more lists.\n\nIn Python, the set-builder's braces are replaced with square brackets, parentheses, or curly braces, giving list, generator, and set objects, respectively. Python uses an English-based syntax. Haskell replaces the set-builder's braces with square brackets and uses symbols, including the standard set-builder vertical bar.\n\nThe same can be achieved in Scala using Sequence Comprehensions, where the \"for\" keyword returns a list of the yielded variables using the \"yield\" keyword.\n\nConsider these set-builder notation examples in some programming languages:\nThe set builder notation and list comprehension notation are both instances of a more general notation known as \"monad comprehensions\", which permits map/filter-like operations over any monad with a zero element.\n\n"}
{"id": "29391369", "url": "https://en.wikipedia.org/wiki?curid=29391369", "title": "Structuralism (philosophy of mathematics)", "text": "Structuralism (philosophy of mathematics)\n\nStructuralism is a theory in the philosophy of mathematics that holds that mathematical theories describe structures of mathematical objects. Mathematical objects are exhaustively defined by their place in such structures. Consequently, structuralism maintains that mathematical objects do not possess any intrinsic properties but are defined by their external relations in a system. For instance, structuralism holds that the integer 1 is exhaustively defined by being the successor of 0 in the structure of the theory of natural numbers. By generalization of this example, any integer is defined by their respective place in this structure of the number line. Other examples of mathematical objects might include lines and planes in geometry, or elements and operations in abstract algebra.\n\nStructuralism is an epistemologically realistic view in that it holds that mathematical statements have an objective truth value. However, its central claim only relates to what \"kind\" of entity a mathematical object is, not to what kind of \"existence\" mathematical objects or structures have (not, in other words, to their ontology). The kind of existence mathematical objects have would clearly be dependent on that of the structures in which they are embedded; different sub-varieties of structuralism make different ontological claims in this regard.\n\nStructuralism in the philosophy of mathematics is particularly associated with Paul Benacerraf, Geoffrey Hellman, Michael Resnik and Stewart Shapiro.\n\nThe historical motivation for the development of structuralism derives from a fundamental problem of ontology. Since Medieval times, philosophers have argued as to whether the ontology of mathematics contains abstract objects. In the philosophy of mathematics, an abstract object is traditionally defined as an entity that: (1) exists independent of the mind; (2) exists independent of the empirical world; and (3) has eternal, unchangeable properties. Traditional mathematical Platonism maintains that some set of mathematical elements–natural numbers, real numbers, functions, relations, systems–are such abstract objects. Contrarily, mathematical nominalism denies the existence of any such abstract objects in the ontology of mathematics. \n\nIn the late 19th and early 20th century, a number of anti-Platonist programs gained in popularity. These included intuitionism, formalism, and predicativism. By the mid-20th century, however, these anti-Platonist theories had a number of their own issues. This subsequently resulted in a resurgence of interest in Platonism. It was in this historic context that the motivations for structuralism developed. In 1965, Paul Benacerraf published a paradigm changing article entitled \"What Numbers Could Not Be\". Benacerraf concluded, on two principal arguments, that set-theoretic Platonism cannot succeed as a philosophical theory of mathematics. \n\nFirstly, Benacerraf argued that Platonic approaches do not pass the ontological test. He developed an argument against the ontology of set-theoretic Platonism, which is now historically referred to as Benacerraf's identification problem. Benacerraf noted that there are elementarily equivalent, set-theoretic ways of relating natural numbers to pure sets. However, if someone asks for the \"true\" identity statements for relating natural numbers to pure sets, then different set-theoretic methods yield contradictory identity statements when these elementarily equivalent sets are related together. This generates a set-theoretic falsehood. Consequently, Benacerraf inferred that this set-theoretic falsehood demonstrates it is impossible for there to be any Platonic method of reducing numbers to sets that reveals any abstract objects. \n\nSecondly, Benacerraf argued that Platonic approaches do not pass the epistemological test. Benacerraf contended that there does not exist an empirical or rational method for accessing abstract objects. If mathematical objects are not spatial or temporal, then Benacerraf infers that such objects are not accessible through the causal theory of knowledge. The fundamental epistemological problem thus arises for the Platonist to offer a plausible account of how a mathematician with a limited, empirical mind is capable of accurately accessing mind-independent, world-independent, eternal truths. It was from these considerations, the ontological argument and the epistemological argument, that Benacerraf's anti-Platonic critiques motivated the development of structuralism in the philosophy of mathematics.\n\nShapiro divides structuralism into three major schools of thought. These schools are referred to as the \"ante rem\", the \"in re\", and the \"post rem\". \n\nThe \"ante rem\" structuralism (\"before the thing\"), or abstract structuralism or abstractionism (particularly associated with Michael Resnik, Stewart Shapiro, and Edward N. Zalta) has a similar ontology to Platonism (see also modal neo-logicism). Structures are held to have a real but abstract and immaterial existence. As such, it faces the standard epistemological problem, as noted by Benacerraf, of explaining the interaction between such abstract structures and flesh-and-blood mathematicians. \n\nThe \"in re\" structuralism (\"in the thing\"), or modal structuralism (particularly associated with Geoffrey Hellman), is the equivalent of Aristotelian realism (realism in truth value, but anti-realism about abstract objects in ontology). Structures are held to exist inasmuch as some concrete system exemplifies them. This incurs the usual issues that some perfectly legitimate structures might accidentally happen not to exist, and that a finite physical world might not be \"big\" enough to accommodate some otherwise legitimate structures.\n\nThe \"post rem\" structuralism (\"after the thing\"), or eliminative structuralism (particularly associated with Paul Benacerraf), is anti-realist about structures in a way that parallels nominalism. Like nominalism, the \"post rem\" approach denies the existence of abstract mathematical objects with properties other than their place in a relational structure. According to this view mathematical \"systems\" exist, and have structural features in common. If something is true of a structure, it will be true of all systems exemplifying the structure. However, it is merely instrumental to talk of structures being \"held in common\" between systems: they in fact have no independent existence.\n\n\n\n\n"}
