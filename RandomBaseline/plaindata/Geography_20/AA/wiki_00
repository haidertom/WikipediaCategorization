{"id": "13348166", "url": "https://en.wikipedia.org/wiki?curid=13348166", "title": "Federal Geographic Data Committee", "text": "Federal Geographic Data Committee\n\nThe Federal Geographic Data Committee (FGDC) is a United States government committee which promotes the coordinated development, use, sharing, and dissemination of geospatial data on a national basis. Its 32 members are representatives from the Executive Office of the President, and Cabinet level and independent Federal agencies. The Secretary of the Department of the Interior chairs the FGDC, with the Deputy Director for Management, Office of Management and Budget (OMB) as Vice-Chair.\n\nThe FGDC coordinates the sharing of geographic data, maps, and online services through an online portal, \"geodata.gov\", that searches metadata held within the NSDI Clearinghouse Network.\n\nOn July 23, 2009, the Energy and Mineral Resources Subcommittee of the House Natural Resources Committee held an oversight hearing on federal geospatial data management. Rep. John Sarbanes of Maryland quoted a U.S. General Accounting Office (GAO) report from his briefing material saying that only 4 of the FGDC member agencies were in compliance.\n"}
{"id": "53452291", "url": "https://en.wikipedia.org/wiki?curid=53452291", "title": "Geography of aging", "text": "Geography of aging\n\nGeography of aging or gerontological geography is an emerging field of knowledge of Human Geography that analyzes the socio-spatial implications of aging of the population from the understanding of the relationships between the physical-social environment and the elderly, at different scales, micro (City, region, country), etc.\n\nSince the 1970s in a number of developed countries such as the United States, Canada, the United Kingdom, Germany, Sweden, France, Spain, Australia, New Zealand and Japan, there have been increasing studies focusing on the understanding of spatial patterns of aging population, as well as aspects related to residential changes and provision of health and social services. Among the geographers of aging is S. Harper, who identified the phenomenon of aging associated with the social construction of old age and the processes of residential mobility of this group to the urban periphery, mainly nursing homes and sheltered housing.\n\nThe contribution of geographers of aging, such as Graham D. Rowles, SM. Golant, S. Harper, G. Laws, are contributing to environmental gerontology by understanding the environmental aspects of gerontology in developed and developing countries. Also in Spain, some geographers, such as Gloria Fernández-Mayoralas, Fermina Rojo-Pérez and Vicente Rodríguez-Rodríguez, have made outstanding contributions to the study of residential strategies, access to health services, and, in general, quality of Life of the elderly, as well as the impacts of Northern European retirees on the Costa del Sol (Spain).\n\nIn Latin America and Spain, Diego Sánchez-González has shed light on the deepening of issues such as the physical-built and social environment and the quality of life of the elderly; the importance of the natural environment (therapeutic natural landscape) on active and healthy aging in the place; residential strategies for the maintenance of the elderly in the communities; the socio-environmental vulnerability of the elderly in the face of climate change; as well as issues related to the attachment to the place (identity and public space); elderly people with disabilities and social exclusion; leisure and tourism of elderly; and the planning of gerontological and geriatric services.\n\n"}
{"id": "1870398", "url": "https://en.wikipedia.org/wiki?curid=1870398", "title": "Geotagging", "text": "Geotagging\n\nGeotagging or GeoTagging, is the process of adding geographical identification metadata to various media such as a geotagged photograph or video, websites, SMS messages, QR Codes or RSS feeds and is a form of geospatial metadata. This data usually consists of latitude and longitude coordinates, though they can also include altitude, bearing, distance, accuracy data, and place names, and perhaps a time stamp.\n\nGeotagging can help users find a wide variety of location-specific information from a device. For instance, someone can find images taken near a given location by entering latitude and longitude coordinates into a suitable image search engine. Geotagging-enabled information services can also potentially be used to find location-based news, websites, or other resources. Geotagging can tell users the location of the content of a given picture or other media or the point of view, and conversely on some media platforms show media relevant to a given location.\n\nThe related term geocoding refers to the process of taking non-coordinate based geographical identifiers, such as a street address, and finding associated geographic coordinates (or vice versa for reverse geocoding). Such techniques can be used together with geotagging to provide alternative search techniques.\n\nGeotagging has become a popular feature on several social media platforms, such as Facebook and Instagram.\n\nFacebook users can geotag photos that can be added to the page of the location they are tagging. Users may also use a feature that allows them to find nearby Facebook friends, by generating a list of people according to the location tracker in their mobile devices.\n\nInstagram uses a map feature that allows users to geotag photos. The map layout pin points specific photos that the user has taken on a world map.\n\nThe geographical location data used in geotagging will, in almost every case, be derived from the global positioning system, and based on a latitude/longitude-coordinate system that presents each location on the earth from 180° west through 180° east along the Equator and 90° north through 90° south along the prime meridian.\n\nThere are two main options for geotagging photos; capturing GPS information at the time the photo is taken or \"attaching\" the photograph to a map after the picture is taken.\n\nIn order to capture GPS data at the time the photograph is captured, the user must have a camera with built in GPS or a standalone GPS along with a digital camera. Because of the requirement for wireless service providers in United States to supply more precise location information for 911 calls by September 11, 2012, more and more cell phones have built-in GPS chips. Most smart phones already use a GPS chip along with built-in cameras to allow users to automatically geotag photos. Others may have the GPS chip and camera but do not have internal software needed to embed the GPS information within the picture. A few digital cameras also have built-on or built-in GPS that allow for automatic geotagging.\nDevices use GPS, A-GPS or both. A-GPS can be faster getting an initial fix if within range of a cell phone tower, and may work better inside buildings. Traditional GPS does not need cell phone towers and uses standard GPS signals outside of urban areas. Traditional GPS tends to use more battery power. Almost any digital camera can be coupled with a stand-alone GPS and post processed with photo mapping software, to write the location information to the image's exif header.\n\nGPS coordinates may be represented in text in a number of ways, with more or fewer decimals:\n\nWith photos stored in JPEG, TIFF and many other file formats, the geotag information, storing camera location and sometimes heading, is typically embedded in the metadata, stored in Exchangeable image file format (Exif) or Extensible Metadata Platform (XMP) format. These data are not visible in the picture itself but are read and written by special programs and most digital cameras and modern scanners. Latitude and longitude are stored in units of degrees with decimals. This geotag information can be read by many programs, such as the cross-platform open source ExifTool. An example readout for a photo might look like:\n\nor the same coordinates could also be presented as decimal degrees:\n\nWhen stored in Exif, the coordinates are represented as a series of rational numbers in the GPS sub-IFD. Here is a hexadecimal dump of the relevant section of the Exif metadata (with big-endian byte order):\n\nIn the field of remote sensing the geotagging goal is to store coordinates of every pixel in the image. One approach is used with the orthophotos where we store coordinates of four corners and all the other pixels can be georeferenced by interpolation. The four corners are stored using GeoTIFF or World file standards. Hyperspectral images take a different approach defining a separate file of the same spatial dimensions as the image where latitude and longitude of each pixel are stored as two 2D layers in so called \"Input geometry data\" (IGM) files, also known as GEO files.\n\nAudio/video files can be geotagged via: metadata, audio encoding, overlay, or with companion files. Metadata records the geospatial data in the encoded video file to be decoded for later analysis. One of the standards used with unmanned aerial vehicle is which allows geocoding of corner points and horizon lines in individual video frames. Audio encoding involves a process of converting gps data into audio data such as modem squawk. Overlay involves overlaying GPS data as text on the recorded video. Companion files are separate data files which correspond to respective audio/video files. Companion files are typically found in the .KML and .GPX data formats. For audio and video files which use the vorbiscomment metadata format (including Opus, Ogg Vorbis, FLAC, Speex, and Ogg Theora), there is a proposed GEO LOCATION field which can be used. Like all vorbiscomments, it is plain text, and it takes the form:\n\ncodice_25\n\nfor example:\n\ncodice_26\n\nThe GeoSMS standard works by embedding one or more 'geo' URIs in the body of an SMS, for example:\n\nRFC 1876 defines a means for expressing location information in the Domain Name System. LOC resources records can specify the latitude, longitude, altitude, precision of the location, and the physical size of on entity attached to an IP address. However, in practice not all IP addresses have such a record, so it is more common to use geolocation services to find the physical location of an IP address.\n\nThe GeoURL method requires the ICBM tag (plus optional Dublin Core metadata), which is used to geotag standard web pages in HTML format:\nThe similar Geo Tag format allows the addition of placename and region tags:\nThe RDF method is defined by W3 Group and presents the information in RDF tags:\n<rdf:RDF xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n</rdf:RDF>\nThe Geo microformat allows coordinates within HyperText Markup Language pages to be marked up in such a way that they can be \"discovered\" by software tools. Example:\nA proposal has been developed to extend Geo to cover other bodies, such as Mars and the Moon.\n\nAn example is the Flickr photo-sharing Web site, which provides geographic data for any geotagged photo in all of the above-mentioned formats.\n\nNo industry standards exist, however there are a variety of techniques for adding geographical identification metadata to an information resource. One convention, established by the website Geobloggers and used by more and more sites, e.g. photo sharing sites Panoramio and Flickr, and the social bookmarking site del.icio.us, enables content to be found via a location search. Such sites allow users to add metadata to an information resource via a set of so-called \"machine tags\" (see folksonomy).\n\nThis describes the geographic coordinates of a particular location in terms of latitude (geo:lat) and longitude (geo:lon). These are expressed in decimal degrees in the WGS84 datum, which has become something of a default geodetic datum with the advent of GPS.\n\nUsing three tags works within the constraint of having tags that can only be single 'words'. Identifying geotagged information resources on sites like Flickr and del.icio.us is done by searching for the 'geotagged' tag, since the tags beginning 'geo:lat=' and 'geo:lon=' are necessarily very variable.\n\nAnother option is to tag with a Geohash:\n\nA further convention proposed by FlickrFly adds tags to specify the suggested viewing angle and range when the geotagged location is viewed in Google Earth:\n\nThese three tags would indicate that the camera is pointed heading 225° (south west), has a 45° tilt and is 560 metres from the subject.\n\nWhere the above methods are in use, their coordinates may differ from those specified by the photo's internal Exif data, for example because of a correction or a difference between the camera's location and the subject's.\n\nIn order to integrate geotags in social media and enhance text readability or oral use, the concept of 'meetag' or tag-to-meet has been proposed. Differing from hashtag construction, meetag includes the geolocation information after an underscore. A meetag is therefore a word or an unspaced phrase prefixed with an underscore (\"_\"). Words in messages on microblogging and social networking services may be tagged by putting \"_\" before them, either as they appear in a sentence, (e.g. \"There is a concert going _montreuxjazzfestival\", \"the world wide web was invented _cern _geneve\", ...) or appended to it.\n\nGeoblogging attaches specific geographic location information to blog entries via \"geotags\". Searching a list of blogs and pictures tagged using geotag technology allows users to select areas of specific interest to them on interactive maps.\n\nThe progression of GPS technology, along with the development of various online applications, has fueled the popularity of such tagged blogging, and the combination of GPS Phones and GSM localization, has led to the moblogging, where blog posts are tagged with exact position of the user. Real-time geotagging relays automatically geotagged media such as photos or video to be published and shared immediately.\n\nFor better integration and readability of geotags into blog texts, the meetag syntax has been proposed, which transforms any word, sentence, or precise geolocalization coordinates prefixed with an underscore into a 'meetag'. It not only lets one express a precise location but also takes in account dynamically changing geolocations.\n\nOne of the first attempts to initiate the geotagging aspect of searching and locating articles seems to be the now-inoperative site Wikinear.com, launched in 2008, which showed the user Wikipedia pages that are geographically closest to one's current location.\n\nThe 2009 app Cyclopedia works relatively well showing geotagged Wikipedia articles located within several miles of ones location, integrated with a street-view mode, and 360-degree mode.\n\nThe app Respotter Wiki, launched in 2009, claims to feature Wikipedia searching via a map, also allowing users to interact with people around them, via messaging and reviews, etc. The app, in its current function, however, seems to give only geotagged photo results.\n\nAs of 2017, the provides a simple map search tool which can display tagged articles near to a particular location, as well as a variety of more sophisticated tools integrated with external mapping services.\n\nFollowing a scientific study and several demonstrative websites, a discussion on the privacy implications of geotagging has raised public attention. In particular, the automatic embedding of geotags in pictures taken with smartphones is often ignored by cell-phone users. As a result, people are often not aware that the photos they publish on the Internet have been geotagged. Many celebrities reportedly gave away their home location without knowing it. According to the study, a significant number of for-sale advertisements on Craigslist, that were otherwise anonymized, contained geotags, thereby revealing the location of high-valued goods—sometimes in combination with clear hints to the absence of the offerer at certain times. Publishing photos and other media tagged with exact geolocation on the Internet allows random people to track an individual's location and correlate it with other information. Therefore, criminals could find out when homes are empty because their inhabitants posted geotagged and timestamped information both about their home address and their vacation residence. These dangers can be avoided by removing geotags with a metadata removal tool for photos before publishing them on the Internet.\n\nIn 2007, four United States Army Apache helicopters were destroyed on the ground by Iraqi insurgent mortar fire; the insurgents had made use of embedded coordinates in web-published photographs (geotagging) taken of the helicopters by soldiers.\n\nAnother newly realised danger of geotagging is the location information provided to criminal gangs and poachers on the whereabouts of often endangered animals. This can effectively make tourists scouts for these poachers, so geotagging should be turned off when photographing these animals.\n\n"}
{"id": "9357204", "url": "https://en.wikipedia.org/wiki?curid=9357204", "title": "Gordon Cullen", "text": "Gordon Cullen\n\nThomas Gordon Cullen (9 August 1914 – 11 August 1994) was an influential British architect and urban designer who was a key motivator in the Townscape movement. He is best known for the book \"Townscape\", first published in 1961. Later editions of \"Townscape \" were published under the title \"The Concise Townscape\".\n\nCullen was born in Calverley, Pudsey, near Leeds, Yorkshire, England. He studied architecture at the Royal Polytechnic Institution, the present day University of Westminster, and subsequently worked as a draughtsman in various architects' offices including that of Berthold Lubetkin and Tecton, but he never qualified or practised as an architect.\nBetween 1944 and 1946 he worked in the planning office of the Development and Welfare Department in Barbados, as his poor eyesight meant that he was unfit to serve in the British armed forces. He later returned to London and joined the \"Architectural Review\" journal, first as a draughtsman and then as a writer on planning policies. There he produced a large number of influential editorials and case studies on the theory of planning and the design of towns. Many improvements in the urban and rural environment in Britain during the 1950s and 1960s. He was also involved in the Festival of Britain in 1951. One of the few large scale Cullen works on public display is the mural in the foyer of the Erno Goldfinger designed Greenside Primary School in west London, completed in 1953. His 1958 ceramic mural in Coventry, depicting the history of the City and its post-war regeneration, is on a much grander scale though now relocated away from its original central location.\n\nHis techniques consisted largely of sketchy drawings that conveyed a particularly clear understanding of his ideas, and these had a considerable influence on subsequent architectural illustration styles. He also illustrated several books by other various authors, before writing his own book - based on the idea of Townscape - in 1961. The Concise Townscape has subsequently been republished around 15 times, proving to be one of the most popular books on Urban Design in the 20th Century.\n\nIn 1956 Cullen became a freelance writer and consultant and, in the years immediately following he advised the cities of Liverpool and Peterborough on their reconstruction and redevelopment plans. In 1960 he was invited to India to advise on the planning aspects of the Ford Foundation's work in New Delhi and Calcutta and so in 1962 he and his family lived in India for 6 months while he worked on the projects. Later, his work included planning advice to the city of Glasgow and during the 1980s the London Docklands Development Corporation.\n\nFor a while Cullen teamed up with a student, David Price, and they formed an architectural firm together - Price & Cullen. They won a competition in London in the 1980s and together designed and oversaw the building of the Swedish Quays housing development in Docklands. They worked together until 1990 as Price's first child was born and because Cullen's health was deteriorating. Price died in 2009 at the age of 53.\n\nCullen lived in the small village of Wraysbury (Berkshire) from 1958 until his death, aged 80, on 11 August 1994, following a serious stroke. After his passing, David Gosling and Norman Foster collected various examples of his work and put them together in the book \"Visions of Urban Design\".\n\nIn 1972 he was elected Honorary Fellow of the RIBA. In 1975 he was awarded with an RDI for illustration and Townscape. The following year he was awarded a medal from The American Institute of Architects. In 1978 he was appointed a CBE for his contribution to architecture from Elizabeth II of the United Kingdom.\n\n\n\n\n"}
{"id": "41677283", "url": "https://en.wikipedia.org/wiki?curid=41677283", "title": "Heading (navigation)", "text": "Heading (navigation)\n\nIn navigation, the heading of a vessel or aircraft is the compass direction in which the craft's bow or nose is pointed. This may not necessarily be the direction that the vehicle actually travels, which is known as its \"course\" or \"track\". Any difference between the heading and course is due to the motion of the underlying medium, the air or water, or other effects like skidding or slipping. The difference is known as the \"drift\", and can be determined by the \"navigational triangle\".\n\nHeading is typically based on compass directions, so 0° (or 360°) indicates a direction toward true North, 90° indicates a direction toward true East, 180° is true South, and 270° is true West.\n"}
{"id": "4277031", "url": "https://en.wikipedia.org/wiki?curid=4277031", "title": "Index of gardening articles", "text": "Index of gardening articles\n\nThis is a list of gardening topics.\n\nAesthetics\n- African Violet Society of America\n- Allotment\n- Aquascaping\n- Arboretum\n- Architectural theory\n\nBonsai\n- Botanical gardens\n\nCalifornia native plants\n- Chelsea Flower Show\n- Community garden\n- Companion planting\n- Compost\n- Composting\n- Conservation\n\nDesign\n\nEnglish garden\n- Environmental design\n\nFlowerbed\n- Fountains\n- French intensive gardening\n- French landscape garden\n\nGarden\n- Garden designer\n- Gardener\n- Gardening\n- Garden buildings\n- List of gardens in fiction\n- Garden tool\n- Garden Gnome Liberation Front\n- Garden à la française\n- Gardens of the French Renaissance\n- Gardens of Versailles\n- \n- Giardino all'italiana \n- Grandi Giardini Italiani\n- Growbag\n- Guerrilla gardening\n\nHistory of gardening\n- History of gardens\n- History of landscape architecture\n- Hedge\n- Herbaceous border\n- Home economics\n- Horticulture\n\nInvasive species\n- Italian Renaissance garden\n\nJapanese garden\n- Japanese rock garden\n\n- \n- \n- List of notable historical gardens\n- Land Arts of the American West\n- Landscape architecture\n- Landscape design\n- Landscape detailing\n- Landscape garden\n- Landscape manager\n- Landscape products\n- Lawn\n- Lawnmower\n- List of botanical gardens in Italy\n- List of botanical gardens in the United States\n- List of Chinese gardens\n- List of garden and horticulture books\n- List of garden features\n- List of garden plants\n- List of gardens in Italy\n- List of invasive species in North America\n- List of organic gardening and farming topics\n- List of professional gardeners\n- Local food\n\nNative plant gardening\n- Natural landscaping\n- Nature and Culture\n- Never Ending Gardens\n- No-dig gardening\n\nOrganic gardening\n- Arboreta\n\nPatio garden\n- Pergola\n- Parterre\n- Permaculture\n- Plant community\n- Planting design\n- Flower pot\n- Pruning\n\nRain gardens\n- Raised bed gardening\n- Remarkable Gardens of France\n- Rock garden\n- Roof garden\n- Roman garden\n\nSculpture garden\n- Sheet mulching\n- Shrub\n- Spanish garden\n- Spanish gardens\n- Square foot gardening\n- Statuary\n- Sustainable art\n- Sustainable design\n- Sustainability\n\nterrace\n- Topiary\n- Tree\n- Tropical garden\n\nVegetable farming\n\nWater garden\n- water feature\n- Wildlife corridor\n- Wildlife garden\n\nXeriscaping\n\nZen garden\n\n"}
{"id": "53091648", "url": "https://en.wikipedia.org/wiki?curid=53091648", "title": "José Antonio Sosa", "text": "José Antonio Sosa\n\nJosé Antonio Sosa Díaz-Saavedra was brought up in a family with an interest in art and history, especially in relation to the Canary Islands. As a result, he was encouraged to study architecture, graduating from the Superior Technical School of Architecture of Madrid in 1981. In 1994, he earned a Ph.D at the School of Architecture, University of Las Palmas de Gran Canaria.\n\nAfter completing his doctoral studies, he obtained the chair of Architectural Projects in the Department of Graphic Expression and Architectural Projects of the EALPGC, School of Architecture of Las Palmas de Gran Canaria, where he has been since 1983.\n\nIn the year 2000 he was \"Visiting Scholar\" and taught in the Department of Architecture, belonging to the Graduate School of Design, Harvard University, he has also participated as a jury in several universities among which is the Swiss Federal Institute of Technology, ETH (Studio Basel).\n\nIn 2015 he joined the University Institute of Intelligent Systems and Numerical Applications in Engineering of the ULPGC, University of Las Palmas de Gran Canaria.\n\nJosé Antonio Sosa has been the main proponent of modern architectural development in the early twentieth century in the Canary Islands and Spain, promoting its place in the history of urban and cultural heritage. As a result, he was appointed a member of the Royal Spanish Academy.\n\nIn 1996 Professor Sosa joined his studio to Magüi González creating the Nred Arquitectos Group, where several projects stand out: the City of Justice (New Law Courts Headquarters) and the rehabilitation of the Town Halls (Town Hall Houses) of Las Palmas de Gran Canaria.\n\nSince 2011, he has founded a new studio for architecture and urban planning together with Evelyn Alonso Rohner.\n\nIn 2015 José Antonio Sosa is the managing editor of the Arquiatesis editorial line for the publication and publication of theses and research works in architecture.\n\nSosa, has participated in the following exhibitions:\n\n\nProfessor Sosa has been awarded in the following competitions:\n\n\nSome of them are:\n\n\nSosa has published the following texts:\n\n\nAmong them are:\n\n"}
{"id": "469799", "url": "https://en.wikipedia.org/wiki?curid=469799", "title": "Land rehabilitation", "text": "Land rehabilitation\n\nLand rehabilitation is the process of returning the land in a given area to some degree of its former state, after some process (industry, natural disasters, etc.) has resulted in its damage. Many projects and developments will result in the land becoming degraded, for example mining, farming and forestry.\n\nModern mine rehabilitation aims to minimize and mitigate the environmental effects of modern mining, which may in the case of open pit mining involve movement of significant volumes of rock. Rehabilitation management is an ongoing process, often resulting in open pit mines being backfilled.\n\nAfter mining finishes, the mine area must undergo rehabilitation. \n\nFor underground mines, rehabilitation is not always a significant problem or cost. This is because of the higher grade of the ore and lower volumes of waste rock and tailings. In some situations, stopes are backfilled with concrete slurry using waste, so that minimal waste is left at surface.\n\nThe removal of plant and infrastructure is not always part of a rehabilitation programme, as many old mine plants have cultural heritage and cultural value. Often in gold mines, rehabilitation is performed by scavenger operations which treat the soil within the plant area for spilled gold using modified placer mining gravity collection plants.\n\nAlso possible is that the section of the mine that is below ground, is kept and used to provide heating, water and/or methane. Heat extraction can be done using heat exchangers, that convey the heat to a nearby city (hence making it be used for district heating purposes.\nWater can be harvested from the mine as well (mines are often filled with water once the mine has been shut down and the pumps no longer operate). Methane is also often present in the mine shafts, in small quantities (often around 0,1%). This can still be recovered though with specialised systems. An added advantage of recovering the methane finally is that the methane does not come into the atmosphere, and so does not contribute to global warming.\n\n\n"}
{"id": "9502", "url": "https://en.wikipedia.org/wiki?curid=9502", "title": "List of explorations", "text": "List of explorations\n\nSome of the most important explorations of State Societies, in chronological order:\n\n"}
{"id": "55792911", "url": "https://en.wikipedia.org/wiki?curid=55792911", "title": "List of international prime ministerial trips made by Jacinda Ardern", "text": "List of international prime ministerial trips made by Jacinda Ardern\n\nThis is a list of international prime ministerial trips made by Jacinda Ardern, the 40th Prime Minister of New Zealand. , Jacinda Ardern has made eight international trips to twelve sovereign countries and two associated states of New Zealand, since her premiership began on 26 October 2017.\n\n"}
{"id": "11486275", "url": "https://en.wikipedia.org/wiki?curid=11486275", "title": "List of towns and cities with 100,000 or more inhabitants/cityname: Z", "text": "List of towns and cities with 100,000 or more inhabitants/cityname: Z\n\n\n"}
{"id": "32374980", "url": "https://en.wikipedia.org/wiki?curid=32374980", "title": "Nautical measured mile", "text": "Nautical measured mile\n\nA nautical measured mile is a nautical mile which is marked by two pairs of towers. A mile is measure by sailing on a given bearing and lining up the pairs of towers. The start of the mile is recorded when the first pair of towers line up and the end of the mile recorded when the second pair line up.\n\nTo accurately measure performance ships must make at least four to six runs in both directions to allow for the wind and tide. \nThere are several nautical measured miles around the British Isles:\n\n"}
{"id": "52744449", "url": "https://en.wikipedia.org/wiki?curid=52744449", "title": "Nightscape", "text": "Nightscape\n\nA nightscape or night view is the visible features of an area of landscape as viewed at night.\n"}
{"id": "12978962", "url": "https://en.wikipedia.org/wiki?curid=12978962", "title": "Plane table", "text": "Plane table\n\nA plane table (plain table prior to 1830) is a device used in surveying and related disciplines to provide a solid and level surface on which to make field drawings, charts and maps. The early use of the name \"plain table\" reflected its simplicity and plainness rather than its flatness.\n\nThe earliest mention of a plane table dates to 1551 in Abel Foullon's \"Usage et description de l'holomètre\", published in Paris. However, since Foullon's description was of a complete, fully developed instrument, it must have been invented earlier.\n\nA brief description was also added to the 1591 edition of Digge's \"Pantometria\". The first mention of the device in English was by Cyprian Lucar in 1590.\n\nSome have credited Johann Richter, also known as Johannes Praetorius, a Nuremberg mathematician, in 1610 with the first plane table, but this appears to be incorrect.\n\nThe plane table became a popular instrument for surveying. Its use was widely taught. Some considered it a substandard instrument compared to other devices such as the theodolite, since it was relatively easy to use. By allowing the use of graphical methods rather than mathematical calculations, it could be used by those with less education than other instruments. The addition of a camera to the plane table, as was done from 1890 by Sebastian Finsterwalder in conjunction with a phototheodolite, established photogrammetry in spatial and temporal surveying.\n\nA plane table consists of a smooth table surface mounted on a sturdy base. The connection between the table top and the base permits one to level the table precisely, using bubble levels, in a horizontal plane. The base, a tripod, is designed to support the table over a specific point on land. By adjusting the length of the legs, one can bring the table level regardless of the roughness of the terrain. \n\nIn use, a plane table is set over a point and brought to precise horizontal level. A drawing sheet is attached to the surface and an alidade is used to sight objects of interest. The alidade, in modern examples of the instrument a rule with a telescopic sight, can then be used to construct a line on the drawing that is in the direction of the object of interest.\n\nBy using the alidade as a surveying level, information on the topography of the site can be directly recorded on the drawing as elevations. Distances to the objects can be measured directly or by the use of stadia marks in the telescope of the alidade.\n\n\n"}
{"id": "1845092", "url": "https://en.wikipedia.org/wiki?curid=1845092", "title": "Political geography", "text": "Political geography\n\nPolitical geography is concerned with the study of both the spatially uneven outcomes of political processes and the ways in which political processes are themselves affected by spatial structures. Conventionally, for the purposes of analysis, political geography adopts a three-scale structure with the study of the state at the centre, the study of international relations (or geopolitics) above it, and the study of localities below it. The primary concerns of the subdiscipline can be summarized as the inter-relationships between people, state, and territory.\n\nThe British geographer Halford Mackinder was also heavily influenced by environmental determinism and in developing his concept of the 'geographical pivot of history' or the Heartland Theory (in 1904) he argued that the era of sea power was coming to an end and that land based powers were in the ascendant, and, in particular, that whoever controlled the heartland of 'Euro-Asia' would control the world. This theory involved concepts diametrically opposed to the ideas of Alfred Thayer Mahan about the significance of \"sea power\" in world conflict. The heartland theory hypothesized the possibility of a huge empire being created which didn't need to use coastal or transoceanic transport to supply its military–industrial complex, and that this empire could not be defeated by the rest of the world allied against it. This perspective proved influential throughout the period of the Cold War, underpinning military thinking about the creation of buffer states between East and West in central Europe.\n\nThe heartland theory depicted a world divided into a \"Heartland\" (Eastern Europe/Western Russia); \"World Island\" (Eurasia and Africa); \"Peripheral Islands\" (British Isles, Japan, Indonesia and Australia) and \"New World\" (The Americas). Mackinder argued that whoever controlled the Heartland would have control of the world. He used these ideas to politically influence events such as the Treaty of Versailles, where buffer states were created between the USSR and Germany, to prevent either of them controlling the Heartland. At the same time, Ratzel was creating a theory of states based around the concepts of Lebensraum and Social Darwinism. He argued that states were analogous to 'organisms' that needed sufficient room in which to live. Both of these writers created the idea of a political and geographical science, with an objective view of the world. Prior to World War II political geography was concerned largely with these issues of global power struggles and influencing state policy, and the above theories were taken on board by German geopoliticians (see Geopolitik) such as Karl Haushofer who - perhaps inadvertently - greatly influenced Nazi political theory, which was a form of politics seen to be legitimated by such 'scientific' theories.\n\nThe close association with environmental determinism and the freezing of political boundaries during the Cold War led to a significant decline in the perceived importance of political geography, which was described by Brian Berry in 1968 as a 'moribund backwater'. Although at this time in most other areas of human geography new approaches, including quantitative spatial science, behavioural studies, and structural Marxism, were invigorating academic research these were largely ignored by political geographers whose main point of reference remained the regional approach. As a result, most of the political geography texts produced during this period were descriptive, and it was not until 1976 that Richard Muir could argue that political geography was no longer a dead duck, but could in fact be a phoenix.\n\nFrom the late-1970s onwards, political geography has undergone a renaissance, and could fairly be described as one of the most dynamic of the sub-disciplines today. The revival was underpinned by the launch of the journal \"Political Geography Quarterly\" (and its expansion to bi-monthly production as \"Political Geography\"). In part this growth has been associated with the adoption by political geographers of the approaches taken up earlier in other areas of human geography, for example, Ron J. Johnston's (1979) work on electoral geography relied heavily on the adoption of quantitative spatial science, Robert Sack's (1986) work on territoriality was based on the behavioural approach, Henry Bakis (1987) showed the impact of information and telecommunications networks on political geography, and Peter Taylor's (e.g. 2007) work on World Systems Theory owed much to developments within structural Marxism. However, the recent growth in vitality and importance of this sub-discipline is also related to the changes in the world as a result of the end of the Cold War. With the emergence of a new world order (which as yet, is only poorly defined) and the development of new research agendas, such as the more recent focus on social movements and political struggles, going beyond the study of nationalism with its explicit territorial basis. There has also been increasing interest in the geography of green politics (see, for example, David Pepper's (1996) work), including the geopolitics of environmental protest, and in the capacity of our existing state apparatus and wider political institutions, to address any contemporary and future environmental problems competently.\n\nPolitical geography has extended the scope of traditional political science approaches by acknowledging that the exercise of power is not restricted to states and bureaucracies, but is part of everyday life. This has resulted in the concerns of political geography increasingly overlapping with those of other human geography sub-disciplines such as economic geography, and, particularly, with those of social and cultural geography in relation to the study of the politics of place (see, for example, the books by David Harvey (1996) and Joe Painter (1995)). Although contemporary political geography maintains many of its traditional concerns (see below) the multi-disciplinary expansion into related areas is part of a general process within human geography which involves the blurring of boundaries between formerly discrete areas of study, and through which the discipline as a whole is enriched.\n\nIn particular, contemporary political geography often considers:\n\nCritical political geography is mainly concerned with the criticism of traditional political geographies vis-a-vis modern trends. As with much of the move towards 'Critical geographies', the arguments have drawn largely from postmodern, post structural and postcolonial theories. Examples include:\n\n\n\n"}
{"id": "12321263", "url": "https://en.wikipedia.org/wiki?curid=12321263", "title": "Softscape", "text": "Softscape\n\nSoftscape refers to the live horticultural elements of a landscape. Softscaping can include, flowers, plants, shrubs, trees, flower beds, and duties like weed/nuisance management, grading, planting, mowing, trimming, aerating, spraying, and digging for everything from plants and shrubs , to flower beds. Wheel barrows and manual tools like rakes, shovels, picks, and gas power tools are commonly used. This is a term that has been popularized in recent pop culture (2006 onwards) on television shows such as Home & Garden Television. The purpose of softscape is to lend character to the landscaping, create an aura, ambience, and reflect the sensibilities of the inhabitants. \n\nThe term softscape stands in contrast to hardscape which represents inanimate objects of a landscape such as : pavers, stones, rocks,planter boxes, arbors , water feature as well as structures of wood and natural stone and concrete, like: retaining walls, patios, fences and decks, pergolas, stairs, Etc\n\n"}
{"id": "3190431", "url": "https://en.wikipedia.org/wiki?curid=3190431", "title": "Spatial analysis", "text": "Spatial analysis\n\nComplex issues arise in spatial analysis, many of which are neither clearly defined nor completely resolved, but form the basis for current research. The most fundamental of these is the problem of defining the spatial location of the entities being studied.\n\nClassification of the techniques of spatial analysis is difficult because of the large number of different fields of research involved, the different fundamental approaches which can be chosen, and the many forms the data can take.\n\nSpatial analysis can perhaps be considered to have arisen with early attempts at cartography and surveying but many fields have contributed to its rise in modern form. Biology contributed through botanical studies of global plant distributions and local plant locations, ethological studies of animal movement, landscape ecological studies of vegetation blocks, ecological studies of spatial population dynamics, and the study of biogeography. Epidemiology contributed with early work on disease mapping, notably John Snow's work of mapping an outbreak of cholera, with research on mapping the spread of disease and with location studies for health care delivery. Statistics has contributed greatly through work in spatial statistics. Economics has contributed notably through spatial econometrics. Geographic information system is currently a major contributor due to the importance of geographic software in the modern analytic toolbox. Remote sensing has contributed extensively in morphometric and clustering analysis. Computer science has contributed extensively through the study of algorithms, notably in computational geometry. Mathematics continues to provide the fundamental tools for analysis and to reveal the complexity of the spatial realm, for example, with recent work on fractals and scale invariance. Scientific modelling provides a useful framework for new approaches.\n\nSpatial analysis confronts many fundamental issues in the definition of its objects of study, in the construction of the analytic operations to be used, in the use of computers for analysis, in the limitations and particularities of the analyses which are known, and in the presentation of analytic results. Many of these issues are active subjects of modern research.\n\nCommon errors often arise in spatial analysis, some due to the mathematics of space, some due to the particular ways data are presented spatially, some due to the tools which are available. Census data, because it protects individual privacy by aggregating data into local units, raises a number of statistical issues. The fractal nature of coastline makes precise measurements of its length difficult if not impossible. A computer software fitting straight lines to the curve of a coastline, can easily calculate the lengths of the lines which it defines. However these straight lines may have no inherent meaning in the real world, as was shown for the coastline of Britain.\n\nThese problems represent a challenge in spatial analysis because of the power of maps as media of presentation. When results are presented as maps, the presentation combines spatial data which are generally accurate with analytic results which may be inaccurate, leading to an impression that analytic results are more accurate than the data would indicate.\n\nThe definition of the spatial presence of an entity constrains the possible analysis which can be applied to that entity and influences the final conclusions that can be reached. While this property is fundamentally true of all analysis, it is particularly important in spatial analysis because the tools to define and study entities favor specific characterizations of the entities being studied. Statistical techniques favor the spatial definition of objects as points because there are very few statistical techniques which operate directly on line, area, or volume elements. Computer tools favor the spatial definition of objects as homogeneous and separate elements because of the limited number of database elements and computational structures available, and the ease with which these primitive structures can be created.\n\nSpatial dependency is the co-variation of properties within geographic space: characteristics at proximal locations appear to be correlated, either positively or negatively. Spatial dependency leads to the spatial autocorrelation problem in statistics since, like temporal autocorrelation, this violates standard statistical techniques that assume independence among observations. For example, regression analyses that do not compensate for spatial dependency can have unstable parameter estimates and yield unreliable significance tests. Spatial regression models (see below) capture these relationships and do not suffer from these weaknesses. It is also appropriate to view spatial dependency as a source of information rather than something to be corrected.\n\nLocational effects also manifest as spatial heterogeneity, or the apparent variation in a process with respect to location in geographic space. Unless a space is uniform and boundless, every location will have some degree of uniqueness relative to the other locations. This affects the spatial dependency relations and therefore the spatial process. Spatial heterogeneity means that overall parameters estimated for the entire system may not adequately describe the process at any given location.\n\nSpatial measurement scale is a persistent issue in spatial analysis; more detail is available at the modifiable areal unit problem (MAUP) topic entry. Landscape ecologists developed a series of scale invariant metrics for aspects of ecology that are fractal in nature. In more general terms, no scale independent method of analysis is widely agreed upon for spatial statistics.\n\nSpatial sampling involves determining a limited number of locations in geographic space for faithfully measuring phenomena that are subject to dependency and heterogeneity. Dependency suggests that since one location can predict the value of another location, we do not need observations in both places. But heterogeneity suggests that this relation can change across space, and therefore we cannot trust an observed degree of dependency beyond a region that may be small. Basic spatial sampling schemes include random, clustered and systematic. These basic schemes can be applied at multiple levels in a designated spatial hierarchy (e.g., urban area, city, neighborhood). It is also possible to exploit ancillary data, for example, using property values as a guide in a spatial sampling scheme to measure educational attainment and income. Spatial models such as autocorrelation statistics, regression and interpolation (see below) can also dictate sample design.\n\nThe fundamental issues in spatial analysis lead to numerous problems in analysis including bias, distortion and outright errors in the conclusions reached. These issues are often interlinked but various attempts have been made to separate out particular issues from each other.\n\nIn a paper by Benoit Mandelbrot on the coastline of Britain it was shown that it is inherently nonsensical to discuss certain spatial concepts despite an inherent presumption of the validity of the concept. Lengths in ecology depend directly on the scale at which they are measured and experienced. So while surveyors commonly measure the length of a river, this length only has meaning in the context of the relevance of the measuring technique to the question under study.\nThe locational fallacy refers to error due to the particular spatial characterization chosen for the elements of study, in particular choice of placement for the spatial presence of the element.\n\nSpatial characterizations may be simplistic or even wrong. Studies of humans often reduce the spatial existence of humans to a single point, for instance their home address. This can easily lead to poor analysis, for example, when considering disease transmission which can happen at work or at school and therefore far from the home.\n\nThe spatial characterization may implicitly limit the subject of study. For example, the spatial analysis of crime data has recently become popular but these studies can only describe the particular kinds of crime which can be described spatially. This leads to many maps of assault but not to any maps of embezzlement with political consequences in the conceptualization of crime and the design of policies to address the issue.\n\nThis describes errors due to treating elements as separate 'atoms' outside of their spatial context. The fallacy is about transferring individual conclusions to spatial units.\n\nThe ecological fallacy describes errors due to performing analyses on aggregate data when trying to reach conclusions on the individual units. Errors occur in part from spatial aggregation. For example, a pixel represents the average surface temperatures within an area. Ecological fallacy would be to assume that all points within the area have the same temperature. This topic is closely related to the modifiable areal unit problem.\n\nA mathematical space exists whenever we have a set of observations and quantitative measures of their attributes. For example, we can represent individuals' incomes or years of education within a coordinate system where the location of each individual can be specified with respect to both dimensions. The distance between individuals within this space is a quantitative measure of their differences with respect to income and education. However, in spatial analysis, we are concerned with specific types of mathematical spaces, namely, geographic space. In geographic space, the observations correspond to locations in a spatial measurement framework that capture their proximity in the real world. The locations in a spatial measurement framework often represent locations on the surface of the Earth, but this is not strictly necessary. A spatial measurement framework can also capture proximity with respect to, say, interstellar space or within a biological entity such as a liver. The fundamental tenet is Tobler's First Law of Geography: if the interrelation between entities increases with proximity in the real world, then representation in geographic space and assessment using spatial analysis techniques are appropriate.\n\nThe Euclidean distance between locations often represents their proximity, although this is only one possibility. There are an infinite number of distances in addition to Euclidean that can support quantitative analysis. For example, \"Manhattan\" (or \"Taxicab\") distances where movement is restricted to paths parallel to the axes can be more meaningful than Euclidean distances in urban settings. In addition to distances, other geographic relationships such as connectivity (e.g., the existence or degree of shared borders) and direction can also influence the relationships among entities. It is also possible to compute minimal cost paths across a cost surface; for example, this can represent proximity among locations when travel must occur across rugged terrain.\n\n Spatial data comes in many varieties and it is not easy to\nUrban and Regional Studies deal with large tables of spatial data obtained from censuses and surveys. It is necessary to simplify the huge amount of detailed information in order to extract the main trends. Multivariable analysis (or Factor analysis, FA) allows a change of variables, transforming the many variables of the census, usually correlated between themselves, into fewer independent \"Factors\" or \"Principal Components\" which are, actually, the eigenvectors of the data correlation matrix weighted by the inverse of their eigenvalues. This change of variables has two main advantages:\n\nFactor analysis depends on measuring distances between observations : the choice of a significant metric is crucial. The Euclidean metric (Principal Component Analysis), the Chi-Square distance (Correspondence Analysis) or the Generalized Mahalanobis distance (Discriminant Analysis ) are among the more widely used. More complicated models, using communalities or rotations have been proposed.\n\nUsing multivariate methods in spatial analysis began really in the 1950s (although some examples go back to the beginning of the century) and culminated in the 1970s, with the increasing power and accessibility of computers. Already in 1948, in a seminal publication, two sociologists, Bell and Shevky, had shown that most city populations in the USA and in the world could be represented with three independent factors : 1- the « socio-economic status » opposing rich and poor districts and distributed in sectors running along highways from the city center, 2- the « life cycle », i.e. the age structure of households, distributed in concentric circles, and 3- « race and ethnicity », identifying patches of migrants located within the city. In 1961, in a groundbreaking study, British geographers used FA to classify British towns. Brian J Berry, at the University of Chicago, and his students made a wide use of the method, applying it to most important cities in the world and exhibiting common social structures. \nThe use of Factor Analysis in Geography, made so easy by modern computers, has been very wide but not always very wise.\n\nSince the vectors extracted are determined by the data matrix, it is not possible to compare factors obtained from different censuses. A solution consists in fusing together several census matrices in a unique table which, then, may be analyzed. This, however, assumes that the definition of the variables has not changed over time and produces very large tables, difficult to manage. A better solution, proposed by psychometricians, groups the data in a « cubic matrix », with three entries (for instance, locations, variables, time periods). A Three-Way Factor Analysis produces then three groups of factors related by a small cubic « core matrix ». This method, which exhibits data evolution over time, has not been widely used in geography. In Los Angeles, however, it has exhibited the role, traditionally ignored, of Downtown as an organizing center for the whole city during several decades.\n\nSpatial autocorrelation statistics measure and analyze the degree of dependency among observations in a geographic space. Classic spatial autocorrelation statistics include Moran's formula_1, Geary's formula_2, Getis's formula_3 and the standard deviational ellipse. These statistics require measuring a spatial weights matrix that reflects the intensity of the geographic relationship between observations in a neighborhood, e.g., the distances between neighbors, the lengths of shared border, or whether they fall into a specified directional class such as \"west\". Classic spatial autocorrelation statistics compare the spatial weights to the covariance relationship at pairs of locations. Spatial autocorrelation that is more positive than expected from random indicate the clustering of similar values across geographic space, while significant negative spatial autocorrelation indicates that neighboring values are more dissimilar than expected by chance, suggesting a spatial pattern similar to a chess board.\n\nSpatial autocorrelation statistics such as Moran's formula_1 and Geary's formula_2 are global in the sense that they estimate the overall degree of spatial autocorrelation for a dataset. The possibility of spatial heterogeneity suggests that the estimated degree of autocorrelation may vary significantly across geographic space. Local spatial autocorrelation statistics provide estimates disaggregated to the level of the spatial analysis units, allowing assessment of the dependency relationships across space. formula_3 statistics compare neighborhoods to a global average and identify local regions of strong autocorrelation. Local versions of the formula_1 and formula_2 statistics are also available.\n\nSpatial stratified heterogeneity, referring to the within-strata variance less than the between strata-variance, is ubiquitous in ecological phenomena, such as ecological zones and many ecological variables. Spatial stratified heterogeneity of an attribute can be measured by geographical detector \"q\"-statistic:\nwhere a population is partitioned into \"h\" = 1, ..., \"L\" strata; \"N\" stands for the size of the population, σ stands for variance of the attribute. The value of \"q\" is within [0, 1], 0 indicates no spatial stratified heterogeneity, 1 indicates perfect spatial stratified heterogeneity.The value of \"q\" indicates the percent of the variance of an attribute explained by the stratification.The \"q\" follows a noncentral \"F\" probability density function.\n\nSpatial interpolation methods estimate the variables at unobserved locations in geographic space based on the values at observed locations. Basic methods include inverse distance weighting: this attenuates the variable with decreasing proximity from the observed location. Kriging is a more sophisticated method that interpolates across space according to a spatial lag relationship that has both systematic and random components. This can accommodate a wide range of spatial relationships for the hidden values between observed locations. Kriging provides optimal estimates given the hypothesized lag relationship, and error estimates can be mapped to determine if spatial patterns exist.\n\nSpatial regression methods capture spatial dependency in regression analysis, avoiding statistical problems such as unstable parameters and unreliable significance tests, as well as providing information on spatial relationships among the variables involved. The estimated spatial relationships can be used on spatial and spatio-temporal predictions. Depending on the specific technique, spatial dependency can enter the regression model as relationships between the independent variables and the dependent, between the dependent variables and a spatial lag of itself, or in the error terms. Geographically weighted regression (GWR) is a local version of spatial regression that generates parameters disaggregated by the spatial units of analysis. This allows assessment of the spatial heterogeneity in the estimated relationships between the independent and dependent variables. The use of Bayesian hierarchical modeling in conjunction with Markov Chain Monte Carlo (MCMC) methods have recently shown to be effective in modeling complex relationships using Poisson-Gamma-CAR, Poisson-lognormal-SAR, or Overdispersed logit models. \nSpatial stochastic processes, such as Gaussian processes are also increasingly being deployed in spatial regression analysis. Model-based versions of GWR, known as spatially varying coefficient models have been applied to conduct Bayesian inference. Spatial stochastic process can become computationally effective and scalable Gaussian process models, such as Gaussian Predictive Processes and Nearest Neighbor Gaussian Processes (NNGP).\n\nSpatial interaction or \"gravity models\" estimate the flow of people, material or information between locations in geographic space. Factors can include origin propulsive variables such as the number of commuters in residential areas, destination attractiveness variables such as the amount of office space in employment areas, and proximity relationships between the locations measured in terms such as driving distance or travel time. In addition, the topological, or connective, relationships between areas must be identified, particularly considering the often conflicting relationship between distance and topology; for example, two spatially close neighborhoods may not display any significant interaction if they are separated by a highway. After specifying the functional forms of these relationships, the analyst can estimate model parameters using observed flow data and standard estimation techniques such as ordinary least squares or maximum likelihood. Competing destinations versions of spatial interaction models include the proximity among the destinations (or origins) in addition to the origin-destination proximity; this captures the effects of destination (origin) clustering on flows. Computational methods such as artificial neural networks can also estimate spatial interaction relationships among locations and can handle noisy and qualitative data.\n\nSpatial interaction models are aggregate and top-down: they specify an overall governing relationship for flow between locations. This characteristic is also shared by urban models such as those based on mathematical programming, flows among economic sectors, or bid-rent theory. An alternative modeling perspective is to represent the system at the highest possible level of disaggregation and study the bottom-up emergence of complex patterns and relationships from behavior and interactions at the individual level. \n\nComplex adaptive systems theory as applied to spatial analysis suggests that simple interactions among proximal entities can lead to intricate, persistent and functional spatial entities at aggregate levels. Two fundamentally spatial simulation methods are cellular automata and agent-based modeling. Cellular automata modeling imposes a fixed spatial framework such as grid cells and specifies rules that dictate the state of a cell based on the states of its neighboring cells. As time progresses, spatial patterns emerge as cells change states based on their neighbors; this alters the conditions for future time periods. For example, cells can represent locations in an urban area and their states can be different types of land use. Patterns that can emerge from the simple interactions of local land uses include office districts and urban sprawl. Agent-based modeling uses software entities (agents) that have purposeful behavior (goals) and can react, interact and modify their environment while seeking their objectives. Unlike the cells in cellular automata, simulysts can allow agents to be mobile with respect to space. For example, one could model traffic flow and dynamics using agents representing individual vehicles that try to minimize travel time between specified origins and destinations. While pursuing minimal travel times, the agents must avoid collisions with other vehicles also seeking to minimize their travel times. Cellular automata and agent-based modeling are complementary modeling strategies. They can be integrated into a common geographic automata system where some agents are fixed while others are mobile.\n\nCalibration plays a pivotal role in both CA and ABM simulation and modelling approaches. Initial approaches to CA proposed robust calibration approaches based on stochastic, Monte Carlo methods. ABM approaches rely on agents' decision rules (in many cases extracted from qualitative research base methods such as questionnaires). Recent Machine Learning Algorithms calibrate using training sets, for instance in order to understand the qualities of the built environment.\n\nSpatial analysis of a conceptual geological model is the main purpose of any MPS algorithm. The method analyzes the spatial statistics of the geological model, called the training image, and generates realizations of the phenomena that honor those input multiple-point statistics.\n\nA recent MPS algorithm used to accomplish this task is the pattern-based method by Honarkhah. In this method, a distance-based approach is employed to analyze the patterns in the training image. This allows the reproduction of the multiple-point statistics, and the complex geometrical features of the training image. Each output of the MPS algorithm is a realization that represents a random field. Together, several realizations may be used to quantify spatial uncertainty.\n\nOne of the recent methods is presented by Tahmasebi et al. uses a cross-correlation function to improve the spatial pattern reproduction. They call their MPS simulation method as the CCSIM algorithm. This method is able to quantify the spatial connectivity, variability and uncertainty. Furthermore, the method is not sensitive to any type of data and is able to simulate both categorical and continuous scenarios. CCSIM algorithm is able to be used for any stationary, non-stationary and multivariate systems and it can provide high quality visual appeal model.,\n\nGeospatial analysis, or just spatial analysis, is an approach to applying statistical analysis and other analytic techniques to data which has a geographical or spatial aspect . Such analysis would typically employ software capable of rendering maps processing spatial data, and applying analytical methods to terrestrial or geographic datasets, including the use of geographic information systems and geomatics.\n\nGeographic information systems (GIS) — a large domain that provides a variety of capabilities designed to capture, store, manipulate, analyze, manage, and present all types of geographical data — utilizes geospatial analysis in a variety of contexts, operations and applications.\n\nGeospatial analysis, using GIS, was developed for problems in the environmental and life sciences, in particular ecology, geology and epidemiology. It has extended to almost all industries including defense, intelligence, utilities, Natural Resources (i.e. Oil and Gas, Forestry ... etc.), social sciences, medicine and Public Safety (i.e. emergency management and criminology), disaster risk reduction and management (DRRM), and climate change adaptation (CCA). Spatial statistics typically result primarily from observation rather than experimentation.\n\nVector-based GIS is typically related to operations such as map overlay (combining two or more maps or map layers according to predefined rules), simple buffering (identifying regions of a map within a specified distance of one or more features, such as towns, roads or rivers) and similar basic operations. This reflects (and is reflected in) the use of the term spatial analysis within the Open Geospatial Consortium (OGC) “simple feature specifications”. For raster-based GIS, widely used in the environmental sciences and remote sensing, this typically means a range of actions applied to the grid cells of one or more maps (or images) often involving filtering and/or algebraic operations (map algebra). These techniques involve processing one or more raster layers according to simple rules resulting in a new map layer, for example replacing each cell value with some combination of its neighbours’ values, or computing the sum or difference of specific attribute values for each grid cell in two matching raster datasets. Descriptive statistics, such as cell counts, means, variances, maxima, minima, cumulative values, frequencies and a number of other measures and distance computations are also often included in this generic term spatial analysis. Spatial analysis includes a large variety of statistical techniques (descriptive, exploratory, and explanatory statistics) that apply to data that vary spatially and which can vary over time. Some more advanced statistical techniques include Getis-ord Gi* or Anselin Local Moran's I which are used to determine clustering patterns of spatially referenced data.\n\nGeospatial analysis goes beyond 2D and 3D mapping operations and spatial statistics. It includes: \n\nTraditionally geospatial computing has been performed primarily on personal computers (PCs) or servers. Due to the increasing capabilities of mobile devices, however, geospatial computing in mobile devices is a fast-growing trend. The portable nature of these devices, as well as the presence of useful sensors, such as Global Navigation Satellite System (GNSS) receivers and barometric pressure sensors, make them useful for capturing and processing geospatial information in the field. In addition to the local processing of geospatial information on mobile devices, another growing trend is cloud-based geospatial computing. In this architecture, data can be collected in the field using mobile devices and then transmitted to cloud-based servers for further processing and ultimate storage. In a similar manner, geospatial information can be made available to connected mobile devices via the cloud, allowing access to vast databases of geospatial information anywhere where a wireless data connection is available.\n\nGeographic information systems (GIS) and the underlying geographic information science that advances these technologies have a strong influence on spatial analysis. The increasing ability to capture and handle geographic data means that spatial analysis is occurring within increasingly data-rich environments. Geographic data capture systems include remotely sensed imagery, environmental monitoring systems such as intelligent transportation systems, and location-aware technologies such as mobile devices that can report location in near-real time. GIS provide platforms for managing these data, computing spatial relationships such as distance, connectivity and directional relationships between spatial units, and visualizing both the raw data and spatial analytic results within a cartographic context.\n\nGeovisualization (GVis) combines scientific visualization with digital cartography to support the exploration and analysis of geographic data and information, including the results of spatial analysis or simulation. GVis leverages the human orientation towards visual information processing in the exploration, analysis and communication of geographic data and information. In contrast with traditional cartography, GVis is typically three- or four-dimensional (the latter including time) and user-interactive.\n\nGeographic knowledge discovery (GKD) is the human-centered process of applying efficient computational tools for exploring massive spatial databases. GKD includes geographic data mining, but also encompasses related activities such as data selection, data cleaning and pre-processing, and interpretation of results. GVis can also serve a central role in the GKD process. GKD is based on the premise that massive databases contain interesting (valid, novel, useful and understandable) patterns that standard analytical techniques cannot find. GKD can serve as a hypothesis-generating process for spatial analysis, producing tentative patterns and relationships that should be confirmed using spatial analytical techniques.\n\nSpatial decision support systems (SDSS) take existing spatial data and use a variety of mathematical models to make projections into the future. This allows urban and regional planners to test intervention decisions prior to implementation.\n\n\n\n\n\n"}
{"id": "13219832", "url": "https://en.wikipedia.org/wiki?curid=13219832", "title": "Survey stakes", "text": "Survey stakes\n\nControl of alignment and grade during construction is established through the use of survey stakes. Stakes are generally made of wood in different sizes. Based on the use of the stake they are called \"alignment stakes, offset stakes, grade stakes, and slope stakes\".\n\nSurvey stakes are markers surveyors use in surveying projects to prepare job sites, mark out property boundaries, and provide information about claims on natural resources like timber and minerals. They can be made from wood, metal, plastic, and other materials and typically come in a range of sizes and colors for different purposes. Sources can include surveying and construction suppliers, and people can also make or order their own for custom applications.\n\nA survey stake is typically small, with a pointed end to make it easy to drive into the earth. It may be color-coded or have a space for people to write information on the stake. Surveyors use stakes when assessing sites to mark out boundaries, record data, and convey information to other people. On a job site, for example, survey stakes indicate where it is necessary to backfill with soil to raise the elevation, or to cut soil away to lower it. Stakes can also provide information about slope and grading for people getting a job site ready for construction.\n"}
{"id": "18062821", "url": "https://en.wikipedia.org/wiki?curid=18062821", "title": "Table (landform)", "text": "Table (landform)\n\nA table or tableland is a butte, flank of a mountain, or mountain, that has a flat top. \n\nThis landform has numerous names in addition to \"table\", including:\n\nThe term \"flat\" is relative when speaking of tables, and often the naming or identification of a table (or table hill or mountain) is based on the appearance of the terrain feature from a distance or from below it. An example is Mesa Verde, Colorado, where the \"flat top\" of the mountain is both rolling terrain and cut by numerous deep canyons and arroyos, but whose rims appear quite flat from almost all directions, terminating in cliffs.\n\n"}
{"id": "248860", "url": "https://en.wikipedia.org/wiki?curid=248860", "title": "Theodolite", "text": "Theodolite\n\nA theodolite is a precision optical instrument for measuring angles between designated visible points in the horizontal and vertical planes. The traditional use has been for land surveying, but they are also used extensively for building and infrastructure construction, and some specialized applications such as meteorology and rocket launching. \n\nIt consists of a moveable telescope mounted so it can rotate around horizontal and vertical axes and provide angular readouts. These indicate the orientation of the telescope, and are used to relate the first point sighted through the telescope to subsequent sightings of other points from the same theodolite position. These angles can be measured with great accuracy, typically to milliradian or seconds of arc. From these readings a plan can be drawn, or objects can be positioned in accordance with an existing plan. The modern theodolite has evolved into what is known as a total station where angles and distances are measured electronically, and are read directly to computer memory. \n\nIn a transit theodolite, the telescope is short enough to rotate through the zenith, otherwise for non-transit instruments vertical (or altitude), rotation is restricted to a limited arc. \n\nThe optical level is sometimes mistaken for a theodolite, but it does not measure vertical angles, and is used only for levelling on a horizontal plane.\n\nTemporary adjustments are a set of operations necessary in order to make a theodolite ready for taking observations at a station. These include its setting up, centering, leveling up and elimination of parallax, and are achieved in four steps:\n\n\nSightings are taken by the surveyor, who adjusts the telescope's vertical and horizontal angular orientation so the cross-hairs align with the desired sighting point. Both angles are read either from exposed or internal scales and recorded. The next object is then sighted and recorded without moving the position of the instrument and tripod.\n\nThe earliest angular readouts were from open vernier scales directly visible to the eye. Gradually these scales were enclosed for physical protection, and finally became an indirect optical readout, with convoluted light paths to bring them to a convenient place on the instrument for viewing. The modern digital theodolites have electronic displays.\n\nIndex error - The angles in the vertical axis should read 90° (100 grad) when the sight axis is horizontal, or 270° (300 grad) when the instrument is transited. Half of the difference between the two positions is called the \"index error\". This can only be checked on transit instruments.\n\nHorizontal axis error - The horizontal and vertical axes of a theodolite must be perpendicular; if not then a \"horizontal axis error\" exists. This can be tested by aligning the tubular spirit bubble parallel to a line between two footscrews and setting the bubble central. A horizontal axis error is present if the bubble runs off central when the tubular spirit bubble is reversed (turned through 180°). To adjust, the operator removes 1/2 the amount the bubble has run off using the adjusting screw, then re-level, test and refine the adjustment.\n\nCollimation error - The optical axis of the telescope, must also be perpendicular to the horizontal axis. If not, then a \"collimation error\" exists.\n\nIndex error, horizontal-axis error ('trunnion-axis error') and collimation error are regularly determined by calibration and are removed by mechanical adjustment. Their existence is taken into account in the choice of measurement procedure in order to eliminate their effect on the measurement results of the theodolite.\n\nThe term \"diopter\" was sometimes used in old texts as a synonym for theodolite. This derives from an older astronomical instrument called a dioptra.\n\nPrior to the theodolite, instruments such as the groma, geometric square and various other graduated circles (see circumferentor) and semicircles (see graphometer) were used to obtain either vertical or horizontal angle measurements. It was only a matter of time before someone put two measuring devices into a single instrument that could measure both angles simultaneously. Gregorius Reisch showed such an instrument in the appendix of his book \"Margarita Philosophica\", which he published in Strasburg in 1512. It was described in the appendix by Martin Waldseemüller, a German topographer and cartographer, who made the device in the same year. Waldseemüller called his instrument the \"polimetrum\".\n\nThe first occurrence of the word \"theodolite\" is found in the surveying textbook \"A geometric practice named Pantometria\" (1571) by Leonard Digges, which was published posthumously by his son, Thomas Digges. The etymology of the word is unknown. The first part of the New Latin \"theo-delitus\" might stem from the Greek \"θεᾶσθαι\", \"to behold or look attentively upon\" or \"θεῖν\" \"to run\", but the second part is more puzzling and is often attributed to an unscholarly variation of one of the following Greek words: \"δῆλος\", meaning \"evident\" or \"clear\", or \"δολιχός\" \"long\", or \"δοῦλος\" \"slave\", or an unattested Neolatin compound combining \"ὁδός\" \"way\" and \"λιτός\" \"plain\". It has been also suggested that \"-delitus\" is a variation of the Latin supine \"deletus\", in the sense of \"crossed out\".\n\nThere is some confusion about the instrument to which the name was originally applied. Some identify the early theodolite as an azimuth instrument only, while others specify it as an altazimuth instrument. In Digges's book, the name \"theodolite\" described an instrument for measuring horizontal angles only. He also described an instrument that measured both altitude and azimuth, which he called a \"topographicall instrument\" . Thus the name originally applied only to the azimuth instrument and only later became associated with the altazimuth instrument. The 1728 \"Cyclopaedia\" compares \"graphometer\" to \"half-theodolite\". Even as late as the 19th century, the instrument for measuring horizontal angles only was called a \"simple theodolite\" and the altazimuth instrument, the \"plain theodolite\".\n\nThe first instrument more like a true theodolite was likely the one built by Joshua Habermel (Erasmus Habermehl) in Germany in 1576, complete with compass and tripod.\n\nThe earliest altazimuth instruments consisted of a base graduated with a full circle at the limb and a vertical angle measuring device, most often a semicircle. An alidade on the base was used to sight an object for horizontal angle measurement, and a second alidade was mounted on the vertical semicircle. Later instruments had a single alidade on the vertical semicircle and the entire semicircle was mounted so as to be used to indicate horizontal angles directly. Eventually, the simple, open-sight alidade was replaced with a sighting telescope. This was first done by Jonathan Sisson in 1725.\n\nThe theodolite became a modern, accurate instrument in 1787, with the introduction of Jesse Ramsden's famous great theodolite, which he created using a very accurate dividing engine of his own design. The demand could not be met by foreign theodolites owing to their inadequate precision, hence all instruments meeting high precision requirements were made in England. Despite the many German instrument builders at the turn of the century, there were no usable German theodolites available. A transition was brought about by Breithaupt and the symbiosis of Utzschneider, Reichenbach and Fraunhofer.\nAs technology progressed, in the 1840s, the vertical partial circle was replaced with a full circle, and both vertical and horizontal circles were finely graduated. This was the \"transit theodolite\". Theodolites were later adapted to a wider variety of mountings and uses. In the 1870s, an interesting waterborne version of the theodolite (using a pendulum device to counteract wave movement) was invented by Edward Samuel Ritchie. It was used by the U.S. Navy to take the first precision surveys of American harbors on the Atlantic and Gulf coasts.\n\nIn the early part of the 20th century, Heinrich Wild produced theodolites that became popular with surveyors. His Wild T2, T3, and A1 instruments were made for many years, and he would go on to develop the DK1, DKM1, DM2, DKM2, and DKM3 for Kern Aarau company. With continuing refinements, instruments steadily evolved into the modern theodolite used by surveyors today.\n\nTriangulation, as invented by Gemma Frisius around 1533, consists of making such direction plots of the surrounding landscape from two separate standpoints. The two graphing papers are superimposed, providing a scale model of the landscape, or rather the targets in it. The true scale can be obtained by measuring one distance both in the real terrain and in the graphical representation.\n\nModern triangulation as, e.g., practised by Snellius, is the same procedure executed by numerical means. Photogrammetric block adjustment of stereo pairs of aerial photographs is a modern, three-dimensional variant.\n\nIn the late 1780s, Jesse Ramsden, a Yorkshireman from Halifax, England who had developed the dividing engine for dividing angular scales accurately to within a second of arc (≈ 0.0048 mrad or 4.8 µrad), was commissioned to build a new instrument for the British Ordnance Survey. The Ramsden theodolite was used over the next few years to map the whole of southern Britain by triangulation.\n\nIn network measurement, the use of forced centering speeds up operations while maintaining the highest precision. The theodolite or the target can be rapidly removed from, or socketed into, the forced centering plate with sub-millimeter precision. Nowadays GPS antennas used for geodetic positioning use a similar mounting system. The height of the reference point of the theodolite—or the target—above the ground benchmark must be measured precisely.\n\nThe term transit theodolite, or transit for short, refers to a type of theodolite that was developed in the early 19th century. It was popular with American railroad engineers pushing west, and it replaced the railroad compass, sextant and octant. It features a vertical circle which is graduated through the full 360 degrees and a telescope that could \"flip over\" (\"transit the scope\"). By reversing the telescope and at the same time rotating the instrument through 180 degrees about the vertical axis, the instrument can be used in 'plate-left' or 'plate-right' modes ('plate' refers to the vertical protractor circle). By measuring the same horizontal and vertical angles in these two modes and then averaging the results, centering and collimating errors in the instrument can be eliminated. Some transit instruments are capable of reading angles directly to thirty arc-seconds (≈ 0.15 mrad). Modern theodolites are usually of the transit-theodolite design, but engraved plates have been replaced with glass plates designed to be read with light-emitting diodes and computer circuitry, greatly improving accuracy.\n\nThere is a long history of theodolite use in measuring winds aloft, by using specially-manufactured theodolites to track the horizontal and vertical angles of special weather balloons called \"ceiling balloons\" or \"pilot balloons\" (\"pibal\"). Early attempts at this were made in the opening years of the nineteenth century, but the instruments and procedures weren't fully developed until a hundred years later. This method was extensively used in World War II and thereafter, and was gradually replaced by radio and GPS measuring systems from the 1980s onward.\n\nThe pibal theodolite uses a prism to bend the optical path by 90 degrees so the operator's eye position does not change as the elevation is changed through a complete 180 degrees. The theodolite is typically mounted on a rugged steel stand, set up so it is level and pointed north, with the altitude and azimuth scales reading zero degrees. A balloon is released in front of the theodolite, and its position is precisely tracked, usually once a minute. The balloons are carefully constructed and filled, so their rate of ascent can be known fairly accurately in advance. Mathematical calculations on time, rate of ascent, azimuth and angular altitude can produce good estimates of wind speed and direction at various altitudes.\n\nIn modern electronic theodolites, the readout of the horizontal and vertical circles is usually done with a rotary encoder. These produce signals indicating the altitude and azimuth of the telescope which are fed to a microprocessor. CCD sensors have been added to the focal plane of the telescope allowing both auto-targeting and the automated measurement of residual target offset. All this is implemented in embedded software of the processor.\n\nMany modern theodolites are equipped with integrated electro-optical distance measuring devices, generally infrared based, allowing the measurement in one step of complete three-dimensional vectors—albeit in instrument-defined polar co-ordinates, which can then be transformed to a pre-existing co-ordinate system in the area by means of a sufficient number of control points. This technique is called a resection solution or free station position surveying and is widely used in mapping surveying.\n\nSuch instruments are \"intelligent\" theodolites called self-registering tacheometers or colloquially \"total stations\", and perform all the necessary angular and distance calculations, and the results or raw data can be downloaded to external processors, such as ruggedized laptops, PDAs or programmable calculators\n\nA gyrotheodolite is used when the north-south reference bearing of the meridian is required in the absence of astronomical star sights. This occurs mainly in the underground mining industry and in tunnel engineering. For example, where a conduit must pass under a river, a vertical shaft on each side of the river might be connected by a horizontal tunnel. A gyrotheodolite can be operated at the surface and then again at the foot of the shafts to identify the directions needed to tunnel between the base of the two shafts. Unlike an artificial horizon or inertial navigation system, a gyrotheodolite cannot be relocated while it is operating. It must be restarted again at each site.\n\nThe gyrotheodolite comprises a normal theodolite with an attachment that contains a gyroscope mounted so as to sense rotation of the Earth and from that the alignment of the meridian. The meridian is the plane that contains both the axis of the Earth's rotation and the observer. The intersection of the meridian plane with the horizontal contains the true north-south geographic reference bearing required. The gyrotheodolite is usually referred to as being able to determine or find true north.\n\nA gyrotheodolite will function at the equator and in both the northern and southern hemispheres. The meridian is undefined at the geographic poles. A gyrotheodolite cannot be used at the poles where the Earth's axis is precisely perpendicular to the horizontal axis of the spinner, indeed it is not normally used within about 15 degrees of the pole because the east-west component of the Earth's rotation is insufficient to obtain reliable results. When available, astronomical star sights are able to give the meridian bearing to better than one hundred times the accuracy of the gyrotheodolite. Where this extra precision is not required, the gyrotheodolite is able to produce a result quickly without the need for night observations.\n\n\n"}
